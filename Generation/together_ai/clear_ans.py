"""
Clear the<start>and<end>generated by the model in inference
"""

import json
import os
import sys

model_name = 'Llama-3-70b-chat-hf'

# result_path = f'/Users/qingyuanzii/Desktop/Versi-Code-Gen/VerisiCode-RAG/Generation/output/library_source_code/withoutRAG/line/meta-llama/Llama-3-70b-chat-hf/library_source_code_line.json' #改 token,line,block
result_path = sys.argv[1]
sample_num = int(sys.argv[2])
# result_path = "/Users/qingyuanzii/Desktop/Versi-Code-Gen/VerisiCode-RAG/Generation/together_ai/tmp/meta-llama/Llama-3-70b-chat-hf/stackoverflow_block.json"

import json

input_file = sys.argv[1]  # 原始 JSONL 文件路径
key_to_check = 'model_output'  # 需要检查的键

clear_list = []
with open(input_file, 'r', encoding='utf-8') as infile:
    load_dic = json.load(infile)
    for line in load_dic["data"]:
        if key_to_check in line.keys():  # 检查字典中是否存在指定的键
           clear_list.append(line)

clear_dict = {"data":clear_list}
sample_num = len(clear_list)


with open(result_path,"w",encoding='utf-8') as f:
    json.dump(clear_dict, f, indent=4, ensure_ascii=False)



with open(result_path, 'r', encoding='utf-8')as fr:
    lodict = json.load(fr)
data_dict = lodict
data_list = data_dict['data']

for idx,data in enumerate(data_list[:sample_num]):
    temp_list = []
    try:
        model_output_list = eval(data['model_output'])  #改
    except:
        print(idx)
    for output in model_output_list:
        if "<start>" in output and "<end>" in output:
            start_index = output.find("<start>") + len("<start>")
            end_index = output.find("<end>")
            content = output[start_index:end_index].replace('```python', '').replace('```', '')
        else:
            content = "no_answer"

        temp_list.append(content)

    data['model_output_token_clear'] = str(temp_list)   #change token,line,block

with open(result_path, 'w', encoding='utf-8')as fw:
    json.dump(data_dict, fw, indent=4, ensure_ascii=False)
print(len(data_dict["data"]))