
<DEPENDENCY.GitPython==0.3.3> <CONTAINS> "CODE.remote.refs.master = RemoteReference('/refs/remotes/origin/master')" .

<DEPENDENCY.Jinja2==2.1> <CONTAINS> """CODE.    from os import path

    class MyCache(BytecodeCache):

        def __init__(self, directory):
            self.directory = directory

        def load_bytecode(self, bucket):
            filename = path.join(self.directory, bucket.key)
            if path.exists(filename):
                with file(filename, 'rb') as f:
                    bucket.load_bytecode(f)

        def dump_bytecode(self, bucket):
            filename = path.join(self.directory, bucket.key)
            with file(filename, 'wb') as f:
                bucket.write_bytecode(f)""" .

<DEPENDENCY.Jinja2==2.11.0> <CONTAINS> """CODE.foo = ChainableUndefined(name='foo')
str(foo.bar['baz'])
foo.bar['baz'] + 42""" .

<DEPENDENCY.Jinja2==2.11.3> <CONTAINS> "CODE.await template.render_async(knights='that say nih; asynchronously')" .

<DEPENDENCY.Jinja2==2.2> <CONTAINS> """CODE.from jinja2 import Environment, meta
env = Environment()
ast = env.parse('{% extends "layout.html" %}{% include helper %}')
list(meta.find_referenced_templates(ast))""",
        """CODE.from jinja2 import Environment, meta
env = Environment()
ast = env.parse('{% set foo = 42 %}{{ bar + foo }}')
meta.find_undeclared_variables(ast)
""" .

<DEPENDENCY.Jinja2==2.4> <CONTAINS> "CODE.EvalContextModifier(options=[Keyword('autoescape', Const(True))])",
        """CODE.ModuleLoader('/path/to/compiled/templates'),
FileSystemLoader('/path/to/templates')""" .

<DEPENDENCY.Jinja2==2.5.1> <CONTAINS> """CODE.Marks a string as being safe for inclusion in HTML/XML output without
needing to be escaped.  This implements the `__html__` interface a couple
of frameworks and web applications use.  :class:`Markup` is a direct
subclass of `unicode` and provides all the methods of `unicode` just that
it escapes arguments passed and always returns `Markup`.

The `escape` function returns markup objects so that double escaping can't
happen.

The constructor of the :class:`Markup` class can be used for three
different things:  When passed an unicode object it's assumed to be safe,
when passed an object with an HTML representation (has an `__html__`
method) that representation is used, otherwise the object passed is
converted into a unicode string and then assumed to be safe:

Markup("Hello <em>World</em>!")
Markup(u'Hello <em>World</em>!')
class Foo(object):
...  def __html__(self):
...   return '<a href="#">foo</a>'
...
Markup(Foo())
Markup(u'<a href="#">foo</a>')

If you want object passed being always treated as unsafe you can use the
:meth:`escape` classmethod to create a :class:`Markup` object:

Markup.escape("Hello <em>World</em>!")
Markup(u'Hello &lt;em&gt;World&lt;/em&gt;')

Operations on a markup string are markup aware which means that all
arguments are passed through the :func:`escape` function:

em = Markup("<em>%s</em>")
em % "foo & bar"
Markup(u'<em>foo &amp; bar</em>')
strong = Markup("<strong>%(text)s</strong>")
strong % {'text': '<blink>hacker here</blink>'}
Markup(u'<strong>&lt;blink&gt;hacker here&lt;/blink&gt;</strong>')
Markup("<em>Hello</em> ") + "<foo>"
Markup(u'<em>Hello</em> &lt;foo&gt;')""",
        """CODE.Markup("Main &raquo;  <em>About</em>").striptags()
u'Main \\xbb About'""",
        "CODE.Markup(\"Main &raquo; <em>About</em>\").unescape()" .

<DEPENDENCY.Jinja2==2.6> <CONTAINS> """CODE.Marks a string as being safe for inclusion in HTML/XML output without
needing to be escaped.  This implements the `__html__` interface a couple
of frameworks and web applications use.  :class:`Markup` is a direct
subclass of `unicode` and provides all the methods of `unicode` just that
it escapes arguments passed and always returns `Markup`.

The `escape` function returns markup objects so that double escaping can't
happen.

The constructor of the :class:`Markup` class can be used for three
different things:  When passed an unicode object it's assumed to be safe,
when passed an object with an HTML representation (has an `__html__`
method) that representation is used, otherwise the object passed is
converted into a unicode string and then assumed to be safe:

Markup("Hello <em>World</em>!")
Markup(u'Hello <em>World</em>!')
class Foo(object):
...  def __html__(self):
...   return '<a href="#">foo</a>'
...
Markup(Foo())
Markup(u'<a href="#">foo</a>')

If you want object passed being always treated as unsafe you can use the
:meth:`escape` classmethod to create a :class:`Markup` object:

Markup.escape("Hello <em>World</em>!")
Markup(u'Hello &lt;em&gt;World&lt;/em&gt;')

Operations on a markup string are markup aware which means that all
arguments are passed through the :func:`escape` function:

em = Markup("<em>%s</em>")
em % "foo & bar"
Markup(u'<em>foo &amp; bar</em>')
strong = Markup("<strong>%(text)s</strong>")
strong % {'text': '<blink>hacker here</blink>'}
Markup(u'<strong>&lt;blink&gt;hacker here&lt;/blink&gt;</strong>')
Markup("<em>Hello</em> ") + "<foo>"
Markup(u'<em>Hello</em> &lt;foo&gt;')""",
        """CODE.Markup("Main &raquo;  <em>About</em>").striptags()
u'Main \\xbb About'""",
        "CODE.Markup(\"Main &raquo; <em>About</em>\").unescape()" .

<DEPENDENCY.Jinja2==2.7> <CONTAINS> """CODE.@contextfunction
def get_exported_names(context):
    return sorted(context.exported_vars)""",
        "CODE.Template('Hello {{ name }}!').stream(name='foo').dump('hello.html')",
        """CODE.def is_undefined(var):
    return isinstance(var, Undefined)

def default(var, default=''):
    if is_undefined(var):
        return default
    return var""",
        """CODE.from jinja2 import Environment
env = Environment()
node = env.parse('{{ (_("foo"), _(), ngettext("foo", "bar", 42)) }}')

def extract_from_ast(node, babel_style=True):
    # implementation here
    pass

list(extract_from_ast(node))
list(extract_from_ast(node, babel_style=False))
""",
        "CODE.self.attr('_my_attribute', lineno=lineno)",
        """CODE.t = Template('{% macro foo() %}42{% endmacro %}23')
unicode(t.module)
u'23'
t.module.foo()
u'42'""",
        """CODE.template.render(knights='that say nih')
template.render({'knights': 'that say nih'})""" .

<DEPENDENCY.Jinja2==2.8> <CONTAINS> """CODE.logger = logging.getLogger(__name__)
LoggingUndefined = make_logging_undefined(
    logger=logger,
    base=Undefined
)
""" .

<DEPENDENCY.Jinja2==2.9> <CONTAINS> "CODE.await template.render_async(knights='that say nih; asynchronously')",
        """CODE.from jinja2 import Environment, select_autoescape
env = Environment(autoescape=select_autoescape(
    enabled_extensions=('html', 'xml'),
    default_for_string=True,
))

from jinja2 import Environment, select_autoescape
env = Environment(autoescape=select_autoescape(
    disabled_extensions=('txt',),
    default_for_string=True,
    default=True,
))
""" .

<DEPENDENCY.MarkupSafe==1.1.1> <CONTAINS> """CODE.value = escape('<User 1>')
value
Markup('&lt;User 1&gt;')
escape(str(value))
Markup('&amp;lt;User 1&amp;gt;')
escape(soft_unicode(value))
Markup('&lt;User 1&gt;')""" .

<DEPENDENCY.MarkupSafe==2.0.0> <CONTAINS> """CODE.value = escape("<User 1>")
value
Markup('&lt;User 1&gt;')
escape(str(value))
Markup('&amp;lt;User 1&amp;gt;')
escape(soft_str(value))
Markup('&lt;User 1&gt;')""" .

<DEPENDENCY.Pygments==0.7> <CONTAINS> """CODE.@simplefilter
def lowercase(lexer, stream, options):
    for ttype, value in stream:
        yield ttype, value.lower()""",
        """CODE.filter = NameHighlightFilter(
    names=['foo', 'bar', 'baz'],
    tokentype=Name.Function,
)""" .

<DEPENDENCY.Pygments==2.15.0> <CONTAINS> """CODE.def __init__(self, **options):
    self.compress = options.get('compress', '')
    Lexer.__init__(self, **options)
""" .

<DEPENDENCY.absl-py==0.13.0> <CONTAINS> """CODE.@absltest.skipThisClass("Shared functionality")
class BaseTest(absltest.TestCase):
    def test_simple_functionality(self):
        self.assertEqual(self.system_under_test.method(), 1)""" .

<DEPENDENCY.absl-py==0.8.0> <CONTAINS> """CODE.@expectedFailureIf(sys.version.major == 2, "Not yet working in py2")
def test_foo(self):
    ...""" .

<DEPENDENCY.accelerate==0.15.0> <CONTAINS> """CODE.from accelerate import Accelerator

accelerator = Accelerator(even_batches=True)
ddp_model, optimizer, dataloader = accelerator.prepare(model, optimizer, dataloader)

with accelerator.join_uneven_inputs([ddp_model], even_batches=False):
...     for input, output in dataloader:
...         outputs = model(input)
...         loss = loss_func(outputs)
...         loss.backward()
...         optimizer.step()
...         optimizer.zero_grad()
""" .

<DEPENDENCY.accelerate==0.16.0> <CONTAINS> """CODE.from accelerate import Accelerator

accelerator = Accelerator()
dataloader, model, optimizer, scheduler = accelerator.prepare(dataloader, model, optimizer, scheduler)

for (input, target) in accelerator.skip_first_batches(dataloader, num_batches=2):
    optimizer.zero_grad()
    output = model(input)
    loss = loss_func(output, target)
    accelerator.backward(loss)
    optimizer.step()
""",
        """CODE.from accelerate.utils import install_xla
install_xla(upgrade=True)
""",
        """CODE.import torch
from accelerate.utils import release_memory

a = torch.ones(1000, 1000).cuda()
b = torch.ones(1000, 1000).cuda()
a, b = release_memory(a, b)
""",
        """CODE.import torch.nn as nn
from accelerate import init_on_device

with init_on_device(device=torch.device("cuda")):
    tst = nn.Liner(100, 100)  # on `cuda` device
""" .

<DEPENDENCY.accelerate==0.17.0> <CONTAINS> """CODE.@accelerator.on_local_process(local_process_index=2)
def print_something():
    print(f"Printed on process {accelerator.local_process_index}")


print_something()
""",
        """CODE.@state.on_local_main_process
def print_something():
    print("This will be printed by process 0 only on each server.")

print_something()
""",
        """CODE.@state.on_main_process
def print_something():
    print("This will be printed by process 0 only.")""",
        """CODE.@state.on_process(process_index=2)
def print_something():
    print(f"Printed on process {state.process_index}")

print_something()""",
        """CODE.from accelerate import Accelerator
accelerator = Accelerator()
with accelerator.main_process_first():
    print(f"This will be printed by process {accelerator.process_index}")
""",
        """CODE.from accelerate import Accelerator
from accelerate.utils import FP8RecipeKwargs

kwargs = FP8RecipeKwargs(fp8_format="HYBRID")
accelerator = Accelerator(mixed_precision="fp8", kwargs_handlers=[kwargs])
""",
        """CODE.from accelerate.state import PartialState
state = PartialState()
with state.local_main_process_first():
    print(f"This will be printed by process {state.local_process_index}")
""",
        """CODE.import time
from accelerate.state import PartialState

state = PartialState()
if state.is_main_process:
    time.sleep(2)
else:
    print("I'm waiting for the main process to finish its sleep...")
state.wait_for_everyone()
print("Everyone is here")
""",
        """CODE.model_1, hook_1 = cpu_offload_with_hook(model_1, cuda_device)
model_2, hook_2 = cpu_offload_with_hook(model_2, cuda_device, prev_module_hook=hook_1)
model_3, hook_3 = cpu_offload_with_hook(model_3, cuda_device, prev_module_hook=hook_2)

hid_1 = model_1(input)
for i in range(50):
    # model1 is offloaded on the CPU at the first iteration, model 2 stays on the GPU for this whole loop.
    hid_2 = model_2(hid_1)
# model2 is offloaded to the CPU just before this forward.
hid_3 = model_3(hid_3)

# For model3, you need to manually call the hook offload method.
hook_3.offload()
""" .

<DEPENDENCY.accelerate==0.20.0> <CONTAINS> """CODE.# Assume there are two processes
from accelerate import Accelerator

accelerator = Accelerator()
with accelerator.split_between_processes(["A", "B", "C"]) as inputs:
    print(inputs)
# Process 0
["A", "B"]
# Process 1
["C"]

with accelerator.split_between_processes(["A", "B", "C"], apply_padding=True) as inputs:
    print(inputs)
# Process 0
["A", "B"]
# Process 1
["C", "C"]
""",
        """CODE.# Assume there are two processes
from accelerate import PartialState

state = PartialState()
with state.split_between_processes(["A", "B", "C"]) as inputs:
    print(inputs)
# Process 0
["A", "B"]
# Process 1
["C"]

with state.split_between_processes(["A", "B", "C"], apply_padding=True) as inputs:
    print(inputs)
# Process 0
["A", "B"]
# Process 1
["C", "C"]
""",
        """CODE.# Assume there are two processes
from accelerate.state import AcceleratorState

state = AcceleratorState()
with state.split_between_processes(["A", "B", "C"]) as inputs:
    print(inputs)
# Process 0
["A", "B"]
# Process 1
["C"]

with state.split_between_processes(["A", "B", "C"], apply_padding=True) as inputs:
    print(inputs)
# Process 0
["A", "B"]
# Process 1
["C", "C"]
""" .

<DEPENDENCY.accelerate==0.21.0> <CONTAINS> """CODE.from accelerate import Accelerator
accelerator = Accelerator()
model = ...
accelerator.save_model(model, save_directory)
""" .

<DEPENDENCY.accelerate==0.22.0> <CONTAINS> """CODE.        from accelerate import Accelerator

        accelerator = Accelerator()
        dataloader, model, optimizer = accelerator.prepare(dataloader, model, optimizer)

        with accelerator.no_sync():
        ...     loss_a = loss_func(model(input_a))  # first forward pass
        ...     loss_b = loss_func(model(input_b))  # second forward pass
        accelerator.backward(loss_a)  # No synchronization across processes, only accumulate gradients
        with accelerator.trigger_sync_in_backward(model):
        ...     accelerator.backward(loss_b)  # Synchronization across all processes
        optimizer.step()
        optimizer.zero_grad()
""",
        """CODE.from accelerate import Accelerator
from accelerate.utils import AutocastKwargs

kwargs = AutocastKwargs(cache_enabled=True)
accelerator = Accelerator(kwargs_handlers=[kwargs])
""",
        """CODE.import os
from accelerate.utils import clear_environment

os.environ["FOO"] = "bar"
with clear_environment():
    print(os.environ)
    os.environ["FOO"] = "new_bar"
    print(os.environ["FOO"])

""" .

<DEPENDENCY.accelerate==0.23.0> <CONTAINS> """CODE.from accelerate import Accelerator

accelerator = Accelerator()
if should_do_breakpoint(loss):
...     accelerator.set_trigger()
if accelerator.check_breakpoint():
...     break
""",
        """CODE.if should_do_breakpoint(loss):
    accelerator.set_trigger()
if accelerator.check_trigger():
    break
""" .

<DEPENDENCY.accelerate==0.8.0> <CONTAINS> """CODE.def convert_file_size_to_int(size):
    if isinstance(size, int):
        return size
    units = {"B": 1, "KB": 1024, "MB": 1024 ** 2, "GB": 1024 ** 3, "TB": 1024 ** 4}
    size_str = str(size)
    for unit in units:
        if size_str.endswith(unit):
            return int(size_str[:-len(unit)]) * units[unit]
    raise ValueError("Invalid size format")
""",
        """CODE.dtype_byte_size(torch.float32)
4
""",
        """CODE.pyton
import torch.nn as nn
from accelerate import init_empty_weights

# Initialize a model with 100 billions parameters in no time and without using any RAM.
with init_empty_weights():
    tst = nn.Sequential(*[nn.Linear(10000, 10000) for _ in range(1000)])
""" .

<DEPENDENCY.appdirs==1.2.0> <CONTAINS> """CODE.#!/usr/bin/env python
if __name__ == "__main__":
    retval = testlib.harness()
    sys.exit(retval)""",
        """CODE.@testlib.tag("knownfailure")
def test_foo(self):
    #...""" .

<DEPENDENCY.attrs==19.2.0> <CONTAINS> """CODE.attr.VersionInfo(19, 1, 0, "final")  <= (19, 2)
attr.VersionInfo(19, 1, 0, "final") < (19, 1, 1)
vi = attr.VersionInfo(19, 2, 0, "final")
vi < (19, 1, 1)
vi < (19,)
vi == (19, 2,)
vi == (19, 2, 1)""" .

<DEPENDENCY.cffi==1.12.0> <CONTAINS> """CODE.merge_flags({"libraries": ["one"]}, {"libraries": ["two"]})
{"libraries": ["one", "two"]}""" .

<DEPENDENCY.click==2.0> <CONTAINS> """CODE.click.echo(click.style('Hello World!', fg='green'))
click.echo(click.style('ATTENTION!', blink=True))
click.echo(click.style('Some things', reverse=True, fg='cyan'))""",
        """CODE.click.launch('http://click.pocoo.org/')
click.launch('/my/downloaded/file', locate=True)""",
        """CODE.click.secho('Hello World!', fg='green')
click.echo(click.style('Hello World!', fg='green'))""" .

<DEPENDENCY.click==3.0> <CONTAINS> """CODE.@click.group()
@click.option('-i', '--input', default=23)
def cli(input):
    return 42

@cli.resultcallback()
def process_result(result, input):
    return result + input""",
        """CODE.with open_file(filename) as f:
    ...
""" .

<DEPENDENCY.click==5.0> <CONTAINS> """CODE.LANG_KEY = __name__ + '.lang'

def set_language(value):
    ctx = get_current_context()
    ctx.meta[LANG_KEY] = value

def get_language():
    return get_current_context().meta.get(LANG_KEY, 'en_US')
""" .

<DEPENDENCY.click==5.1> <CONTAINS> """CODE.unpack_args(range(6), [1, 2, 1, -1])
unpack_args(range(6), [1, 2, 1])
unpack_args(range(6), [-1])
unpack_args(range(6), [1, 1])
unpack_args(range(6), [-1,1,1,1,1])""" .

<DEPENDENCY.click==7.1.2> <CONTAINS> """CODE.@click.group()
@click.option('-i', '--input', default=23)
def cli(input):
    return 42

@cli.resultcallback()
def process_result(result, input):
    return result + input""" .

<DEPENDENCY.click==8.0.0> <CONTAINS> """CODE.@click.group()
@click.option("--name")
@click.pass_context
def cli(ctx):
    ctx.obj = ctx.with_resource(connect_db(name))""",
        """CODE.@click.group()
@click.option('-i', '--input', default=23)
def cli(input):
    return 42

@cli.result_callback()
def process_result(result, input):
    return result + input""",
        """CODE.with Context(cli) as ctx:
    info = ctx.to_info_dict()""" .

<DEPENDENCY.cloudpickle==0.1.1> <CONTAINS> "CODE.subprocess_pickle_echo([1, 'a', None])" .

<DEPENDENCY.cloudpickle==2.0.0> <CONTAINS> "CODE.testutils.subprocess_pickle_string([1, 'a', None], protocol=2)" .

<DEPENDENCY.datasets==1.1.3> <CONTAINS> """CODE.# You can use this method in the context manager (recommended)
with lock.acquire():
    pass

# Or use an equivalent try-finally construct:
lock.acquire()
try:
    pass
finally:
    lock.release()
""" .

<DEPENDENCY.datasets==1.13.0> <CONTAINS> """CODE.from datasets.data_files import resolve_patterns_in_dataset_repository
base_path = "/Users/username/Desktop/hf/datasets"
resolve_patterns_locally_or_by_urls(base_path, ["src/**/*.yaml"])
""",
        """CODE.import huggingface_hub
from datasets.data_files import resolve_patterns_in_dataset_repository

dataset_info = huggingface_hub.HfApi().dataset_info("lhoestq/demo1")
resolve_patterns_in_dataset_repository(dataset_info, ["*.csv"])
""" .

<DEPENDENCY.datasets==1.16.0> <CONTAINS> """CODE.dataset.push_to_hub("<organization>/<dataset_id>", split="evaluation")
""",
        "CODE.xbasename(\"zip://folder1/file.txt::https://host.com/archive.zip\")" .

<DEPENDENCY.datasets==1.18.0> <CONTAINS> """CODE.def xsplitext(path):
    parts = path.split("::")
    first_part, *rest = parts
    base, ext = os.path.splitext(first_part)
    return (base + "".join(rest), ext)
""" .

<DEPENDENCY.datasets==1.2.0> <CONTAINS> """CODE.t = {'name': 'Ferry', 'hobbies': ['programming', 'sci-fi']}
deepupdate(t, {'hobbies': ['gaming']})
print(t)
{'name': 'Ferry', 'hobbies': ['programming', 'sci-fi', 'gaming']}""" .

<DEPENDENCY.datasets==1.3.0> <CONTAINS> """CODE.import datasets
s3 = datasets.filesystems.S3FileSystem(anon=True)
s3.ls('public-datasets/imdb/train')

import datasets
s3 = datasets.filesystems.S3FileSystem(key=aws_access_key_id, secret=aws_secret_access_key)
s3.ls('my-private-datasets/imdb/train')

import botocore
from datasets.filesystems import S3Filesystem
s3_session = botocore.session.Session(profile_name='my_profile_name')
s3 = S3FileSystem(session=s3_session)

from datasets import load_from_disk
from datasets.filesystems import S3Filesystem
s3 = S3FileSystem(key=aws_access_key_id, secret=aws_secret_access_key)
dataset = load_from_disk('s3://my-private-datasets/imdb/train',fs=s3)
print(len(dataset))

from datasets import load_dataset
from datasets.filesystems import S3Filesystem
dataset = load_dataset("imdb")
s3 = S3FileSystem(key=aws_access_key_id, secret=aws_secret_access_key)
dataset.save_to_disk('s3://my-private-datasets/imdb/train',fs=s3)""" .

<DEPENDENCY.datasets==1.8.0> <CONTAINS> """CODE.from datasets import Features, Sequence, Value

f1 = Features({"root": Sequence({"a": Value("string"), "b": Value("string")})})
f2 = Features({"root": {"b": Sequence(Value("string")), "a": Sequence(Value("string"))}})
assert f1.type != f2.type
f1.reorder_fields_as(f2)
assert f1.reorder_fields_as(f2).type == f2.type
""" .

<DEPENDENCY.datasets==1.9.0> <CONTAINS> """CODE.import importlib
from datasets.load import prepare_module
from datasets.streaming import patch_submodule, xjoin

snli_module_path, _ = prepare_module("snli")
snli_module = importlib.import_module(snli_module_path)
patcher = patch_submodule(snli_module, "os.path.join", xjoin)
patcher.start()
assert snli_module.os.path.join is xjoin""" .

<DEPENDENCY.datasets==2.0.0> <CONTAINS> """CODE.import pandas as pd
import pyarrow as pa
df = pd.DataFrame({
    'int': [1, 2],
    'str': ['a', 'b']
})
pa.Table.from_pandas(df)
""" .

<DEPENDENCY.datasets==2.10.0> <CONTAINS> """CODE.from datasets import load_dataset
ds = load_dataset("rotten_tomatoes")
ds.select_columns("text")
""",
        """CODE.from datasets import load_dataset
ds = load_dataset("rotten_tomatoes", split="train", streaming=True)
next(iter(ds))
{'text': 'the rock is destined to be the 21st century's new " conan " and that he's going to make a splash even greater than arnold schwarzenegger , jean-claud van damme or steven segal .', 'label': 1}
ds = ds.select_columns("text")
next(iter(ds))
{'text': 'the rock is destined to be the 21st century's new " conan " and that he's going to make a splash even greater than arnold schwarzenegger , jean-claud van damme or steven segal .'}
""",
        """CODE.from datasets import load_dataset
ds = load_dataset("rotten_tomatoes", split="validation")
ds.select_columns(['text'])
Dataset({
    features: ['text'],
    num_rows: 1066
})
""",
        """CODE.ids = ds.to_iterable_dataset()
for example in ids:
...     pass


ids = ds.to_iterable_dataset()
ids = ids.filter(filter_fn).map(process_fn)  # will filter and process on-the-fly when you start iterating over the iterable dataset
for example in ids:
...     pass


ids = ds.to_iterable_dataset(num_shards=64)  # the dataset is split into 64 shards to be iterated over
ids = ids.shuffle(buffer_size=10_000)  # will shuffle the shards order and use a shuffle buffer for fast approximate shuffling when you start iterating
for example in ids:
...     pass


import torch
ids = ds.to_iterable_dataset(num_shards=64)
ids = ids.filter(filter_fn).map(process_fn)
dataloader = torch.utils.data.DataLoader(ids, num_workers=4)  # will assign 64 / 4 = 16 shards to each worker to load, filter and process when you start iterating
for example in ids:
...     pass


import torch
ids = ds.to_iterable_dataset(num_shards=64)
ids = ids.shuffle(buffer_size=10_000)  # will shuffle the shards order and use a shuffle buffer when you start iterating
dataloader = torch.utils.data.DataLoader(ids, num_workers=4)  # will assign 64 / 4 = 16 shards from the shuffled list of shards to each worker when you start iterating
for example in ids:
...     pass


from datasets.distributed import split_dataset_by_node
ids = ds.to_iterable_dataset(num_shards=512)
ids = ids.shuffle(buffer_size=10_000)  # will shuffle the shards order and use a shuffle buffer when you start iterating
ids = split_dataset_by_node(ds, world_size=8, rank=0)  # will keep only 512 / 8 = 64 shards from the shuffled lists of shards when you start iterating
dataloader = torch.utils.data.DataLoader(ids, num_workers=4)  # will assign 64 / 4 = 16 shards from this node's list of shards to each worker when you start iterating
for example in ids:
...     pass


ids = ds.to_iterable_dataset(num_shards=64)
ids = ids.shuffle(buffer_size=10_000, seed=42)  # will shuffle the shards order and use a shuffle buffer when you start iterating
for epoch in range(n_epochs):
...     ids.set_epoch(epoch)  # will use effective_seed = seed + epoch to shuffle the shards and for the shuffle buffer when you start iterating
...     for example in ids:
...         pass

""" .

<DEPENDENCY.datasets==2.11.0> <CONTAINS> """CODE.ds.to_list()
""",
        """CODE.from datasets import load_dataset
ds = load_dataset("rotten_tomatoes", split="validation", streaming=True)
ds.column_names
['text', 'label']
""" .

<DEPENDENCY.datasets==2.12.0> <CONTAINS> """CODE.df = spark.createDataFrame(
    data=[[1, "Elia"], [2, "Teo"], [3, "Fang"]],
    columns=["id", "name"],
)
ds = Dataset.from_spark(df)
""" .

<DEPENDENCY.datasets==2.13.0> <CONTAINS> """CODE.@experimental
def my_function():
    print("Hello world!")
""",
        """CODE.df = spark.createDataFrame(
    data=[[1, "Elia"], [2, "Teo"], [3, "Fang"]],
    columns=["id", "name"],
)
ds = IterableDataset.from_spark(df)
""",
        """CODE.with parallel_backend('spark'):
  dataset = load_dataset(..., num_proc=2)
""" .

<DEPENDENCY.datasets==2.13.2> <CONTAINS> """CODE.from datasets.data_files import resolve_patterns_locally_or_by_urls

base_path = "."
resolve_patterns_locally_or_by_urls(base_path, ["src/**/*.yaml"])
""",
        """CODE.import huggingface_hub
from datasets.data_files import resolve_patterns_in_dataset_repository
dataset_info = huggingface_hub.HfApi().dataset_info("lhoestq/demo1")
resolve_patterns_in_dataset_repository(dataset_info, ["data/*.csv"])
""" .

<DEPENDENCY.datasets==2.14.0> <CONTAINS> """CODE.from datasets.data_files import resolve_pattern
base_path = "."
resolve_pattern("docs/**/*.py", base_path)
""" .

<DEPENDENCY.datasets==2.15.0> <CONTAINS> """CODE.# You can use this method in the context manager (recommended)
with lock.acquire():
    pass

# Or use an equivalent try-finally construct:
lock.acquire()
try:
    pass
finally:
    lock.release()
""" .

<DEPENDENCY.datasets==2.16.0> <CONTAINS> """CODE.from datasets import get_dataset_default_config_name
get_dataset_default_config_name("openbookqa")
""" .

<DEPENDENCY.datasets==2.2.0> <CONTAINS> """CODE.def convert_file_size_to_int(size):
    if isinstance(size, int):
        return size
    units = {"B": 1, "KB": 1024, "MB": 1024 ** 2, "GB": 1024 ** 3, "TB": 1024 ** 4}
    number, unit = int(size[:-2]), size[-2:]
    return number * units[unit]

""",
        "CODE.xsplit(\"zip://folder1/file.txt::https://host.com/archive.zip\")" .

<DEPENDENCY.datasets==2.2.2> <CONTAINS> """CODE.downloaded_files = dl_manager.download('https://storage.googleapis.com/seldon-datasets/sentence_polarity_v1/rt-polaritydata.tar.gz')
""",
        """CODE.downloaded_files = dl_manager.download('https://storage.googleapis.com/seldon-datasets/sentence_polarity_v1/rt-polaritydata.tar.gz')
extracted_files = dl_manager.extract(downloaded_files)
""" .

<DEPENDENCY.datasets==2.3.0> <CONTAINS> """CODE.from datasets import Features
features = Features({'x': Array2D(shape=(1, 3), dtype='int32')})
""",
        """CODE.from datasets import Features
features = Features({'x': Array3D(shape=(1, 2, 3), dtype='int32')})
""",
        """CODE.from datasets import Features
features = Features({'x': Array4D(shape=(1, 2, 2, 3), dtype='int32')})
""",
        """CODE.from datasets import Features
features = Features({'x': Array5D(shape=(1, 2, 2, 3, 3), dtype='int32')})
""",
        """CODE.from datasets import load_dataset
ds = load_dataset("rotten_tomatoes", split="validation")
ds._select_contiguous(0, 4)
Dataset({
    features: ['text', 'label'],
    num_rows: 4
})
""",
        """CODE.from datasets import load_dataset
ds = load_dataset("rotten_tomatoes", split="validation")
ds._select_with_indices_mapping(range(4))
Dataset({
    features: ['text', 'label'],
    num_rows: 4
})
""" .

<DEPENDENCY.datasets==2.4.0> <CONTAINS> """CODE._is_inside_unrequested_special_dir("__pycache__/b.txt", "**")
True
_is_inside_unrequested_special_dir("__pycache__/b.txt", "*/b.txt")
True
_is_inside_unrequested_special_dir("__pycache__/b.txt", "__pycache__/*")
False
_is_inside_unrequested_special_dir("__pycache__/b.txt", "__*/*")
False""",
        """CODE.def _is_unrequested_hidden_file_or_is_inside_unrequested_hidden_dir(filepath, pattern):
    # code here
    pass
""",
        """CODE.ds3 = _concatenate_iterable_datasets([ds1, ds2])
""",
        """CODE.ds3 = _concatenate_map_style_datasets([ds1, ds2])
""",
        """CODE.fs = mock_fs(["data/train.txt", "data.test.txt"])
assert fsspec.get_filesystem_class("mock").__name__ == "DummyTestFS"
assert type(fs).__name__ == "DummyTestFS"
print(fs.glob("**"))
["data", "data/train.txt", "data.test.txt"]
""" .

<DEPENDENCY.datasets==2.5.0> <CONTAINS> """CODE.def gen():
    yield {"text": "Good", "label": 0}
    yield {"text": "Bad", "label": 1}

ds = Dataset.from_generator(gen)""" .

<DEPENDENCY.datasets==2.6.0> <CONTAINS> """CODE.def gen():
    yield {"text": "Good", "label": 0}
    yield {"text": "Bad", "label": 1}

ds = IterableDataset.from_generator(gen)
""",
        """CODE.ds.to_sql("data", "sqlite:///my_own_db.sql")
import sqlite3
con = sqlite3.connect("my_own_db.sql")
with con:
...     ds.to_sql("data", con)
""",
        """CODE.from datasets import Dataset

# Fetch a database table
ds = Dataset.from_sql("test_data", "postgres:///db_name")
# Execute a SQL query on the table
ds = Dataset.from_sql("SELECT sentence FROM test_data", "postgres:///db_name")
# Use a Selectable object to specify the query
from sqlalchemy import select, text
stmt = select([text("sentence")]).select_from(text("test_data"))
ds = Dataset.from_sql(stmt, "postgres:///db_name")
""" .

<DEPENDENCY.datasets==2.7.0> <CONTAINS> """CODE._distribute_shards(2, max_num_jobs=4)
_distribute_shards(10, max_num_jobs=3)""" .

<DEPENDENCY.decorator==4.3.0> <CONTAINS> """CODE.a = Action()
a.user = User()
a.view()
a.insert() # doctest: +IGNORE_EXCEPTION_DETAIL
Traceback (most recent call last):
   ...
PermissionError: User does not have the permission to run insert!""" .

<DEPENDENCY.decorator==4.4.1> <CONTAINS> "CODE.operation1()",
        "CODE.operation2()" .

<DEPENDENCY.decorator==4.4.2> <CONTAINS> """CODE.ba.__class__.__name__
hello('michele')
BEFORE
hello michele
AFTER""" .

<DEPENDENCY.dill==0.2.3> <CONTAINS> """CODE.with capture('stdout') as out:
    print "foo!"
print out.getvalue()""" .

<DEPENDENCY.dill==0.3.5> <CONTAINS> """CODE.pickler.save(Reduce(*reduction))
pickler.save_reduce(*reduction, obj=reduction)""" .

<DEPENDENCY.dill==0.3.6> <CONTAINS> """CODE.import dill
dill.detect.trace(True)
dill.dump_session()
from dill import detect
D = {'a': 42, 'b': {'x': None}}
with detect.trace():
    dumps(D)
squared = lambda x: x**2
with detect.trace('output.txt', mode='w') as log:
    log("> D = %r", D)
    dumps(D)
    log("> squared = %r", squared)
    dumps(squared)
""",
        """CODE.import dill
squared = lambda x: x*x
dill.dump_module() # save state of __main__ to /tmp/session.pkl

import dill
import pox
pox.plus_one = lambda x: x+1
dill.dump_module('pox_session.pkl', module=pox)

import dill
from types import ModuleType
foo = ModuleType('foo')
foo.values = [1,2,3]
import math
foo.sin = math.sin
dill.dump_module('foo_session.pkl', module=foo, refimported=True)

import dill
dill.load_module()
squared(2)
pox = dill.load_module('pox_session.pkl')
pox.plus_one(1)
foo = dill.load_module('foo_session.pkl')
[foo.sin(x) for x in foo.values]
""",
        """CODE.lambda filename: vars(dill.load_module(filename)).copy()
import dill
alist = [1, 2, 3]
anum = 42
dill.dump_module()
anum = 0
new_var = 'spam'
main = dill.load_module_asdict()
main['__name__'], main['__session__']
('__main__', '/tmp/session.pkl')
main is globals() # loaded objects don't reference globals
False
main['alist'] == alist
True
main['alist'] is alist # was saved by value
False
main['anum'] == anum # changed after the session was saved
False
new_var in main # would be True if the option 'update' was set""",
        """CODE.with match(args) as m:
    if   m.case(('x', 'y')):
        # use m.x and m.y
    elif m.case(('x', 'y', 'z')):
        # use m.x, m.y and m.z

Equivalent native code for Python >= 3.10:
match args:
    case (x, y):
        # use x and y
    case (x, y, z):
        # use x, y and z""" .

<DEPENDENCY.evaluate==0.2.0> <CONTAINS> """CODE.combine(["accuracy", "f1", "precision","recall"])
clf_metrics.compute(predictions=[0,1], references=[1,1])""",
        """CODE.from evaluate import evaluator
from datasets import load_dataset
task_evaluator = evaluator("image-classification")
data = load_dataset("beans", split="test[:40]")
results = task_evaluator.compute(
    model_or_pipeline="nateraw/vit-base-beans",
    data=data,
    label_column="labels",
    metric="accuracy",
    label_mapping={'angular_leaf_spot': 0, 'bean_rust': 1, 'healthy': 2},
    strategy="bootstrap"
)
""",
        """CODE.from evaluate import evaluator
from datasets import load_dataset
task_evaluator = evaluator("question-answering")
data = load_dataset("squad", split="validation[:2]")
results = task_evaluator.compute(
    model_or_pipeline="sshleifer/tiny-distilbert-base-cased-distilled-squad",
    data=data,
    metric="squad",
)


from evaluate import evaluator
from datasets import load_dataset
task_evaluator = evaluator("question-answering")
data = load_dataset("squad_v2", split="validation[:2]")
results = task_evaluator.compute(
    model_or_pipeline="mrm8488/bert-tiny-finetuned-squadv2",
    data=data,
    metric="squad_v2",
    squad_v2_format=True,
)
""" .

<DEPENDENCY.evaluate==0.3.0> <CONTAINS> """CODE.from evaluate import evaluator
from datasets import load_dataset
task_evaluator = evaluator("summarization")
data = load_dataset("cnn_dailymail", "3.0.0", split="validation[:40]")
results = task_evaluator.compute(
    model_or_pipeline="facebook/bart-large-cnn",
    data=data,
    input_column="article",
    label_column="highlights",
)""",
        """CODE.from evaluate import evaluator
from datasets import load_dataset
task_evaluator = evaluator("text2text-generation")
data = load_dataset("cnn_dailymail", "3.0.0", split="validation[:40]")
results = task_evaluator.compute(
    model_or_pipeline="facebook/bart-large-cnn",
    data=data,
    input_column="article",
    label_column="highlights",
    metric="rouge",
)""",
        """CODE.from evaluate import evaluator
from datasets import load_dataset
task_evaluator = evaluator("translation")
data = load_dataset("wmt19", "fr-de", split="validation[:40]")
data = data.map(lambda x: {"text": x["translation"]["de"], "label": x["translation"]["fr"]})
results = task_evaluator.compute(
    model_or_pipeline="Helsinki-NLP/opus-mt-de-fr",
    data=data,
)""" .

<DEPENDENCY.evaluate==0.4.0> <CONTAINS> """CODE.from evaluate import EvaluationSuite
suite = EvaluationSuite.load("evaluate/evaluation-suite-ci")
results = suite.run("lvwerra/distilbert-imdb")
""",
        """CODE.from evaluate import evaluator
from datasets import load_dataset
task_evaluator = evaluator("summarization")
data = load_dataset("cnn_dailymail", "3.0.0", split="validation[:40]")
results = task_evaluator.compute(
    model_or_pipeline="facebook/bart-large-cnn",
    data=data,
    input_column="article",
    label_column="highlights",
)""",
        """CODE.from evaluate import evaluator
from datasets import load_dataset
task_evaluator = evaluator("text2text-generation")
data = load_dataset("cnn_dailymail", "3.0.0", split="validation[:40]")
results = task_evaluator.compute(
    model_or_pipeline="facebook/bart-large-cnn",
    data=data,
    input_column="article",
    label_column="highlights",
    metric="rouge",
)""",
        """CODE.from evaluate import evaluator
from datasets import load_dataset
task_evaluator = evaluator("translation")
data = load_dataset("wmt19", "fr-de", split="validation[:40]")
data = data.map(lambda x: {"text": x["translation"]["de"], "label": x["translation"]["fr"]})
results = task_evaluator.compute(
    model_or_pipeline="Helsinki-NLP/opus-mt-de-fr",
    data=data,
)""" .

<DEPENDENCY.faiss-cpu==1.7.1> <CONTAINS> """CODE.supported_instruction_sets()  # for x86
{"SSE2", "AVX2", ...}
supported_instruction_sets()  # for PPC
{"VSX", "VSX2", ...}
supported_instruction_sets()  # for ARM
{"NEON", "ASIMD", ...}""" .

<DEPENDENCY.faiss-cpu==1.7.4> <CONTAINS> """CODE.timer = RepeatTimer(warmup=1, nt=1, runs=6)
for _ in timer:
    # perform operation
print(f"time={timer.get_ms():.1f} Â± {timer.get_ms_std():.1f} ms")""" .

<DEPENDENCY.filelock==2.0.0> <CONTAINS> """CODE... code-block:: python
    with lock.acquire_(timeout = 20):
        pass""",
        """CODE.with lock.acquire_(timeout = 20):
        pass""" .

<DEPENDENCY.fire==0.2.0> <CONTAINS> "CODE._as_arg_name_and_type(\"foo (int)\")",
        "CODE._as_arg_names(\"a, b, c\") == [\"a\", \"b\", \"c\"]",
        "CODE._matches_section_title('Yields', 'yield') == True",
        "CODE.fn_with_code_in_docstring()" .

<DEPENDENCY.flax==0.3.2> <CONTAINS> """CODE.class Foo(nn.Module):
    @nn.compact
    def __call__(self, x):
      h = nn.Dense(4)(x)
      self.sow('intermediates', 'h', h)
      return nn.Dense(2)(h)
  y, state = Foo.apply(params, x, mutable=['intermediates'])
  print(state['intermediates'])  # {'h': (...,)}

class Foo(nn.Module):
    @nn.compact
    def __call__(self, x):
      init_fn = lambda: 0
      reduce_fn = lambda a, b: a + b
      self.sow('intermediates', x, h,
               init_fn=init_fn, reduce_fn=reduce_fn)
      self.sow('intermediates', x * 2, h,
               init_fn=init_fn, reduce_fn=reduce_fn)
      return x
  y, state = Foo.apply(params, 1, mutable=['intermediates'])
  print(state['intermediates'])  # ==> {'h': 3}
""",
        """CODE.params = convert_pre_linen(pre_linen_params)

batch_stats = convert_pre_linen(flax.traverse_util.unflatten_dict({
    tuple(k.split('/')[1:]): v
    for k, v in pre_linen_model_state.as_dict().items()
}))

variables = {'params': params, 'batch_stats': batch_stats}
""",
        "CODE.state, params = variables.pop('params')" .

<DEPENDENCY.flax==0.3.3> <CONTAINS> """CODE.class AutoEncoder(nn.Module):
    def setup(self):
      self.encoder = nn.Dense(3)
      self.decoder = nn.Dense(5)

ae = AutoEncoder()
model = ae.bind(variables)
z = model.encode(x)
x_reconstructed = model.decode(z)
""",
        """CODE.def init_with_output(fn, module, mutable=False):
    def init_fn(rngs, *args, **kwargs):
        variables = module.init(rngs)
        return fn(module, *args, **kwargs), variables
    return init_fn
""" .

<DEPENDENCY.flax==0.3.4> <CONTAINS> """CODE.def flip_sequences(inputs, lengths):
    flipped_inputs = []
    for i in range(len(inputs)):
        seq = inputs[i][:lengths[i]]
        flipped_seq = seq[::-1] + inputs[i][lengths[i]:]
        flipped_inputs.append(flipped_seq)
    return flipped_inputs
""",
        """CODE.def sequence_mask(lengths, max_length):
    mask = []
    for length in lengths:
        mask.append([True] * length + [False] * (max_length - length))
    return mask
""" .

<DEPENDENCY.flax==0.3.5> <CONTAINS> """CODE.@nowrap
def _make_dense(self, num_features):
  return nn.Dense(num_features)

@compact
def __call__(self, x):
  # now safe to use constructor helper even if using named_call
  dense = self._dense(self.num_features)
  return dense(x)""",
        """CODE.import jax
import jax.numpy as jnp
import flax.linen as nn

class AutoEncoder(nn.Module):
  def setup(self):
    self.encoder = nn.Dense(3)
    self.decoder = nn.Dense(5)

  def __call__(self, x):
    return self.decoder(self.encoder(x))

x = jnp.ones((16, 9))
ae = AutoEncoder()
variables = ae.init(jax.random.PRNGKey(0), x)
model = ae.bind(variables)
z = model.encoder(x)
x_reconstructed = model.decoder(z)
""",
        """CODE.import jax
import jax.numpy as jnp
import flax.linen as nn

class Foo(nn.Module):
  @nn.compact
  def __call__(self, x):
    h = nn.Dense(4)(x)
    self.sow('intermediates', 'h', h)
    return nn.Dense(2)(h)

x = jnp.ones((16, 9))
model = Foo()
variables = model.init(jax.random.PRNGKey(0), x)
y, state = model.apply(variables, x, mutable=['intermediates'])
print(state['intermediates'])  # {'h': (...,)}

class Foo2(nn.Module):
  @nn.compact
  def __call__(self, x):
    init_fn = lambda: 0
    reduce_fn = lambda a, b: a + b
    self.sow('intermediates', 'h', x,
             init_fn=init_fn, reduce_fn=reduce_fn)
    self.sow('intermediates', 'h', x * 2,
             init_fn=init_fn, reduce_fn=reduce_fn)
    return x

model = Foo2()
variables = model.init(jax.random.PRNGKey(0), x)
y, state = model.apply(variables, jnp.ones((1, 1)), mutable=['intermediates'])
print(state['intermediates'])  # ==> {'h': [[3.]]}
""",
        """CODE.key = self.make_rng('stats')
mean = self.variable('stats', 'mean', lecun_normal(), key, (2, 2))
""" .

<DEPENDENCY.flax==0.3.6> <CONTAINS> """CODE.@nn.module
def DenseLayer(x, features):
    x = flax.nn.Dense(x, features)
    x = flax.nn.relu(x)
    return x

class DenseLayer(nn.Module):
    def apply(self, x, features):
        x = flax.nn.Dense(x, features)
        x = flax.nn.relu(x)
        return x""",
        """CODE.class Example(nn.Module):
    def apply(self, inputs, decay=0.9):
      ema = self.state('ema', inputs.shape, initializers.zeros)
      ema.value = decay * ema.value + (1 - decay) * inputs
      return inputs
""",
        """CODE.class MyLinearModule(nn.Module):
    def apply(self, x, features, kernel_init):
      kernel = self.param('kernel', (x.shape[-1], features), kernel_init)
      return jnp.dot(x, kernel)

    @nn.module_method
    def apply_transpose(self, x, **kwargs):
      kernel = self.get_param('kernel')
      return jnp.dot(x, kernel.transpose((1, 0)))

class AutoEncoder(nn.module):
    def apply(self, x, features):
      linear_fn = MyLinearModule.shared(features=features)
      h = linear_fn(x)
      y = linear_fn.apply_transpose(h)
      return y
""",
        """CODE.def learn_scale(scope, x):
    p = scope.param('scale', nn.initializers.zeros, ())
    return p * x

def f(scope, x):
    vars_t = jax.tree_map(jnp.ones_like, scope.variables().get('params', {}))
    x, out_t = lift.jvp(
        learn_scale, scope, (x,), (jnp.zeros_like(x),),
        variable_tangents={'params': vars_t})
    return out_t""",
        """CODE.def learn_scale(scope, x):
    p = scope.param('scale', nn.initializers.zeros, ())
    return p * x

def f(scope, x):
    y, bwd = lift.vjp(learn_scale, scope, x)
    params_grad, x_grad = bwd(jnp.ones(y.shape))
    return y, params_grad, x_grad""",
        """CODE.with nn.stochastic(rng):
    x = random.normal(nn.make_rng(), shape)
    x_drop = nn.dropout(x, 0.5)
""" .

<DEPENDENCY.flax==0.4.0> <CONTAINS> """CODE.  import jax
  import jax.numpy as jnp
  import flax.linen as nn

  class Foo(nn.Module):
    @nn.compact
    def __call__(self, x):
      h = nn.Dense(4)(x)
      self.sow('intermediates', 'h', h)
      return nn.Dense(2)(h)

  x = jnp.ones((16, 9))
  model = Foo()
  variables = model.init(jax.random.PRNGKey(0), x)
  y, state = model.apply(variables, x, mutable=['intermediates'])
  print(state['intermediates'])  # {'h': (...,)}

  class Foo2(nn.Module):
    @nn.compact
    def __call__(self, x):
      init_fn = lambda: 0
      reduce_fn = lambda a, b: a + b
      self.sow('intermediates', 'h', x,
               init_fn=init_fn, reduce_fn=reduce_fn)
      self.sow('intermediates', 'h', x * 2,
               init_fn=init_fn, reduce_fn=reduce_fn)
      return x

  model = Foo2()
  variables = model.init(jax.random.PRNGKey(0), x)
  y, state = model.apply(variables, jnp.ones((1, 1)), mutable=['intermediates'])
  print(state['intermediates'])  # ==> {'h': [[3.]]}
""",
        """CODE.import jax
import jax.numpy as jnp
import flax.linen as nn

class AutoEncoder(nn.Module):
  def setup(self):
    self.encoder = nn.Dense(3)
    self.decoder = nn.Dense(5)

  def __call__(self, x):
    return self.decoder(self.encoder(x))

x = jnp.ones((16, 9))
ae = AutoEncoder()
variables = ae.init(jax.random.PRNGKey(0), x)
model = ae.bind(variables)
z = model.encoder(x)
x_reconstructed = model.decoder(z)
""",
        """CODE.key = self.make_rng('stats')
mean = self.variable('stats', 'mean', lecun_normal(), key, (2, 2))
""" .

<DEPENDENCY.flax==0.4.1> <CONTAINS> """CODE.def cond_fn(scope, c):
    return scope.get_variable('state', 'acc') < 10

def body_fn(scope, c):
    acc = scope.variable('state', 'acc')
    acc += 1
    y = scope.child(nn.dense)(c, c.shape[-1])
    return y

c = x
c = body_fn(scope, c)
return lift.while_loop(cond_fn, body_fn, scope, (),
                       carry_variables='state')""",
        """CODE.nn.Sequential([nn.Dense(4),
                            nn.relu,
                            nn.Dense(2),
                            nn.log_softmax])(x)""" .

<DEPENDENCY.flax==0.4.2> <CONTAINS> """CODE.def cond_example(scope, x, pred):
    scope.variable('state', 'true_count', lambda: 0)
    scope.variable('state', 'false_count', lambda: 0)
    def true_fn(scope, x):
        scope.variable('state', 'true_count').value += 1
        return scope.child(nn.dense)(x, 2)
    def false_fn(scope, x):
        scope.variable('state', 'false_count').value += 1
        return -scope.child(nn.dense)(x, 2)
    return lift.cond(pred, true_fn, false_fn, scope, x)""",
        """CODE.def cond_example(scope, x, pred):
    scope.variable('state', 'true_count', lambda: 0)
    scope.variable('state', 'false_count', lambda: 0)
    def true_fn(scope, x):
      scope.variable('state', 'true_count').value += 1
      return scope.child(nn.dense)(x, 2)
    def false_fn(scope, x):
      scope.variable('state', 'false_count').value += 1
      return -scope.child(nn.dense)(x, 2)
    return lift.cond(pred, true_fn, false_fn, scope, x)""" .

<DEPENDENCY.flax==0.5.1> <CONTAINS> """CODE.import jax
import jax.numpy as jnp
import flax.linen as nn

class Foo(nn.Module):
    @nn.compact
    def __call__(self, x):
        h = nn.Dense(4)(x)
        return nn.Dense(2)(h)

x = jnp.ones((16, 9))

print(Foo().tabulate(jax.random.PRNGKey(0), x))
""",
        """CODE.import jax
import jax.numpy as jnp
import flax.linen as nn

class Foo(nn.Module):
    @nn.compact
    def __call__(self, x):
        h = nn.Dense(4)(x)
        return nn.Dense(2)(h)

x = jnp.ones((16, 9))
tabulate_fn = nn.tabulate(Foo(), jax.random.PRNGKey(0))

print(tabulate_fn(x))
""" .

<DEPENDENCY.flax==0.5.3> <CONTAINS> """CODE.am = AsyncManager()
save_checkpoint(..., async_manager=am)""",
        """CODE.def switch_example(scope, x, index):
    scope.variable('state', 'a_count', lambda: 0)
    scope.variable('state', 'b_count', lambda: 0)
    scope.variable('state', 'c_count', lambda: 0)
    def a_fn(scope, x):
      scope.variable('state', 'a_count').value += 1
      return scope.child(nn.dense)(x, 2)
    def b_fn(scope, x):
      scope.variable('state', 'b_count').value += 1
      return -scope.child(nn.dense)(x, 2)
    def c_fn(scope, x):
      scope.variable('state', 'c_count').value += 1
      return scope.child(nn.dense)(x, 2)
    return lift.switch(index, [a_fn, b_fn, c_fn], scope, x)

def multihead_switch_example(scope, x, index):
    def a_fn(scope, x):
      x = scope.child(nn.dense)(x, 10)
      x = scope.child(nn.dense)(x, 7)
      return scope.child(nn.dense)(x, 5)
    def b_fn(scope, x):
      x = scope.child(nn.dense)(x, 11)
      return scope.child(nn.dense)(x, 5)
    def c_fn(scope, x):
      return scope.child(nn.dense)(x, 5)

    branches = [a_fn, b_fn, c_fn]

    # run all branches on init
    if scope.is_mutable_collection('params'):
      for branch in branches:
        _ = branch(scope, x)

    return lift.switch(index, branches, scope, x)""",
        """CODE.kernels = traverse_util.ModelParamTraversal(lambda path, _: 'kernel' in path)
biases = traverse_util.ModelParamTraversal(lambda path, _: 'bias' in path)
kernel_opt = optim.Momentum(learning_rate=0.01)
bias_opt = optim.Momentum(learning_rate=0.1)
opt_def = MultiOptimizer((kernels, kernel_opt), (biases, bias_opt))
optimizer = opt_def.create(model)

hparams = optimizer.optimizer_def.hyper_params
new_optimizer = optimizer.apply_gradient(
    grads,
    hyper_params=[
        hparams[0].replace(learning_rate=0.2),
        hparams[1].replace(learning_rate=jnp.where(step < 1000, 0., lr)),
    ])
""" .

<DEPENDENCY.flax==0.6.1> <CONTAINS> """CODE.import jax
import jax.numpy as jnp
import flax.linen as nn

class Foo(nn.Module):
    @nn.compact
    def __call__(self, x):
        x = nn.Dense(3)(x)
        x = self.perturb('dense3', x)
        return nn.Dense(2)(x)

def loss(params, perturbations, inputs, targets):
    variables = {'params': params, 'perturbations': perturbations}
    preds = model.apply(variables, inputs)
    return jnp.square(preds - targets).mean()

x = jnp.ones((2, 9))
y = jnp.ones((2, 2))
model = Foo()
variables = model.init(jax.random.PRNGKey(0), x)
intm_grads = jax.grad(loss, argnums=1)(variables['params'], variables['perturbations'], x, y)
print(intm_grads['dense3']) # ==> [[-1.456924   -0.44332537  0.02422847]
                            #      [-1.456924   -0.44332537  0.02422847]]""",
        """CODE.import jax.numpy as jnp
from flax import traverse_util

params = {'a': {'x': 10, 'y': 3}, 'b': {'x': 20}}
f = lambda path, x: x + 5 if 'x' in path else -x
traverse_util.path_aware_map(f, params)
""" .

<DEPENDENCY.flax==0.6.4> <CONTAINS> """CODE.import jax, jax.numpy as jnp
from flax.linen.initializers import ones_init
ones_initializer = ones_init()
ones_initializer(jax.random.PRNGKey(42), (3, 2), jnp.float32)""",
        """CODE.import jax, jax.numpy as jnp
from flax.linen.initializers import zeros_init
zeros_initializer = zeros_init()
zeros_initializer(jax.random.PRNGKey(42), (2, 3), jnp.float32)""" .

<DEPENDENCY.flax==0.6.6> <CONTAINS> """CODE.import jax.numpy as jnp
import jax
import flax.linen as nn

x = jax.random.uniform(jax.random.PRNGKey(0), (2, 3))
layer = nn.RMSNorm()
variables = layer.init(jax.random.PRNGKey(1), x)
y = layer.apply(variables, x)""",
        """CODE.model = nn.Dense(features=256)
variables = model.lazy_init(rng, jax.ShapeDtypeStruct((1, 128), jnp.float32))
""" .

<DEPENDENCY.flax==0.6.7> <CONTAINS> """CODE.import jax.numpy as jnp
import jax
import flax.linen as nn

x = jnp.ones((10, 50, 32)) # (batch, time, features)
lstm = nn.RNN(nn.LSTMCell(), cell_size=64)
variables = lstm.init(jax.random.PRNGKey(0), x)
y = lstm.apply(variables, x)
y.shape # (batch, time, cell_size)

x = jnp.ones((10, 50, 32, 32, 3)) # (batch, time, height, width, features)
conv_lstm = nn.RNN(nn.ConvLSTMCell(64, kernel_size=(3, 3)), cell_size=(32, 32, 64))
y, variables = conv_lstm.init_with_output(jax.random.PRNGKey(0), x)
y.shape # (batch, time, height, width, features)

x = jnp.ones((50, 10, 32)) # (time, batch, features)
lstm = nn.RNN(nn.LSTMCell(), cell_size=64, time_major=True)
variables = lstm.init(jax.random.PRNGKey(0), x)
y = lstm.apply(variables, x)
y.shape # (time, batch, cell_size)

x = jnp.ones((10, 50, 32)) # (batch, time, features)
lstm = nn.RNN(nn.LSTMCell(), cell_size=64, return_carry=True)
variables = lstm.init(jax.random.PRNGKey(0), x)
carry, y = lstm.apply(variables, x)
jax.tree_map(jnp.shape, carry) # ((batch, cell_size), (batch, cell_size))
y.shape # (batch, time, cell_size)
""" .

<DEPENDENCY.flax==0.7.4> <CONTAINS> """CODE.from flax.cursor import cursor

dict_obj = {'a': 1, 'b': (2, 3), 'c': [4, 5]}
modified_dict_obj = cursor(dict_obj)['b'][0].set(10)
assert modified_dict_obj == {'a': 1, 'b': (10, 3), 'c': [4, 5]}


from flax.cursor import cursor

dict_obj = {'a': 1, 'b': (2, 3), 'c': [4, 5]}
c = cursor(dict_obj)
c['b'][0] = 10
c['a'] = (100, 200)
modified_dict_obj = c.build()
assert modified_dict_obj == {'a': (100, 200), 'b': (10, 3), 'c': [4, 5]}


from flax.cursor import cursor
from flax.training import train_state
import optax

def update_fn(path, value):
    '''Replace params with empty dictionary.'''
    if 'params' in path:
        return {}
    return value

state = train_state.TrainState.create(
    apply_fn=lambda x: x,
    params={'a': 1, 'b': 2},
    tx=optax.adam(1e-3),
)
c = cursor(state)
state2 = c.apply_update(update_fn).build()
assert state2.params == {}
assert state.params == {'a': 1, 'b': 2} # make sure original params are unchanged
""",
        """CODE.from flax.cursor import cursor
from flax.training import train_state
import optax

dict_obj = {'a': 1, 'b': (2, 3), 'c': [4, 5]}
c = cursor(dict_obj)
c['b'][0] = 10
c['a'] = (100, 200)
modified_dict_obj = c.build()
assert modified_dict_obj == {'a': (100, 200), 'b': (10, 3), 'c': [4, 5]}

state = train_state.TrainState.create(
    apply_fn=lambda x: x,
    params=dict_obj,
    tx=optax.adam(1e-3),
)
new_fn = lambda x: x + 1
c = cursor(state)
c.params['b'][1] = 10
c.apply_fn = new_fn
modified_state = c.build()
assert modified_state.params == {'a': 1, 'b': (2, 10), 'c': [4, 5]}
assert modified_state.apply_fn == new_fn""",
        """CODE.from flax.cursor import cursor
from flax.training import train_state
import optax

dict_obj = {'a': 1, 'b': (2, 3), 'c': [4, 5]}
modified_dict_obj = cursor(dict_obj)['b'][0].set(10)
assert modified_dict_obj == {'a': 1, 'b': (10, 3), 'c': [4, 5]}

state = train_state.TrainState.create(
    apply_fn=lambda x: x,
    params=dict_obj,
    tx=optax.adam(1e-3),
)
modified_state = cursor(state).params['b'][1].set(10)
assert modified_state.params == {'a': 1, 'b': (2, 10), 'c': [4, 5]}""",
        """CODE.import flax.linen as nn
from flax.cursor import cursor
import jax
import jax.numpy as jnp

class Model(nn.Module):
  @nn.compact
  def __call__(self, x):
    x = nn.Dense(3)(x)
    x = nn.relu(x)
    x = nn.Dense(3)(x)
    x = nn.relu(x)
    x = nn.Dense(3)(x)
    x = nn.relu(x)
    return x

params = Model().init(jax.random.PRNGKey(0), jnp.empty((1, 2)))['params']

def update_fn(path, value):
  '''Multiply all dense kernel params by 2 and add 1.
  Subtract the Dense_1 bias param by 1.'''
  if 'kernel' in path:
    return value * 2 + 1
  elif 'Dense_1' in path and 'bias' in path:
    return value - 1
  return value

c = cursor(params)
new_params = c.apply_update(update_fn).build()
for layer in ('Dense_0', 'Dense_1', 'Dense_2'):
  assert (new_params[layer]['kernel'] == 2 * params[layer]['kernel'] + 1).all()
  if layer == 'Dense_1':
    assert (new_params[layer]['bias'] == jnp.array([-1, -1, -1])).all()
  else:
    assert (new_params[layer]['bias'] == params[layer]['bias']).all()

assert jax.tree_util.tree_all(
      jax.tree_util.tree_map(
          lambda x, y: (x == y).all(),
          params,
          Model().init(jax.random.PRNGKey(0), jnp.empty((1, 2)))[
              'params'
          ],
      )
  ) # make sure original params are unchanged""",
        """CODE.import flax.linen as nn
import jax.numpy as jnp

class Foo(nn.Module):
...   def __call__(self, x):
...     return x

def my_interceptor1(next_fun, args, kwargs, context):
...   print('calling my_interceptor1')
...   return next_fun(*args, **kwargs)

foo = Foo()
with nn.intercept_methods(my_interceptor1):
...   _ = foo(jnp.ones([1]))

def my_interceptor2(next_fun, args, kwargs, context):
...   print('calling my_interceptor2')
...   return next_fun(*args, **kwargs)

with nn.intercept_methods(my_interceptor1), \\
...      nn.intercept_methods(my_interceptor2):
...   _ = foo(jnp.ones([1]))

def my_interceptor3(next_fun, args, kwargs, context):
...   print('calling my_interceptor3')
...   return context.orig_method(*args, **kwargs)
with nn.intercept_methods(my_interceptor3), \\
...      nn.intercept_methods(my_interceptor1), \\
...      nn.intercept_methods(my_interceptor2):
...   _ = foo(jnp.ones([1]))
""" .

<DEPENDENCY.flax==0.7.5> <CONTAINS> """CODE.class Baz(nn.Module):
  @nn.compact
  def __call__(self, x):
    return nn.Dense(2)(x)

class Bar(nn.Module):
  @nn.compact
  def __call__(self, x):
    x = Baz()(x)
    x = nn.Dense(3)(x)
    x = Baz()(x)
    x = nn.Dense(3)(x)
    return x

class Foo(nn.Module):
  @nn.compact
  def __call__(self, x):
    x = nn.Dense(3)(x)
    # l2-normalize all params of the second Dense layer
    x = nn.WeightNorm(nn.Dense(4), variable_filter=None)(x)
    x = nn.Dense(5)(x)
    # l2-normalize all kernels in the Bar submodule and all params in the
    # Baz submodule
    x = nn.WeightNorm(Bar(), variable_filter={'kernel', 'Baz'})(x)
    return x

# init
x = jnp.ones((1, 2))
model = Foo()
variables = model.init(jax.random.key(0), x)

variables
# {
#   params: {
#     ...
#     WeightNorm_0: {
#         Dense_1/bias/scale: Array([1., 1., 1., 1.], dtype=float32),
#         Dense_1/kernel/scale: Array([1., 1., 1., 1.], dtype=float32),
#     },
#     ...
#     WeightNorm_1: {
#         Bar_0/Baz_0/Dense_0/bias/scale: Array([1., 1.], dtype=float32),
#         Bar_0/Baz_0/Dense_0/kernel/scale: Array([1., 1.], dtype=float32),
#         Bar_0/Baz_1/Dense_0/bias/scale: Array([1., 1.], dtype=float32),
#         Bar_0/Baz_1/Dense_0/kernel/scale: Array([1., 1.], dtype=float32),
#         Bar_0/Dense_0/kernel/scale: Array([1., 1., 1.], dtype=float32),
#         Bar_0/Dense_1/kernel/scale: Array([1., 1., 1.], dtype=float32),
#     },
#     ...
#   }
# }
""",
        """CODE.class Foo(nn.Module):
  @nn.compact
  def __call__(self, x, train):
    x = nn.Dense(3)(x)
    # only spectral normalize the params of the second Dense layer
    x = nn.SpectralNorm(nn.Dense(4))(x, update_stats=train)
    x = nn.Dense(5)(x)
    return x

# init
x = jnp.ones((1, 2))
y = jnp.ones((1, 5))
model = Foo()
variables = model.init(jax.random.PRNGKey(0), x, train=False)

# train
def train_step(variables, x, y):
  def loss_fn(params):
    logits, updates = model.apply(
        {'params': params, 'batch_stats': variables['batch_stats']},
        x,
        train=True,
        mutable=['batch_stats'],
    )
    loss = jnp.mean(optax.l2_loss(predictions=logits, targets=y))
    return loss, updates

  (loss, updates), grads = jax.value_and_grad(loss_fn, has_aux=True)(
      variables['params']
  )
  return {
      'params': jax.tree_map(
          lambda p, g: p - 0.1 * g, variables['params'], grads
      ),
      'batch_stats': updates['batch_stats'],
  }, loss
for _ in range(10):
  variables, loss = train_step(variables, x, y)

# inference / eval
out = model.apply(variables, x, train=False)
""",
        """CODE.import flax.linen as nn
from flax.cursor import cursor
import jax
import jax.numpy as jnp

class Model(nn.Module):
  @nn.compact
  def __call__(self, x):
    x = nn.Dense(3)(x)
    x = nn.relu(x)
    x = nn.Dense(3)(x)
    x = nn.relu(x)
    x = nn.Dense(3)(x)
    x = nn.relu(x)
    return x

params = Model().init(jax.random.PRNGKey(0), jnp.empty((1, 2)))['params']

def cond_fn(path, value):
  '''Find all dense layer params.'''
  return 'Dense' in path

c = cursor(params)
for dense_params in c.find_all(cond_fn):
  dense_params['bias'] += 1
new_params = c.build()

for layer in ('Dense_0', 'Dense_1', 'Dense_2'):
  assert (new_params[layer]['bias'] == params[layer]['bias'] + 1).all()

assert jax.tree_util.tree_all(
      jax.tree_util.tree_map(
          lambda x, y: (x == y).all(),
          params,
          Model().init(jax.random.PRNGKey(0), jnp.empty((1, 2)))[
              'params'
          ],
      )
  ) # make sure original params are unchanged""",
        """CODE.import flax.linen as nn
from flax.cursor import cursor
import jax
import jax.numpy as jnp

class Model(nn.Module):
  @nn.compact
  def __call__(self, x):
    x = nn.Dense(3)(x)
    x = nn.relu(x)
    x = nn.Dense(3)(x)
    x = nn.relu(x)
    x = nn.Dense(3)(x)
    x = nn.relu(x)
    return x

params = Model().init(jax.random.PRNGKey(0), jnp.empty((1, 2)))['params']

def cond_fn(path, value):
  '''Find the second dense layer params.'''
  return 'Dense_1' in path

new_params = cursor(params).find(cond_fn)['bias'].set(params['Dense_1']['bias'] + 1)

for layer in ('Dense_0', 'Dense_1', 'Dense_2'):
  if layer == 'Dense_1':
    assert (new_params[layer]['bias'] == params[layer]['bias'] + 1).all()
  else:
    assert (new_params[layer]['bias'] == params[layer]['bias']).all()

c = cursor(params)
c2 = c.find(cond_fn)
c2['kernel'] += 2
c2['bias'] += 2
new_params = c.build()

for layer in ('Dense_0', 'Dense_1', 'Dense_2'):
  if layer == 'Dense_1':
    assert (new_params[layer]['kernel'] == params[layer]['kernel'] + 2).all()
    assert (new_params[layer]['bias'] == params[layer]['bias'] + 2).all()
  else:
    assert (new_params[layer]['kernel'] == params[layer]['kernel']).all()
    assert (new_params[layer]['bias'] == params[layer]['bias']).all()

assert jax.tree_util.tree_all(
      jax.tree_util.tree_map(
          lambda x, y: (x == y).all(),
          params,
          Model().init(jax.random.PRNGKey(0), jnp.empty((1, 2)))[
              'params'
          ],
      )
  ) # make sure original params are unchanged""" .

<DEPENDENCY.flax==0.8.0> <CONTAINS> """CODE.    class TransformerBlock(nn.Module):
    ...   @nn.compact
    ...   def __call__(self, x):
    ...     x = nn.Dense(2)(x)
    ...     x = nn.GeGLU()(x) # initialized
    ...     return x""",
        """CODE.class LearnScale(nn.Module):
    @nn.compact
    def __call__(self, x, y):
      p = self.param('scale', nn.initializers.zeros_init(), ())
      return p * x * y

class Foo(nn.Module):
    @nn.compact
    def __call__(self, x, y):
      x_grad, y_grad = nn.grad(
          lambda mdl, x, y: mdl(x, y), LearnScale(), x, y)
      return x_grad, y_grad
""",
        """CODE.def learn_scale(scope, x, y):
    p = scope.param('scale', nn.initializers.zeros_init(), ())
    return p * x * y

def f(scope, x, y):
    z, x_grad, y_grad = lift.value_and_grad(learn_scale, scope, x, y)
    return z, x_grad, y_grad
""",
        """CODE.def learn_scale(scope, x, y):
    p = scope.param('scale', nn.initializers.zeros_init(), ())
    return p * x * y
def f(scope, x, y):
    z, x_grad, y_grad = lift.value_and_grad(learn_scale, scope, x, y)
    return z, x_grad, y_grad
""",
        """CODE.import flax.linen as nn
import jax

layer = nn.MultiHeadAttention(num_heads=8, qkv_features=16)
key1, key2, key3, key4, key5, key6 = jax.random.split(jax.random.key(0), 6)
shape = (4, 3, 2, 5)
q, k, v = jax.random.uniform(key1, shape), jax.random.uniform(key2, shape), jax.random.uniform(key3, shape)
variables = layer.init(jax.random.key(0), q)

out = layer.apply(variables, q, k, v)
out = layer.apply(variables, q, k)
out = layer.apply(variables, q)

attention_kwargs = dict(
    num_heads=8,
    qkv_features=16,
    kernel_init=nn.initializers.ones,
    bias_init=nn.initializers.zeros,
    dropout_rate=0.5,
    deterministic=False,
    )
class Module(nn.Module):
  attention_kwargs: dict

  @nn.compact
  def __call__(self, x, dropout_rng=None):
    out1 = nn.MultiHeadAttention(**self.attention_kwargs)(x, dropout_rng=dropout_rng)
    out2 = nn.MultiHeadAttention(**self.attention_kwargs)(x, dropout_rng=dropout_rng)
    return out1, out2
module = Module(attention_kwargs)
variables = module.init({'params': key1, 'dropout': key2}, q)

out1, out2 = module.apply(variables, q, rngs={'dropout': key3})
out3, out4 = module.apply(variables, q, rngs={'dropout': key4})
out1, out2 = module.apply(variables, q, dropout_rng=key5)
out3, out4 = module.apply(variables, q, rngs={'dropout': key6}, dropout_rng=key5)""",
        """CODE.import flax.linen as nn
import jax

layer = nn.MultiHeadAttention(num_heads=8, qkv_features=16)
key1, key2, key3, key4, key5, key6 = jax.random.split(jax.random.key(0), 6)
shape = (4, 3, 2, 5)
q, k, v = jax.random.uniform(key1, shape), jax.random.uniform(key2, shape), jax.random.uniform(key3, shape)
variables = layer.init(jax.random.key(0), q)

out = layer.apply(variables, q, k, v)
out = layer.apply(variables, q, k)
out = layer.apply(variables, q)

attention_kwargs = dict(
    num_heads=8,
    qkv_features=16,
    kernel_init=nn.initializers.ones,
    bias_init=nn.initializers.zeros,
    dropout_rate=0.5,
    deterministic=False,
    )
class Module(nn.Module):
  attention_kwargs: dict

  @nn.compact
  def __call__(self, x, dropout_rng=None):
    out1 = nn.MultiHeadAttention(**self.attention_kwargs)(x, dropout_rng=dropout_rng)
    out2 = nn.MultiHeadAttention(**self.attention_kwargs)(x, dropout_rng=dropout_rng)
    return out1, out2
module = Module(attention_kwargs)
variables = module.init({'params': key1, 'dropout': key2}, q)

out1, out2 = module.apply(variables, q, rngs={'dropout': key3})
out3, out4 = module.apply(variables, q, rngs={'dropout': key4})
out1, out2 = module.apply(variables, q, dropout_rng=key5)
out3, out4 = module.apply(variables, q, rngs={'dropout': key6}, dropout_rng=key5)
""",
        """CODE.import flax.linen as nn
import jax, jax.numpy as jnp

layer = nn.LinearGeneral(features=4)
layer = nn.LinearGeneral(features=(4, 5))
params = layer.init(jax.random.key(0), jnp.ones((1, 3)))
jax.tree_map(jnp.shape, params)
layer = nn.LinearGeneral(features=(4, 5), axis=(1, -1))
params = layer.init(jax.random.key(0), jnp.ones((1, 3, 6, 7))
jax.tree_map(jnp.shape, params)""",
        """CODE.import jax, jax.numpy as jnp

a = jax.random.normal(jax.random.key(0), [2, 3, 4])
b = jax.random.normal(jax.random.key(1), [4])

def raises(a, b):
  if len(a.shape) != 2:
    raise ValueError("a must be shape 2")
  if len(b.shape) != 1:
    raise ValueError("b must be shape 1")
  return jnp.dot(a, b)

out = BatchApply(raises)(a, b)
expected_merged_leading = raises(a.reshape(2*3, 4), b)
expected = expected_merged_leading.reshape((2, 3) + expected_merged_leading.shape[1:])
np.testing.assert_array_equal(out, expected)
""" .

<DEPENDENCY.flax==0.8.1> <CONTAINS> """CODE.@nn.compact_name_scope
def up(self, x):
    return nn.Dense(3)(x)

@nn.compact_name_scope
def down(self, x):
    return nn.Dense(3)(x)""",
        """CODE.class Foo(nn.Module):
  @nn.compact
  def __call__(self, x):
    h = nn.Dense(4)(x)
    return nn.Dense(2)(h)


x = jnp.ones((16, 9))
modules = Foo().module_paths(jax.random.key(0), x)
print({
    p: type(m).__name__ for p, m in modules.items()
})
""",
        """CODE.import flax.linen as nn
import jax
import numpy as np

x = jax.random.normal(jax.random.key(0), (2, 3, 4, 5))
layer = nn.InstanceNorm()
variables = layer.init(jax.random.key(1), x)
y = layer.apply(variables, x)

y2 = nn.LayerNorm(reduction_axes=[1, 2], feature_axes=-1).apply(variables, x)
np.testing.assert_allclose(y, y2, atol=1e-7)
y3 = nn.GroupNorm(num_groups=x.shape[-1]).apply(variables, x)
np.testing.assert_allclose(y, y3, atol=1e-7)""" .

<DEPENDENCY.git-python==0.3.3> <CONTAINS> "CODE.remote.refs.master # yields RemoteReference('/refs/remotes/origin/master')" .

<DEPENDENCY.gym==0.12.6> <CONTAINS> """CODE.from gym.spaces import Box
import numpy as np

space = Box(low=0, high=1, shape=(3,), dtype=np.float32)
out = np.zeros((2, 3), dtype=np.float32)
items = [space.sample() for _ in range(2)]
concatenate(items, out, space)
""",
        """CODE.from gym.spaces import Box, Dict
space = Dict({
'position': Box(low=0, high=1, shape=(3,), dtype=np.float32),
'velocity': Box(low=0, high=1, shape=(2,), dtype=np.float32)})
create_empty_array(space, n=2, fn=np.zeros)""",
        """CODE.import gym
env = gym.vector.make('CartPole-v1', 3)
env.reset()
array([[-0.04456399,  0.04653909,  0.01326909, -0.02099827],
       [ 0.03073904,  0.00145001, -0.03088818, -0.03131252],
       [ 0.03468829,  0.01500225,  0.01230312,  0.01825218]],
      dtype=float32)""" .

<DEPENDENCY.gym==0.15.0> <CONTAINS> """CODE.import gym
env = gym.make('CartPole-v1')
env = TransformReward(env, lambda r: 0.01*r)
env.reset()
observation, reward, done, info = env.step(env.action_space.sample())
reward
0.01""" .

<DEPENDENCY.gym==0.15.4> <CONTAINS> "CODE.RescaleAction(env, a, b).action_space == Box(a,b)",
        """CODE.import gym
env = gym.make('CartPole-v1')
env = TransformObservation(env, lambda obs: obs + 0.1*np.random.randn(*obs.shape))
env.reset()
array([-0.08319338,  0.04635121, -0.07394746,  0.20877492])""" .

<DEPENDENCY.gym==0.15.6> <CONTAINS> """CODE.self.observation_space = spaces.MultiBinary(5)
self.observation_space.sample()""" .

<DEPENDENCY.gym==0.17.2> <CONTAINS> """CODE.def flatten_space(space):
    if isinstance(space, Box):
        return Box(np.prod(space.shape),)
    elif isinstance(space, Discrete):
        return Box(space.n,)
    elif isinstance(space, Dict):
        return Box(sum([flatten_space(subspace).shape[0] for subspace in space.spaces]),)
    else:
        raise NotImplementedError
""" .

<DEPENDENCY.gym==0.22.0> <CONTAINS> """CODE.closer = Closer()
class Example(object):
    def __init__(self):
        self._id = closer.register(self)

    def close(self):
        # Probably worth making idempotent too!
        ...
        closer.unregister(self._id)

    def __del__(self):
        self.close()
""",
        """CODE.from gym.spaces import Box, Dict
space = Dict({
... 'position': Box(low=0, high=1, shape=(2, 3), dtype=np.float32),
... 'velocity': Box(low=0, high=1, shape=(2, 2), dtype=np.float32)})
items = space.sample()
it = iterate(space, items)
next(it)
{'position': array([-0.99644893, -0.08304597, -0.7238421 ], dtype=float32),
'velocity': array([0.35848552, 0.1533453 ], dtype=float32)}
next(it)
{'position': array([-0.67958736, -0.49076623,  0.38661423], dtype=float32),
'velocity': array([0.7975036 , 0.93317133], dtype=float32)}
next(it)""",
        """CODE.import gym
env = gym.make("BipedalWalker-v3", hardcore=True)
""",
        """CODE.specs = EnvSpecTree()

specs["My/Env-v0"] = EnvSpec(...)
assert specs["My/Env-v0"] == EnvSpec(...)

assert specs.tree["My"]["Env"]["0"] == specs["My/Env-v0"]
""" .

<DEPENDENCY.gym==0.23.1> <CONTAINS> """CODE.specs = EnvSpecTree()

specs["My/Env-v0"] = EnvSpec(...)
assert specs["My/Env-v0"] == EnvSpec(...)

assert specs.tree["My"]["Env"]["0"] == specs["My/Env-v0"]
""" .

<DEPENDENCY.gym==0.24.0> <CONTAINS> """CODE.    class ClipReward(gym.RewardWrapper):
        def __init__(self, env, min_reward, max_reward):
            super().__init__(env)
            self.min_reward = min_reward
            self.max_reward = max_reward
            self.reward_range = (min_reward, max_reward)

        def reward(self, reward):
            return np.clip(reward, self.min_reward, self.max_reward)""",
        """CODE.class DiscreteActions(gym.ActionWrapper):
    def __init__(self, env, disc_to_cont):
        super().__init__(env)
        self.disc_to_cont = disc_to_cont
        self.action_space = Discrete(len(disc_to_cont))

    def action(self, act):
        return self.disc_to_cont[act]

if __name__ == "__main__":
    env = gym.make("LunarLanderContinuous-v2")
    wrapped_env = DiscreteActions(env, [np.array([1,0]), np.array([-1,0]),
                                        np.array([0,1]), np.array([0,-1])])
    print(wrapped_env.action_space)         #Discrete(4)
""",
        """CODE.class RelativePosition(gym.ObservationWrapper):
    def __init__(self, env):
        super().__init__(env)
        self.observation_space = Box(shape=(2,), low=-np.inf, high=np.inf)

    def observation(self, obs):
        return obs["target"] - obs["agent"]
""",
        """CODE.from gym.envs.classic_control import CartPoleEnv
env = CartPoleEnv()
env = OrderEnforcing(env)
env.reset()
env.render()
env.step(0)
""",
        """CODE.from gym.envs.classic_control import CartPoleEnv
from gym.wrappers import TimeLimit
env = CartPoleEnv()
env = TimeLimit(env, max_episode_steps=1000)""" .

<DEPENDENCY.gym==0.25.0> <CONTAINS> "CODE.StepAPICompatibility(CustomEnv(), new_step_api=True) # manually using wrapper on unregistered envs",
        """CODE.import string
Text(5)
Text(min_length = 1,
     max_length = 10,
     charset = string.digits)""" .

<DEPENDENCY.gym==0.26.0> <CONTAINS> """CODE.save_video(
    env.render(),
    "videos",
    fps=env.metadata["render_fps"],
    step_starting_index=step_starting_index,
    episode_index=episode_index
)
step_starting_index = step_index + 1
episode_index += 1
env.reset()""" .

<DEPENDENCY.gym==0.7.4> <CONTAINS> """CODE.box_to_multi_discrete = BoxToMultiDiscrete(multi_discrete)
box_action = box_to_multi_discrete.sample()
multi_discrete_action = box_to_multi_discrete(box_action)
""",
        """CODE.def callback(obs_t, obs_tp1, rew, done, info):
    return [rew,]

env_plotter = EnvPlotter(callback, 30 * 5, ["reward"])

env = gym.make("Pong-v3")
play(env, callback=env_plotter.callback)""",
        """CODE.discrete_to_multi_discrete = DiscreteToMultiDiscrete(multi_discrete)
discrete_action = discrete_to_multi_discrete.sample()
multi_discrete_action = discrete_to_multi_discrete(discrete_action)""" .

<DEPENDENCY.h5py==2.2.0> <CONTAINS> """CODE.with dataset.astype('f8'):
    double_precision = dataset[0:100:2]""" .

<DEPENDENCY.h5py==2.5.0> <CONTAINS> """CODE.sel = HyperSelection((10,20))  # Initially 200 points
sel[:,5:15] = False            # Now 100 points
sel[:,10]   = True             # Now 110 points
sel[...]    = XOR              # Now 90 points
""" .

<DEPENDENCY.h5py==3.0.0> <CONTAINS> "CODE.2d_coords = dataset.fields(['x', 'y'])[:]",
        "CODE.str_array = dataset.asstr()[:]" .

<DEPENDENCY.huggingface-hub==0.0.13> <CONTAINS> """CODE.with Repository("text-files", clone_from="<user>/text-files", use_auth_token=True).commit("My first file :)"):
    with open("file.txt", "w+") as f:
        f.write(json.dumps({"hey": 8}))
import torch
model = torch.nn.Transformer()
with Repository("torch-model", clone_from="<user>/torch-model", use_auth_token=True).commit("My cool model :)"):
    torch.save(model.state_dict(), "model.pt")""" .

<DEPENDENCY.huggingface-hub==0.0.14> <CONTAINS> """CODE.    class MyModel(nn.Module, ModelHubMixin):
       def __init__(self, **kwargs):
           super().__init__()
           self.config = kwargs.pop("config", None)
           self.layer = ...
       def forward(self, ...)
           return ...

    model = MyModel()
    model.save_pretrained("mymodel", push_to_hub=False) # Saving model weights in the directory
    model.push_to_hub("mymodel", "model-1") # Pushing model-weights to hf-hub

    # Downloading weights from hf-hub & model will be initialized from those weights
    model = MyModel.from_pretrained("username/mymodel@main")""",
        """CODE.from huggingface_hub.inference_api import InferenceApi
api = InferenceApi("bert-base-uncased")
api(inputs="The goal of life is [MASK].")
api = InferenceApi("deepset/roberta-base-squad2")
inputs = {"question":"What's my name?", "context":"My name is Clara and I live in Berkeley."}
api(inputs)
api = InferenceApi("typeform/distilbert-base-uncased-mnli")
inputs = "Hi, I recently bought a device from your company but it is not working as advertised and I would like to get reimbursed!"
params = {"candidate_labels":["refund", "legal", "faq"]}
api(inputs, params)
api = InferenceApi("bert-base-uncased", task="feature-extraction")""" .

<DEPENDENCY.huggingface-hub==0.0.15> <CONTAINS> """CODE.    class MyModel(nn.Module, PyTorchModelHubMixin):
       def __init__(self, **kwargs):
           super().__init__()
           self.config = kwargs.pop("config", None)
           self.layer = ...
       def forward(self, ...)
           return ...

    model = MyModel()
    model.save_pretrained("mymodel", push_to_hub=False) # Saving model weights in the directory
    model.push_to_hub("mymodel", "model-1") # Pushing model-weights to hf-hub

    # Downloading weights from hf-hub & model will be initialized from those weights
    model = MyModel.from_pretrained("username/mymodel@main")""" .

<DEPENDENCY.huggingface-hub==0.0.16> <CONTAINS> """CODE.class MyModel(tf.keras.Model, KerasModelHubMixin):
...    def __init__(self, **kwargs):
...        super().__init__()
...        self.config = kwargs.pop("config", None)
...        self.dummy_inputs = ...
...        self.layer = ...
...    def call(self, ...)
...        return ...""" .

<DEPENDENCY.huggingface-hub==0.0.7> <CONTAINS> """CODE.    class MyModel(nn.Module, ModelHubMixin):
       def __init__(self, **kwargs):
           super().__init__()
           self.config = kwargs.pop("config", None)
           self.layer = ...
       def forward(self, ...)
           return ...

    model = MyModel()
    model.save_pretrained("mymodel", push_to_hub=False) # Saving model weights in the directory
    model.push_to_hub("mymodel", "model-1") # Pushing model-weights to hf-hub

    # Downloading weights from hf-hub & model will be initialized from those weights
    model = MyModel.from_pretrained("username/mymodel@main")""" .

<DEPENDENCY.huggingface-hub==0.0.9> <CONTAINS> """CODE.with open("./local/filepath", "rb") as fobj:
...     upload_file(
...         path_or_fileobj=fileobj,
...         path_in_repo="remote/file/path.h5",
...         repo_id="username/my-dataset",
...         repo_type="datasets",
...         token="my_token",
...    )
"https://huggingface.co/datasets/username/my-dataset/blob/main/remote/file/path.h5"

upload_file(
...     path_or_fileobj=".\\\\local\\\\file\\\\path",
...     path_in_repo="remote/file/path.h5",
...     repo_id="username/my-model",
...     token="my_token",
... )
"https://huggingface.co/username/my-model/blob/main/remote/file/path.h5"
""" .

<DEPENDENCY.huggingface-hub==0.10.0> <CONTAINS> """CODE.        import requests
        from huggingface_hub.utils import hf_raise_for_status, HfHubHTTPError

        response = requests.post(...)
        try:
            hf_raise_for_status(response)
        except HfHubHTTPError as e:
            print(str(e)) # formatted message
            e.request_id, e.server_message # details returned by server

            # Complete the error message with additional information once it's raised
            e.append_to_message("
`create_commit` expects the repository to exist.")
            raise
""",
        """CODE.        import requests
        from huggingface_hub.utils import hf_raise_for_status, HfHubHTTPError

        response = requests.post(...)
        try:
            hf_raise_for_status(response)
        except HfHubHTTPError as e:
            print(str(e)) # formatted message
            e.request_id, e.server_message # details returned by server

            # Complete the error message with additional information once it's raised
            e.append_to_message("
`create_commit` expects the repository to exist.")
            raise
    """,
        """CODE.@patch("something.foo")
@patch("something_else.foo.bar") # order doesn't matter
@handle_injection # after @patch calls
def TestHelloWorld(unittest.TestCase):

    def test_hello_foo(self, mock_foo: Mock) -> None:
        (...)

    def test_hello_bar(self, mock_bar: Mock) -> None
        (...)

    def test_hello_both(self, mock_foo: Mock, mock_bar: Mock) -> None:
        (...)
""",
        """CODE.@pytest.mark.usefixtures("fx_cache_dir")
class TestWithCache(unittest.TestCase):
    cache_dir: Path

    def test_cache_dir(self) -> None:
        self.assertTrue(self.cache_dir.is_dir())
""",
        """CODE._get_expectations_str(hf_cache_info, selected_hashes)
'7 revisions selected counting for 4.3G.'""",
        """CODE._read_manual_review_tmp_file(tmp_path)
['123456789', 'an_older_hash']""",
        """CODE.class TestHelloWorld(unittest.TestCase):
    def test_hello_world(self):
        with capture_output() as output:
            print("hello world")
        self.assertEqual(output.getvalue(), "hello world
")
""",
        """CODE.def TestHelloWorld(unittest.TestCase):

    @patch("something.foo")
    @patch("something_else.foo.bar") # order doesn't matter
    @handle_injection_in_test # after @patch calls
    def test_hello_foo(self, mock_foo: Mock) -> None:
        (...)
""",
        """CODE.def build_hf_headers(use_auth_token=None, is_write_action=False, library_name=None, library_version=None, user_agent=None):
    headers = {}

    if use_auth_token is not None:
        if isinstance(use_auth_token, str):
            headers["authorization"] = f"Bearer {use_auth_token}"
        elif use_auth_token is True:
            # token is read from the machine (cache or env variable)
            pass
        elif use_auth_token is False:
            # authorization header is not set
            pass
        else:
            # token is read from the machine only except if HF_HUB_DISABLE_IMPLICIT_TOKEN env variable is set
            pass

    if is_write_action:
        if use_auth_token is None or use_auth_token.startswith("api_org"):
            raise ValueError("You must use your personal account token for write-access methods.")

    if library_name is not None and library_version is not None:
        headers["user-agent"] = f"{library_name}/{library_version}; hf_hub/0.10.2; python/3.10.4; tensorflow/1.55"

    return headers

""",
        """CODE.from huggingface_hub import DatasetCard, DatasetCardData

# Using the Default Template
card_data = DatasetCardData(
    language='en',
    license='mit',
    annotations_creators='crowdsourced',
    task_categories=['text-classification'],
    task_ids=['sentiment-classification', 'text-scoring'],
    multilinguality='monolingual',
    pretty_name='My Text Classification Dataset',
)
card = DatasetCard.from_template(
    card_data,
    pretty_name=card_data.pretty_name,
)

# Using a Custom Template
card_data = DatasetCardData(
    language='en',
    license='mit',
)
card = DatasetCard.from_template(
    card_data=card_data,
    template_path='./src/huggingface_hub/templates/datasetcard_template.md',
    custom_template_var='custom value',  # will be replaced in template if it exists
)
""",
        """CODE.from huggingface_hub import ModelCard, ModelCardData, EvalResult

# Using the Default Template
card_data = ModelCardData(
...     language='en',
...     license='mit',
...     library_name='timm',
...     tags=['image-classification', 'resnet'],
...     datasets='beans',
...     metrics=['accuracy'],
... )
card = ModelCard.from_template(
...     card_data,
...     model_description='This model does x + y...'
... )

# Including Evaluation Results
card_data = ModelCardData(
...     language='en',
...     tags=['image-classification', 'resnet'],
...     eval_results=[
...         EvalResult(
...             task_type='image-classification',
...             dataset_type='beans',
...             dataset_name='Beans',
...             metric_type='accuracy',
...             metric_value=0.9,
...         ),
...     ],
...     model_name='my-cool-model',
... )
card = ModelCard.from_template(card_data)

# Using a Custom Template
card_data = ModelCardData(
...     language='en',
...     tags=['image-classification', 'resnet']
... )
card = ModelCard.from_template(
...     card_data=card_data,
...     template_path='./src/huggingface_hub/templates/modelcard_template.md',
...     custom_template_var='custom value',  # will be replaced in template if it exists
... )
""",
        """CODE.from huggingface_hub import ModelCardData
card_data = ModelCardData(
    language="en",
    license="mit",
    library_name="timm",
    tags=['image-classification', 'resnet'],
)
card_data.to_dict()
{'language': 'en', 'license': 'mit', 'library_name': 'timm', 'tags': ['image-classification', 'resnet']}
""",
        """CODE.from huggingface_hub import scan_cache_dir

hf_cache_info = scan_cache_dir()
""",
        """CODE.from huggingface_hub import scan_cache_dir
cache_info = scan_cache_dir()
delete_strategy = cache_info.delete_revisions(
    "81fd1d6e7847c99f5862c9fb81387956d99ec7aa"
)
print(f"Will free {delete_strategy.expected_freed_size_str}.")
delete_strategy.execute()


from huggingface_hub import scan_cache_dir
scan_cache_dir().delete_revisions(
    "81fd1d6e7847c99f5862c9fb81387956d99ec7aa",
    "e2983b237dccf3ab4937c97fa717319a9ca1a96d",
    "6c0e6080953db56375760c0471a8c5f2929baf11",
).execute()
""",
        """CODE.from huggingface_hub.repocard import RepoCard
card = RepoCard("---\\nlanguage: en\\n---\\n# This is a test repo card")
card.save("/tmp/test.md")
""",
        """CODE.from huggingface_hub.repocard import RepoCard
card = RepoCard.load("nateraw/food")
assert card.data.tags == ["generated_from_trainer", "image-classification", "pytorch"]
""",
        """CODE.from huggingface_hub.repocard_data import eval_results_to_model_index, EvalResult
# Define minimal eval_results
eval_results = [
...     EvalResult(
...         task_type="image-classification",  # Required
...         dataset_type="beans",  # Required
...         dataset_name="Beans",  # Required
...         metric_type="accuracy",  # Required
...         metric_value=0.9,  # Required
...     )
... ]
eval_results_to_model_index("my-cool-model", eval_results)
[{'name': 'my-cool-model', 'results': [{'task': {'type': 'image-classification'}, 'dataset': {'name': 'Beans', 'type': 'beans'}, 'metrics': [{'type': 'accuracy', 'value': 0.9}]}]}
""",
        """CODE.from huggingface_hub.repocard_data import model_index_to_eval_results
# Define a minimal model index
model_index = [
...     {
...         "name": "my-cool-model",
...         "results": [
...             {
...                 "task": {
...                     "type": "image-classification"
...                 },
...                 "dataset": {
...                     "type": "beans",
...                     "name": "Beans"
...                 },
...                 "metrics": [
...                     {
...                         "type": "accuracy",
...                         "value": 0.9
...                     }
...                 ]
...             }
...         ]
...     }
... ]
model_name, eval_results = model_index_to_eval_results(model_index)
model_name
'my-cool-model'
eval_results[0].task_type
'image-classification'
eval_results[0].metric_type
'accuracy'
""",
        """CODE.from huggingface_hub.utils import validate_hf_hub_args

@validate_hf_hub_args
... def my_cool_method(repo_id: str):
...     print(repo_id)

my_cool_method(repo_id="valid_repo_id")
valid_repo_id

my_cool_method("other..repo..id")
huggingface_hub.utils._validators.HFValidationError: Cannot have -- or .. in repo_id: 'other..repo..id'.

my_cool_method(repo_id="other..repo..id")
huggingface_hub.utils._validators.HFValidationError: Cannot have -- or .. in repo_id: 'other..repo..id'.
""",
        """CODE.from huggingface_hub.utils import validate_repo_id
validate_repo_id(repo_id="valid_repo_id")
validate_repo_id(repo_id="other..repo..id")
""" .

<DEPENDENCY.huggingface-hub==0.10.1> <CONTAINS> """CODE.from huggingface_hub import KerasModelHubMixin

class MyModel(tf.keras.Model, KerasModelHubMixin):
    def __init__(self, **kwargs):
        super().__init__()
        self.config = kwargs.pop("config", None)
        self.dummy_inputs = ...
        self.layer = ...

    def call(self, *args):
        return ...

# Init and compile the model as you normally would
model = MyModel()
model.compile(...)
# Build the graph by training it or passing dummy inputs
_ = model(model.dummy_inputs)
# You can save your model like this
model.save_pretrained("local_model_dir/", push_to_hub=False)
# Or, you can push to a new public model repo like this
model.push_to_hub(
    "super-cool-model",
    git_user="your-hf-username",
    git_email="you@somesite.com",
)

# Downloading weights from hf-hub & model will be initialized from those weights
model = MyModel.from_pretrained("username/mymodel@main")
""",
        """CODE.from huggingface_hub import PyTorchModelHubMixin

class MyModel(nn.Module, PyTorchModelHubMixin):
    def __init__(self, **kwargs):
        super().__init__()
        self.config = kwargs.pop("config", None)
        self.layer = ...

    def forward(self, *args):
        return ...

model = MyModel()
model.save_pretrained(
    "mymodel", push_to_hub=False
)  # Saving model weights in the directory
model.push_to_hub(
    repo_id="mymodel", commit_message="model-1"
)  # Pushing model-weights to hf-hub

# Downloading weights from hf-hub & model will be initialized from those weights
model = MyModel.from_pretrained("username/mymodel@main")
""" .

<DEPENDENCY.huggingface-hub==0.11.0> <CONTAINS> """CODE.from .testing_utils import use_tmp_repo

class HfApiCommonTest(unittest.TestCase):
    _api = HfApi(endpoint=ENDPOINT_STAGING, token=TOKEN)
    _user = USER
    _repo_id: str

    @use_tmp_repo()
    def test_create_tag_on_model(self) -> None:
        self._repo_id  # populated
        (...)

    @use_tmp_repo("dataset")
    def test_create_tag_on_dataset(self) -> None:
        self._repo_id  # populated
        (...)
""",
        """CODE.from huggingface_hub import cached_assets_path

cached_assets_path(library_name="datasets", namespace="SQuAD", subfolder="download")
PosixPath('/home/wauplin/.cache/huggingface/extra/datasets/SQuAD/download')

cached_assets_path(library_name="datasets", namespace="SQuAD", subfolder="extracted")
PosixPath('/home/wauplin/.cache/huggingface/extra/datasets/SQuAD/extracted')

cached_assets_path(library_name="datasets", namespace="Helsinki-NLP/tatoeba_mt")
PosixPath('/home/wauplin/.cache/huggingface/extra/datasets/Helsinki-NLP--tatoeba_mt/default')

cached_assets_path(library_name="datasets", assets_dir="/tmp/tmp123456")
PosixPath('/tmp/tmp123456/datasets/default/default')
""",
        """CODE.from huggingface_hub.utils import chunk_iterable

for items in chunk_iterable(range(17), chunk_size=8):
    print(items)
""" .

<DEPENDENCY.huggingface-hub==0.12.0> <CONTAINS> """CODE.@pytest.mark.usefixtures("fx_production_space")
class TestSpaceAPI(unittest.TestCase):
    repo_id: str
    api: HfApi

    def test_space(self) -> None:
        api.repo_info(repo_id, repo_type="space")""",
        """CODE.assert SpaceHardware.CPU_BASIC == "cpu-basic"
""",
        """CODE.assert SpaceStage.BUILDING == "BUILDING"
""",
        """CODE.def test_push_to_hub():
    # Pull from production Hub
    with production_endpoint():
        model = ...from_pretrained("modelname")

    # Push to staging Hub
    model.push_to_hub()
""",
        """CODE.from huggingface_hub import HfApi
api = HfApi()
api.list_repo_refs("gpt2")
GitRefs(branches=[GitRefInfo(name='main', ref='refs/heads/main', target_commit='e7da7f221d5bf496a48136c0cd264e630fe9fcc8')], converts=[], tags=[])

api.list_repo_refs("bigcode/the-stack", repo_type='dataset')
GitRefs(
    branches=[
        GitRefInfo(name='main', ref='refs/heads/main', target_commit='18edc1591d9ce72aa82f56c4431b3c969b210ae3'),
        GitRefInfo(name='v1.1.a1', ref='refs/heads/v1.1.a1', target_commit='f9826b862d1567f3822d3d25649b0d6d22ace714')
    ],
    converts=[],
    tags=[
        GitRefInfo(name='v1.0', ref='refs/tags/v1.0', target_commit='c37a8cd1e382064d8aced5e05543c5f7753834da')
    ]
)
""",
        """CODE.from huggingface_hub import like, list_liked_repos, unlike
like("gpt2")
"gpt2" in list_liked_repos().models
True
unlike("gpt2")
"gpt2" in list_liked_repos().models
False
""",
        """CODE.from huggingface_hub import list_liked_repos

likes = list_liked_repos("julien-c")

likes.user
"julien-c"

likes.models
["osanseviero/streamlit_1.15", "Xhaheen/ChatGPT_HF", ...]
""",
        """CODE.with tqdm_stream_file("config.json") as f:
    requests.put(url, data=f)
config.json: 100%|âââââââââââââââââââââââââ| 8.19k/8.19k [00:02<00:00, 3.72kB/s]
""" .

<DEPENDENCY.huggingface-hub==0.13.0> <CONTAINS> """CODE.from huggingface_hub import HfApi
api = HfApi()

# Commits are sorted by date (last commit first)
initial_commit = api.list_repo_commits("gpt2")[-1]

# Initial commit is always a system commit containing the `.gitattributes` file.
initial_commit
GitCommitInfo(
    commit_id='9b865efde13a30c13e0a33e536cf3e4a5a9d71d8',
    authors=['system'],
    created_at=datetime.datetime(2019, 2, 18, 10, 36, 15, tzinfo=datetime.timezone.utc),
    title='initial commit',
    message='',
    formatted_title=None,
    formatted_message=None
)

# Create an empty branch by deriving from initial commit
api.create_branch("gpt2", "new_empty_branch", revision=initial_commit.commit_id)
""",
        """CODE.from huggingface_hub import SpaceCardData
card_data = SpaceCardData(
...     title="Dreambooth Training",
...     license="mit",
...     sdk="gradio",
...     duplicated_from="multimodalart/dreambooth-training"
... )
card_data.to_dict()
{'title': 'Dreambooth Training', 'sdk': 'gradio', 'license': 'mit', 'duplicated_from': 'multimodalart/dreambooth-training'}
""",
        """CODE.from huggingface_hub import duplicate_space

# Duplicate a Space to your account
duplicate_space("multimodalart/dreambooth-training")
RepoUrl('https://huggingface.co/spaces/nateraw/dreambooth-training',...)

# Can set custom destination id and visibility flag.
duplicate_space("multimodalart/dreambooth-training", to_id="my-dreambooth", private=True)
RepoUrl('https://huggingface.co/spaces/nateraw/my-dreambooth',...)
""",
        """CODE.from huggingface_hub.utils import send_telemetry

# Send telemetry without library information
send_telemetry("ping")

# Send telemetry to subtopic with library information
send_telemetry("gradio/local_link", library_name="gradio", library_version="3.22.1")

# Send telemetry with additional data
send_telemetry(
...     topic="examples",
...     library_name="transformers",
...     library_version="4.26.0",
...     user_agent={"pipeline": "text_classification", "framework": "flax"},
... )
""" .

<DEPENDENCY.huggingface-hub==0.14.0> <CONTAINS> """CODE.@experimental
def my_function():
    print("Hello world!")

my_function()
""",
        """CODE.def backend_factory() -> requests.Session:
    session = requests.Session()
    session.proxies = {"http": "http://10.10.1.10:3128", "https": "https://10.10.1.11:1080"}
    return session

configure_http_backend(backend_factory=backend_factory)

session = get_session()
""",
        """CODE.from huggingface_hub import HfApi, plan_multi_commits
addition_commits, deletion_commits = plan_multi_commits(
...     operations=[
...          CommitOperationAdd(...),
...          CommitOperationAdd(...),
...          CommitOperationDelete(...),
...          CommitOperationDelete(...),
...          CommitOperationAdd(...),
...     ],
... )
HfApi().create_commits_on_pr(
...     repo_id="my-cool-model",
...     addition_commits=addition_commits,
...     deletion_commits=deletion_commits,
...     (...)
...     verbose=True,
... )
""",
        """CODE.from huggingface_hub import WebhooksServer, WebhookPayload

app = WebhooksServer()

@app.add_webhook
async def trigger_training(payload: WebhookPayload):
    if payload.repo.type == "dataset" and payload.event.action == "update":
        # Trigger a training job if a dataset is updated
        ...

app.run()
""",
        """CODE.from huggingface_hub import list_files_info
files_info = list_files_info("lysandre/arxiv-nlp", ["README.md", "config.json"])
files_info
<generator object HfApi.list_files_info at 0x7f93b848e730>
list(files_info)
[
    RepoFile: {"blob_id": "43bd404b159de6fba7c2f4d3264347668d43af25", "lfs": None, "rfilename": "README.md", "size": 391},
    RepoFile: {"blob_id": "2f9618c3a19b9a61add74f70bfb121335aeef666", "lfs": None, "rfilename": "config.json", "size": 554},
]

from huggingface_hub import list_files_info
[info.rfilename for info in list_files_info("stabilityai/stable-diffusion-2", "vae") if info.lfs is not None]
['vae/diffusion_pytorch_model.bin', 'vae/diffusion_pytorch_model.safetensors']

from huggingface_hub import list_files_info
[info.rfilename for info in list_files_info("glue", repo_type="dataset")]
['.gitattributes', 'README.md', 'dataset_infos.json', 'glue.py']
""",
        """CODE.from huggingface_hub import webhook_endpoint, WebhookPayload

@webhook_endpoint
async def trigger_training(payload: WebhookPayload):
    if payload.repo.type == "dataset" and payload.event.action == "update":
        # Trigger a training job if a dataset is updated
        ...

# Server is automatically started at the end of the script.


from huggingface_hub import webhook_endpoint, WebhookPayload

@webhook_endpoint
async def trigger_training(payload: WebhookPayload):
    if payload.repo.type == "dataset" and payload.event.action == "update":
        # Trigger a training job if a dataset is updated
        ...

# Start the server manually
trigger_training.run()
""",
        """CODE.import gradio as gr
from huggingface_hub import WebhooksServer, WebhookPayload

with gr.Blocks() as ui:
    ...

app = WebhooksServer(ui=ui, webhook_secret="my_secret_key")

@app.add_webhook("/say_hello")
async def hello(payload: WebhookPayload):
    return {"message": "hello"}

app.run()
""",
        """CODE.import hffs

fs = hffs.HfFileSystem()

# List files
fs.glob("my-username/my-model/*.bin")
fs.ls("datasets/my-username/my-dataset", detail=False)

# Read/write files
with fs.open("my-username/my-model/pytorch_model.bin") as f:
    data = f.read()
with fs.open("my-username/my-model/pytorch_model.bin", "wb") as f:
    f.write(data)
""",
        """CODE.import requests
from huggingface_hub import configure_http_backend, get_session

# Create a factory function that returns a Session with configured proxies
def backend_factory() -> requests.Session:
    session = requests.Session()
    session.proxies = {"http": "http://10.10.1.10:3128", "https": "https://10.10.1.11:1080"}
    return session

# Set it as the default session factory
configure_http_backend(backend_factory=backend_factory)

# In practice, this is mostly done internally in `huggingface_hub`
session = get_session()
""" .

<DEPENDENCY.huggingface-hub==0.15.0> <CONTAINS> """CODE.from huggingface_hub import HfApi
api = HfApi()
future = api.run_as_future(api.whoami) # instant
future.done()
False
future.result() # wait until complete and return result
(...)
future.done()
True
""",
        """CODE.from huggingface_hub import InferenceClient
client = InferenceClient()

image = client.text_to_image("An astronaut riding a horse on the moon.")
image.save("astronaut.png")

image = client.text_to_image(
...     "An astronaut riding a horse on the moon.",
...     negative_prompt="low resolution, blurry",
...     model="stabilityai/stable-diffusion-2-1",
... )
image.save("better_astronaut.png")
""",
        """CODE.from huggingface_hub import InferenceClient
client = InferenceClient()
client.audio_classification("audio.wav")
[{'score': 0.4976358711719513, 'label': 'hap'}, {'score': 0.3677836060523987, 'label': 'neu'},...]
""",
        """CODE.from huggingface_hub import InferenceClient
client = InferenceClient()
client.automatic_speech_recognition("hello_world.wav")
"hello world"
""",
        """CODE.from huggingface_hub import InferenceClient
client = InferenceClient()
client.feature_extraction("Hi, who are you?")
array([[ 2.424802  ,  2.93384   ,  1.1750331 , ...,  1.240499, -0.13776633, -0.7889173 ],
[-0.42943227, -0.6364878 , -1.693462  , ...,  0.41978157, -2.4336355 ,  0.6162071 ],
...,
[ 0.28552425, -0.928395  , -1.2077185 , ...,  0.76810825, -2.1069427 ,  0.6236161 ]], dtype=float32)
""",
        """CODE.from huggingface_hub import InferenceClient
client = InferenceClient()
client.image_classification("https://upload.wikimedia.org/wikipedia/commons/thumb/4/43/Cute_dog.jpg/320px-Cute_dog.jpg")
[{'score': 0.9779096841812134, 'label': 'Blenheim spaniel'}, ...]
""",
        """CODE.from huggingface_hub import InferenceClient
client = InferenceClient()
client.image_segmentation("cat.jpg"):
[{'score': 0.989008, 'label': 'LABEL_184', 'mask': <PIL.PngImagePlugin.PngImageFile image mode=L size=400x300 at 0x7FDD2B129CC0>}, ...]
""",
        """CODE.from huggingface_hub import InferenceClient
client = InferenceClient()
client.image_to_text("cat.jpg")
'a cat standing in a grassy field '
client.image_to_text("https://upload.wikimedia.org/wikipedia/commons/thumb/4/43/Cute_dog.jpg/320px-Cute_dog.jpg")
'a dog laying on the grass next to a flower pot '
""",
        """CODE.from huggingface_hub import InferenceClient
client = InferenceClient()
client.sentence_similarity(
...     "Machine learning is so easy.",
...     other_sentences=[
...         "Deep learning is so straightforward.",
...         "This is so difficult, like rocket science.",
...         "I can't believe how much I struggled with this.",
...     ],
... )
[0.7785726189613342, 0.45876261591911316, 0.2906220555305481]
""",
        """CODE.from huggingface_hub import InferenceClient
client = InferenceClient()
client.summarization("The Eiffel tower...")
'The Eiffel tower is one of the most famous landmarks in the world....'
""",
        """CODE.from huggingface_hub import InferenceClient
client = InferenceClient()
image = client.image_to_image("cat.jpg", prompt="turn the cat into a tiger")
image.save("tiger.jpg")
""",
        """CODE.from huggingface_hub import InferenceClient
client = InferenceClient()
output = client.conversational("Hi, who are you?")
output
{'generated_text': 'I am the one who knocks.', 'conversation': {'generated_responses': ['I am the one who knocks.'], 'past_user_inputs': ['Hi, who are you?']}, 'warnings': ['Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.']}
client.conversational(
...     "Wow, that's scary!",
...     generated_responses=output["conversation"]["generated_responses"],
...     past_user_inputs=output["conversation"]["past_user_inputs"],
... )
""",
        """CODE.from pathlib import Path
from huggingface_hub import InferenceClient
client = InferenceClient()

audio = client.text_to_speech("Hello world")
Path("hello_world.wav").write_bytes(audio)
""" .

<DEPENDENCY.huggingface-hub==0.16.0> <CONTAINS> """CODE.from huggingface_hub import HFSummaryWriter

# Logs are automatically pushed every 15 minutes
logger = HFSummaryWriter(repo_id="test_hf_logger", commit_every=15)
logger.add_scalar("a", 1)
logger.add_scalar("b", 2)
...

# You can also trigger a push manually
logger.scheduler.trigger()


from huggingface_hub import HFSummaryWriter

# Logs are automatically pushed every 5 minutes (default) + when exiting the context manager
with HFSummaryWriter(repo_id="test_hf_logger") as logger:
...     logger.add_scalar("a", 1)
...     logger.add_scalar("b", 2)
""",
        """CODE.from huggingface_hub import InferenceClient
client = InferenceClient()

client.zero_shot_image_classification(
...     "https://upload.wikimedia.org/wikipedia/commons/thumb/4/43/Cute_dog.jpg/320px-Cute_dog.jpg",
...     labels=["dog", "cat", "horse"],
... )
[{"label": "dog", "score": 0.956}, ...]
""",
        """CODE.from huggingface_hub import InferenceClient
client = InferenceClient()
client.text_generation("The huggingface_hub library is ", max_new_tokens=12)
'100% open source and built to be easy to use.'
for token in client.text_generation("The huggingface_hub library is ", max_new_tokens=12, stream=True):
...     print(token)
100
%
open
source
and
built
to
be
easy
to
use
.
client.text_generation("The huggingface_hub library is ", max_new_tokens=12, details=True)
TextGenerationResponse(
    generated_text='100% open source and built to be easy to use.',
    details=Details(
        finish_reason=<FinishReason.Length: 'length'>,
        generated_tokens=12,
        seed=None,
        prefill=[
            InputToken(id=487, text='The', logprob=None),
            InputToken(id=53789, text=' hugging', logprob=-13.171875),
            (...)
            InputToken(id=204, text=' ', logprob=-7.0390625)
        ],
        tokens=[
            Token(id=1425, text='100', logprob=-1.0175781, special=False),
            Token(id=16, text='%', logprob=-0.0463562, special=False),
            (...)
            Token(id=25, text='.', logprob=-0.5703125, special=False)
        ],
        best_of_sequences=None
    )
)
for details in client.text_generation("The huggingface_hub library is ", max_new_tokens=12, details=True, stream=True):
...     print(details)
...
TextGenerationStreamResponse(token=Token(id=1425, text='100', logprob=-1.0175781, special=False), generated_text=None, details=None)
TextGenerationStreamResponse(token=Token(id=16, text='%', logprob=-0.0463562, special=False), generated_text=None, details=None)
TextGenerationStreamResponse(token=Token(id=1314, text=' open', logprob=-1.3359375, special=False), generated_text=None, details=None)
TextGenerationStreamResponse(token=Token(id=3178, text=' source', logprob=-0.28100586, special=False), generated_text=None, details=None)
TextGenerationStreamResponse(token=Token(id=273, text=' and', logprob=-0.5961914, special=False), generated_text=None, details=None)
TextGenerationStreamResponse(token=Token(id=3426, text=' built', logprob=-1.9423828, special=False), generated_text=None, details=None)
TextGenerationStreamResponse(token=Token(id=271, text=' to', logprob=-1.4121094, special=False), generated_text=None, details=None)
TextGenerationStreamResponse(token=Token(id=314, text=' be', logprob=-1.5224609, special=False), generated_text=None, details=None)
TextGenerationStreamResponse(token=Token(id=1833, text=' easy', logprob=-2.1132812, special=False), generated_text=None, details=None)
TextGenerationStreamResponse(token=Token(id=271, text=' to', logprob=-0.08520508, special=False), generated_text=None, details=None)
TextGenerationStreamResponse(token=Token(id=745, text=' use', logprob=-0.39453125, special=False), generated_text=None, details=None)
TextGenerationStreamResponse(token=Token(
    id=25,
    text='.',
    logprob=-0.5703125,
    special=False),
    generated_text='100% open source and built to be easy to use.',
    details=StreamDetails(finish_reason=<FinishReason.Length: 'length'>, generated_tokens=12, seed=None)
)
""",
        """CODE.from pathlib import Path
from huggingface_hub import CommitScheduler

# Scheduler uploads every 10 minutes
csv_path = Path("watched_folder/data.csv")
CommitScheduler(repo_id="test_scheduler", repo_type="dataset", folder_path=csv_path.parent, every=10)

with csv_path.open("a") as f:
...     f.write("first line")

# Some time later (...)
with csv_path.open("a") as f:
...     f.write("second line")
""" .

<DEPENDENCY.huggingface-hub==0.17.0> <CONTAINS> """CODE.assert SpaceStorage.SMALL == "small"
""",
        """CODE.from huggingface_hub import HfApi
api = HfApi()

# Create repo
repo_id = api.create_repo("test-squash").repo_id

# Make a lot of commits.
api.upload_file(repo_id=repo_id, path_in_repo="file.txt", path_or_fileobj=b"content")
api.upload_file(repo_id=repo_id, path_in_repo="lfs.bin", path_or_fileobj=b"content")
api.upload_file(repo_id=repo_id, path_in_repo="file.txt", path_or_fileobj=b"another_content")

# Squash history
api.super_squash_history(repo_id=repo_id)
""",
        """CODE.from huggingface_hub import InferenceClient
client = InferenceClient()

# Discover zero-shot-classification models currently deployed
models = client.list_deployed_models()
models["zero-shot-classification"]
['Narsil/deberta-large-mnli-zero-cls', 'facebook/bart-large-mnli', ...]

# List from only 1 framework
client.list_deployed_models("text-generation-inference")
{'text-generation': ['bigcode/starcoder', 'meta-llama/Llama-2-70b-chat-hf', ...], ...}
""",
        """CODE.from huggingface_hub import InferenceClient
client = InferenceClient()
client.document_question_answering(image="https://huggingface.co/spaces/impira/docquery/resolve/2359223c1837a7587402bda0f2643382a6eefeab/invoice.png", question="What is the invoice number?")
[{'score': 0.42515629529953003, 'answer': 'us-001', 'start': 16, 'end': 16}]
""",
        """CODE.from huggingface_hub import InferenceClient
client = InferenceClient()
client.fill_mask("The goal of life is <mask>.")
[{'score': 0.06897063553333282,
'token': 11098,
'token_str': ' happiness',
'sequence': 'The goal of life is happiness.'},
{'score': 0.06554922461509705,
'token': 45075,
'token_str': ' immortality',
'sequence': 'The goal of life is immortality.'}]
""",
        """CODE.from huggingface_hub import InferenceClient
client = InferenceClient()
client.get_model_status("bigcode/starcoder")
ModelStatus(loaded=True, state='Loaded', compute_type='gpu', framework='text-generation-inference')
""",
        """CODE.from huggingface_hub import InferenceClient
client = InferenceClient()
client.object_detection("people.jpg"):
[{"score":0.9486683011054993,"label":"person","box":{"xmin":59,"ymin":39,"xmax":420,"ymax":510}}, ... ]
""",
        """CODE.from huggingface_hub import InferenceClient
client = InferenceClient()
client.question_answering(question="What's my name?", context="My name is Clara and I live in Berkeley.")
{'score': 0.9326562285423279, 'start': 11, 'end': 16, 'answer': 'Clara'}
""",
        """CODE.from huggingface_hub import InferenceClient
client = InferenceClient()
client.text_classification("I like you")
[{'label': 'POSITIVE', 'score': 0.9998695850372314}, {'label': 'NEGATIVE', 'score': 0.0001304351753788069}]
""",
        """CODE.from huggingface_hub import InferenceClient
client = InferenceClient()
client.token_classification("My name is Sarah Jessica Parker but you can call me Jessica")
[{'entity_group': 'PER',
'score': 0.9971321225166321,
'word': 'Sarah Jessica Parker',
'start': 11,
'end': 31},
{'entity_group': 'PER',
'score': 0.9773476123809814,
'word': 'Jessica',
'start': 52,
'end': 59}]
""",
        """CODE.from huggingface_hub import InferenceClient
client = InferenceClient()
client.translation("My name is Wolfgang and I live in Berlin")
'Mein Name ist Wolfgang und ich lebe in Berlin.'
client.translation("My name is Wolfgang and I live in Berlin", model="Helsinki-NLP/opus-mt-en-fr")
"Je m'appelle Wolfgang et je vis Ã  Berlin."
""",
        """CODE.from huggingface_hub import InferenceClient
client = InferenceClient()
client.visual_question_answering(
...     image="https://huggingface.co/datasets/mishig/sample_images/resolve/main/tiger.jpg",
...     question="What is the animal doing?"
... )
[{'score': 0.778609573841095, 'answer': 'laying down'},{'score': 0.6957435607910156, 'answer': 'sitting'}, ...]
""",
        """CODE.from huggingface_hub import InferenceClient
client = InferenceClient()
query = "How many stars does the transformers repository have?"
table = {"Repository": ["Transformers", "Datasets", "Tokenizers"], "Stars": ["36542", "4512", "3934"]}
client.table_question_answering(table, query, model="google/tapas-base-finetuned-wtq")
{'answer': 'AVERAGE > 36542', 'coordinates': [[0, 1]], 'cells': ['36542'], 'aggregator': 'AVERAGE'}
""",
        """CODE.from huggingface_hub import InferenceClient
client = InferenceClient()
table = {
    "fixed_acidity": ["7.4", "7.8", "10.3"],
    "volatile_acidity": ["0.7", "0.88", "0.32"],
    "citric_acid": ["0", "0", "0.45"],
    "residual_sugar": ["1.9", "2.6", "6.4"],
    "chlorides": ["0.076", "0.098", "0.073"],
    "free_sulfur_dioxide": ["11", "25", "5"],
    "total_sulfur_dioxide": ["34", "67", "13"],
    "density": ["0.9978", "0.9968", "0.9976"],
    "pH": ["3.51", "3.2", "3.23"],
    "sulphates": ["0.56", "0.68", "0.82"],
    "alcohol": ["9.4", "9.8", "12.6"],
}
client.tabular_classification(table=table, model="julien-c/wine-quality")
""",
        """CODE.from huggingface_hub import InferenceClient
client = InferenceClient()
table = {
...     "Height": ["11.52", "12.48", "12.3778"],
...     "Length1": ["23.2", "24", "23.9"],
...     "Length2": ["25.4", "26.3", "26.5"],
...     "Length3": ["30", "31.2", "31.1"],
...     "Species": ["Bream", "Bream", "Bream"],
...     "Width": ["4.02", "4.3056", "4.6961"],
... }
client.tabular_regression(table, model="scikit-learn/Fish-Weight")
[110, 120, 130]
""",
        """CODE.from huggingface_hub import InferenceClient
client = InferenceClient()
text = (
...     "A new model offers an explanation for how the Galilean satellites formed around the solar system's"
...     "largest world. Konstantin Batygin did not set out to solve one of the solar system's most puzzling"
...     " mysteries when he went for a run up a hill in Nice, France."
... )
labels = ["space & cosmos", "scientific discovery", "microbiology", "robots", "archeology"]
client.zero_shot_classification(text, labels)
[
    {"label": "scientific discovery", "score": 0.7961668968200684},
    {"label": "space & cosmos", "score": 0.18570658564567566},
    {"label": "microbiology", "score": 0.00730885099619627},
    {"label": "archeology", "score": 0.006258360575884581},
    {"label": "robots", "score": 0.004559356719255447},
]
client.zero_shot_classification(text, labels, multi_label=True)
[
    {"label": "scientific discovery", "score": 0.9829297661781311},
    {"label": "space & cosmos", "score": 0.755190908908844},
    {"label": "microbiology", "score": 0.0005462635890580714},
    {"label": "archeology", "score": 0.00047131875180639327},
    {"label": "robots", "score": 0.00030448526376858354},
]
""",
        """CODE.from huggingface_hub import file_exists
file_exists("bigcode/starcoder", "config.json")
True
file_exists("bigcode/starcoder", "not-a-file")
False
file_exists("bigcode/not-a-repo", "config.json")
False
""",
        """CODE.from huggingface_hub import repo_exists
repo_exists("huggingface/transformers")
True
repo_exists("huggingface/not-a-repo")
False
""" .

<DEPENDENCY.huggingface-hub==0.18.0> <CONTAINS> """CODE.args = DatasetSearchArguments()
args.author.huggingface
args.language.en
""",
        """CODE.args = ModelSearchArguments()
args.author.huggingface
args.language.en
""",
        """CODE.d = AttributeDictionary()
d["test"] = "a"
print(d.test)  # prints "a"
""",
        """CODE.from huggingface_hub import CommitOperationAdd, preupload_lfs_files, create_commit, create_repo

repo_id = create_repo("test_preupload").repo_id

# Generate and preupload LFS files one by one
operations = [] # List of all `CommitOperationAdd` objects that will be generated
for i in range(5):
...     content = ... # generate binary content
...     addition = CommitOperationAdd(path_in_repo=f"shard_{i}_of_5.bin", path_or_fileobj=content)
...     preupload_lfs_files(repo_id, additions=[addition]) # upload + free memory
...     operations.append(addition)

# Create commit
create_commit(repo_id, operations=operations, commit_message="Commit all shards")
""",
        """CODE.from huggingface_hub import add_collection_item
collection = add_collection_item(
    collection_slug="davanstrien/climate-64f99dc2a5067f6b65531bab",
    item_id="pierre-loic/climate-news-articles",
    item_type="dataset"
)
collection.items[-1].item_id
"pierre-loic/climate-news-articles"

add_collection_item(
    collection_slug="davanstrien/climate-64f99dc2a5067f6b65531bab",
    item_id="datasets/climate_fever",
    item_type="dataset",
    note="This dataset adopts the FEVER methodology that consists of 1,535 real-world claims regarding climate-change collected on the internet."
)
""",
        """CODE.from huggingface_hub import create_collection
collection = create_collection(
    title="ICCV 2023",
    description="Portfolio of models, papers and demos I presented at ICCV 2023",
)
collection.slug
"username/iccv-2023-64f9a55bb3115b4f513ec026"
""",
        """CODE.from huggingface_hub import delete_collection
collection = delete_collection("username/useless-collection-64f9a55bb3115b4f513ec026", missing_ok=True)
""",
        """CODE.from huggingface_hub import get_collection
collection = get_collection("TheBloke/recent-models-64f9a55bb3115b4f513ec026")
collection.title
'Recent models'
len(collection.items)
37
collection.items[0]
CollectionItem: {
    {'item_object_id': '6507f6d5423b46492ee1413e',
    'item_id': 'TheBloke/TigerBot-70B-Chat-GPTQ',
    'author': 'TheBloke',
    'item_type': 'model',
    'lastModified': '2023-09-19T12:55:21.000Z',
    (...)
}}
""",
        """CODE.from huggingface_hub import get_collection, delete_collection_item

# Get collection first
collection = get_collection("TheBloke/recent-models-64f9a55bb3115b4f513ec026")

# Delete item based on its ID
delete_collection_item(
...     collection_slug="TheBloke/recent-models-64f9a55bb3115b4f513ec026",
...     item_object_id=collection.items[-1].item_object_id,
... )
""",
        """CODE.from huggingface_hub import get_collection, update_collection_item

# Get collection first
collection = get_collection("TheBloke/recent-models-64f9a55bb3115b4f513ec026")

# Update item based on its ID (add note + update position)
update_collection_item(
...     collection_slug="TheBloke/recent-models-64f9a55bb3115b4f513ec026",
...     item_object_id=collection.items[-1].item_object_id,
...     note="Newly updated model!"
...     position=0,
... )
""",
        """CODE.from huggingface_hub import update_collection_metadata
collection = update_collection_metadata(
    collection_slug="username/iccv-2023-64f9a55bb3115b4f513ec026",
    title="ICCV Oct. 2023"
    description="Portfolio of models, datasets, papers and demos I presented at ICCV Oct. 2023",
    private=False,
    theme="pink",
)
collection.slug
""" .

<DEPENDENCY.huggingface-hub==0.19.0> <CONTAINS> """CODE.from huggingface_hub import HfApi
api = HfApi()
api.list_inference_endpoints()
[InferenceEndpoint(name='my-endpoint', ...), ...]
""",
        """CODE.from huggingface_hub import HfApi
api = HfApi()
create_inference_endpoint(
    "my-endpoint-name",
    repository="gpt2",
    framework="pytorch",
    task="text-generation",
    accelerator="cpu",
    vendor="aws",
    region="us-east-1",
    type="protected",
    instance_size="medium",
    instance_type="c6i"
)
endpoint
InferenceEndpoint(name='my-endpoint-name', status="pending",...)

# Run inference on the endpoint
endpoint.client.text_generation(...)
"..."
""",
        """CODE.from huggingface_hub import HfApi
api = HfApi()
endpoint = api.get_inference_endpoint("my-text-to-image")
endpoint
InferenceEndpoint(name='my-text-to-image', ...)

# Get status
endpoint.status
'running'
endpoint.url
'https://my-text-to-image.region.vendor.endpoints.huggingface.cloud'

# Run inference
endpoint.client.text_to_image(...)
""",
        """CODE.from huggingface_hub import get_inference_endpoint
endpoint = get_inference_endpoint("my-text-to-image")
endpoint
InferenceEndpoint(name='my-text-to-image', ...)

# Get status
endpoint.status
'running'
endpoint.url
'https://my-text-to-image.region.vendor.endpoints.huggingface.cloud'

# Run inference
endpoint.client.text_to_image(...)

# Pause endpoint to save $$$
endpoint.pause()

# ...
# Resume and wait for deployment
endpoint.resume()
endpoint.wait()
endpoint.client.text_to_image(...)
""" .

<DEPENDENCY.huggingface-hub==0.20.0> <CONTAINS> """CODE.# Parse repo with single weights file
metadata = get_safetensors_metadata("bigscience/bloomz-560m")
metadata
metadata.files_metadata["model.safetensors"].metadata
{'format': 'pt'}

# Parse repo with sharded model
metadata = get_safetensors_metadata("bigscience/bloom")
metadata
len(metadata.files_metadata)
72  # All safetensors files have been fetched

# Parse repo with sharded model
get_safetensors_metadata("runwayml/stable-diffusion-v1-5")
NotASafetensorsRepoError: 'runwayml/stable-diffusion-v1-5' is not a safetensors repo. Couldn't find 'model.safetensors.index.json' or 'model.safetensors' files.
""",
        """CODE.from huggingface_hub import get_paths_info
paths_info = get_paths_info("allenai/c4", ["README.md", "en"], repo_type="dataset")
paths_info
[
    RepoFile(path='README.md', size=2379, blob_id='f84cb4c97182890fc1dbdeaf1a6a468fd27b4fff', lfs=None, last_commit=None, security=None),
    RepoFolder(path='en', tree_id='dc943c4c40f53d02b31ced1defa7e5f438d5862e', last_commit=None)
]
""",
        """CODE.from huggingface_hub import list_accepted_access_requests

requests = list_accepted_access_requests("meta-llama/Llama-2-7b")
len(requests)
411
requests[0]
[
    AccessRequest(
        username='clem',
        fullname='Clem ð¤',
        email='***',
        timestamp=datetime.datetime(2023, 11, 23, 18, 4, 53, 828000, tzinfo=datetime.timezone.utc),
        status='accepted',
        fields=None,
    ),
    ...
]
""",
        """CODE.from huggingface_hub import list_pending_access_requests, accept_access_request

# List pending requests
requests = list_pending_access_requests("meta-llama/Llama-2-7b")
len(requests)
411
requests[0]
[
    AccessRequest(
        username='clem',
        fullname='Clem ð¤',
        email='***',
        timestamp=datetime.datetime(2023, 11, 23, 18, 4, 53, 828000, tzinfo=datetime.timezone.utc),
        status='pending',
        fields=None,
    ),
    ...
]

# Accept Clem's request
accept_access_request("meta-llama/Llama-2-7b", "clem")
""",
        """CODE.from huggingface_hub import list_rejected_access_requests

requests = list_rejected_access_requests("meta-llama/Llama-2-7b")
len(requests)
411
requests[0]
[
    AccessRequest(
        username='clem',
        fullname='Clem ð¤',
        email='***',
        timestamp=datetime.datetime(2023, 11, 23, 18, 4, 53, 828000, tzinfo=datetime.timezone.utc),
        status='rejected',
        fields=None,
    ),
    ...
]
""",
        """CODE.from huggingface_hub import list_repo_tree
repo_tree = list_repo_tree("lysandre/arxiv-nlp")
repo_tree
<generator object HfApi.list_repo_tree at 0x7fa4088e1ac0>
list(repo_tree)
[
    RepoFile(path='.gitattributes', size=391, blob_id='ae8c63daedbd4206d7d40126955d4e6ab1c80f8f', lfs=None, last_commit=None, security=None),
    RepoFile(path='README.md', size=391, blob_id='43bd404b159de6fba7c2f4d3264347668d43af25', lfs=None, last_commit=None, security=None),
    RepoFile(path='config.json', size=554, blob_id='2f9618c3a19b9a61add74f70bfb121335aeef666', lfs=None, last_commit=None, security=None),
    RepoFile(
        path='flax_model.msgpack', size=497764107, blob_id='8095a62ccb4d806da7666fcda07467e2d150218e',
        lfs={'size': 497764107, 'sha256': 'd88b0d6a6ff9c3f8151f9d3228f57092aaea997f09af009eefd7373a77b5abb9', 'pointer_size': 134}, last_commit=None, security=None
    ),
    RepoFile(path='merges.txt', size=456318, blob_id='226b0752cac7789c48f0cb3ec53eda48b7be36cc', lfs=None, last_commit=None, security=None),
    RepoFile(
        path='pytorch_model.bin', size=548123560, blob_id='64eaa9c526867e404b68f2c5d66fd78e27026523',
        lfs={'size': 548123560, 'sha256': '9be78edb5b928eba33aa88f431551348f7466ba9f5ef3daf1d552398722a5436', 'pointer_size': 134}, last_commit=None, security=None
    ),
    RepoFile(path='vocab.json', size=898669, blob_id='b00361fece0387ca34b4b8b8539ed830d644dbeb', lfs=None, last_commit=None, security=None)]
]

""" .

<DEPENDENCY.huggingface-hub==0.4.0> <CONTAINS> """CODE.d = AttributeDictionary()
d["test"] = "a"
print(d.test) # prints "a"
""" .

<DEPENDENCY.huggingface-hub==0.8.1> <CONTAINS> """CODE.with open("path/to/file", "rb") as file:
...     with SliceFileObj(file, seek_from=128, read_limit=200) as fslice:
...         fslice.read(...)

import os
chunk_size = 512
file_size = os.getsize("path/to/file")
with open("path/to/file", "rb") as file:
...     for chunk_idx in range(ceil(file_size / chunk_size)):
...         with SliceFileObj(file, seek_from=chunk_idx * chunk_size, read_limit=chunk_size) as fslice:
...             chunk = fslice.read(...)
""",
        """CODE.with operation.as_file() as file:
    content = file.read()
""" .

<DEPENDENCY.huggingface-hub==0.9.0> <CONTAINS> """CODE.@expect_deprecation("push_to_hub"):
def test_push_to_hub_git_version(self):
    (...)
    push_to_hub(repo_url="something") <- Should warn with FutureWarnings
    (...)
""",
        """CODE._parse_revision_from_pr_url("https://huggingface.co/bigscience/bloom/discussions/2")
"refs/pr/2\"""",
        """CODE.comment = \"\"\"
Hello @otheruser!

# This is a title

**This is bold**, *this is italic* and ~this is strikethrough~
And [this](http://url) is a link
\"\"\"

HfApi().comment_discussion(
    repo_id="username/repo_name",
    discussion_num=34,
    comment=comment
)
""",
        """CODE.from huggingface_hub import get_repo_discussions
discussions_list = list(get_repo_discussions(repo_id="bert-base-uncased"))

from huggingface_hub import get_repo_discussions
for discussion in get_repo_discussions(repo_id="bert-base-uncased"):
    print(discussion.num, discussion.title)
""",
        """CODE.from huggingface_hub import metadata_eval_result
metadata_eval_result(
    model_pretty_name="RoBERTa fine-tuned on ReactionGIF",
    task_pretty_name="Text Classification",
    task_id="text-classification",
    metrics_pretty_name="Accuracy",
    metrics_id="accuracy",
    metrics_value=0.2662102282047272,
    dataset_pretty_name="ReactionJPEG",
    dataset_id="julien-c/reactionjpeg",
    dataset_config="default",
    dataset_split="test",
)
""",
        """CODE.list(filter_repo_objects(
...     ["aaa.PDF", "bbb.jpg", ".ccc.pdf", ".ddd.png"],
...     allow_patterns=["*.pdf"],
...     ignore_patterns=[".*"],
... ))
["aaa.pdf"]


list(filter_repo_objects(
... [
...     CommitOperationAdd(path_or_fileobj="/tmp/aaa.pdf", path_in_repo="aaa.pdf")
...     CommitOperationAdd(path_or_fileobj="/tmp/bbb.jpg", path_in_repo="bbb.jpg")
...     CommitOperationAdd(path_or_fileobj="/tmp/.ccc.pdf", path_in_repo=".ccc.pdf")
...     CommitOperationAdd(path_or_fileobj="/tmp/.ddd.png", path_in_repo=".ddd.png")
... ],
... allow_patterns=["*.pdf"],
... ignore_patterns=[".*"],
... key=lambda x: x.repo_in_path
... ))
[CommitOperationAdd(path_or_fileobj="/tmp/aaa.pdf", path_in_repo="aaa.pdf")]
""",
        """CODE.new_title = "New title, fixing a typo"
HfApi().rename_discussion(
    repo_id="username/repo_name",
    discussion_num=34,
    new_title=new_title
)
""",
        """CODE.parse_datetime('2022-08-19T07:19:38.123Z')
datetime.datetime(2022, 8, 19, 7, 19, 38, 123000, tzinfo=timezone.utc)
""",
        """CODE.repo_name()
repo-2fe93f-16599646671840
repo_name("my-space", prefix='space')
space-my-space-16599481979701""" .

<DEPENDENCY.imageio==1.4> <CONTAINS> """CODE.data = numpy.random.rand(2, 5, 3, 301, 219)
with TiffWriter('temp.tif', bigtiff=True) as tif:
...     for i in range(data.shape[0]):
...         tif.save(data[i], compress=6)""",
        """CODE.def product(iterable):
    result = 1
    for num in iterable:
        result *= num
    return result

print(product([2**8, 2**30]))
print(product([]))""",
        """CODE.excel_datetime(40237.029999999795)
datetime.datetime(2010, 2, 28, 0, 43, 11, 999982)""",
        """CODE.import struct
import numpy as np

def unpackrgb(data, dtype, bitspersample, rescale=True):
    result = []
    for i in range(0, len(data), len(bitspersample)):
        sample = struct.unpack(dtype, data[i:i+len(bitspersample)])[0]
        if rescale:
            sample = sample * (2**len(bitspersample) - 1) // 255
        result.append(sample)
    return np.array(result)

data = struct.pack('BBBB', 0x21, 0x08, 0xff, 0xff)
print(unpackrgb(data, '<B', (5, 6, 5), False))
print(unpackrgb(data, '<B', (5, 6, 5)))
print(unpackrgb(data, '<B', (5, 5, 5)))
""",
        """CODE.imsave('temp.tif', data, compress=6,
       extratags=[(270, 's', 0, description, True)])""",
        """CODE.julian_datetime(2451576, 54362783)
datetime.datetime(2000, 2, 2, 15, 6, 2, 783)""",
        "CODE.natural_sorted(['f1', 'f2', 'f10'])",
        """CODE.sequence(1)
(1,)
sequence([1])
[1]""",
        """CODE.squeeze_axes((5, 1, 2, 1, 1), 'TZYXC')
((5, 2, 1), 'TYX')""",
        """CODE.stripascii(b'string\\x00string\\n\\x01\\x00')
stripascii(b'\\x00')""",
        "CODE.test_tifffile(verbose=False)",
        "CODE.transpose_axes(numpy.zeros((2, 3, 4, 5)), 'TYXC', asaxes='CTZYX').shape" .

<DEPENDENCY.imageio==1.5> <CONTAINS> """CODE.data = struct.pack('BBBB', 0x21, 0x08, 0xff, 0xff)
print(unpackrgb(data, '<B', (5, 6, 5), False))
print(unpackrgb(data, '<B', (5, 6, 5)))
print(unpackrgb(data, '<B', (5, 5, 5)))
""",
        """CODE.import struct
import numpy as np

def unpackrgb(data, dtype, bitspersample, rescale=True):
    result = []
    for i in range(0, len(data), len(bitspersample)):
        sample = struct.unpack(dtype, data[i:i+len(bitspersample)])[0]
        if rescale:
            sample = sample * (2**len(bitspersample) - 1) // 255
        result.append(sample)
    return np.array(result)

data = struct.pack('BBBB', 0x21, 0x08, 0xff, 0xff)
print(unpackrgb(data, '<B', (5, 6, 5), False))
print(unpackrgb(data, '<B', (5, 6, 5)))
print(unpackrgb(data, '<B', (5, 5, 5)))
""" .

<DEPENDENCY.imageio==1.6> <CONTAINS> """CODE.data = struct.pack('BBBB', 0x21, 0x08, 0xff, 0xff)
print(unpack_rgb(data, '<B', (5, 6, 5), False))
print(unpack_rgb(data, '<B', (5, 6, 5)))
print(unpack_rgb(data, '<B', (5, 5, 5)))
""",
        """CODE.image = numpy.arange(256, dtype='uint8')
colormap = numpy.vstack([image, image, image]).astype('uint16') * 256
apply_colormap(image, colormap)[-1]
array([65280, 65280, 65280], dtype=uint16)
""",
        """CODE.image_description((256, 256, 3), axes='YXS')  # doctest: +SKIP
b'{"shape": [256, 256, 3], "axes": "YXS"}'""",
        """CODE.image_description_dict(b'shape=(256, 256, 3)')
description = b'{"shape": [256, 256, 3], "axes": "YXS"}'
image_description_dict(description)  # doctest: +SKIP""",
        """CODE.imagej_shape((2, 3, 4, 5, 3), False)
(2, 3, 4, 5, 3, 1)""",
        """CODE.import struct
import numpy as np

def unpack_rgb(data, dtype, bitspersample, rescale=True):
    result = []
    for i in range(0, len(data), len(bitspersample)):
        sample = struct.unpack(dtype, data[i:i+len(bitspersample)])[0]
        if rescale:
            sample = sample * (2**len(bitspersample) - 1) // 255
        result.append(sample)
    return np.array(result)

data = struct.pack('BBBB', 0x21, 0x08, 0xff, 0xff)
print(unpack_rgb(data, '<B', (5, 6, 5), False))
print(unpack_rgb(data, '<B', (5, 6, 5)))
print(unpack_rgb(data, '<B', (5, 5, 5)))
""",
        """CODE.reshape_axes('YXS', (219, 301, 1), (219, 301))
reshape_axes('IYX', (12, 219, 301), (3, 4, 219, 1, 301, 1)""",
        """CODE.reverse_bitorder(b'd')
data = numpy.array([1, 666], dtype='uint16')
reverse_bitorder(data)""" .

<DEPENDENCY.imageio==2.1.2> <CONTAINS> "CODE.test_tifffile(verbose=False)" .

<DEPENDENCY.imageio==2.10.0> <CONTAINS> """CODE.Request.Mode("rI")
Request.Mode("wv")""",
        """CODE.import imageio.v3 as iio
with iio.imopen("/path/to/image.png", "r") as file:
    im = file.read()

with iio.imopen("/path/to/output.jpg", "w") as file:
    file.write(im)""" .

<DEPENDENCY.imageio==2.10.5> <CONTAINS> """CODE.import imageio.v3 as iio
with iio.imopen("/path/to/image.png", "r") as file:
    im = file.read()

with iio.imopen("/path/to/output.jpg", "w") as file:
    file.write(im)""" .

<DEPENDENCY.imageio==2.11.0> <CONTAINS> """CODE.import imageio.v3 as iio
with iio.imopen("/path/to/image.png", "r") as file:
    im = file.read()

with iio.imopen("/path/to/output.jpg", "w") as file:
    file.write(im)
""" .

<DEPENDENCY.imageio==2.15.0> <CONTAINS> """CODE.with working_directory('/my/new/path'):
    # Do something in new directory""" .

<DEPENDENCY.imageio==2.16.1> <CONTAINS> """CODE.image_file = my_plugin(Request, "r")
...
image_file.close()""" .

<DEPENDENCY.imageio==2.2.0> <CONTAINS> """CODE.def parse_kwargs(kwargs, *keys, **keyvals):
...     for key in keys:
...         if key in kwargs:
...             del kwargs[key]
...     kwargs.update(keyvals)
...     return kwargs""",
        """CODE.description = b'ImageJ=1.11a\\nimages=510\\nhyperstack=true\\n'
imagej_description_dict(description)  # doctest: +SKIP
{'ImageJ': '1.11a', 'images': 510, 'hyperstack': True}""",
        """CODE.image_description((256, 256, 3), axes='YXS')  # doctest: +SKIP
b'{"shape": [256, 256, 3], "axes": "YXS"}'""",
        """CODE.image_description_dict(b'shape=(256, 256, 3)')
description = b'{"shape": [256, 256, 3], "axes": "YXS"}'
image_description_dict(description)  # doctest: +SKIP""",
        """CODE.imagej_description_dict(description)  # doctest: +SKIP
{'ImageJ': '1.11a', 'images': 510, 'hyperstack': True}""",
        """CODE.reshape_nd(numpy.empty(0), 1).shape
reshape_nd(numpy.empty(1), 2).shape
reshape_nd(numpy.empty((2, 3)), 3).shape
reshape_nd(numpy.empty((3, 4, 5)), 3).shape""",
        "CODE.update_kwargs(kwargs, one=None, two=2)" .

<DEPENDENCY.imageio==2.3.0> <CONTAINS> """CODE.asbool(b' False ')
False""",
        """CODE.byteorder_isnative('=')
True""",
        """CODE.d = matlabstr2py("SI.hChannels.channelType = {'stripe' 'stripe'}\\n"
...                  "SI.hChannels.channelsActive = 2")
d['SI.hChannels.channelType']
['stripe', 'stripe']""",
        """CODE.descr = ('[Intensity Mapping]\\nMap Ch0: Range=00000 to 02047\\n'
...          '[Intensity Mapping End]')
fluoview_description_metadata(descr)
{'Intensity Mapping': {'Map Ch0: Range': '00000 to 02047'}}""",
        """CODE.description = 'ImageJ=1.11a\\nimages=510\\nhyperstack=true\\n'
imagej_description_metadata(description)
{'ImageJ': '1.11a', 'images': 510, 'hyperstack': True}""",
        """CODE.enumarg(TIFF.PHOTOMETRIC, 2)
enumarg(TIFF.PHOTOMETRIC, 'RGB')""",
        "CODE.identityfunc('arg')",
        """CODE.json_description((256, 256, 3), axes='YXS')  # doctest: +SKIP
b'{"shape": [256, 256, 3], "axes": "YXS"}'""",
        """CODE.json_description_metadata(description)  # doctest: +SKIP
json_description_metadata('shape=(256, 256, 3)')""",
        """CODE.memmap('temp.tif', shape=(256, 256), dtype='float32')
im[255, 255] = 1.0
im.flush()
im.shape, im.dtype
((256, 256), dtype('float32'))
del im

memmap('temp.tif', page=0)
im[255, 255]
1.0""",
        "CODE.nullfunc('arg', kwarg='kwarg')",
        """CODE.pilatus_description_metadata('# Pixel_size 172e-6 m x 172e-6 m')
{'Pixel_size': (0.000172, 0.000172)}""",
        """CODE.repeat_nd([[1, 2], [3, 4]], (2, 2))
array([[1, 1, 2, 2],
       [1, 1, 2, 2],
       [3, 3, 4, 4],
       [3, 3, 4, 4]])""",
        "CODE.snipstr('abcdefghijklmnop', 8)",
        """CODE.svs_description_metadata('Aperio Image Library v1.0')
{'Aperio Image Library': 'v1.0'}""",
        """CODE.with NullContext():
    pass""",
        "CODE.xml2dict('<?xml version=\"1.0\" ?><root attr=\"name\"><key>1</key></root>')" .

<DEPENDENCY.imageio==2.5.0> <CONTAINS> "CODE.hexdump(binascii.unhexlify('49492a00080000000e00fe0004000100'))",
        """CODE.isprintable('abc')
isprintable(b'\\x01')""" .

<DEPENDENCY.jax==0.1.59> <CONTAINS> """CODE.x = np.arange(4)
y = jax.pmap(lambda x: jax.lax.pmean(x, 'i'), axis_name='i')(x)
print(y)
[ 1.5         1.5         1.5         1.5       ]
y = jax.pmap(lambda x: x / jax.lax.pmean(x, 'i'), axis_name='i')(x)
print(y)""" .

<DEPENDENCY.jax==0.1.60> <CONTAINS> """CODE.@jax.profiler.trace_function
def f(x):
    return jnp.dot(x, x.T).block_until_ready()

@partial(jax.profiler.trace_function, name="event_name")
def f(x):
    return jnp.dot(x, x.T).block_until_ready()""",
        """CODE.@register_pytree_node_class
class Special:
    def __init__(self, x, y):
        self.x = x
        self.y = y
    def tree_flatten(self):
        return ((self.x, self.y), None)
    @classmethod
    def tree_unflatten(cls, aux_data, children):
        return cls(*children)""",
        """CODE.jax.nn.one_hot(np.array([0, 1, 2]), 3)
DeviceArray([[1., 0., 0.],
             [0., 1., 0.],
             [0., 0., 1.]], dtype=float32)
jax.nn.one_hot(np.array([-1, 3]), 3)
DeviceArray([[0., 0., 0.],
             [0., 0., 0.]], dtype=float32)""",
        """CODE.with jax.profiler.TraceContext("acontext"):
    jnp.dot(x, x.T).block_until_ready()""" .

<DEPENDENCY.jax==0.1.62> <CONTAINS> """CODE.x = np.arange(4)
y = jax.pmap(lambda x: jax.lax.all_gather(x, 'i'), axis_name='i')(x)
print(y)
[[0 1 2 3]
 [0 1 2 3]
 [0 1 2 3]
 [0 1 2 3]]
""" .

<DEPENDENCY.jax==0.1.63> <CONTAINS> """CODE.@jax.custom_jvp
def f(x, y):
    return np.sin(x) * y

@f.defjvp
def f_jvp(primals, tangents):
    x, y = primals
    x_dot, y_dot = tangents
    primal_out = f(x, y)
    tangent_out = np.cos(x) * x_dot * y - np.sin(x) * y_dot
    return primal_out, tangent_out""",
        """CODE.@jax.custom_vjp
def f(x, y):
    return np.sin(x) * y

def f_fwd(x, y):
    return f(x, y), (np.cos(x), np.sin(x), y)

def f_bwd(res, g):
    cos_x, sin_x, y = res
    return (cos_x * g * y, -sin_x * g)

f.defvjp(f_fwd, f_bwd)""",
        """CODE.@partial(pmap, axis_name='i')
@partial(pmap, axis_name='j')
def f(_):
  return lax.axis_index('i'), lax.axis_index('j')""",
        """CODE.tree = {"a": [1, 2, 3]}
assert all_leaves(jax.tree_leaves(tree))
assert not all_leaves([tree])""" .

<DEPENDENCY.jax==0.1.64> <CONTAINS> """CODE.``x[idx] = minimum(x[idx], y)``
``x.at[idx].min(y)`` is syntactic sugar for
``jax.ops.index_min(x, jax.ops.index[idx], y)``, and
returns the value of ``x`` that would result from the NumPy-style
:mod:indexed assignment <numpy.doc.indexing>`
``x[idx] = minimum(x[idx], y)``.""",
        """CODE.x = jax.numpy.ones((5, 6))
jax.ops.index_mul(x, jax.ops.index[2:4, 3:], 6.)""" .

<DEPENDENCY.jax==0.1.67> <CONTAINS> """CODE.with outfeed_receiver():
    jax.jit(func)(args)
    ...
    jax.pmap(another_func)(args)

The ``outfeed_receiver`` must be started outside any jitted computation.
""" .

<DEPENDENCY.jax==0.1.68> <CONTAINS> """CODE.# Example 1: Partials sums of numbers.

np.associative_scan(operator.add, np.arange(0, 4))
# ==> [ 0, 1, 3, 6]

# Example 2: Partial products of random matrices.

np.associative_scan(np.matmul, matrices)
""" .

<DEPENDENCY.jax==0.1.69> <CONTAINS> """CODE.c = xc.XlaBuilder("example")
p0 = xb.parameter(c, 1, xc.shape_from_pyval(jnp.ones([1])))
p1 = xb.parameter(c, 2, xc.shape_from_pyval(jnp.ones([2])))
p2 = xb.parameter(c, 3, xc.shape_from_pyval(jnp.ones([3])))
o = xops.Tuple(c, [p0, p1, p2])
flatten_shape(c.GetShape(o))
flatten_shape(c.GetShape(p0))""",
        """CODE.rebase_donate_argnums((3, 4), (0, 1))
(1, 2)""" .

<DEPENDENCY.jax==0.1.70> <CONTAINS> """CODE.def switch(index, branches, operand):
    index = clamp(0, index, len(branches) - 1)
    return branches[index](operand)""" .

<DEPENDENCY.jax==0.1.71> <CONTAINS> """CODE.@jax.checkpoint
def g(x):
  y = jnp.sin(x)
  z = jnp.sin(y)
  return z

recursive_checkpoint(funs):
  if len(funs) == 1:
    return funs[0]
  elif len(funs) == 2:
    f1, f2 = funs
    return lambda x: f1(f2(x))
  else:
    f1 = recursive_checkpoint(funs[:len(funs)//2])
    f2 = recursive_checkpoint(funs[len(funs)//2:])
    return lambda x: f1(jax.checkpoint(f2)(x))""" .

<DEPENDENCY.jax==0.1.73> <CONTAINS> """CODE.import jax

while global_step < NUM_STEPS:
    with jax.profiler.StepTraceContext("train", step_num=global_step):
        train_step()
        global_step += 1
""" .

<DEPENDENCY.jax==0.2.0> <CONTAINS> """CODE.f = lambda x, y: 0.5 * x - 0.5 * y
scalar = types.SimpleNamespace(shape=(), dtype=np.float32)
f_transpose = jax.linear_transpose(f, scalar, scalar)
f_transpose(1.0)""",
        """CODE.from jax import api, numpy as jnp
devices = api.local_devices()
x = [jnp.ones(5) for device in devices]
y = api.device_put_sharded(x, devices)
np.allclose(y, jnp.stack(x))
x = [(i, jnp.arange(i, i + 4)) for i in range(len(devices))]
y = api.device_put_sharded(x, devices)
type(y)
<class 'tuple'>
y0 = api.device_put_sharded([a for a, b in x], devices)
y1 = api.device_put_sharded([b for a, b in x], devices)
np.allclose(y[0], y0)
np.allclose(y[1], y1)
""" .

<DEPENDENCY.jax==0.2.10> <CONTAINS> """CODE.Passing a tracer in place of an integer
    from jax import jit, partial
    import numpy as np

    @jit
    ... def func(x, axis):
    ...   return np.split(x, 2, axis)

    func(np.arange(4), 0)  # doctest: +IGNORE_EXCEPTION_DETAIL
    Traceback (most recent call last):
        ...
    TracerIntegerConversionError: The __index__() method was called on the JAX Tracer object

  When this happens, the solution is often to mark the problematic argument as static::

    @partial(jit, static_argnums=1)
    ... def func(x, axis):
    ...   return np.split(x, 2, axis)

    func(np.arange(10), 0)
    [DeviceArray([0, 1, 2, 3, 4], dtype=int32),
     DeviceArray([5, 6, 7, 8, 9], dtype=int32)]

Indexing a list with a Tracer
    import jax.numpy as jnp
    from jax import jit, partial

    L = [1, 2, 3]

    @jit
    ... def func(i):
    ...   return L[i]

    func(0)  # doctest: +IGNORE_EXCEPTION_DETAIL
    Traceback (most recent call last):
        ...
    TracerIntegerConversionError: The __index__() method was called on the JAX Tracer object

  Depending on the context, you can generally fix this either by converting the list
  to a JAX array::

    @jit
    ... def func(i):
    ...   return jnp.array(L)[i]

    func(0)
    DeviceArray(1, dtype=int32)

  or by declaring the index as a static argument::

    @partial(jit, static_argnums=0)
    ... def func(i):
    ...   return L[i]

    func(0)
    DeviceArray(1, dtype=int32)""",
        """CODE.devices = np.array(jax.devices())[:4].reshape((2, 2)
with mesh(devices, ('x', 'y')):
    distributed_out = xmap(
      jnp.vdot,
      in_axes=({0: 'left', 1: 'right'}),
      out_axes=['left', 'right', ...],
      axis_resources={'left': 'x', 'right': 'y'})(x, x.T)""" .

<DEPENDENCY.jax==0.2.11> <CONTAINS> """CODE.@jax.profiler.trace_function
def f(x):
    return jnp.dot(x, x.T).block_until_ready()

@partial(jax.profiler.trace_function, name="event_name")
def f(x):
    return jnp.dot(x, x.T).block_until_ready()""",
        """CODE.@jax.profiler.trace_function
def f(x):
    return jnp.dot(x, x.T).block_until_ready()

f(jnp.ones((1000, 1000))

@partial(jax.profiler.trace_function, name="event_name")
def f(x):
    return jnp.dot(x, x.T).block_until_ready()

f(jnp.ones((1000, 1000))""",
        """CODE.enable_foo = config.define_bool_state(
    name='jax_enable_foo',
    default=False,
    help='Enable foo.')

with enable_foo(True):
    ...
""",
        """CODE.import jax

while global_step < NUM_STEPS:
    with jax.profiler.StepTraceContext("train", step_num=global_step):
        train_step()
        global_step += 1
""",
        """CODE.with jax.profiler.TraceContext("acontext"):
    jnp.dot(x, x.T).block_until_ready()""" .

<DEPENDENCY.jax==0.2.12> <CONTAINS> """CODE.@jax.profiler.annotate_function
def f(x):
    return jnp.dot(x, x.T).block_until_ready()

@partial(jax.profiler.trace_function, name="event_name")
def f(x):
    return jnp.dot(x, x.T).block_until_ready()""",
        """CODE.import jax

while global_step < NUM_STEPS:
    with jax.profiler.StepTraceAnnotation("train", step_num=global_step):
        train_step()
        global_step += 1
""",
        """CODE.val = jnp.uint32(0xFFFFFFFF)
val.astype('int32')

_convert_and_clip_integer(val, 'int32')""",
        """CODE.with jax.profiler.TraceAnnotation("my_label"):
    jnp.dot(x, x.T).block_until_ready()""" .

<DEPENDENCY.jax==0.2.13> <CONTAINS> """CODE.data = jnp.arange(6)
segment_ids = jnp.array([0, 0, 1, 1, 2, 2])
segment_max(data, segment_ids)

from jax import jit
jit(segment_max, static_argnums=2)(data, segment_ids, 3)
""",
        """CODE.data = jnp.arange(6)
segment_ids = jnp.array([0, 0, 1, 1, 2, 2])
segment_min(data, segment_ids)

from jax import jit
jit(segment_min, static_argnums=2)(data, segment_ids, 3)
""" .

<DEPENDENCY.jax==0.2.14> <CONTAINS> """CODE.with loop('l', 4):
    out = xmap(
      lambda x: jnp.sin(x) * 5,
      in_axes=['i'], out_axes=['i'],
      axis_resources={'i': 'l'})(x)""" .

<DEPENDENCY.jax==0.2.19> <CONTAINS> """CODE.x = np.arange(16).reshape(4,4)
y = jax.pmap(lambda x: jax.lax.psum_scatter(x, 'i'), axis_name='i')(x)
y = jax.pmap(lambda x: jax.lax.psum_scatter(x, 'i', tiled=True), axis_name='i')(x)
def f(x):
...   return jax.lax.psum_scatter(
...       x, 'i', axis_index_groups=[[0, 2], [3, 1]], tiled=True)
y = jax.pmap(f, axis_name='i')(x)""" .

<DEPENDENCY.jax==0.2.20> <CONTAINS> """CODE.@sparse.sparsify
def f(M, v):
  return 2 * M.T @ v

M = sparse.BCOO.fromdense(jnp.arange(12).reshape(3, 4))

v = jnp.array([3, 4, 2])

f(M, v)""",
        """CODE.import jax
x = jax.numpy.array([1., 2., 3.])
jax.device_get(x)

import jax
jax.device_get(1)
""" .

<DEPENDENCY.jax==0.2.26> <CONTAINS> """CODE.jax.distributed.initialize('10.0.0.1:1234', 2, 0)  # doctest: +SKIP
jax.distributed.initialize('10.0.0.1:1234', 2, 1)  # doctest: +SKIP""" .

<DEPENDENCY.jax==0.2.27> <CONTAINS> """CODE.# Logical mesh is (hosts, devices)
assert global_mesh.shape == {'x': 4, 'y': 8}

global_input_shape = (64, 32)
mesh_axes = P('x', 'y')

# Dummy example data; in practice we wouldn't necessarily materialize global data
# in a single process.
global_input_data = np.arange(
    np.prod(global_input_shape)).reshape(global_input_shape)

def get_local_data_slice(index):
  # index will be a tuple of slice objects, e.g. (slice(0, 16), slice(0, 4))
  # This method will be called per-local device from the GSDA constructor.
  return global_input_data[index]

gda = GlobalDeviceArray.from_callback(
        global_input_shape, global_mesh, mesh_axes, get_local_data_slice)

f = pjit(lambda x: x @ x.T, out_axis_resources = P('y', 'x'))

with mesh(global_mesh.shape, global_mesh.axis_names):
  out = f(gda)

print(type(out))  # GlobalDeviceArray
print(out.shape)  # global shape == (64, 64)
print(out.local_shards[0].data)  # Access the data on a single local device,
                                 # e.g. for checkpointing
print(out.local_shards[0].data.shape)  # per-device shape == (8, 16)
print(out.local_shards[0].index) # Numpy-style index into the global array that
                                 # this data shard corresponds to

# `out` can be passed to another pjit call, out.local_shards can be used to
# export the data to non-jax systems (e.g. for checkpointing or logging), etc.
""",
        """CODE.import jax
import jax.numpy as jnp

@jax.jit
def f(x):
  with jax.ensure_compile_time_eval():
    y = jnp.sin(3.0)
    z = jnp.sin(y)
  if z > 0:  # the value of z is availble and can be used in control flow
    return jnp.sin(x)
  else:
    return jnp.cos(x)

import jax
import jax.numpy as jnp
from jax import random

@jax.jit
def jax_fn(x):
  with jax.ensure_compile_time_eval():
    y = random.randint(random.PRNGKey(0), (1000,1000), 0, 100)
  y2 = y @ y
  x2 = jnp.sum(y2) * x
  return x2""" .

<DEPENDENCY.jax==0.2.28> <CONTAINS> """CODE.global_input_shape = (8, 2)
global_input_data = np.arange(
    prod(global_input_shape)).reshape(global_input_shape)
def batched_cb(indices):
    self.assertEqual(len(indices),len(global_mesh.local_devices))
    return [global_input_data[index] for index in indices]
gda = GlobalDeviceArray.from_batched_callback(global_input_shape, global_mesh, mesh_axes, batched_cb)""",
        """CODE.global_input_shape = (8, 2)
global_input_data = np.arange(prod(global_input_shape)).reshape(global_input_shape)
def cb(index):
    return global_input_data[index]
gda = GlobalDeviceArray.from_callback(global_input_shape, global_mesh, mesh_axes, cb)""",
        """CODE.global_input_shape = (8, 2)
global_input_data = np.arange(prod(global_input_shape), dtype=np.float32).reshape(global_input_shape)
def cb(cb_inp):
    self.assertLen(cb_inp, len(global_mesh.local_devices))
    dbs = []
    for inp in cb_inp:
        index, devices = inp
        array = global_input_data[index]
        dbs.extend([jax.device_put(array, device) for device in devices])
    return dbs
gda = GlobalDeviceArray.from_batched_callback_with_devices(global_input_shape, global_mesh, mesh_axes, cb)""" .

<DEPENDENCY.jax==0.2.6> <CONTAINS> "CODE.np.prod(jax2tf.shape_as_value(x))" .

<DEPENDENCY.jax==0.2.9> <CONTAINS> """CODE.def cond(pred, true_fun, false_fun, operand):
    if pred:
        return true_fun(operand)
    else:
        return false_fun(operand)

jax.lax.cond(
    get_predicate_value(),
    lambda _: 23,
    lambda _: 42,
    operand=None)""",
        """CODE.with disable_x64():
  ...   print(jnp.arange(10.0).dtype)""",
        """CODE.with enable_x64():
  ...   print(jnp.arange(10.0).dtype)""" .

<DEPENDENCY.jax==0.3.0> <CONTAINS> """CODE.@jax.jit
def f(x):
  y = jnp.sin(x)
  return x+y
err, out = checkify.checkify(f, errors=checkify.float_checks)(jnp.inf)
err.throw()  # doctest: +IGNORE_EXCEPTION_DETAIL
""",
        """CODE.ann_recall(result_neighbors, ground_truth_neighbors)
ann_recall(result_neighbors, ground_truth_neighbors[:,0:10])""",
        """CODE.def check_error(err: Error) -> None:
    err.throw()  # can raise ValueError

error, out = jax.jit(checked_f)(x)
checkify.check_error(error)""",
        """CODE.def f(x):
    checkify.check(x!=0, "cannot be zero!")
    return 1/x

checked_f = checkify.checkify(f)
err, out = jax.jit(checked_f)(0)
err.throw()  # doctest: +IGNORE_EXCEPTION_DETAIL""",
        """CODE.xmap(f, in_axes=['i'], out_axes=[i], axis_resources={'i': SerialLoop(20)})(x)
xmap(h, in_axes=(['i'], ['j']), out_axes=['i', 'j'],
     axis_resources={'i': SerialLoop(20), 'j': SerialLoop(20)})(x, y)
""" .

<DEPENDENCY.jax==0.3.1> <CONTAINS> """CODE.ann_recall(result_neighbors, ground_truth_neighbors)
ann_recall(result_neighbors, ground_truth_neighbors[:,0:10])""",
        "CODE.jax.ops.index_add(x, jnp.index_exp[2:4, 3:], 6.)",
        "CODE.jax.ops.index_update(x, jnp.index_exp[::2, 3:], 6.)",
        """CODE.x = jax.numpy.ones((5, 6))
jax.ops.index_max(x, jnp.index_exp[2:4, 3:], 6.)
""",
        """CODE.x[idx] *= y
x = jax.numpy.ones((5, 6))
jax.ops.index_mul(x, jnp.index_exp[2:4, 3:], 6.)""",
        """CODE.x[idx] = minimum(x[idx], y)
x = jax.numpy.ones((5, 6))
jax.ops.index_min(x, jnp.index_exp[2:4, 3:], 0.)""" .

<DEPENDENCY.jax==0.3.14> <CONTAINS> """CODE.
sequence_mask([1, 2], 3)
[[True, False, False],
 [True, True, False]]
""",
        """CODE.import jax

@jax.jit
def layer(w, x):
  with jax.named_scope("dot_product"):
    logits = w.dot(x)
  with jax.named_scope("activation"):
    return jax.nn.relu(logits)
""",
        """CODE.inputs = [[1, 0, 0],
          [2, 3, 0]
          [4, 5, 6]]
lengths = [1, 2, 3]
flip_sequences(inputs, lengths) = [[1, 0, 0],
                                   [3, 2, 0],
                                   [6, 5, 4]]
""" .

<DEPENDENCY.jax==0.3.15> <CONTAINS> """CODE.def for_loop(nsteps, body, init_state):
  refs = tree_map(make_ref, init_state)
  for i in range(nsteps):
    body(i, refs)
  return tree_map(ref_get, refs)
""",
        """CODE.for i in range(5):
    s.out += 1
""",
        """CODE.with Scope() as s:
    s.data = 0.
    for i in s.range(5):
      s.data += 1.
    return s.data""" .

<DEPENDENCY.jax==0.3.19> <CONTAINS> """CODE.import jax
import jax.numpy as jnp
from jax.experimental.maps import Mesh
from jax.experimental.pjit import PartitionSpec, pjit

x = jnp.arange(8, dtype=jnp.float32)
def f_(x):
    x = jnp.sin(x)
    jax.debug.inspect_array_sharding(x, callback=print)
    return jnp.square(x)
f = pjit(f_, in_axis_resources=PartitionSpec('dev'),
         out_axis_resources=PartitionSpec('dev'))
with Mesh(jax.devices(), ('dev',)):
    f.lower(x).compile()  # doctest: +SKIP
""" .

<DEPENDENCY.jax==0.3.2> <CONTAINS> """CODE.h0, h1, h2 = 0.5**3., 3.*0.5**2., 6.*0.5
f, df, ddf = np.sin, np.cos, lambda *args: -np.sin(*args)
f0, (f1, f2) =  jet(f, (h0,), ((h1, h2),))
print(f0,  f(h0))
print(f1, df(h0) * h1)
print(f2, ddf(h0) * h1 ** 2 + df(h0) * h2""",
        """CODE.import jax, jax.numpy as jnp
initializer = jax.nn.initializers.constant(-7)
initializer(jax.random.PRNGKey(42), (2, 3), jnp.float32)""",
        """CODE.import jax, jax.numpy as jnp
initializer = jax.nn.initializers.glorot_normal()
initializer(jax.random.PRNGKey(42), (2, 3), jnp.float32)  # doctest: +SKIP""",
        """CODE.import jax, jax.numpy as jnp
initializer = jax.nn.initializers.glorot_uniform()
initializer(jax.random.PRNGKey(42), (2, 3), jnp.float32)  # doctest: +SKIP""",
        """CODE.import jax, jax.numpy as jnp
initializer = jax.nn.initializers.lecun_normal()
initializer(jax.random.PRNGKey(42), (2, 3), jnp.float32)  # doctest: +SKIP
""",
        """CODE.import jax, jax.numpy as jnp
initializer = jax.nn.initializers.lecun_uniform()
initializer(jax.random.PRNGKey(42), (2, 3), jnp.float32)  # doctest: +SKIP""",
        """CODE.import jax, jax.numpy as jnp
jax.nn.initializers.ones(jax.random.PRNGKey(42), (3, 2), jnp.float32)""",
        """CODE.import jax, jax.numpy as jnp
jax.nn.initializers.zeros(jax.random.PRNGKey(42), (2, 3), jnp.float32)""" .

<DEPENDENCY.jax==0.3.24> <CONTAINS> "CODE.__getattr__, __dir__, __all__ = lazy_loader.attach(__name__, [\"sub1\", \"sub2\"])" .

<DEPENDENCY.jax==0.3.25> <CONTAINS> """CODE.arrays = [
    jax.device_put(inp_data[index], d)
    for d, index in sharding.addressable_devices_indices_map(shape).items()]
arr = jax.make_array_from_single_device_arrays(shape, sharding, arrays)
arr.addressable_data(0).shape
(4, 2)
""",
        """CODE.from jax.experimental.maps import Mesh
from jax.experimental import PartitionSpec as P
import numpy as np

input_shape = (8, 8)
global_input_data = np.arange(prod(input_shape)).reshape(input_shape)
global_mesh = Mesh(np.array(jax.devices()).reshape(2, 4), ('x', 'y'))
inp_sharding = jax.sharding.NamedSharding(global_mesh, P('x', 'y'))

def cb(index):
 return global_input_data[index]

arr = jax.make_array_from_callback(input_shape, inp_sharding, cb)
arr.addressable_data(0).shape
""",
        "CODE.named_sharding = jax.sharding.NamedSharding(mesh, spec)",
        "CODE.single_device_sharding = jax.sharding.SingleDeviceSharding(jax.devices()[0])" .

<DEPENDENCY.jax==0.3.7> <CONTAINS> """CODE.c = xc.XlaBuilder("example")
p0 = parameter(c, 1, xc.shape_from_pyval(jnp.ones([1])))
p1 = parameter(c, 2, xc.shape_from_pyval(jnp.ones([2])))
p2 = parameter(c, 3, xc.shape_from_pyval(jnp.ones([3]))
o = xops.Tuple(c, [p0, p1, p2])
flatten_shape(c.GetShape(o))
flatten_shape(c.GetShape(p0))""",
        """CODE.devices = np.array(jax.devices())[:4].reshape((2, 2)
with mesh(devices, ('x', 'y')):
    distributed_out = xmap(
      jnp.vdot,
      in_axes=({0: 'left', 1: 'right'}),
      out_axes=['left', 'right', ...],
      axis_resources={'left': 'x', 'right': 'y'})(x, x.T)""" .

<DEPENDENCY.jax==0.3.8> <CONTAINS> """CODE.from jax.experimental.maps import Mesh
from jax.experimental.pjit import pjit
from jax.experimental import PartitionSpec as P
import numpy as np

inp = np.arange(16).reshape((8, 2))
devices = np.array(jax.devices()).reshape(4, 2)

global_mesh = Mesh(devices, ('x', 'y'))
with global_mesh:
  out = pjit(lambda x: x, in_axis_resources=None, out_axis_resources=None)(inp)

with Mesh(devices, ('x', 'y')) as global_mesh:
  out = pjit(lambda x: x, in_axis_resources=None, out_axis_resources=None)(inp)

global_mesh = Mesh(devices, ('x', 'y'))
with global_mesh as m:
  out = pjit(lambda x: x, in_axis_resources=None, out_axis_resources=None)(inp)

with Mesh(devices, ('x', 'y')):
  out = pjit(lambda x: x, in_axis_resources=None, out_axis_resources=None)(inp)""" .

<DEPENDENCY.jax==0.3.9> <CONTAINS> """CODE.manager = GlobalAsyncCheckpointManager()

train_state = manager.deserialize(...)

while ...:
  if step % num_steps_between_checkpoints == 0:
    manager.serialize(train_state)
    train_state = train_step(train_state, input)
    manager.check_for_errors()

manager.serialize(train_state)
manager.wait_until_finished()""" .

<DEPENDENCY.jax==0.4.14> <CONTAINS> """CODE.from jax import random
from jax import dtypes
key = random.key(0)
jnp.issubdtype(key.dtype, dtypes.prng_key)""",
        """CODE.from jax import random
from jax._src import dtypes
key = random.key(0)
jnp.issubdtype(key.dtype, dtypes.extended)""" .

<DEPENDENCY.jax==0.4.16> <CONTAINS> """CODE.with set_env(my_var=None, other_var='some_value'):
    print("my_var is set:", 'my_var' in os.environ)
    print("other_var =", os.environ['other_var'])""" .

<DEPENDENCY.jax==0.4.19> <CONTAINS> """CODE.config.define_bool_state(
      name='jax_enable_foo',
      default=False,
      help='Enable foo.'
)""" .

<DEPENDENCY.jax==0.4.2> <CONTAINS> """CODE.def f(x):
    checkify.debug_check(x!=0, "cannot be zero!")
    return x
""" .

<DEPENDENCY.jax==0.4.20> <CONTAINS> """CODE.config.define_bool_state(
      name='jax_enable_foo',
      default=False,
      help='Enable foo.')

with enable_foo(True):
    ...""" .

<DEPENDENCY.jax==0.4.21> <CONTAINS> """CODE.def cpu_code(*args): ...
def tpu_code(*args): ...
def other_platforms_code(*args): ...
res = platform_dependent(*args, cpu=cpu_code, tpu=tpu_code,
                         default=other_platforms_code)
""" .

<DEPENDENCY.jax==0.4.24> <CONTAINS> """CODE.from absl import app
import jax
...

if __name__ == '__main__':
  jax.config.config_with_absl()
  app.run(main)
""" .

<DEPENDENCY.jax==0.4.6> <CONTAINS> """CODE.@register_pytree_with_keys_class
class Special:
    def __init__(self, x, y):
        self.x = x
        self.y = y
    def tree_flatten_with_keys(self):
        return (((GetAttrKey('x'), self.x), (GetAttrKey('y'), self.y)), None)
    @classmethod
    def tree_unflatten(cls, aux_data, children):
        return cls(*children)""",
        """CODE.def cb(cb_inp):
    dbs = []
    for inp in cb_inp:
        index, devices = inp
        array = global_input_data[index]
        dbs.extend([jax.device_put(array, device) for device in devices])
    return dbs

gda = GlobalDeviceArray.from_batched_callback_with_devices(
    global_input_shape, global_mesh, mesh_axes, cb)

gda.addressable_data(0).shape
(1, 2)
""",
        """CODE.def cb(index):
    return global_input_data[index]

gda = GlobalDeviceArray.from_callback(global_input_shape, global_mesh, mesh_axes, cb)
gda.addressable_data(0).shape
(4, 2)""",
        """CODE.global_input_shape = (8, 2)
mesh_axes = P('x')
global_mesh = Mesh(np.array(jax.devices()).reshape(4, 2), ('x', 'y'))
global_input_data = np.arange(math.prod(global_input_shape)).reshape(global_input_shape)

def batched_cb(indices):
  assert len(indices) == len(global_mesh.local_devices)
  return [global_input_data[index] for index in indices]

gda = GlobalDeviceArray.from_batched_callback(global_input_shape, global_mesh, mesh_axes, batched_cb)
gda.addressable_data(0).shape
(2, 2)
""" .

<DEPENDENCY.jax==0.4.9> <CONTAINS> """CODE.import jax
from jax.experimental import jax2tf
from jax import numpy as jnp

f = lambda A, x: jnp.sin(jnp.dot(A, x))
A = jax.ShapeDtypeStruct((2000, 3000), jnp.float32)
x = jax.ShapeDtypeStruct((3000, 1000), jnp.float32)
out_spec, out_poly_shape = jax2tf.eval_polymorphic_shape(f, polymorphic_shapes=["a, b", "b, c"])(A, x)
print(out_spec.shape)
print(out_poly_shape)
res_spec, res_poly_shape = jax2tf.eval_polymorphic_shape(lambda x: x.T, polymorphic_shapes=[out_poly_shape])(out_spec)
print(res_poly_shape)
""" .

<DEPENDENCY.jaxlib==0.4.14> <CONTAINS> """CODE.from jax import random
from jax import dtypes
key = random.key(0)
jnp.issubdtype(key.dtype, dtypes.prng_key)""",
        """CODE.from jax import random
from jax._src import dtypes
key = random.key(0)
jnp.issubdtype(key.dtype, dtypes.extended)
True""" .

<DEPENDENCY.jaxlib==0.4.16> <CONTAINS> """CODE.with set_env(my_var=None, other_var='some_value'):
    print("my_var is set:", 'my_var' in os.environ)
    print("other_var =", os.environ['other_var'])""" .

<DEPENDENCY.jaxlib==0.4.19> <CONTAINS> """CODE.enable_foo = config.define_bool_state(
    name='jax_enable_foo',
    default=False,
    help='Enable foo.')

with enable_foo(True):
    ...
""" .

<DEPENDENCY.jaxlib==0.4.2> <CONTAINS> """CODE.import jax
import jax.numpy as jnp
from jax.experimental import checkify
def f(x):
  checkify.debug_check(x!=0, "cannot be zero!")
  return x
_ = f(0)  # running without checkify means no debug_check is run.
checked_f = checkify.checkify(f)
err, out = jax.jit(checked_f)(0)  # running with checkify runs debug_check.
err.throw()  # doctest: +IGNORE_EXCEPTION_DETAIL
""" .

<DEPENDENCY.jaxlib==0.4.21> <CONTAINS> """CODE.def cpu_code(*args): ...
def tpu_code(*args): ...
def other_platforms_code(*args): ...
res = platform_dependent(*args, cpu=cpu_code, tpu=tpu_code,
                         default=other_platforms_code)
""" .

<DEPENDENCY.jaxlib==0.4.24> <CONTAINS> """CODE.from absl import app
import jax
...

if __name__ == '__main__':
  jax.config.config_with_absl()
  app.run(main)
""" .

<DEPENDENCY.jaxlib==0.4.6> <CONTAINS> """CODE.@register_pytree_with_keys_class
class Special:
    def __init__(self, x, y):
        self.x = x
        self.y = y
    def tree_flatten_with_keys(self):
        return (((GetAttrKey('x'), self.x), (GetAttrKey('y'), self.y)), None)
    @classmethod
    def tree_unflatten(cls, aux_data, children):
        return cls(*children)""",
        """CODE.def cb(cb_inp):
    dbs = []
    for inp in cb_inp:
        index, devices = inp
        array = global_input_data[index]
        dbs.extend([jax.device_put(array, device) for device in devices])
    return dbs

gda = GlobalDeviceArray.from_batched_callback_with_devices(
    global_input_shape, global_mesh, mesh_axes, cb)

gda.addressable_data(0).shape
(1, 2)
""",
        """CODE.global_input_shape = (8, 2)
mesh_axes = P('x')
global_mesh = Mesh(np.array(jax.devices()).reshape(4, 2), ('x', 'y'))
global_input_data = np.arange(math.prod(global_input_shape)).reshape(global_input_shape)

def batched_cb(indices):
    assert len(indices) == len(global_mesh.local_devices)
    return [global_input_data[index] for index in indices]

gda = GlobalDeviceArray.from_batched_callback(global_input_shape, global_mesh, mesh_axes, batched_cb)
gda.addressable_data(0).shape
""",
        """CODE.global_input_shape = (8, 8)
mesh_axes = P('x', 'y')
global_mesh = Mesh(np.array(jax.devices()).reshape(2, 4), ('x', 'y'))
global_input_data = np.arange(math.prod(global_input_shape)).reshape(global_input_shape)

def cb(index):
    return global_input_data[index]

gda = GlobalDeviceArray.from_callback(global_input_shape, global_mesh, mesh_axes, cb)
gda.addressable_data(0).shape
""" .

<DEPENDENCY.jedi==0.14.0> <CONTAINS> """CODE.from os.path import abspath
transform_path_to_dotted([abspath("/foo")], abspath('/foo/bar/baz.py'))
(('bar', 'baz'), False)""" .

<DEPENDENCY.jedi==0.15.2> <CONTAINS> """CODE.class A(object):
    @underscore_memoization
    def x(self):
        return 10""" .

<DEPENDENCY.jedi==0.16.0> <CONTAINS> """CODE.def f(a, b=1):
    "Document for function f."

print(doc)
print(script.infer(1, len('def f'))[0].docstring())
print(script.infer(1, len('def f'))[0].docstring(raw=True))""",
        """CODE.from jedi import Script
source = 'import json'
script = Script(source, path='example.py')
d = script.infer()[0]
print(d.module_name)  # doctest: +ELLIPSIS""",
        """CODE.import os
os.path.join

from jedi import Script
source = '''
import os
os.path.join'''
script = Script(source, path='example.py')
print(script.infer(3, len('os.path.join'))[0].full_name)""" .

<DEPENDENCY.jedi==0.17.0> <CONTAINS> """CODE.def f(a, b=1):
    "Document for function f."

print(doc)
print(script.infer(1, len('def f'))[0].docstring())
print(script.infer(1, len('def f'))[0].docstring(raw=True))""",
        """CODE.from jedi import Script
source = 'import json'
script = Script(source, path='example.py')
d = script.infer()[0]
print(d.module_name)  # doctest: +ELLIPSIS""",
        """CODE.import os
os.path.join

from jedi import Script
source = '''
import os
os.path.join'''
script = Script(source, path='example.py')
print(script.infer(3, len('os.path.join'))[0].full_name)""" .

<DEPENDENCY.jedi==0.7.0> <CONTAINS> """CODE._strip_rest_role(':class:`ClassName`')
_strip_rest_role(':py:obj:`module.Object`')
_strip_rest_role('ClassName')""",
        """CODE.from os.path import join
namespace = locals()
script = Interpreter('join().up', [namespace])
print(script.completions()[0].name)""",
        """CODE.try:
    from jedi.utils import setup_readline
    setup_readline()
except ImportError:
    # Fallback to the stdlib readline completer if it is installed.
    # Taken from http://docs.python.org/2/library/rlcompleter.html
    print("Jedi is not installed, falling back to readline")
    try:
        import readline
        import rlcompleter
        readline.parse_and_bind("tab: complete")
    except ImportError:
        print("Readline is not installed either. No tab completion is enabled.")

ran<TAB> # doctest: +SKIP

range(10).cou<TAB> # doctest: +SKIP
""" .

<DEPENDENCY.jedi==0.8.0> <CONTAINS> """CODE.@underscore_memoization
def x(self):
    return 10""",
        """CODE._strip_rst_role(':class:`ClassName`')
_strip_rst_role(':py:obj:`module.Object`')
_strip_rst_role('ClassName')""",
        """CODE.def f(a, b=1):
    "Document for function f."

script = Script(source, 1, len('def f'), 'example.py')
doc = script.goto_definitions()[0].docstring()
print(doc)
print(script.goto_definitions()[0].docstring(raw=True))""",
        """CODE.repr(Token(1, "test", (1, 1)))
"<Token: ('NAME', 'test', (1, 1))>"
Token(1, 'bar', (3, 4)).__getstate__()
(1, 'bar', 3, 4)
a = Token(0, 'baz', (0, 0))
a.__setstate__((1, 'foo', 3, 4))
a
<Token: ('NAME', 'foo', (3, 4))>
a.start_pos
(3, 4)
a.string
'foo'
a._start_pos_col
4
Token(1, u("ð·"), (1 ,1)).string + "p" == u("ð·p")
True""" .

<DEPENDENCY.jedi==0.8.1> <CONTAINS> """CODE.from jedi._compatibility import u
from jedi.parser import Parser
parser = Parser(u('''
a = x
b = y
b.c = z
'''))
parser.module.get_defined_names()""",
        """CODE.from jedi._compatibility import u
from jedi.parser import Parser
parser = Parser(u('''
x = ['a', 'b', 'c']
def func():
    y = None
'''))
scope = parser.module.subscopes[0]

from jedi.evaluate import Evaluator
pairs = list(get_names_of_scope(Evaluator(), scope))
pairs[0]
pairs[1]
pairs[2]
pairs[3]                                        #doctest: +ELLIPSIS
""",
        """CODE.repr(Token(1, "test", (1, 1)))
"<Token: ('NAME', 'test', (1, 1))>"
Token(1, 'bar', (3, 4)).__getstate__()
(1, 'bar', 3, 4)
a = Token(0, 'baz', (0, 0))
a.__setstate__((1, 'foo', 3, 4))
a
<Token: ('NAME', 'foo', (3, 4))>
a.start_pos
(3, 4)
a.string
'foo'
a._start_pos_col
4
Token(1, u("ð·"), (1 ,1)).string + "p" == u("ð·p")
True""" .

<DEPENDENCY.jiwer==3.0.0> <CONTAINS> """CODE.import jiwer

sentences = [" this is an example ", "  hello goodbye  ", "  "]

print(jiwer.Strip()(sentences))
# prints: ['this is an example', "hello goodbye", ""]
# note that there is an empty string left behind which might need to be cleaned up
""",
        """CODE.import jiwer

sentences = ["You're PRETTY"]

print(jiwer.ToLowerCase()(sentences))""",
        """CODE.import jiwer

sentences = ["You're amazing"]

print(jiwer.ToUpperCase()(sentences))""",
        """CODE.import jiwer

sentences = ["hi", "this is an example"]

print(jiwer.ReduceToListOfListOfChars()(sentences))
# prints: [['h', 'i'], ['t', 'h', 'i', 's', ' ', 'i', 's', ' ', 'a', 'n', ' ', 'e', 'x', 'a', 'm', 'p', 'l', 'e']]
""",
        """CODE.import jiwer

sentences = ["is the world doomed or loved?", "edibles are allegedly cultivated"]

print(jiwer.SubstituteRegexes({r"doom": r"sacr", r"(\\w+)ed": r""})(sentences))""",
        """CODE.import jiwer

sentences = ["she'll make sure you can't make it", "let's party!"]

print(jiwer.ExpandCommonEnglishContractions()(sentences))
# prints: ["she will make sure you can not make it", "let us party!"]
""",
        """CODE.import jiwer

sentences = ["this is an example!", "hello. goodbye"]

print(jiwer.RemovePunctuation()(sentences))
# prints: ['this is an example', "hello goodbye"]
""",
        """CODE.import jiwer

sentences = ["this is an example", "hello world "]

print(jiwer.RemoveWhiteSpace()(sentences))
# prints: ["thisisanexample", "helloworld"]

print(jiwer.RemoveWhiteSpace(replace_by_space=True)(sentences))
# prints: ["this is an example", "hello world  "]
# note the trailing spaces
""",
        """CODE.import jiwer

sentences = ["yhe awesome", "the apple is not a pear", "yhe"]

print(jiwer.RemoveSpecificWords(["yhe", "the", "a"])(sentences))
# prints: ['  awesome', '  apple is not   pear', ' ']
# note the extra spaces
""",
        """CODE.import jiwer

sentences = ["you <unk> like [laugh]"]

print(jiwer.RemoveKaldiNonWords()(sentences))

# prints: ["you  like "]
# note the extra spaces
""",
        """CODE.import jiwer

sentences = ["you're pretty", "your book", "foobar"]

print(jiwer.SubstituteWords({"pretty": "awesome", "you": "i", "'re": " am", 'foo': 'bar'})(sentences))

# prints: ["i am awesome", "your book", "foobar"]
""",
        """CODE.import jiwer
sentences = ["", "this is an example", " ",  "                "]
print(jiwer.RemoveEmptyStrings()(sentences))""",
        """CODE.import jiwer
sentences = ["this is   an   example ", "  hello goodbye  ", "  "]
print(jiwer.RemoveMultipleSpaces()(sentences))""",
        """CODE.python3
import jiwer

jiwer.Compose([
    jiwer.RemoveMultipleSpaces(),
    jiwer.ReduceToListOfListOfWords()
])
""" .

<DEPENDENCY.jsonschema==0.8.0> <CONTAINS> """CODE.validate([2, 3, 4], {"maxItems" : 2})
""" .

<DEPENDENCY.jsonschema==4.3.0> <CONTAINS> """CODE.schema = {
...     "type" : "array",
...     "items" : {"enum" : [1, 2, 3]},
...     "maxItems" : 2,
... }
v = Draft3Validator(schema)
for error in sorted(v.iter_errors([2, 3, 4]), key=str):
...     print(error.message)
4 is not one of [1, 2, 3]
[2, 3, 4] is too long""",
        """CODE.schema = {"maxItems" : 2}
Draft3Validator(schema).is_valid([2, 3, 4])""",
        """CODE.schema = {"maxItems" : 2}
Draft3Validator(schema).validate([2, 3, 4])""",
        """CODE.validator = Draft202012Validator({})
validator.evolve(schema={"type": "number"})""" .

<DEPENDENCY.keras==0.1.3> <CONTAINS> """CODE.tokenize('Bob dropped the apple. Where is the apple?')
['Bob', 'dropped', 'the', 'apple', '.', 'Where', 'is', 'the', 'apple', '?']""" .

<DEPENDENCY.keras==0.3.1> <CONTAINS> """CODE.def generate_arrays_from_file(path):
    while 1:
        f = open(path)
        for line in f:
            # create numpy arrays of input data
            # and labels, from each line in the file
            x, y = process_line(line)
            yield x, y
        f.close()

model.fit_generator(generate_arrays_from_file('/my_file.txt'),
                    samples_per_epoch=10000, nb_epoch=10)
""",
        """CODE.def generate_arrays_from_file(path):
    while 1:
        f = open(path)
        for line in f:
            # create numpy arrays of input data
            # and labels, from each line in the file
            x1, x2, y = process_line(line)
            yield {'input_1': x1, 'input_2': x2, 'output': y}
        f.close()

graph.fit_generator(generate_arrays_from_file('/my_file.txt'),
                    samples_per_epoch=10000, nb_epoch=10)
""" .

<DEPENDENCY.keras==0.3.3> <CONTAINS> """CODE.    from keras.layers import containers, AutoEncoder, Dense
    from keras import models

    # input shape: (nb_samples, 32)
    encoder = containers.Sequential([Dense(16, input_dim=32), Dense(8)])
    decoder = containers.Sequential([Dense(16, input_dim=8), Dense(32)])

    autoencoder = AutoEncoder(encoder=encoder, decoder=decoder, output_reconstruction=True)
    model = models.Sequential()
    model.add(autoencoder)

    # training the autoencoder:
    model.compile(optimizer='sgd', loss='mse')
    model.fit(X_train, X_train, nb_epoch=10)

    # predicting compressed representations of inputs:
    autoencoder.output_reconstruction = False  # the model has to be recompiled after modifying this property
    model.compile(optimizer='sgd', loss='mse')
    representations = model.predict(X_test)

    # the model is still trainable, although it now expects compressed representations as targets:
    model.fit(X_test, representations, nb_epoch=1)  # in this case the loss will be 0, so it's useless

    # to keep training against the original inputs, just switch back output_reconstruction to True:
    autoencoder.output_reconstruction = True
    model.compile(optimizer='sgd', loss='mse')
    model.fit(X_train, X_train, nb_epoch=10)
""",
        """CODE.model = Sequential()
model.add(TimeDistributed(Dense(8), input_shape=(10, 16)))


model = Sequential()
model.add(TimeDistributed(Convolution2D(64, 3, 3), input_shape=(10, 3, 299, 299)))
""" .

<DEPENDENCY.keras==1.0.0> <CONTAINS> """CODE.    def generate_arrays_from_file(path):
        while 1:
            f = open(path)
            for line in f:
                # create numpy arrays of input data
                # and labels, from each line in the file
                x1, x2, y = process_line(line)
                yield ({'input_1': x1, 'input_2': x2}, {'output': y})
            f.close()

    model.fit_generator(generate_arrays_from_file('/my_file.txt'),
                        samples_per_epoch=10000, nb_epoch=10)
""",
        """CODE.a = Input(shape=(32,))
b = Dense(16, activation='softmax')(a)
model = Model(input=a, output=b)
""",
        """CODE.model.add_shared_node(my_dense, name='shared_dense', inputs=['node_a', 'node_b'], ...)


model.add_shared_node(my_dense, name='shared_dense', inputs=['node_a', 'node_b'],
                      outputs=['dense_output_a', 'dense_outputs_b'])
""",
        """CODE.tensor_a = Input(shape=(32,))
tensor_b = Input(shape=(32,))
merged_tensor = merge([tensor_a, tensor_b], mode='concat', concat_axis=1)
""" .

<DEPENDENCY.keras==1.0.6> <CONTAINS> """CODE.    model = Sequential()
    model.add(LocallyConnected2D(64, 3, 3, input_shape=(3, 32, 32)))
    model.add(LocallyConnected2D(32, 3, 3))
""",
        """CODE.model = Sequential()
model.add(AtrousConvolution2D(64, 3, 3, atrous_rate=(2,2), border_mode='valid', input_shape=(3, 256, 256)))
""",
        """CODE.model = Sequential()
model.add(LocallyConnected1D(64, 3, input_shape=(10, 32)))
model.add(LocallyConnected1D(32, 3))
""" .

<DEPENDENCY.keras==1.0.7> <CONTAINS> """CODE.from keras.models import load_model

model.save('my_model.h5')  # creates a HDF5 file 'my_model.h5'
del model  # deletes the existing model

# returns a compiled model
# identical to the previous one
model = load_model('my_model.h5')
""" .

<DEPENDENCY.keras==1.0.8> <CONTAINS> """CODE.    # Crop the input 2D images or feature maps
    model = Sequential()
    model.add(Cropping2D(cropping=((2, 2), (4, 4)), input_shape=(3, 28, 28)))
    # now model.output_shape == (None, 3, 24, 20)
    model.add(Convolution2D(64, 3, 3, border_mode='same))
    model.add(Cropping2D(cropping=((2, 2), (2, 2))))
    # now model.output_shape == (None, 64, 20, 16)
""",
        """CODE.model = Sequential()
model.add(Bidirectional(LSTM(10, return_sequences=True), input_shape=(5, 10)))
model.add(Bidirectional(LSTM(10)))
model.add(Dense(5))
model.add(Activation('softmax'))
model.compile(loss='categorical_crossentropy', optimizer='rmsprop')""" .

<DEPENDENCY.keras==1.1.0> <CONTAINS> """CODE.create_ngram_set([1, 4, 9, 4, 1, 4], ngram_value=2)
{(4, 9), (4, 1), (1, 4), (9, 4)}
create_ngram_set([1, 4, 9, 4, 1, 4], ngram_value=3)
[(1, 4, 9), (4, 9, 4), (9, 4, 1), (4, 1, 4)]""",
        """CODE.def add_ngram(sequences, token_indice, ngram_range):
...     for i in range(len(sequences)):
...         for n in range(2, ngram_range + 1):
...             for j in range(len(sequences[i]) - n + 1):
...                 ngram = tuple(sequences[i][j:j + n])
...                 if ngram in token_indice:
...                     sequences[i].append(token_indice[ngram])
...     return sequences
...
sequences = [[1, 3, 4, 5], [1, 3, 7, 9, 2]]
token_indice = {(1, 3): 1337, (9, 2): 42, (4, 5): 2017}
add_ngram(sequences, token_indice, ngram_range=2)
[[1, 3, 4, 5, 1337, 2017], [1, 3, 7, 9, 2, 1337, 42]]

sequences = [[1, 3, 4, 5], [1, 3, 7, 9, 2]]
token_indice = {(1, 3): 1337, (9, 2): 42, (4, 5): 2017, (7, 9, 2): 2018}
add_ngram(sequences, token_indice, ngram_range=3)
[[1, 3, 4, 5, 1337], [1, 3, 7, 9, 2, 1337, 2018]]""",
        """CODE.model = Sequential()
model.add(AtrousConvolution1D(64, 3, atrous_rate=2, border_mode='same', input_shape=(10, 32)))
model.add(AtrousConvolution1D(32, 3, atrous_rate=2, border_mode='same'))
""" .

<DEPENDENCY.keras==1.1.1> <CONTAINS> """CODE.    def generate_arrays_from_file(path):
        while 1:
            f = open(path)
            for line in f:
                # create Numpy arrays of input data
                # and labels, from each line in the file
                x1, x2, y = process_line(line)
                yield ({'input_1': x1, 'input_2': x2, 'output': y})
            f.close()

    graph.fit_generator(generate_arrays_from_file('/my_file.txt'),
                        samples_per_epoch=10000, nb_epoch=10)
""",
        """CODE.# Print the batch number at the beginning of every batch.
batch_print_callback = LambdaCallback(on_batch_begin=lambda batch, logs: print(batch))

# Plot the loss after every epoch.
import numpy as np
import matplotlib.pyplot as plt
plot_loss_callback = LambdaCallback(on_epoch_end=lambda epoch, logs: plt.plot(np.arange(epoch), logs['loss']))

# Terminate some processes after having finished model training.
processes = ...
cleanup_callback = LambdaCallback(on_train_end=lambda logs: [p.terminate() for p in processes if p.is_alive()])

model.fit(..., callbacks=[batch_print_callback, plot_loss_callback, cleanup_callback])
""",
        """CODE.X_data = HDF5Matrix('input/file.hdf5', 'data')
model.predict(X_data)
""",
        """CODE.csv_logger = CSVLogger('training.log')
model.fit(X_train, Y_train, callbacks=[csv_logger])
""",
        """CODE.model.add_shared_node(my_dense, name='shared_dense', inputs=['node_a', 'node_b'], ...)


model.add_shared_node(my_dense, name='shared_dense', inputs=['node_a', 'node_b'],
                      outputs=['dense_output_a', 'dense_outputs_b'])
""",
        """CODE.reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2,
                              patience=5, min_lr=0.001)
model.fit(X_train, Y_train, callbacks=[reduce_lr])
""" .

<DEPENDENCY.keras==1.2.0> <CONTAINS> """CODE.    # TensorFlow example
    kvar = K.random_normal_variable((2,3), 0, 1)
    kvar
    K.eval(kvar)
""",
        """CODE.    from keras import backend as K
    b = K.placeholder((2, 2), sparse=True)
    print(K.is_sparse(b))
    c = K.to_dense(b)
    print(K.is_sparse(c))
""",
        """CODE.from keras import backend as K
K.floatx()
'float32'
K.set_floatx('float16')
K.floatx()
'float16'
""",
        """CODE.from keras import backend as K
a = K.placeholder((2, 2), sparse=False)
print(K.is_sparse(a))
b = K.placeholder((2, 2), sparse=True)
print(K.is_sparse(b))
""",
        """CODE.from keras import backend as K
np_var = numpy.array([1, 2])
K.is_keras_tensor(np_var)
keras_var = K.variable(np_var)
K.is_keras_tensor(keras_var)  # A variable is not a Tensor.
keras_placeholder = K.placeholder(shape=(2, 4, 5))
K.is_keras_tensor(keras_placeholder)  # A placeholder is a Tensor.
""",
        """CODE.keras.backend.get_uid('dense')
1
keras.backend.get_uid('dense')
2""",
        """CODE.kvar = K.random_uniform_variable((2,3), 0, 1)
K.eval(kvar)
""" .

<DEPENDENCY.keras==1.2.2> <CONTAINS> """CODE.    with CustomObjectScope({"MyObject":MyObject}):
        layer = Dense(..., W_regularizer="MyObject")
        # save, load, etc. will recognize custom object by name
""",
        """CODE.    with custom_object_scope({"MyObject":MyObject}):
        layer = Dense(..., W_regularizer="MyObject")
        # save, load, etc. will recognize custom object by name
""",
        """CODE.get_custom_objects().clear()
get_custom_objects()["MyObject"] = MyObject
""",
        """CODE.model = Sequential()
model.add(AtrousConvolution1D(64, 3, atrous_rate=2,
                              border_mode='same',
                              input_shape=(10, 32)))
model.add(AtrousConvolution1D(32, 3, atrous_rate=2,
                              border_mode='same'))
""",
        """CODE.model = Sequential()
model.add(AtrousConvolution2D(64, 3, 3, atrous_rate=(2,2),
                              border_mode='valid',
                              input_shape=(3, 256, 256)))
""",
        """CODE.model = Sequential()
model.add(Convolution1D(64, 3, border_mode='same', input_shape=(10, 32)))
model.add(Convolution1D(32, 3, border_mode='same'))
""",
        """CODE.model = Sequential()
model.add(Convolution2D(64, 3, 3,
                        border_mode='same',
                        input_shape=(3, 256, 256)))
model.add(Convolution2D(32, 3, 3, border_mode='same'))
""",
        """CODE.model = Sequential()
model.add(Deconvolution2D(3, 3, 3, output_shape=(None, 3, 14, 14),
                          border_mode='valid',
                          input_shape=(3, 12, 12)))
dummy_input = np.ones((32, 3, 12, 12))
preds = model.predict(dummy_input)
print(preds.shape)

model = Sequential()
model.add(Deconvolution2D(3, 3, 3, output_shape=(None, 3, 25, 25),
                          subsample=(2, 2),
                          border_mode='valid',
                          input_shape=(3, 12, 12)))
model.summary()
dummy_input = np.ones((32, 3, 12, 12))
preds = model.predict(dummy_input)
print(preds.shape)
""" .

<DEPENDENCY.keras==2.0.0> <CONTAINS> """CODE.from keras import backend as K
K.set_image_data_format('channels_last')
""",
        """CODE.keras.backend.image_data_format()
'channels_first'
""" .

<DEPENDENCY.keras==2.0.3> <CONTAINS> """CODE.from keras.data_utils import _hash_file
_hash_file('/path/to/file.zip')
""" .

<DEPENDENCY.keras==2.0.6> <CONTAINS> """CODE.enqueuer = SequenceEnqueuer(...)
enqueuer.start()
datas = enqueuer.get()
for data in datas:
    # Use the inputs; training, evaluating, predicting.
    # ... stop sometime.
enqueuer.close()
""",
        """CODE.from skimage.io import imread
from skimage.transform import resize
import numpy as np

class CIFAR10Sequence(Sequence):
    def __init__(self, x_set, y_set, batch_size):
        self.X,self.y = x_set,y_set
        self.batch_size = batch_size

    def __len__(self):
        return len(self.X) // self.batch_size

    def __getitem__(self,idx):
        batch_x = self.X[idx*self.batch_size:(idx+1)*self.batch_size]
        batch_y = self.y[idx*self.batch_size:(idx+1)*self.batch_size]

        return np.array([
            resize(imread(file_name), (200,200))
               for file_name in batch_x]), np.array(batch_y)
""" .

<DEPENDENCY.keras==2.0.7> <CONTAINS> """CODE.    import keras

    input1 = keras.layers.Input(shape=(16,))
    x1 = keras.layers.Dense(8, activation='relu')(input1)
    input2 = keras.layers.Input(shape=(32,))
    x2 = keras.layers.Dense(8, activation='relu')(input2)
    subtracted = keras.layers.subtract([x1, x2])

    out = keras.layers.Dense(4)(subtracted)
    model = keras.models.Model(inputs=[input1, input2], outputs=out)
""",
        """CODE.import keras

input1 = keras.layers.Input(shape=(16,))
x1 = keras.layers.Dense(8, activation='relu')(input1)
input2 = keras.layers.Input(shape=(32,))
x2 = keras.layers.Dense(8, activation='relu')(input2)
subtracted = keras.layers.Subtract()([x1, x2])

out = keras.layers.Dense(4)(subtracted)
model = keras.models.Model(inputs=[input1, input2], outputs=out)
""" .

<DEPENDENCY.keras==2.0.9> <CONTAINS> """CODE.    # First, let's define a RNN Cell, as a layer subclass.

    class MinimalRNNCell(keras.layers.Layer):

        def __init__(self, units, **kwargs):
            self.units = units
            self.state_size = units
            super(MinimalRNNCell, self).__init__(**kwargs)

        def build(self, input_shape):
            self.kernel = self.add_weight(shape=(input_shape[-1], self.units),
                                          initializer='uniform',
                                          name='kernel')
            self.recurrent_kernel = self.add_weight(
                shape=(self.units, self.units),
                initializer='uniform',
                name='recurrent_kernel')
            self.built = True

        def call(self, inputs, states):
            prev_output = states[0]
            h = K.dot(inputs, self.kernel)
            output = h + K.dot(prev_output, self.recurrent_kernel)
            return output, [output]

    # Let's use this cell in a RNN layer:

    cell = MinimalRNNCell(32)
    x = keras.Input((None, 5))
    layer = RNN(cell)
    y = layer(x)

    # Here's how to use the cell to build a stacked RNN:

    cells = [MinimalRNNCell(32), MinimalRNNCell(64)]
    x = keras.Input((None, 5))
    layer = RNN(cells)
    y = layer(x)
""",
        """CODE.    import tensorflow as tf
    from keras.applications import Xception
    from keras.utils import multi_gpu_model
    import numpy as np

    num_samples = 1000
    height = 224
    width = 224
    num_classes = 1000

    # Instantiate the base model
    # (here, we do it on CPU, which is optional).
    with tf.device('/cpu:0'):
        model = Xception(weights=None,
                         input_shape=(height, width, 3),
                         classes=num_classes)

    # Replicates the model on 8 GPUs.
    # This assumes that your machine has 8 available GPUs.
    parallel_model = multi_gpu_model(model, gpus=8)
    parallel_model.compile(loss='categorical_crossentropy',
                           optimizer='rmsprop')

    # Generate dummy data.
    x = np.random.random((num_samples, height, width, 3))
    y = np.random.random((num_samples, num_classes))

    # This `fit` call will be distributed on 8 GPUs.
    # Since the batch size is 256, each GPU will process 32 samples.
    parallel_model.fit(x, y, epochs=20, batch_size=256)
""" .

<DEPENDENCY.keras==2.1.5> <CONTAINS> """CODE.from keras.preprocessing.sequence import TimeseriesGenerator
import numpy as np

data = np.array([[i] for i in range(50)])
targets = np.array([[i] for i in range(50)])

data_gen = TimeseriesGenerator(data, targets,
                               length=10, sampling_rate=2,
                               batch_size=2)
assert len(data_gen) == 20

batch_0 = data_gen[0]
x, y = batch_0
assert np.array_equal(x,
                      np.array([[[0], [2], [4], [6], [8]],
                                [[1], [3], [5], [7], [9]]]))
assert np.array_equal(y,
                      np.array([[10], [11]]))
""" .

<DEPENDENCY.keras==2.1.6> <CONTAINS> """CODE.    def generate_arrays_from_file(path):
        while True:
            with open(path) as f:
                for line in f:
                    # create Numpy arrays of input data
                    # and labels, from each line in the file
                    x, y = process_line(line)
                    yield (x, y)

    model.fit_generator(generate_arrays_from_file('/my_file.txt'),
                        steps_per_epoch=1000, epochs=10)
""",
        """CODE.from keras.models import load_model

model.save('my_model.h5')  # creates a HDF5 file 'my_model.h5'
del model  # deletes the existing model

# returns a compiled model
# identical to the previous one
model = load_model('my_model.h5')
""",
        """CODE.from keras.preprocessing.sequence import TimeseriesGenerator
import numpy as np

data = np.array([[i] for i in range(50)])
targets = np.array([[i] for i in range(50)])

data_gen = TimeseriesGenerator(data, targets,
                               length=10, sampling_rate=2,
                               batch_size=2)
assert len(data_gen) == 20

batch_0 = data_gen[0]
x, y = batch_0
assert np.array_equal(x,
                      np.array([[[0], [2], [4], [6], [8]],
                                [[1], [3], [5], [7], [9]]]))
assert np.array_equal(y,
                      np.array([[10], [11]]))
""",
        """CODE.model = Sequential()
model.add(Dense(32, input_shape=(500,)))
model.add(Dense(10, activation='softmax'))
model.compile(optimizer='rmsprop',
              loss='categorical_crossentropy',
              metrics=['accuracy'])
""",
        """CODE.model1 = Sequential()
model1.add(Dense(32, input_dim=32))
model2 = Sequential()
model2.add(Dense(32, input_dim=32))
merged_model = Sequential()
merged_model.add(Merge([model1, model2], mode='concat', concat_axis=1))
""",
        """CODE.tensor_a = Input(shape=(32,))
tensor_b = Input(shape=(32,))
merged_tensor = merge([tensor_a, tensor_b], mode='concat', concat_axis=1)
""" .

<DEPENDENCY.keras==2.10.0> <CONTAINS> """CODE.  def step(iterator):
    data = next(iterator)
    # result <= Do something with data
    return result
  tf_step = tf.function(step, reduce_retracing=True)

  # Assume x is a tf.data Dataset.
  data_handler = data_adapter.get_data_handler(x=x)
  # Epoch iteration
  for epo_idx, iterator in data_handler.enumerate_epochs():
      # Stop on dataset exhaustion.
      with data_handler.catch_stop_iteration():
        for step in data_handler.steps(): # Step iteration
            step_result = step(iterator)
""",
        """CODE.class SubclassModel(tf.keras.Model):

  def __init__(self, name=None):
    super().__init__(name=name)
    self.d1 = tf.keras.layers.Dense(10)
    self.d2 = tf.keras.layers.Dense(20)

  def call(self, inputs):
    x = self.d1(inputs)
    return self.d2(x)

model = SubclassModel()
model(tf.zeros((10, 10)))
weight_paths = model.get_weight_paths()

inputs = tf.keras.Input((10,), batch_size=10)
x = tf.keras.layers.Dense(20, name='d1')(inputs)
output = tf.keras.layers.Dense(30, name='d2')(x)
model = tf.keras.Model(inputs, output)
d1 = model.layers[1]
d2 = model.layers[2]
weight_paths = model.get_weight_paths()
""",
        """CODE.import tensorflow as tf

# Create a tf.keras model.
model = tf.keras.Sequential()
model.add(tf.keras.layers.Dense(1, input_shape=[10]))
model.summary()

# Save the tf.keras model in the SavedModel format.
path = '/tmp/simple_keras_model'
tf.compat.v1.keras.experimental.export_saved_model(model, path)

# Load the saved keras model back.
new_model = tf.compat.v1.keras.experimental.load_from_saved_model(path)
new_model.summary()
""",
        """CODE.layout_map = layout_map_lib.LayoutMap(mesh=self.mesh)
layout_map['d1.kernel'] = layout_1
layout_map['d1.bias'] = layout_2
layout_map['d2.kernel'] = layout_3
layout_map['d2.bias'] = layout_4

## Subclassed model
class SubclassModel(tf.keras.Model):

  def __init__(self, name=None):
    super().__init__(name=name)
    self.d1 = tf.keras.layers.Dense(1000)
    self.d2 = tf.keras.layers.Dense(1000)

  def call(self, inputs):
    x = self.d1(inputs)
    return self.d2(x)

with layout_map.scope():
  model = SubclassModel()
inputs = tf.zeros((10, 10))
results = model(inputs)

model.d1.kernel.layout == layout_1
model.d1.bias.layout == layout_2
model.d2.kernel.layout == layout_3
model.d2.bias.layout == layout_4

## Functional model
with layout_map.scope():
  inputs = tf.keras.Input((10,), batch_size=10)
  x = tf.keras.layers.Dense(20, name='d1')(inputs)
  output = tf.keras.layers.Dense(30, name='d2')(x)

  model = tf.keras.Model(inputs, output)

d1 = model.layers[1]
d2 = model.layers[2]

d1.kernel.layout == layout_1
d1.bias.layout == layout_2
d1.kernel.layout == layout_3
d1.bias.layout == layout_4

## Sequential model
with layout_map.scope():
  model = tf.keras.Sequential([
      tf.keras.layers.Dense(20, name='d1', input_shape=(10,)),
      tf.keras.layers.Dense(30, name='d2')
  ])

d1 = model.layers[0]
d2 = model.layers[1]

d1.kernel.layout == layout_1
d1.bias.layout == layout_2
d1.kernel.layout == layout_3
d1.bias.layout == layout_4
""" .

<DEPENDENCY.keras==2.11.0> <CONTAINS> """CODE.class SubclassLayer(BaseImageAugmentationLayer):
  def __init__(self):
    super().__init__()
    self.auto_vectorize = False


class RandomContrast(BaseImageAugmentationLayer):

  def __init__(self, factor=(0.5, 1.5), **kwargs):
    super().__init__(**kwargs)
    self._factor = factor

  def augment_image(self, image, transformation):
    random_factor = tf.random.uniform([], self._factor[0], self._factor[1])
    mean = tf.math.reduced_mean(inputs, axis=-1, keep_dim=True)
    return (inputs - mean) * random_factor + mean
""",
        """CODE.constraint = UnitNorm()
config = constraint.get_config()
constraint = UnitNorm.from_config(config)
""",
        """CODE.import keras
import numpy as np
import tensorflow as tf

def warmstart_embedding_matrix(base_vocabulary, new_vocabulary, base_embeddings, new_embeddings_initializer):
    # code for warmstarting embedding matrix
    pass

# Example usage of warmstart_embedding_matrix util
vocab_base = tf.convert_to_tensor(["unk", "a", "b", "c"])
vocab_new = tf.convert_to_tensor(["unk", "unk", "a", "b", "c", "d", "e"])
vectorized_vocab_base = np.random.rand(vocab_base.shape[0], 3)
vectorized_vocab_new = np.random.rand(vocab_new.shape[0], 3)
warmstarted_embedding_matrix = warmstart_embedding_matrix(
    base_vocabulary=vocab_base,
    new_vocabulary=vocab_new,
    base_embeddings=vectorized_vocab_base,
    new_embeddings_initializer=keras.initializers.Constant(vectorized_vocab_new))

# Example usage of getting vocabulary and embedding weights from layers
base_vocabulary = old_text_vectorization_layer.get_vocabulary()
new_vocabulary = new_text_vectorization_layer.get_vocabulary()
embedding_weights_base = model.get_layer('embedding').get_weights()[0]
warmstarted_embedding = keras.utils.warmstart_embedding_matrix(
    base_vocabulary,
    new_vocabulary,
    base_embeddings=embedding_weights_base,
    new_embeddings_initializer="uniform")
updated_embedding_variable = tf.Variable(warmstarted_embedding)

# Update embedding layer weights and continue with model training
model.layers[1].embeddings = updated_embedding_variable
model.fit(..)""" .

<DEPENDENCY.keras==2.12.0> <CONTAINS> """CODE.# Create the artifact
model.export("path/to/location")

# Later, in a different process / environment...
reloaded_artifact = tf.saved_model.load("path/to/location")
predictions = reloaded_artifact.serve(input_data)
""",
        """CODE.# Plain float values.
FeatureSpace.float(name=None)

# Float values to be preprocessed via featurewise standardization
# (i.e. via a `keras.layers.Normalization` layer).
FeatureSpace.float_normalized(name=None)

# Float values to be preprocessed via linear rescaling
# (i.e. via a `keras.layers.Rescaling` layer).
FeatureSpace.float_rescaled(scale=1., offset=0., name=None)

# Float values to be discretized. By default, the discrete
# representation will then be one-hot encoded.
FeatureSpace.float_discretized(
    num_bins, bin_boundaries=None, output_mode="one_hot", name=None)

# Integer values to be indexed. By default, the discrete
# representation will then be one-hot encoded.
FeatureSpace.integer_categorical(
    max_tokens=None, num_oov_indices=1, output_mode="one_hot", name=None)

# String values to be indexed. By default, the discrete
# representation will then be one-hot encoded.
FeatureSpace.string_categorical(
    max_tokens=None, num_oov_indices=1, output_mode="one_hot", name=None)

# Integer values to be hashed into a fixed number of bins.
# By default, the discrete representation will then be one-hot encoded.
FeatureSpace.integer_hashed(num_bins, output_mode="one_hot", name=None)

# String values to be hashed into a fixed number of bins.
# By default, the discrete representation will then be one-hot encoded.
FeatureSpace.string_hashed(num_bins, output_mode="one_hot", name=None)
""",
        """CODE.export_archive = ExportArchive()
export_archive.track(model)
export_archive.add_endpoint(
    name="serve",
    fn=model.call,
    input_signature=[tf.TensorSpec(shape=(None, 3), dtype=tf.float32)],
)


export_archive = ExportArchive()
export_archive.track(model)
export_archive.add_endpoint(
    name="serve",
    fn=model.call,
    input_signature=[
        tf.TensorSpec(shape=(None, 3), dtype=tf.float32),
        tf.TensorSpec(shape=(None, 4), dtype=tf.float32),
    ],
)


model = keras.Model(inputs=[x1, x2], outputs=outputs)

export_archive = ExportArchive()
export_archive.track(model)
export_archive.add_endpoint(
    name="serve",
    fn=model.call,
    input_signature=[
        [
            tf.TensorSpec(shape=(None, 3), dtype=tf.float32),
            tf.TensorSpec(shape=(None, 4), dtype=tf.float32),
        ],
    ],
)


model = keras.Model(inputs={"x1": x1, "x2": x2}, outputs=outputs)

export_archive = ExportArchive()
export_archive.track(model)
export_archive.add_endpoint(
    name="serve",
    fn=model.call,
    input_signature=[
        {
            "x1": tf.TensorSpec(shape=(None, 3), dtype=tf.float32),
            "x2": tf.TensorSpec(shape=(None, 4), dtype=tf.float32),
        },
    ],
)


@tf.function()
def serving_fn(x):
    return model(x)

# The function must be traced, i.e. it must be called at least once.
serving_fn(tf.random.normal(shape=(2, 3)))

export_archive = ExportArchive()
export_archive.track(model)
export_archive.add_endpoint(name="serve", fn=serving_fn)
""",
        """CODE.export_archive = ExportArchive()
export_archive.track(model)
export_archive.add_endpoint(
    name="serve",
    fn=model.call,
    input_signature=[tf.TensorSpec(shape=(None, 3), dtype=tf.float32)],
)
export_archive.add_variable_collection(
    name="optimizer_variables", variables=model.optimizer.variables)
export_archive.write_out("path/to/location")

revived_object = tf.saved_model.load("path/to/location")
optimizer_variables = revived_object.optimizer_variables
""",
        """CODE.export_archive = ExportArchive()
export_archive.track(model)
export_archive.add_endpoint(
    name="serve",
    fn=model.call,
    input_signature=[tf.TensorSpec(shape=(None, 3), dtype=tf.float32)],
)
export_archive.write_out("path/to/location")

export_archive = ExportArchive()
export_archive.track(model)
export_archive.add_endpoint(
    name="call_inference",
    fn=lambda x: model.call(x, training=False),
    input_signature=[tf.TensorSpec(shape=(None, 3), dtype=tf.float32)],
)
export_archive.add_endpoint(
    name="call_training",
    fn=lambda x: model.call(x, training=True),
    input_signature=[tf.TensorSpec(shape=(None, 3), dtype=tf.float32)],
)
export_archive.write_out("path/to/location")
""",
        """CODE.feature_space.save("myfeaturespace.keras")
reloaded_feature_space = keras.models.load_model("myfeaturespace.keras")
""",
        """CODE.model.export("path/to/artifact")
reloaded_layer = ReloadedLayer("path/to/artifact")
outputs = reloaded_layer(inputs)
""",
        """CODE.model.export("path/to/location")
reloaded_artifact = tf.saved_model.load("path/to/location")
predictions = reloaded_artifact.serve(input_data)""" .

<DEPENDENCY.keras==2.13.1> <CONTAINS> """CODE.Wrap `keras.layers.Conv2D`:
x = np.random.rand(1, 10, 10, 1)
conv2d = SpectralNormalization(tf.keras.layers.Conv2D(2, 2))
y = conv2d(x)
y.shape
TensorShape([1, 9, 9, 2])

Wrap `keras.layers.Dense`:
x = np.random.rand(1, 10, 10, 1)
dense = SpectralNormalization(tf.keras.layers.Dense(10))
y = dense(x)
y.shape
TensorShape([1, 10, 10, 10])""",
        """CODE.b2 = beta ** 2
f_beta_score = (1 + b2) * (precision * recall) / (precision * b2 + recall)

metric = tf.keras.metrics.FBetaScore(beta=2.0, threshold=0.5)
y_true = np.array([[1, 1, 1],
                   [1, 0, 0],
                   [1, 1, 0]], np.int32)
y_pred = np.array([[0.2, 0.6, 0.7],
                   [0.2, 0.6, 0.6],
                   [0.6, 0.8, 0.0]], np.float32)
metric.update_state(y_true, y_pred)
result = metric.result()
result.numpy()
""",
        """CODE.class TimedLogIterations(keras.utils.TimedThread):
    def __init__(self, model, interval):
        self.model = model
        super().__init__(interval)

    def on_interval(self):
        # Logs Optimizer iterations every x seconds
        try:
            opt_iterations = self.model.optimizer.iterations.numpy()
            print(f"Epoch: {epoch}, Optimizer Iterations: {opt_iterations}")
        except Exception as e:
            print(str(e))  # To prevent thread from getting killed

timed_logs = TimedLogIterations(model=model, interval=5)
timed_logs.start()
try:
    model.fit(...)
finally:
    timed_logs.stop()

with TimedLogIterations(model=model, interval=5):
    model.fit(...)

class LogThreadCallback(
    keras.utils.TimedThread, keras.callbacks.Callback
):
    def __init__(self, interval):
        self._epoch = 0
        keras.utils.TimedThread.__init__(self, interval)
        keras.callbacks.Callback.__init__(self)

    def on_interval(self):
        if self.epoch:
            opt_iter = self.model.optimizer.iterations.numpy()
            logging.info(f"Epoch: {self._epoch}, Opt Iteration: {opt_iter}")

    def on_epoch_begin(self, epoch, logs=None):
        self._epoch = epoch

with LogThreadCallback(interval=5) as thread_callback:
    model.fit(..., callbacks=[thread_callback])
""",
        """CODE.def mish(x):
    return x * tanh(softplus(x))

def softplus(x):
    return log(exp(x) + 1)
""",
        """CODE.from sklearn.metrics import jaccard_score
import tensorflow as tf

class JaccardScore(tf.keras.metrics.experimental.PyMetric):

  def __init__(self, name='jaccard_score', **kwargs):
    super().__init__(name=name, **kwargs)

  def update_state(self, y_true, y_pred, sample_weight=None):
    self.jaccard_sum += jaccard_score(y_pred, y_true, average="macro")
    self.count += 1

  def reset_state(self):
    self.jaccard_sum = 0.
    self.count = 0.

  def result(self):
    return self.jaccard_sum / self.count
""",
        """CODE.model.compile(loss=..., optimizer=...,
              metrics=['accuracy'])
sidecar_evaluator = keras.utils.SidecarEvaluator(
    model=model,
    data=dataset,
    checkpoint_dir=checkpoint_dir,
    max_evaluations=1,
    callbacks=[
        SidecarEvaluatorModelExport(
            export_filepath=os.path.join(checkpoint_dir,
                                  'best_model_eval',
                                  'best-model-{epoch:04d}'),
            checkpoint_filepath=os.path.join(checkpoint_dir,
            'ckpt-{epoch:04d}'),
            save_freq="eval",
            save_weights_only=True,
            monitor="loss",
            mode="min",
            verbose=1,
        ),
    ],
)
sidecar_evaluator.start()
""",
        """CODE.y_true = [[0., 1., 0.], [0., 0., 1.]]
y_pred = [[0.05, 0.95, 0], [0.1, 0.8, 0.1]]
# Using 'auto'/'sum_over_batch_size' reduction type.
cce = tf.keras.losses.CategoricalFocalCrossentropy()
cce(y_true, y_pred).numpy()
0.23315276

# Calling with 'sample_weight'.
cce(y_true, y_pred, sample_weight=tf.constant([0.3, 0.7])).numpy()
0.1632

# Using 'sum' reduction type.
cce = tf.keras.losses.CategoricalFocalCrossentropy(
...     reduction=tf.keras.losses.Reduction.SUM)
cce(y_true, y_pred).numpy()
0.46631

# Using 'none' reduction type.
cce = tf.keras.losses.CategoricalFocalCrossentropy(
...     reduction=tf.keras.losses.Reduction.NONE)
cce(y_true, y_pred).numpy()
array([3.2058331e-05, 4.6627346e-01], dtype=float32)

model.compile(optimizer='adam',
              loss=tf.keras.losses.CategoricalFocalCrossentropy())
""" .

<DEPENDENCY.keras==2.15.0> <CONTAINS> """CODE._hash_file('/path/to/file.zip')
'e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855'
""" .

<DEPENDENCY.keras==2.2.0> <CONTAINS> """CODE.from keras.models import load_model

model.save('my_model.h5')  # creates a HDF5 file 'my_model.h5'
del model  # deletes the existing model

# returns a compiled model
# identical to the previous one
model = load_model('my_model.h5')
""" .

<DEPENDENCY.keras==2.2.1> <CONTAINS> """CODE.from keras import backend as K
K.normalize_data_format(None)
K.normalize_data_format('channels_last')
""" .

<DEPENDENCY.keras==2.2.3> <CONTAINS> """CODE.from keras.preprocessing.sequence import TimeseriesGenerator
import numpy as np

data = np.array([[i] for i in range(50)])
targets = np.array([[i] for i in range(50)])

data_gen = TimeseriesGenerator(data, targets,
                               length=10, sampling_rate=2,
                               batch_size=2)
assert len(data_gen) == 20

batch_0 = data_gen[0]
x, y = batch_0
assert np.array_equal(x,
                      np.array([[[0], [2], [4], [6], [8]],
                                [[1], [3], [5], [7], [9]]]))
assert np.array_equal(y,
                      np.array([[10], [11]]))
""",
        """CODE.from keras.utils.generic_utils import transpose_shape
transpose_shape((16, 128, 128, 32),'channels_first', spatial_axes=(1, 2))
transpose_shape((16, 128, 128, 32), 'channels_last', spatial_axes=(1, 2))
transpose_shape((128, 128, 32), 'channels_first', spatial_axes=(0, 1))
""" .

<DEPENDENCY.keras==2.2.5> <CONTAINS> """CODE.with tf_file_io_proxy('keras.engine.saving.tf_file_io') as file_io_proxy:
    gcs_filepath = file_io_proxy.get_filepath(filename='model.h5')
    save_model(model, gcs_filepath)
    file_io_proxy.assert_exists(gcs_filepath)
    new_model_gcs = load_model(gcs_filepath)
    file_io_proxy.delete_file(gcs_filepath)  # cleanup
""" .

<DEPENDENCY.keras==2.3.0> <CONTAINS> """CODE.bce = keras.losses.BinaryCrossentropy()
loss = bce([0., 0., 1., 1.], [1., 1., 1., 0.])
model = keras.Model(inputs, outputs)
model.compile('sgd', loss=keras.losses.BinaryCrossentropy())
""",
        """CODE.cce = keras.losses.CategoricalCrossentropy()
loss = cce(
    [[1., 0., 0.], [0., 1., 0.], [0., 0., 1.]],
    [[.9, .05, .05], [.5, .89, .6], [.05, .01, .94]])

model = keras.Model(inputs, outputs)
model.compile('sgd', loss=keras.losses.CategoricalCrossentropy())
""",
        """CODE.cce = keras.losses.SparseCategoricalCrossentropy()
loss = cce(
    [0, 1, 2],
    [[.9, .05, .05], [.5, .89, .6], [.05, .01, .94]])


model = keras.Model(inputs, outputs)
model.compile('sgd', loss=keras.losses.SparseCategoricalCrossentropy())
""",
        """CODE.class MeanSquaredError(Loss):
    def call(self, y_true, y_pred):
        y_pred = ops.convert_to_tensor(y_pred)
        y_true = math_ops.cast(y_true, y_pred.dtype)
        return K.mean(math_ops.square(y_pred - y_true), axis=-1)
""",
        """CODE.from keras import backend as K
val = np.array([[1, 2], [3, 4]])
kvar = K.variable(value=val)
K.size(inputs)
""",
        """CODE.from keras import backend as K
val = np.array([[1, 2], [3, 4]])
kvar = K.variable(value=val)
K.size(inputs)
<tf.Tensor: id=9, shape=(), dtype=int32, numpy=4>
""",
        """CODE.loss = 0.5 * x**2 if abs(x) <= d
loss = 0.5 * d**2 + d * (abs(x) - d) if abs(x) > d


model = keras.Model(inputs, outputs)
model.compile('sgd', loss=keras.losses.Huber())
""",
        """CODE.loss = y_pred - y_true * log(y_pred)
model = keras.Model(inputs, outputs)
model.compile('sgd', loss=keras.losses.Poisson())
""",
        """CODE.m = keras.metrics.Mean()
m.update_state([1, 3, 5, 7])
m.result()
""",
        """CODE.m = keras.metrics.Sum()
m.update_state([1, 3, 5, 7])
m.result()
""",
        """CODE.mae = keras.losses.MeanAbsoluteError()
loss = mae([0., 0., 1., 1.], [1., 1., 1., 0.])
model = keras.Model(inputs, outputs)
model.compile('sgd', loss=keras.losses.MeanAbsoluteError())""",
        """CODE.mae = keras.losses.MeanAbsoluteError()
loss = mae([0., 0., 1., 1.], [1., 1., 1., 0.])
model = keras.Model(inputs, outputs)
model.compile('sgd', loss=keras.losses.MeanAbsoluteError())
""",
        """CODE.mape = keras.losses.MeanAbsolutePercentageError()
loss = mape([0., 0., 1., 1.], [1., 1., 1., 0.])
model = keras.Model(inputs, outputs)
model.compile('sgd', loss=keras.losses.MeanAbsolutePercentageError())""",
        """CODE.mape = keras.losses.MeanAbsolutePercentageError()
loss = mape([0., 0., 1., 1.], [1., 1., 1., 0.])
model = keras.Model(inputs, outputs)
model.compile('sgd', loss=keras.losses.MeanAbsolutePercentageError())
""",
        """CODE.model = keras.Model(inputs, outputs)
model.compile(
    'sgd',
    loss='mse',
    metrics=[keras.metrics.CategoricalAccuracy()])
""",
        """CODE.model = keras.Model(inputs, outputs)
model.compile(
    'sgd',
    loss='mse',
    metrics=[keras.metrics.MeanIoU(num_classes=2)])
""",
        """CODE.model = keras.Model(inputs, outputs)
model.compile(
    'sgd',
    loss='mse',
    metrics=[keras.metrics.SensitivityAtSpecificity()])
""",
        """CODE.model = keras.Model(inputs, outputs)
model.compile(
    'sgd',
    loss='mse',
    metrics=[keras.metrics.SparseCategoricalAccuracy()])
""",
        """CODE.model = keras.Model(inputs, outputs)
model.compile(
    'sgd',
    loss='mse',
    metrics=[keras.metrics.SpecificityAtSensitivity()])
""",
        """CODE.model = keras.Model(inputs, outputs)
model.compile(
    'sgd',
    metrics=[keras.metrics.SparseTopKCategoricalAccuracy()])
""",
        """CODE.model = keras.Model(inputs, outputs)
model.compile(
  'sgd',
  loss='mse',
  metrics=[keras.metrics.CosineSimilarity(axis=1)])
""",
        """CODE.model = keras.Model(inputs, outputs)
model.compile('sgd', loss='mse', metrics=[keras.metrics.BinaryAccuracy()])
""",
        """CODE.model = keras.Model(inputs, outputs)
model.compile('sgd', loss='mse', metrics=[keras.metrics.FalseNegatives()])
""",
        """CODE.model = keras.Model(inputs, outputs)
model.compile('sgd', loss='mse', metrics=[keras.metrics.FalsePositives()])
""",
        """CODE.model = keras.Model(inputs, outputs)
model.compile('sgd', loss='mse', metrics=[keras.metrics.Precision()])
""",
        """CODE.model = keras.Model(inputs, outputs)
model.compile('sgd', loss='mse', metrics=[keras.metrics.TrueNegatives()])
""",
        """CODE.model = keras.Model(inputs, outputs)
model.compile('sgd', loss='mse', metrics=[keras.metrics.TruePositives()])
""",
        """CODE.model = keras.Model(inputs, outputs)
model.compile('sgd', loss=keras.losses.CategoricalHinge())""",
        """CODE.model = keras.Model(inputs, outputs)
model.compile('sgd', loss=keras.losses.CategoricalHinge())
""",
        """CODE.model = keras.Model(inputs, outputs)
model.compile('sgd', loss=keras.losses.Hinge())
""",
        """CODE.model = keras.Model(inputs, outputs)
model.compile('sgd', loss=keras.losses.KLDivergence())
""",
        """CODE.model = keras.Model(inputs, outputs)
model.compile('sgd', loss=keras.losses.LogCosh())
""",
        """CODE.model = keras.Model(inputs, outputs)
model.compile('sgd', loss=keras.losses.SquaredHinge())
""",
        """CODE.model = keras.Model(inputs, outputs)
model.compile('sgd', metrics=[keras.metrics.RootMeanSquaredError()])
""",
        """CODE.model = keras.Model(inputs, outputs)
model.compile('sgd', metrics=[keras.metrics.TopKCategoricalAccuracy()])
""",
        """CODE.model.compile('sgd', metrics=[keras.metrics.LogCoshError()])
""",
        """CODE.mse = keras.losses.MeanSquaredError()
loss = mse([0., 0., 1., 1.], [1., 1., 1., 0.])
model = keras.Model(inputs, outputs)
model.compile('sgd', loss=keras.losses.MeanSquaredError())""",
        """CODE.mse = keras.losses.MeanSquaredError()
loss = mse([0., 0., 1., 1.], [1., 1., 1., 0.])
model = keras.Model(inputs, outputs)
model.compile('sgd', loss=keras.losses.MeanSquaredError())
""",
        """CODE.msle = keras.losses.MeanSquaredLogarithmicError()
loss = msle([0., 0., 1., 1.], [1., 1., 1., 0.])
model = keras.Model(inputs, outputs)
model.compile('sgd', loss=keras.losses.MeanSquaredLogarithmicError())
""" .

<DEPENDENCY.keras==2.3.1> <CONTAINS> """CODE.    # Crop the input 2D images or feature maps
    model = Sequential()
    model.add(Cropping2D(cropping=((2, 2), (4, 4)),
                         input_shape=(28, 28, 3)))
    # now model.output_shape == (None, 24, 20, 3)
    model.add(Conv2D(64, (3, 3), padding='same'))
    model.add(Cropping2D(cropping=((2, 2), (2, 2)))
    # now model.output_shape == (None, 20, 16, 64)
""",
        """CODE.    # First, let's define a RNN Cell, as a layer subclass.

    class MinimalRNNCell(keras.layers.Layer):

        def __init__(self, units, **kwargs):
            self.units = units
            self.state_size = units
            super(MinimalRNNCell, self).__init__(**kwargs)

        def build(self, input_shape):
            self.kernel = self.add_weight(shape=(input_shape[-1], self.units),
                                          initializer='uniform',
                                          name='kernel')
            self.recurrent_kernel = self.add_weight(
                shape=(self.units, self.units),
                initializer='uniform',
                name='recurrent_kernel')
            self.built = True

        def call(self, inputs, states):
            prev_output = states[0]
            h = K.dot(inputs, self.kernel)
            output = h + K.dot(prev_output, self.recurrent_kernel)
            return output, [output]

    # Let's use this cell in a RNN layer:

    cell = MinimalRNNCell(32)
    x = keras.Input((None, 5))
    layer = RNN(cell)
    y = layer(x)

    # Here's how to use the cell to build a stacked RNN:

    cells = [MinimalRNNCell(32), MinimalRNNCell(64)]
    x = keras.Input((None, 5))
    layer = RNN(cells)
    y = layer(x)
""",
        """CODE.    # TensorFlow example
    kvar = K.random_normal_variable((2,3), 0, 1)
    kvar
    K.eval(kvar)
""",
        """CODE.    # dot product between tensors
    x = K.placeholder(shape=(2, 3))
    y = K.placeholder(shape=(3, 4))
    xy = K.dot(x, y)
    xy


    # dot product between tensors
    x = K.placeholder(shape=(32, 28, 3))
    y = K.placeholder(shape=(3, 4))
    xy = K.dot(x, y)
    xy


    # Theano-like behavior example
    x = K.random_uniform_variable(shape=(2, 3), low=0, high=1)
    y = K.ones((4, 3, 5))
    xy = K.dot(x, y)
    K.int_shape(xy)
""",
        """CODE.    class CIFAR10Sequence(Sequence):

        def __init__(self, x_set, y_set, batch_size):
            self.x, self.y = x_set, y_set
            self.batch_size = batch_size

        def __len__(self):
            return int(np.ceil(len(self.x) / float(self.batch_size)))

        def __getitem__(self, idx):
            batch_x = self.x[idx * self.batch_size:(idx + 1) * self.batch_size]
            batch_y = self.y[idx * self.batch_size:(idx + 1) * self.batch_size]

            return np.array([
                resize(imread(file_name), (200, 200))
                   for file_name in batch_x]), np.array(batch_y)
""",
        """CODE.    def antirectifier(x):
        x -= K.mean(x, axis=1, keepdims=True)
        x = K.l2_normalize(x, axis=1)
        pos = K.relu(x)
        neg = K.relu(-x)
        return K.concatenate([pos, neg], axis=1)

    def antirectifier_output_shape(input_shape):
        shape = list(input_shape)
        assert len(shape) == 2  # only valid for 2D tensors
        shape[-1] *= 2
        return tuple(shape)

    model.add(Lambda(antirectifier,
                     output_shape=antirectifier_output_shape))

    def hadamard_product_sum(tensors):
        out1 = tensors[0] * tensors[1]
        out2 = K.sum(out1, axis=-1)
        return [out1, out2]

    def hadamard_product_sum_output_shape(input_shapes):
        shape1 = list(input_shapes[0])
        shape2 = list(input_shapes[1])
        assert shape1 == shape2  # else hadamard product isn't possible
        return [tuple(shape1), tuple(shape2[:-1])]

    x1 = Dense(32)(input_1)
    x2 = Dense(32)(input_2)
    layer = Lambda(hadamard_product_sum, hadamard_product_sum_output_shape)
    x_hadamard, x_sum = layer([x1, x2])
""",
        """CODE.    enqueuer = SequenceEnqueuer(...)
    enqueuer.start()
    datas = enqueuer.get()
    for data in datas:
        # Use the inputs; training, evaluating, predicting.
        # ... stop sometime.
    enqueuer.close()
""",
        """CODE.    from keras import backend as K
    K.epsilon()
    K.set_epsilon(1e-05)
    K.epsilon()
""",
        """CODE.    from keras import backend as K
    b = K.placeholder((2, 2), sparse=True)
    print(K.is_sparse(b))
    c = K.to_dense(b)
    print(K.is_sparse(c))
""",
        """CODE.    from keras import backend as K
    from keras.layers import Input, Dense
    np_var = numpy.array([1, 2])
    K.is_keras_tensor(np_var) # A numpy array is not a symbolic tensor.
    k_var = tf.placeholder('float32', shape=(1,1))
    K.is_keras_tensor(k_var)
    keras_var = K.variable(np_var)
    K.is_keras_tensor(keras_var)
    keras_placeholder = K.placeholder(shape=(2, 4, 5))
    K.is_keras_tensor(keras_placeholder)
    keras_input = Input([10])
    K.is_keras_tensor(keras_input) # An Input is a Keras tensor.
    keras_layer_output = Dense(10)(keras_input)
    K.is_keras_tensor(keras_layer_output)
""",
        """CODE.    from keras import backend as K
    tf_session = K.get_session()
    val = np.array([[1, 2], [3, 4]])
    kvar = K.variable(value=val)
    inputs = keras.backend.placeholder(shape=(2, 4, 5))
    K.shape(kvar)
    K.shape(inputs)
    K.shape(kvar).eval(session=tf_session)
    K.shape(inputs).eval(session=tf_session)
""",
        """CODE.    import keras

    input1 = keras.layers.Input(shape=(16,))
    x1 = keras.layers.Dense(8, activation='relu')(input1)
    input2 = keras.layers.Input(shape=(32,))
    x2 = keras.layers.Dense(8, activation='relu')(input2)
    added = keras.layers.add([x1, x2])

    out = keras.layers.Dense(4)(added)
    model = keras.models.Model(inputs=[input1, input2], outputs=out)
""",
        """CODE.    import keras

    input1 = keras.layers.Input(shape=(16,))
    x1 = keras.layers.Dense(8, activation='relu')(input1)
    input2 = keras.layers.Input(shape=(32,))
    x2 = keras.layers.Dense(8, activation='relu')(input2)
    subtracted = keras.layers.subtract([x1, x2])

    out = keras.layers.Dense(4)(subtracted)
    model = keras.models.Model(inputs=[input1, input2], outputs=out)
""",
        """CODE.    import tensorflow as tf
    from keras.applications import Xception
    from keras.utils import multi_gpu_model
    import numpy as np

    num_samples = 1000
    height = 224
    width = 224
    num_classes = 1000

    # Instantiate the base model (or "template" model).
    # We recommend doing this with under a CPU device scope,
    # so that the model's weights are hosted on CPU memory.
    # Otherwise they may end up hosted on a GPU, which would
    # complicate weight sharing.
    with tf.device('/cpu:0'):
        model = Xception(weights=None,
                         input_shape=(height, width, 3),
                         classes=num_classes)

    # Replicates the model on 8 GPUs.
    # This assumes that your machine has 8 available GPUs.
    parallel_model = multi_gpu_model(model, gpus=8)
    parallel_model.compile(loss='categorical_crossentropy',
                           optimizer='rmsprop')

    # Generate dummy data.
    x = np.random.random((num_samples, height, width, 3))
    y = np.random.random((num_samples, num_classes))

    # This `fit` call will be distributed on 8 GPUs.
    # Since the batch size is 256, each GPU will process 32 samples.
    parallel_model.fit(x, y, epochs=20, batch_size=256)

    # Save model via the template model (which shares the same weights):
    model.save('my_model.h5')


     ..
     # Not needed to change the device scope for model definition:
     model = Xception(weights=None, ..)

     try:
         parallel_model = multi_gpu_model(model, cpu_relocation=True)
         print("Training using multiple GPUs..")
     except ValueError:
         parallel_model = model
         print("Training using single GPU or CPU..")
     parallel_model.compile(..)
     ..


     ..
     # Not needed to change the device scope for model definition:
     model = Xception(weights=None, ..)

     try:
         parallel_model = multi_gpu_model(model, cpu_merge=False)
         print("Training using multiple GPUs..")
     except:
         parallel_model = model
         print("Training using single GPU or CPU..")

     parallel_model.compile(..)
     ..
""",
        """CODE.    inner_products = []
    for xi, yi in zip(x, y):
        inner_products.append(xi.dot(yi))
    result = stack(inner_products)

    x_batch = K.ones(shape=(32, 20, 1))
    y_batch = K.ones(shape=(32, 30, 20))
    xy_batch_dot = K.batch_dot(x_batch, y_batch, axes=(1, 2))
    K.int_shape(xy_batch_dot)
    (32, 1, 30)
""",
        """CODE.    model = Sequential()
    model.add(Masking(mask_value=0., input_shape=(timesteps, features)))
    model.add(LSTM(32))
""",
        """CODE.    model = Sequential()
    model.add(TimeDistributed(Dense(8), input_shape=(10, 16)))
    # now model.output_shape == (None, 10, 8)


    model.add(TimeDistributed(Dense(32)))
    # now model.output_shape == (None, 10, 32)


    model = Sequential()
    model.add(TimeDistributed(Conv2D(64, (3, 3)),
                              input_shape=(10, 299, 299, 3)))
""",
        """CODE.    with CustomObjectScope({'MyObject':MyObject}):
        layer = Dense(..., kernel_regularizer='MyObject')
        # save, load, etc. will recognize custom object by name
""",
        """CODE.    with custom_object_scope({'MyObject':MyObject}):
        layer = Dense(..., kernel_regularizer='MyObject')
        # save, load, etc. will recognize custom object by name
""",
        """CODE.    x = K.print_tensor(x, message="x is: ")
""",
        """CODE.    x_data = HDF5Matrix('input/file.hdf5', 'data')
    model.predict(x_data)
""",
        """CODE.# Consider an array of 5 labels out of a set of 3 classes {0, 1, 2}:
labels
array([0, 2, 1, 2, 0])
# `to_categorical` converts this into a matrix with as many
# columns as there are classes. The number of rows
# stays the same.
to_categorical(labels)
array([[ 1.,  0.,  0.],
       [ 0.,  0.,  1.],
       [ 0.,  1.,  0.],
       [ 0.,  0.,  1.],
       [ 1.,  0.,  0.]], dtype=float32)
""",
        """CODE.# as first layer in a sequential model:
model = Sequential()
model.add(Dense(32, input_shape=(16,)))
# now the model will take as input arrays of shape (*, 16)
# and output arrays of shape (*, 32)

# after the first layer, you don't need to specify
# the size of the input anymore:
model.add(Dense(32))
""",
        """CODE.batch_print_callback = LambdaCallback(
    on_batch_begin=lambda batch,logs: print(batch))

json_log = open('loss_log.json', mode='wt', buffering=1)
json_logging_callback = LambdaCallback(
    on_epoch_end=lambda epoch, logs: json_log.write(
        json.dumps({'epoch': epoch, 'loss': logs['loss']}) + '\\n'),
    on_train_end=lambda logs: json_log.close()
)

processes = ...
cleanup_callback = LambdaCallback(
    on_train_end=lambda logs: [
        p.terminate() for p in processes if p.is_alive()])

model.fit(...,
          callbacks=[batch_print_callback,
                     json_logging_callback,
                     cleanup_callback])
""",
        """CODE.bce = keras.losses.BinaryCrossentropy()
loss = bce([0., 0., 1., 1.], [1., 1., 1., 0.])
model = keras.Model(inputs, outputs)
model.compile('sgd', loss=keras.losses.BinaryCrossentropy())
""",
        """CODE.cce = keras.losses.CategoricalCrossentropy()
loss = cce(
    [[1., 0., 0.], [0., 1., 0.], [0., 0., 1.]],
    [[.9, .05, .05], [.5, .89, .6], [.05, .01, .94]])

model = keras.Model(inputs, outputs)
model.compile('sgd', loss=keras.losses.CategoricalCrossentropy())
""",
        """CODE.cce = keras.losses.CategoricalCrossentropy()
loss = cce(
    [[1., 0., 0.], [0., 1., 0.], [0., 0., 1.]],
    [[.9, .05, .05], [.5, .89, .6], [.05, .01, .94]])
model = keras.Model(inputs, outputs)
model.compile('sgd', loss=keras.losses.CategoricalCrossentropy())
""",
        """CODE.cce = keras.losses.SparseCategoricalCrossentropy()
loss = cce(
    [0, 1, 2],
    [[.9, .05, .05], [.5, .89, .6], [.05, .01, .94]])


model = keras.Model(inputs, outputs)
model.compile('sgd', loss=keras.losses.SparseCategoricalCrossentropy())
""",
        """CODE.class MeanSquaredError(Loss):
    def call(self, y_true, y_pred):
        y_pred = ops.convert_to_tensor(y_pred)
        y_true = math_ops.cast(y_true, y_pred.dtype)
        return K.mean(math_ops.square(y_pred - y_true), axis=-1)
""",
        """CODE.csv_logger = CSVLogger('training.log')
model.fit(X_train, Y_train, callbacks=[csv_logger])
""",
        """CODE.def foldr(fn, elems, initializer=None, name=None):
    if not callable(fn):
        raise TypeError("fn must be callable")
    if not isinstance(initializer, tf.Tensor) and initializer is not None:
        raise TypeError("initializer must be a tensor or None value")
    if not isinstance(elems, tf.Tensor):
        raise TypeError("elems must be a tensor")

    accumulator = initializer
    for elem in reversed(elems):
        if accumulator is None:
            accumulator = elem
        else:
            accumulator = fn(accumulator, elem)

    return accumulator

# Example usage
result = foldr(lambda acc, x: acc + x, tf.constant([1, 2, 3, 4]), initializer=tf.constant(0))
print(result)
""",
        """CODE.def generate_arrays_from_file(path):
    while True:
        with open(path) as f:
            for line in f:
                # create numpy arrays of input data
                # and labels, from each line in the file
                x1, x2, y = process_line(line)
                yield ({'input_1': x1, 'input_2': x2}, {'output': y})

model.fit_generator(generate_arrays_from_file('/my_file.txt'),
                    steps_per_epoch=10000, epochs=10)
""",
        """CODE.from keras import backend as K
K.eval(K.eye(3))
K.eval(K.eye((2, 3)))
""",
        """CODE.from keras import backend as K
K.floatx()
arr = numpy.array([1.0, 2.0], dtype='float64')
arr.dtype
new_arr = K.cast_to_floatx(arr)
new_arr
new_arr.dtype
""",
        """CODE.from keras import backend as K
K.normalize_data_format(None)
'channels_first'
K.normalize_data_format('channels_last')
'channels_last'
""",
        """CODE.from keras import backend as K
K.normalize_data_format(None)
K.normalize_data_format('channels_last')
""",
        """CODE.from keras import backend as K
K.set_epsilon(1e-05)
""",
        """CODE.from keras import backend as K
K.set_floatx('float16')
""",
        """CODE.from keras import backend as K
K.set_image_data_format('channels_last')
""",
        """CODE.from keras import backend as K
a = K.placeholder((2, 2), sparse=False)
print(K.is_sparse(a))
b = K.placeholder((2, 2), sparse=True)
print(K.is_sparse(b))
""",
        """CODE.from keras import backend as K
from keras.layers import Input, Dense
np_var = numpy.array([1, 2])
K.is_keras_tensor(np_var)
k_var = tf.placeholder('float32', shape=(1,1))
K.is_keras_tensor(k_var)
keras_var = K.variable(np_var)
K.is_keras_tensor(keras_var)
keras_placeholder = K.placeholder(shape=(2, 4, 5))
K.is_keras_tensor(keras_placeholder)
keras_input = Input([10])
K.is_keras_tensor(keras_input)
keras_layer_output = Dense(10)(keras_input)
K.is_keras_tensor(keras_layer_output)
""",
        """CODE.from keras import backend as K
import numpy as np

kvar = K.variable(np.random.random((2, 3)))
kvar_tile = K.tile(K.eye(2), (2, 3))
K.eval(kvar_tile)
""",
        """CODE.from keras import backend as K
import numpy as np

tf_session = K.get_session()
val = np.array([[1, 2], [3, 4]])
kvar = K.variable(value=val)
inputs = keras.backend.placeholder(shape=(2, 4, 5))
print(K.shape(kvar))
print(K.shape(inputs))
print(K.shape(kvar).eval(session=tf_session))
print(K.shape(inputs).eval(session=tf_session)
""",
        """CODE.from keras import backend as K
input = K.placeholder((2, 3), dtype='float32')
K.cast(input, dtype='float16')
input = K.cast(input, dtype='float16')
""",
        """CODE.from keras import backend as K
input_ph = K.placeholder(shape=(2, 4, 5))
input_ph._keras_shape
input_ph
""",
        """CODE.from keras import backend as K
inputs = K.placeholder(shape=(2, 4, 5))
K.int_shape(inputs)
val = np.array([[1, 2], [3, 4]])
kvar = K.variable(value=val)
K.int_shape(kvar)
""",
        """CODE.from keras import backend as K
inputs = K.placeholder(shape=(2, 4, 5))
val = np.array([[1, 2], [3, 4]])
kvar = K.variable(value=val)
K.ndim(inputs)
K.ndim(kvar)
""",
        """CODE.from keras import backend as K
kvar = K.ones((3,4))
K.eval(kvar)
array([[ 1.,  1.,  1.,  1.],
       [ 1.,  1.,  1.,  1.],
       [ 1.,  1.,  1.,  1.]], dtype=float32)
""",
        """CODE.from keras import backend as K
kvar = K.variable(np.array([[1, 2], [3, 4]]), dtype='float32')
K.eval(kvar)
""",
        """CODE.from keras import backend as K
kvar = K.variable(np.random.random((2,3)))
kvar_ones = K.ones_like(kvar)
K.eval(kvar_ones)
""",
        """CODE.from keras import backend as K
kvar = K.variable(np.random.random((2,3)))
kvar_zeros = K.zeros_like(kvar)
K.eval(kvar_zeros)
""",
        """CODE.from keras import backend as K
kvar = K.zeros((3,4))
K.eval(kvar)
array([[ 0.,  0.,  0.,  0.],
       [ 0.,  0.,  0.,  0.],
       [ 0.,  0.,  0.,  0.]], dtype=float32)
""",
        """CODE.from keras import backend as K
print(K.dtype(K.placeholder(shape=(2,4,5))))
print(K.dtype(K.placeholder(shape=(2,4,5), dtype='float32')))
print(K.dtype(K.placeholder(shape=(2,4,5), dtype='float64')))
kvar = K.variable(np.array([[1, 2], [3, 4]]))
print(K.dtype(kvar))
kvar = K.variable(np.array([[1, 2], [3, 4]]), dtype='float32')
print(K.dtype(kvar))
""",
        """CODE.from keras import backend as K
val = np.array([[1, 2], [3, 4]])
kvar = K.variable(value=val)
K.size(inputs)
<tf.Tensor: id=9, shape=(), dtype=int32, numpy=4>
""",
        """CODE.from keras.models import load_model

model.save('my_model.h5')  # creates a HDF5 file 'my_model.h5'
del model  # deletes the existing model

# returns a compiled model
# identical to the previous one
model = load_model('my_model.h5')
""",
        """CODE.from keras.preprocessing.sequence import TimeseriesGenerator
import numpy as np

data = np.array([[i] for i in range(50)])
targets = np.array([[i] for i in range(50)])

data_gen = TimeseriesGenerator(data, targets,
                               length=10, sampling_rate=2,
                               batch_size=2)
assert len(data_gen) == 20

batch_0 = data_gen[0]
x, y = batch_0
assert np.array_equal(x,
                      np.array([[[0], [2], [4], [6], [8]],
                                [[1], [3], [5], [7], [9]]]))
assert np.array_equal(y,
                      np.array([[10], [11]]))
""",
        """CODE.from keras.utils.data_utils import _hash_file
_hash_file('/path/to/file.zip')
""",
        """CODE.get_custom_objects().clear()
get_custom_objects()['MyObject'] = MyObject
""",
        """CODE.import keras
input1 = keras.layers.Input(shape=(16,))
x1 = keras.layers.Dense(8, activation='relu')(input1)
input2 = keras.layers.Input(shape=(32,))
x2 = keras.layers.Dense(8, activation='relu')(input2)
added = keras.layers.Add()([x1, x2])
out = keras.layers.Dense(4)(added)
model = keras.models.Model(inputs=[input1, input2], outputs=out)""",
        """CODE.import keras
input1 = keras.layers.Input(shape=(16,))
x1 = keras.layers.Dense(8, activation='relu')(input1)
input2 = keras.layers.Input(shape=(32,))
x2 = keras.layers.Dense(8, activation='relu')(input2)
subtracted = keras.layers.Subtract()([x1, x2])
out = keras.layers.Dense(4)(subtracted)
model = keras.models.Model(inputs=[input1, input2], outputs=out)""",
        """CODE.inner_products = []
for xi, yi in zip(x, y):
    inner_products.append(xi.dot(yi))
result = stack(inner_products)


x_batch = K.ones(shape=(32, 20, 1))
y_batch = K.ones(shape=(32, 30, 20))
xy_batch_dot = K.batch_dot(x_batch, y_batch, axes=(1, 2))
K.int_shape(xy_batch_dot)
(32, 1, 30)
""",
        """CODE.keras.backend.backend()
'tensorflow'
""",
        """CODE.keras.backend.epsilon()
1e-07
""",
        """CODE.keras.backend.floatx()
'float32'
""",
        """CODE.keras.backend.get_uid('dense')
1
keras.backend.get_uid('dense')
2
""",
        """CODE.keras.backend.get_uid('dense')
keras.backend.get_uid('dense')
""",
        """CODE.keras.backend.image_data_format()
'channels_first'
""",
        """CODE.kvar = K.random_uniform_variable((2,3), 0, 1)
kvar
K.eval(kvar)
""",
        """CODE.loss = 0.5 * x**2 if abs(x) <= d else 0.5 * d**2 + d * (abs(x) - d)


model = keras.Model(inputs, outputs)
model.compile('sgd', loss=keras.losses.Huber())
""",
        """CODE.loss = y_pred - y_true * log(y_pred)
model = keras.Model(inputs, outputs)
model.compile('sgd', loss=keras.losses.Poisson())
""",
        """CODE.m = keras.metrics.Mean()
m.update_state([1, 3, 5, 7])
m.result()
""",
        """CODE.m = keras.metrics.Sum()
m.update_state([1, 3, 5, 7])
m.result()
""",
        """CODE.mae = keras.losses.MeanAbsoluteError()
loss = mae([0., 0., 1., 1.], [1., 1., 1., 0.])
model = keras.Model(inputs, outputs)
model.compile('sgd', loss=keras.losses.MeanAbsoluteError())""",
        """CODE.mae = keras.losses.MeanAbsoluteError()
loss = mae([0., 0., 1., 1.], [1., 1., 1., 0.])
model = keras.Model(inputs, outputs)
model.compile('sgd', loss=keras.losses.MeanAbsoluteError())
""",
        """CODE.mape = keras.losses.MeanAbsolutePercentageError()
loss = mape([0., 0., 1., 1.], [1., 1., 1., 0.])
model = keras.Model(inputs, outputs)
model.compile('sgd', loss=keras.losses.MeanAbsolutePercentageError())""",
        """CODE.mape = keras.losses.MeanAbsolutePercentageError()
loss = mape([0., 0., 1., 1.], [1., 1., 1., 0.])
model = keras.Model(inputs, outputs)
model.compile('sgd', loss=keras.losses.MeanAbsolutePercentageError())
""",
        """CODE.model = Sequential()
model.add(Bidirectional(LSTM(10, return_sequences=True), input_shape=(5, 10)))
model.add(Bidirectional(LSTM(10)))
model.add(Dense(5))
model.add(Activation('softmax'))
model.compile(loss='categorical_crossentropy', optimizer='rmsprop')
""",
        """CODE.model = Sequential()
model.add(Conv2D(64, (3, 3),
                 input_shape=(3, 32, 32), padding='same',))
# now: model.output_shape == (None, 64, 32, 32)

model.add(Flatten())
# now: model.output_shape == (None, 65536)
""",
        """CODE.model = Sequential()
model.add(Dense(32, input_dim=32))
model.add(RepeatVector(3))""",
        """CODE.model = Sequential()
model.add(Dense(32, input_shape=(500,)))
model.add(Dense(32))
model = Sequential()
model.add(Dense(32, input_dim=500))
model = Sequential()
model.add(Dense(32, batch_input_shape=(None, 500)))
model = Sequential()
model.add(Dense(32))
model.add(Dense(32))
model.compile(optimizer=optimizer, loss=loss)
model.fit(x, y, batch_size=32, epochs=10)
model = Sequential()
model.add(Dense(32))
model.add(Dense(32))
model.weights  # returns []
model = Sequential()
model.add(Dense(32, input_shape=(500,)))
model.add(Dense(32))
model.weights  # returns list of length 4
model = Sequential()
model.add(Dense(32))
model.add(Dense(32))
model.build((None, 500))
model.weights  # returns list of length 4
""",
        """CODE.model = Sequential()
model.add(Embedding(1000, 64, input_length=10))
input_array = np.random.randint(1000, size=(32, 10))
model.compile('rmsprop', 'mse')
output_array = model.predict(input_array)
assert output_array.shape == (32, 10, 64)
""",
        """CODE.model = Sequential()
model.add(LocallyConnected1D(64, 3, input_shape=(10, 32)))
model.add(LocallyConnected1D(32, 3))
""",
        """CODE.model = Sequential()
model.add(LocallyConnected2D(64, (3, 3), input_shape=(32, 32, 3)))
model.add(LocallyConnected2D(32, (3, 3)))
""",
        """CODE.model = Sequential()
model.add(Permute((2, 1), input_shape=(10, 64)))
# now: model.output_shape == (None, 64, 10)
# note: `None` is the batch dimension
""",
        """CODE.model = Sequential()
model.add(Reshape((3, 4), input_shape=(12,)))
model.add(Reshape((6, 2)))
model.add(Reshape((-1, 2, 2)))
""",
        """CODE.model = keras.Model(inputs, outputs)
model.compile(
    'sgd',
    loss='mse',
    metrics=[keras.metrics.CategoricalAccuracy()])
""",
        """CODE.model = keras.Model(inputs, outputs)
model.compile(
    'sgd',
    loss='mse',
    metrics=[keras.metrics.MeanIoU(num_classes=2)])
""",
        """CODE.model = keras.Model(inputs, outputs)
model.compile(
    'sgd',
    loss='mse',
    metrics=[keras.metrics.SensitivityAtSpecificity()])
""",
        """CODE.model = keras.Model(inputs, outputs)
model.compile(
    'sgd',
    loss='mse',
    metrics=[keras.metrics.SparseCategoricalAccuracy()])
""",
        """CODE.model = keras.Model(inputs, outputs)
model.compile(
    'sgd',
    loss='mse',
    metrics=[keras.metrics.SpecificityAtSensitivity()])
""",
        """CODE.model = keras.Model(inputs, outputs)
model.compile(
    'sgd',
    metrics=[keras.metrics.SparseTopKCategoricalAccuracy()])
""",
        """CODE.model = keras.Model(inputs, outputs)
model.compile(
  'sgd',
  loss='mse',
  metrics=[keras.metrics.CosineSimilarity(axis=1)])
""",
        """CODE.model = keras.Model(inputs, outputs)
model.compile('sgd', loss='mse', metrics=[keras.metrics.BinaryAccuracy()])
""",
        """CODE.model = keras.Model(inputs, outputs)
model.compile('sgd', loss='mse', metrics=[keras.metrics.FalseNegatives()])
""",
        """CODE.model = keras.Model(inputs, outputs)
model.compile('sgd', loss='mse', metrics=[keras.metrics.FalsePositives()])
""",
        """CODE.model = keras.Model(inputs, outputs)
model.compile('sgd', loss='mse', metrics=[keras.metrics.Recall()])
""",
        """CODE.model = keras.Model(inputs, outputs)
model.compile('sgd', loss='mse', metrics=[keras.metrics.TrueNegatives()])
""",
        """CODE.model = keras.Model(inputs, outputs)
model.compile('sgd', loss='mse', metrics=[keras.metrics.TruePositives()])
""",
        """CODE.model = keras.Model(inputs, outputs)
model.compile('sgd', loss=keras.losses.CategoricalHinge())""",
        """CODE.model = keras.Model(inputs, outputs)
model.compile('sgd', loss=keras.losses.Hinge())
""",
        """CODE.model = keras.Model(inputs, outputs)
model.compile('sgd', loss=keras.losses.KLDivergence())
""",
        """CODE.model = keras.Model(inputs, outputs)
model.compile('sgd', loss=keras.losses.LogCosh())
""",
        """CODE.model = keras.Model(inputs, outputs)
model.compile('sgd', loss=keras.losses.Poisson())
""",
        """CODE.model = keras.Model(inputs, outputs)
model.compile('sgd', loss=keras.losses.SquaredHinge())
""",
        """CODE.model = keras.Model(inputs, outputs)
model.compile('sgd', metrics=[keras.metrics.LogCoshError()])
""",
        """CODE.model = keras.Model(inputs, outputs)
model.compile('sgd', metrics=[keras.metrics.RootMeanSquaredError()])
""",
        """CODE.model = keras.Model(inputs, outputs)
model.compile('sgd', metrics=[keras.metrics.TopKCategoricalAccuracy()])
""",
        "CODE.model.compile('sgd', loss=keras.losses.CategoricalHinge())",
        """CODE.mse = keras.losses.MeanSquaredError()
loss = mse([0., 0., 1., 1.], [1., 1., 1., 0.])
model = keras.Model(inputs, outputs)
model.compile('sgd', loss=keras.losses.MeanSquaredError())
""",
        """CODE.msle = keras.losses.MeanSquaredLogarithmicError()
loss = msle([0., 0., 1., 1.], [1., 1., 1., 0.])
model = keras.Model(inputs, outputs)
model.compile('sgd', loss=keras.losses.MeanSquaredLogarithmicError())
""",
        """CODE.reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2,
                              patience=5, min_lr=0.001)
model.fit(X_train, Y_train, callbacks=[reduce_lr])
""",
        """CODE.var = K.variable([[1, 2, 3], [4, 5, 6]])
K.eval(var)
var_transposed = K.transpose(var)
K.eval(var_transposed)


inputs = K.placeholder((2, 3))
inputs
input_transposed = K.transpose(inputs)
input_transposed
""",
        """CODE.with tf_file_io_proxy('keras.engine.saving.tf_file_io') as file_io_proxy:
    gcs_filepath = file_io_proxy.get_filepath(filename='model.h5')
    save_model(model, gcs_filepath)
    file_io_proxy.assert_exists(gcs_filepath)
    new_model_gcs = load_model(gcs_filepath)
    file_io_proxy.delete_file(gcs_filepath)  # cleanup
""",
        """CODE.x = Input(shape=(32,))
y = Dense(16, activation='softmax')(x)
model = Model(x, y)
""" .

<DEPENDENCY.keras==2.4.0> <CONTAINS> """CODE.create_ngram_set([1, 4, 9, 4, 1, 4], ngram_value=2)
{(4, 9), (4, 1), (1, 4), (9, 4)}
create_ngram_set([1, 4, 9, 4, 1, 4], ngram_value=3)
[(1, 4, 9), (4, 9, 4), (9, 4, 1), (4, 1, 4)]""",
        """CODE.def add_ngram(sequences, token_indice, ngram_range):
...     for i in range(len(sequences)):
...         for n in range(2, ngram_range + 1):
...             for j in range(len(sequences[i]) - n + 1):
...                 ngram = tuple(sequences[i][j:j + n])
...                 if ngram in token_indice:
...                     sequences[i].append(token_indice[ngram])
...     return sequences
...
sequences = [[1, 3, 4, 5], [1, 3, 7, 9, 2]]
token_indice = {(1, 3): 1337, (9, 2): 42, (4, 5): 2017}
add_ngram(sequences, token_indice, ngram_range=2)
[[1, 3, 4, 5, 1337, 2017], [1, 3, 7, 9, 2, 1337, 42]]

sequences = [[1, 3, 4, 5], [1, 3, 7, 9, 2]]
token_indice = {(1, 3): 1337, (9, 2): 42, (4, 5): 2017, (7, 9, 2): 2018}
add_ngram(sequences, token_indice, ngram_range=3)
[[1, 3, 4, 5, 1337, 2017], [1, 3, 7, 9, 2, 1337, 42, 2018]]""",
        """CODE.from keras.utils.generic_utils import transpose_shape
transpose_shape((16, 128, 128, 32),'channels_first', spatial_axes=(1, 2))
transpose_shape((16, 128, 128, 32), 'channels_last', spatial_axes=(1, 2))
transpose_shape((128, 128, 32), 'channels_first', spatial_axes=(0, 1))
""",
        "CODE.tokenize('Bob dropped the apple. Where is the apple?')",
        """CODE.tokenize('Bob dropped the apple. Where is the apple?')
['Bob', 'dropped', 'the', 'apple', '.', 'Where', 'is', 'the', 'apple', '?']""" .

<DEPENDENCY.keras==2.6.0> <CONTAINS> """CODE.
  f(x) = max_value if x >= max_value
  f(x) = x if threshold <= x < max_value
  f(x) = negative_slope * (x - threshold) otherwise



layer = tf.keras.layers.ReLU()
output = layer([-3.0, -1.0, 0.0, 2.0])
list(output.numpy())
[0.0, 0.0, 0.0, 2.0]



layer = tf.keras.layers.ReLU(max_value=1.0)
output = layer([-3.0, -1.0, 0.0, 2.0])
list(output.numpy())
[0.0, 0.0, 0.0, 1.0]



layer = tf.keras.layers.ReLU(negative_slope=1.0)
output = layer([-3.0, -1.0, 0.0, 2.0])
list(output.numpy())
[-3.0, -1.0, 0.0, 2.0]



layer = tf.keras.layers.ReLU(threshold=1.5)
output = layer([-3.0, -1.0, 1.0, 2.0])
list(output.numpy())
[0.0, 0.0, 0.0, 2.0]
""",
        """CODE.
branch_a = [Input(shape=(2,), name='a'), Dense(), Dense()]
branch_b = [Input(shape=(3,), name='b'), Dense(), Dense()]

model = get_multi_io_model(branch_a, branch_b)



input_branch_a = [Input(shape=(2,), name='a'), Dense(), Dense()]
input_branch_b = [Input(shape=(3,), name='b'), Dense(), Dense()]
shared_output_branch = [Concatenate(), Dense(), Dense()]

model = get_multi_io_model(input_branch_a, input_branch_b,
                           shared_output_branch=shared_output_branch)



shared_input_branch = [Input(shape=(2,), name='in'), Dense(), Dense()]
output_branch_a = [Dense(), Dense()]
output_branch_b = [Dense(), Dense()]

model = get_multi_io_model(output__branch_a, output_branch_b,
                           shared_input_branch=shared_input_branch)
""",
        """CODE.    enqueuer = SequenceEnqueuer(...)
    enqueuer.start()
    datas = enqueuer.get()
    for data in datas:
        # Use the inputs; training, evaluating, predicting.
        # ... stop sometime.
    enqueuer.stop()
""",
        """CODE.    import keras

    input1 = keras.layers.Input(shape=(16,))
    x1 = keras.layers.Dense(8, activation='relu')(input1)
    input2 = keras.layers.Input(shape=(32,))
    x2 = keras.layers.Dense(8, activation='relu')(input2)
    subtracted = keras.layers.subtract([x1, x2])

    out = keras.layers.Dense(4)(subtracted)
    model = keras.models.Model(inputs=[input1, input2], outputs=out)
""",
        """CODE.    import tensorflow as tf
    from keras.applications import Xception
    from keras.utils import multi_gpu_model
    import numpy as np

    num_samples = 1000
    height = 224
    width = 224
    num_classes = 1000

    # Instantiate the base model (or "template" model).
    # We recommend doing this with under a CPU device scope,
    # so that the model's weights are hosted on CPU memory.
    # Otherwise they may end up hosted on a GPU, which would
    # complicate weight sharing.
    with tf.device('/cpu:0'):
        model = Xception(weights=None,
                         input_shape=(height, width, 3),
                         classes=num_classes)

    # Replicates the model on 8 GPUs.
    # This assumes that your machine has 8 available GPUs.
    parallel_model = multi_gpu_model(model, gpus=8)
    parallel_model.compile(loss='categorical_crossentropy',
                           optimizer='rmsprop')

    # Generate dummy data.
    x = np.random.random((num_samples, height, width, 3))
    y = np.random.random((num_samples, num_classes))

    # This `fit` call will be distributed on 8 GPUs.
    # Since the batch size is 256, each GPU will process 32 samples.
    parallel_model.fit(x, y, epochs=20, batch_size=256)

    # Save model via the template model (which shares the same weights):
    model.save('my_model.h5')


     ..
     # Not needed to change the device scope for model definition:
     model = Xception(weights=None, ..)

     try:
         model = multi_gpu_model(model, cpu_relocation=True)
         print("Training using multiple GPUs..")
     except:
         print("Training using single GPU or CPU..")

     model.compile(..)
     ..


     ..
     # Not needed to change the device scope for model definition:
     model = Xception(weights=None, ..)

     try:
         model = multi_gpu_model(model, cpu_merge=False)
         print("Training using multiple GPUs..")
     except:
         print("Training using single GPU or CPU..")
     model.compile(..)
     ..
""",
        """CODE.  class MinimalRNNCell(AbstractRNNCell):

    def __init__(self, units, **kwargs):
      self.units = units
      super(MinimalRNNCell, self).__init__(**kwargs)

    @property
    def state_size(self):
      return self.units

    def build(self, input_shape):
      self.kernel = self.add_weight(shape=(input_shape[-1], self.units),
                                    initializer='uniform',
                                    name='kernel')
      self.recurrent_kernel = self.add_weight(
          shape=(self.units, self.units),
          initializer='uniform',
          name='recurrent_kernel')
      self.built = True

    def call(self, inputs, states):
      prev_output = states[0]
      h = backend.dot(inputs, self.kernel)
      output = h + backend.dot(prev_output, self.recurrent_kernel)
      return output, output
""",
        """CODE. y = tf.compat.v1.layers.average_pooling1d(x, pool_size=2, strides=2)

 x = tf.keras.Input((28, 28, 1))
 y = tf.keras.layers.AveragePooling1D(pool_size=2, strides=2)(x)
 model = tf.keras.Model(x, y)
""",
        """CODE. y = tf.compat.v1.layers.average_pooling2d(x, pool_size=2, strides=2)

 x = tf.keras.Input((28, 28, 1))
 y = tf.keras.layers.AveragePooling2D(pool_size=2, strides=2)(x)
 model = tf.keras.Model(x, y)
""",
        """CODE. y = tf.compat.v1.layers.average_pooling3d(x, pool_size=2, strides=2)
""",
        """CODE. y = tf.compat.v1.layers.max_pooling1d(x, pool_size=2, strides=2)
 x = tf.keras.Input((28, 28, 1))
 y = tf.keras.layers.MaxPooling1D(pool_size=2, strides=2)(x)
 model = tf.keras.Model(x, y)
""",
        """CODE. y = tf.compat.v1.layers.max_pooling2d(x, pool_size=2, strides=2)

 x = tf.keras.Input((28, 28, 1))
 y = tf.keras.layers.MaxPooling2D(pool_size=2, strides=2)(x)
 model = tf.keras.Model(x, y)
""",
        """CODE. y = tf.compat.v1.layers.max_pooling3d(x, pool_size=2, strides=2)

 x = tf.keras.Input((28, 28, 1))
 y = tf.keras.layers.MaxPooling3D(pool_size=2, strides=2)(x)
 model = tf.keras.Model(x, y)
""",
        """CODE.# Create 2 PeepholeLSTMCells
peephole_lstm_cells = [PeepholeLSTMCell(size) for size in [128, 256]]
# Create a layer composed sequentially of the peephole LSTM cells.
layer = RNN(peephole_lstm_cells)
input = keras.Input((timesteps, input_dim))
output = layer(input)
""",
        """CODE.# Create a test Sequential model.
model = keras.Sequential([
    keras.Input(shape=(728,)),
    keras.layers.Dense(32, activation='relu'),
    keras.layers.Dense(1, activation='sigmoid'),
])
# Create a copy of the test model (with freshly initialized weights).
new_model = clone_model(model)

new_model = model.__class__.from_config(model.get_config())
""",
        """CODE.# First, let's define a RNN Cell, as a layer subclass.

class MinimalRNNCell(keras.layers.Layer):

    def __init__(self, units, **kwargs):
        self.units = units
        self.state_size = units
        super(MinimalRNNCell, self).__init__(**kwargs)

    def build(self, input_shape):
        self.kernel = self.add_weight(shape=(input_shape[-1], self.units),
                                      initializer='uniform',
                                      name='kernel')
        self.recurrent_kernel = self.add_weight(
            shape=(self.units, self.units),
            initializer='uniform',
            name='recurrent_kernel')
        self.built = True

    def call(self, inputs, states):
        prev_output = states[0]
        h = backend.dot(inputs, self.kernel)
        output = h + backend.dot(prev_output, self.recurrent_kernel)
        return output, [output]

# Let's use this cell in a RNN layer:

cell = MinimalRNNCell(32)
x = keras.Input((None, 5))
layer = RNN(cell)
y = layer(x)

# Here's how to use the cell to build a stacked RNN:

cells = [MinimalRNNCell(32), MinimalRNNCell(64)]
x = keras.Input((None, 5))
layer = RNN(cells)
y = layer(x)
""",
        """CODE.# Print the batch number at the beginning of every batch.
batch_print_callback = LambdaCallback(
    on_batch_begin=lambda batch,logs: print(batch))

# Stream the epoch loss to a file in JSON format. The file content
# is not well-formed JSON but rather has a JSON object per line.
import json
json_log = open('loss_log.json', mode='wt', buffering=1)
json_logging_callback = LambdaCallback(
    on_epoch_end=lambda epoch, logs: json_log.write(
        json.dumps({'epoch': epoch, 'loss': logs['loss']}) + '\\n'),
    on_train_end=lambda logs: json_log.close()
)

# Terminate some processes after having finished model training.
processes = ...
cleanup_callback = LambdaCallback(
    on_train_end=lambda logs: [
        p.terminate() for p in processes if p.is_alive()])

model.fit(...,
          callbacks=[batch_print_callback,
                     json_logging_callback,
                     cleanup_callback])
""",
        """CODE.# Retrieve the training sequences.
(x_train, _), _ = keras.datasets.imdb.load_data()
# Retrieve the word index file mapping words to indices
word_index = keras.datasets.imdb.get_word_index()
# Reverse the word index to obtain a dict mapping indices to words
inverted_word_index = dict((i, word) for (word, i) in word_index.items())
# Decode the first sequence in the dataset
decoded_sequence = " ".join(inverted_word_index[i] for i in x_train[0])
""",
        """CODE.# Variable-length int sequences.
query_input = tf.keras.Input(shape=(None,), dtype='int32')
value_input = tf.keras.Input(shape=(None,), dtype='int32')

# Embedding lookup.
token_embedding = tf.keras.layers.Embedding(input_dim=1000, output_dim=64)
# Query embeddings of shape [batch_size, Tq, dimension].
query_embeddings = token_embedding(query_input)
# Value embeddings of shape [batch_size, Tv, dimension].
value_embeddings = token_embedding(value_input)

# CNN layer.
cnn_layer = tf.keras.layers.Conv1D(
    filters=100,
    kernel_size=4,
    # Use 'same' padding so outputs have the same shape as inputs.
    padding='same')
# Query encoding of shape [batch_size, Tq, filters].
query_seq_encoding = cnn_layer(query_embeddings)
# Value encoding of shape [batch_size, Tv, filters].
value_seq_encoding = cnn_layer(value_embeddings)

# Query-value attention of shape [batch_size, Tq, filters].
query_value_attention_seq = tf.keras.layers.Attention()(
    [query_seq_encoding, value_seq_encoding])

# Reduce over the sequence axis to produce encodings of shape
# [batch_size, filters].
query_encoding = tf.keras.layers.GlobalAveragePooling1D()(
    query_seq_encoding)
query_value_attention = tf.keras.layers.GlobalAveragePooling1D()(
    query_value_attention_seq)

# Concatenate query and document encodings to produce a DNN input layer.
input_layer = tf.keras.layers.Concatenate()(
    [query_encoding, query_value_attention])

# Add DNN layers, and create Model.
# ...
""",
        """CODE.@property
@tracking.cached_per_instance
def thing(self):
  # `thing` is expensive to compute (and may not even be requested), so we
  # want to lazily compute it and then cache it.
  return compute_thing(self)""",
        """CODE.@testing_utils.run_with_all_saved_model_formats
def test_foo(self):
    save_format = testing_utils.get_save_format()
    saved_model_dir = '/tmp/saved_model/'
    model = keras.models.Sequential()
    model.add(keras.layers.Dense(2, input_shape=(3,)))
    model.add(keras.layers.Dense(3))
    model.compile(loss='mse', optimizer='sgd', metrics=['acc'])

    keras.models.save_model(model, saved_model_dir, save_format=save_format)
    model = keras.models.load_model(saved_model_dir)""",
        """CODE.a = tf.constant([-1.0, 0.0, 1.0], dtype = tf.float32)
b = tf.keras.activations.softsign(a)
b.numpy()""",
        """CODE.a = tf.constant([-1.0, 0.0, 1.0], dtype = tf.float32)
b = tf.keras.activations.softsign(a)
b.numpy()
array([-0.5,  0. ,  0.5], dtype=float32)""",
        """CODE.a = tf.constant([-20, -1.0, 0.0, 1.0, 20], dtype = tf.float32)
b = tf.keras.activations.sigmoid(a)
b.numpy()
array([2.0611537e-09, 2.6894143e-01, 5.0000000e-01, 7.3105860e-01,
         1.0000000e+00], dtype=float32)""",
        """CODE.a = tf.constant([-20, -1.0, 0.0, 1.0, 20], dtype = tf.float32)
b = tf.keras.activations.softplus(a)
b.numpy()""",
        """CODE.a = tf.constant([-3.0,-1.0, 0.0,1.0,3.0], dtype = tf.float32)
b = tf.keras.activations.exponential(a)
b.numpy()""",
        """CODE.a = tf.constant([-3.0,-1.0, 0.0,1.0,3.0], dtype = tf.float32)
b = tf.keras.activations.hard_sigmoid(a)
b.numpy()""",
        """CODE.a = tf.constant([-3.0,-1.0, 0.0,1.0,3.0], dtype = tf.float32)
b = tf.keras.activations.linear(a)
b.numpy()
array([-3., -1.,  0.,  1.,  3.], dtype=float32)""",
        """CODE.a = tf.constant([-3.0,-1.0, 0.0,1.0,3.0], dtype = tf.float32)
b = tf.keras.activations.tanh(a)
b.numpy()""",
        """CODE.a = tf.constant([1., 0., 0., 0., 1., 0., 0., 0., 1.], shape=[3,3])
b = tf.constant([.9, .05, .05, .05, .89, .06, .05, .01, .94], shape=[3,3])
loss = tf.keras.backend.categorical_crossentropy(a, b)
loss = tf.keras.backend.categorical_crossentropy(a, a)
print(np.around(loss, 5))""",
        """CODE.a = tf.constant([1., 0., 0., 0., 1., 0., 0., 0., 1.], shape=[3,3])
print(a)
tf.Tensor(
  [[1. 0. 0.]
   [0. 1. 0.]
   [0. 0. 1.]], shape=(3, 3), dtype=float32)
b = tf.constant([.9, .05, .05, .05, .89, .06, .05, .01, .94], shape=[3,3])
print(b)
tf.Tensor(
  [[0.9  0.05 0.05]
   [0.05 0.89 0.06]
   [0.05 0.01 0.94]], shape=(3, 3), dtype=float32)
loss = tf.keras.backend.categorical_crossentropy(a, b)
print(np.around(loss, 5))
[0.10536 0.11653 0.06188]
loss = tf.keras.backend.categorical_crossentropy(a, a)
print(np.around(loss, 5))
[0. 0. 0.]""",
        """CODE.a = tf.constant([[1, 2, 3], [4, 5, 6], [7, 8, 9])
b = tf.constant([[10, 20, 30], [40, 50, 60], [70, 80, 90]])
tf.keras.backend.concatenate((a, b), axis=-1)""",
        """CODE.a = tf.constant([[1, 2, 3], [4, 5, 6], [7, 8, 9], [10, 11, 12]])
tf.keras.backend.permute_dimensions(a, pattern=(1, 0))
""",
        """CODE.a = tf.constant([[1, 2, 3], [4, 5, 6], [7, 8, 9], [10, 11, 12]])
tf.keras.backend.reshape(a, shape=(2, 6))""",
        """CODE.a = tf.constant([[1, 2],[3, 4]])
b = tf.constant([[10, 20],[30, 40]])
tf.keras.backend.stack((a, b))""",
        """CODE.a = tf.keras.backend.placeholder((2, 2), sparse=False)
print(tf.keras.backend.is_sparse(a))
False
b = tf.keras.backend.placeholder((2, 2), sparse=True)
print(tf.keras.backend.is_sparse(b))
True""",
        """CODE.a = tf.keras.backend.placeholder((2, 2), sparse=False)
print(tf.keras.backend.is_sparse(a))
b = tf.keras.backend.placeholder((2, 2), sparse=True)
print(tf.keras.backend.is_sparse(b) )""",
        """CODE.a = tf.keras.utils.to_categorical([0, 1, 2, 3], num_classes=4)
a = tf.constant(a, shape=[4, 4])
print(a)
tf.Tensor(
  [[1. 0. 0. 0.]
   [0. 1. 0. 0.]
   [0. 0. 1. 0.]
   [0. 0. 0. 1.]], shape=(4, 4), dtype=float32)

b = tf.constant([.9, .04, .03, .03,
...                  .3, .45, .15, .13,
...                  .04, .01, .94, .05,
...                  .12, .21, .5, .17],
...                 shape=[4, 4])
loss = tf.keras.backend.categorical_crossentropy(a, b)
print(np.around(loss, 5))
[0.10536 0.82807 0.1011  1.77196]

loss = tf.keras.backend.categorical_crossentropy(a, a)
print(np.around(loss, 5))
[0. 0. 0. 0.]""",
        """CODE.acc = np.dot(sample_weight, np.equal(y_true, np.argmax(y_pred, axis=1))
model.compile(
    optimizer='sgd',
    loss='mse',
    metrics=[tf.keras.metrics.SparseCategoricalAccuracy()])
""",
        """CODE.adapt_data = np.array([1., 2., 3., 4., 5.], dtype='float32')
input_data = np.array([1., 2., 3.], dtype='float32')
layer = tf.keras.layers.Normalization(axis=None)
layer.adapt(adapt_data)
layer(input_data)


adapt_data = np.array([[0., 7., 4.],
                       [2., 9., 6.],
                       [0., 7., 4.],
                       [2., 9., 6.]], dtype='float32')
input_data = np.array([[0., 7., 4.]], dtype='float32')
layer = tf.keras.layers.Normalization(axis=-1)
layer.adapt(adapt_data)
layer(input_data)


input_data = np.array([[1.], [2.], [3.]], dtype='float32')
layer = tf.keras.layers.Normalization(mean=3., variance=2.)
layer(input_data)
""",
        """CODE.b = tf.keras.backend.placeholder((2, 2), sparse=True)
print(tf.keras.backend.is_sparse(b))
c = tf.keras.backend.to_dense(b)
print(tf.keras.backend.is_sparse(c) )""",
        """CODE.batch_size = 3
sentence_max_length = 5
n_features = 2
new_shape = (batch_size, sentence_max_length, n_features)
x = tf.constant(np.reshape(np.arange(30), new_shape), dtype = tf.float32)

rnn_cells = [tf.keras.layers.LSTMCell(128) for _ in range(2)]
stacked_lstm = tf.keras.layers.StackedRNNCells(rnn_cells)
lstm_layer = tf.keras.layers.RNN(stacked_lstm)

result = lstm_layer(x)
""",
        """CODE.cache = ContextValueCache(int)
cache[None] += 2
cache[None] += 4
assert cache[None] == 6

with tf.Graph().as_default() as g:
  cache[None] += 5
  cache[g] += 3
assert cache[g] == 8

cache = ContextValueCache(lambda x: x + 1)
g = tf.get_default_graph()

value = cache.setdefault(key=g, kwargs={'x': 3})
assert cache[g] == 4""",
        """CODE.cast_to_floatx(arr)
new_arr
array([1.,  2.], dtype=float32)
new_arr.dtype
dtype('float32')""",
        """CODE.class BinaryTruePositives(tf.keras.metrics.Metric):

  def __init__(self, name='binary_true_positives', **kwargs):
    super(BinaryTruePositives, self).__init__(name=name, **kwargs)
    self.true_positives = self.add_weight(name='tp', initializer='zeros')

  def update_state(self, y_true, y_pred, sample_weight=None):
    y_true = tf.cast(y_true, tf.bool)
    y_pred = tf.cast(y_pred, tf.bool)

    values = tf.logical_and(tf.equal(y_true, True), tf.equal(y_pred, True))
    values = tf.cast(values, self.dtype)
    if sample_weight is not None:
      sample_weight = tf.cast(sample_weight, self.dtype)
      sample_weight = tf.broadcast_to(sample_weight, values.shape)
      values = tf.multiply(values, sample_weight)
    self.true_positives.assign_add(tf.reduce_sum(values))

  def result(self):
    return self.true_positives
""",
        """CODE.class Foo(object):
  def __init__(self, input_):
    self._input = input_
  def value(self):
    return tf.constant(42.)

tf.register_tensor_conversion_function(
    Foo, lambda x, *args, **kwargs: x.value())

tf.keras.__internal__.utils.register_symbolic_tensor_type(Foo)

layer = tf.keras.layers.Lambda(lambda input_: Foo(input_))
""",
        """CODE.class InterruptingCallback(tf.keras.callbacks.Callback):
    def on_epoch_begin(self, epoch, logs=None):
        if epoch == 4:
            raise RuntimeError('Interrupting!')

callback = tf.keras.callbacks.experimental.BackupAndRestore(
    backup_dir="/tmp/backup")

model = tf.keras.models.Sequential([tf.keras.layers.Dense(10)])
model.compile(tf.keras.optimizers.SGD(), loss='mse')

try:
    model.fit(np.arange(100).reshape(5, 20), np.zeros(5), epochs=10,
              batch_size=1, callbacks=[callback, InterruptingCallback()],
              verbose=0)
except:
    pass

history = model.fit(np.arange(100).reshape(5, 20), np.zeros(5), epochs=10,
                    batch_size=1, callbacks=[callback], verbose=0)""",
        """CODE.class MeanSquaredError(Loss):

  def call(self, y_true, y_pred):
    y_pred = tf.convert_to_tensor_v2(y_pred)
    y_true = tf.cast(y_true, y_pred.dtype)
    return tf.reduce_mean(math_ops.square(y_pred - y_true), axis=-1)


with strategy.scope():
  loss_obj = tf.keras.losses.CategoricalCrossentropy(
      reduction=tf.keras.losses.Reduction.NONE)
  ....
  loss = (tf.reduce_sum(loss_obj(labels, predictions)) *
          (1. / global_batch_size))
""",
        """CODE.class MyCallback(tf.keras.callbacks.Callback):
    def on_train_end(self, logs=None):
        global training_finished
        training_finished = True

model = tf.keras.Sequential([tf.keras.layers.Dense(1, input_shape=(1,))])
model.compile(loss='mean_squared_error')
model.fit(tf.constant([[1.0]]), tf.constant([[1.0]]),
          callbacks=[MyCallback()])
assert training_finished == True
""",
        """CODE.class MyLayer(Layer):
    def __init__(self):
        super(MyLayer, self).__init__()
        # The layer will accept inputs with shape (?, 28, 28) & (?, 28, 28, 1)
        # and raise an appropriate error message otherwise.
        self.input_spec = InputSpec(
            shape=(None, 28, 28, 1),
            allow_last_axis_squeeze=True)
""",
        """CODE.class MyLayer(tf.keras.layers.Layer):
    def call(self, inputs):
        self.add_loss(tf.abs(tf.reduce_mean(inputs)))
        return inputs

inputs = tf.keras.Input(shape=(10,))
x = tf.keras.layers.Dense(10)(inputs)
outputs = tf.keras.layers.Dense(1)(x)
model = tf.keras.Model(inputs, outputs)
len(model.losses)
0
model.add_loss(tf.abs(tf.reduce_mean(x)))
len(model.losses)
1

inputs = tf.keras.Input(shape=(10,))
d = tf.keras.layers.Dense(10, kernel_initializer='ones')
x = d(inputs)
outputs = tf.keras.layers.Dense(1)(x)
model = tf.keras.Model(inputs, outputs)
model.add_loss(lambda: tf.reduce_mean(d.kernel))
model.losses
[<tf.Tensor: shape=(), dtype=float32, numpy=1.0>]""",
        """CODE.class MyLayer(tf.keras.layers.Layer):
    def call(self, inputs):
        self.add_loss(tf.abs(tf.reduce_mean(inputs)))
        return inputs

inputs = tf.keras.Input(shape=(10,))
x = tf.keras.layers.Dense(10)(inputs)
outputs = tf.keras.layers.Dense(1)(x)
model = tf.keras.Model(inputs, outputs)
len(model.losses)
model.add_loss(tf.abs(tf.reduce_mean(x)))

inputs = tf.keras.Input(shape=(10,))
d = tf.keras.layers.Dense(10, kernel_initializer='ones')
x = d(inputs)
outputs = tf.keras.layers.Dense(1)(x)
model = tf.keras.Model(inputs, outputs)
model.add_loss(lambda: tf.reduce_mean(d.kernel))
model.losses""",
        """CODE.class MyLayer(tf.keras.layers.Layer):
  def call(self, inputs):
    self.add_loss(tf.abs(tf.reduce_mean(inputs)))
    return inputs


inputs = tf.keras.Input(shape=(10,))
x = tf.keras.layers.Dense(10)(inputs)
outputs = tf.keras.layers.Dense(1)(x)
model = tf.keras.Model(inputs, outputs)
# Activity regularization.
model.add_loss(tf.abs(tf.reduce_mean(x)))


inputs = tf.keras.Input(shape=(10,))
d = tf.keras.layers.Dense(10)
x = d(inputs)
outputs = tf.keras.layers.Dense(1)(x)
model = tf.keras.Model(inputs, outputs)
# Weight regularization.
model.add_loss(lambda: tf.reduce_mean(d.kernel))
""",
        """CODE.class MyLayer(tf.keras.layers.Layer):
  def call(self, inputs):
    self.add_loss(tf.abs(tf.reduce_mean(inputs)))
    return inputs

inputs = tf.keras.Input(shape=(10,))
x = tf.keras.layers.Dense(10)(inputs)
outputs = tf.keras.layers.Dense(1)(x)
model = tf.keras.Model(inputs, outputs)
len(model.losses)
0
model.add_loss(tf.abs(tf.reduce_mean(x)))
len(model.losses)
1

inputs = tf.keras.Input(shape=(10,))
d = tf.keras.layers.Dense(10, kernel_initializer='ones')
x = d(inputs)
outputs = tf.keras.layers.Dense(1)(x)
model = tf.keras.Model(inputs, outputs)
model.add_loss(lambda: tf.reduce_mean(d.kernel))
model.losses
[<tf.Tensor: shape=(), dtype=float32, numpy=1.0>]""",
        """CODE.class MyMetricLayer(tf.keras.layers.Layer):
  def __init__(self):
    super(MyMetricLayer, self).__init__(name='my_metric_layer')
    self.mean = tf.keras.metrics.Mean(name='metric_1')

  def call(self, inputs):
    self.add_metric(self.mean(inputs))
    self.add_metric(tf.reduce_sum(inputs), name='metric_2')
    return inputs


inputs = tf.keras.Input(shape=(10,))
x = tf.keras.layers.Dense(10)(inputs)
outputs = tf.keras.layers.Dense(1)(x)
model = tf.keras.Model(inputs, outputs)
model.add_metric(math_ops.reduce_sum(x), name='metric_1')


inputs = tf.keras.Input(shape=(10,))
x = tf.keras.layers.Dense(10)(inputs)
outputs = tf.keras.layers.Dense(1)(x)
model = tf.keras.Model(inputs, outputs)
model.add_metric(tf.keras.metrics.Mean()(x), name='metric_1')
""",
        """CODE.class MyModel(tf.keras.Model):

  def train_step(self, data):
    # If `sample_weight` is not provided, all samples will be weighted
    # equally.
    x, y, sample_weight = tf.keras.utils.unpack_x_y_sample_weight(data)

    with tf.GradientTape() as tape:
      y_pred = self(x, training=True)
      loss = self.compiled_loss(
        y, y_pred, sample_weight, regularization_losses=self.losses)
      trainable_variables = self.trainable_variables
      gradients = tape.gradient(loss, trainable_variables)
      self.optimizer.apply_gradients(zip(gradients, trainable_variables))

    self.compiled_metrics.update_state(y, y_pred, sample_weight)
    return {m.name: m.result() for m in self.metrics}
""",
        """CODE.class MyTests(testing_utils.KerasTestCase):

  @testing_utils.run_all_keras_modes
  def test_foo(self):
    model = testing_utils.get_small_functional_mlp(1, 4, input_dim=3)
    optimizer = RMSPropOptimizer(learning_rate=0.001)
    loss = 'mse'
    metrics = ['mae']
    model.compile(
        optimizer, loss, metrics=metrics,
        run_eagerly=testing_utils.should_run_eagerly())

    inputs = np.zeros((10, 3))
    targets = np.zeros((10, 4))
    dataset = dataset_ops.Dataset.from_tensor_slices((inputs, targets))
    dataset = dataset.repeat(100)
    dataset = dataset.batch(10)

    model.fit(dataset, epochs=1, steps_per_epoch=2, verbose=1)

if __name__ == "__main__":
  tf.test.main()
""",
        """CODE.class MyTests(testing_utils.KerasTestCase):

  @testing_utils.run_with_all_model_types(
    exclude_models = ['sequential'])
  def test_foo(self):
    model = testing_utils.get_small_mlp(1, 4, input_dim=3)
    optimizer = RMSPropOptimizer(learning_rate=0.001)
    loss = 'mse'
    metrics = ['mae']
    model.compile(optimizer, loss, metrics=metrics)

    inputs = np.zeros((10, 3))
    targets = np.zeros((10, 4))
    dataset = dataset_ops.Dataset.from_tensor_slices((inputs, targets))
    dataset = dataset.repeat(100)
    dataset = dataset.batch(10)

    model.fit(dataset, epochs=1, steps_per_epoch=2, verbose=1)

@testing_utils.run_with_all_model_types(exclude_models = ['sequential'])
class MyTests(testing_utils.KerasTestCase):

  def test_foo(self):
    model = testing_utils.get_small_mlp(1, 4, input_dim=3)
    optimizer = RMSPropOptimizer(learning_rate=0.001)
    loss = 'mse'
    metrics = ['mae']
    model.compile(optimizer, loss, metrics=metrics)

    inputs = np.zeros((10, 3))
    targets = np.zeros((10, 4))
    dataset = dataset_ops.Dataset.from_tensor_slices((inputs, targets))
    dataset = dataset.repeat(100)
    dataset = dataset.batch(10)

    model.fit(dataset, epochs=1, steps_per_epoch=2, verbose=1)
""",
        """CODE.class NonNegative(tf.keras.constraints.Constraint):
    def __call__(self, w):
        return w * tf.cast(tf.math.greater_equal(w, 0.), w.dtype)""",
        """CODE.class RNNModel(tf.keras.Model):

  def __init__(self, name):
    super(RNNModel, self).__init__(name=name)
    self.rnn = tf.compat.v1.nn.rnn_cell.MultiRNNCell(
      [tf.compat.v1.nn.rnn_cell.LSTMCell(64) for _ in range(2)])

  def call(self, input, state):
    return self.rnn(input, state)

model_1 = RNNModel("model_1")
model_2 = RNNModel("model_2")

# OK
output_1, next_state_1 = model_1(input, state)
# Raises an error about trying to create an already existing variable.
output_2, next_state_2 = model_2(input, state)

with keras_style_scope():
  model_1 = RNNModel("model_1")
  model_2 = RNNModel("model_2")

  # model_1 and model_2 are guaranteed to create their own variables.
  output_1, next_state_1 = model_1(input, state)
  output_2, next_state_2 = model_2(input, state)

  assert len(model_1.weights) > 0
  assert len(model_2.weights) > 0
  assert(model_1.weights != model_2.weights)
""",
        """CODE.class SimpleDense(Layer):

  def __init__(self, units=32):
      super(SimpleDense, self).__init__()
      self.units = units

  def build(self, input_shape):  # Create the state of the layer (weights)
    w_init = tf.random_normal_initializer()
    self.w = tf.Variable(
        initial_value=w_init(shape=(input_shape[-1], self.units),
                             dtype='float32'),
        trainable=True)
    b_init = tf.zeros_initializer()
    self.b = tf.Variable(
        initial_value=b_init(shape=(self.units,), dtype='float32'),
        trainable=True)

  def call(self, inputs):  # Defines the computation from inputs to outputs
      return tf.matmul(inputs, self.w) + self.b

# Instantiates the layer.
linear_layer = SimpleDense(4)

# This will also call `build(input_shape)` and create the weights.
y = linear_layer(tf.ones((2, 2)))
assert len(linear_layer.weights) == 2

# These weights are trainable, so they're listed in `trainable_weights`:
assert len(linear_layer.trainable_weights) == 2


class SimpleDense(Layer):

  def __init__(self, units=32):
      super(SimpleDense, self).__init__()
      self.units = units

  def build(self, input_shape):
      self.w = self.add_weight(shape=(input_shape[-1], self.units),
                               initializer='random_normal',
                               trainable=True)
      self.b = self.add_weight(shape=(self.units,),
                               initializer='random_normal',
                               trainable=True)

  def call(self, inputs):
      return tf.matmul(inputs, self.w) + self.b


class ComputeSum(Layer):

  def __init__(self, input_dim):
      super(ComputeSum, self).__init__()
      # Create a non-trainable weight.
      self.total = tf.Variable(initial_value=tf.zeros((input_dim,)),
                               trainable=False)

  def call(self, inputs):
      self.total.assign_add(tf.reduce_sum(inputs, axis=0))
      return self.total

my_sum = ComputeSum(2)
x = tf.ones((2, 2))

y = my_sum(x)
print(y.numpy())  # [2. 2.]

y = my_sum(x)
print(y.numpy())  # [4. 4.]

assert my_sum.weights == [my_sum.total]
assert my_sum.non_trainable_weights == [my_sum.total]
assert my_sum.trainable_weights == []
""",
        """CODE.class TestLayer(tf.keras.Layer):
  def build():
    with no_manual_dependency_tracking_scope(self):
      var = self.add_variable("name1")  # Creates a var and doesn't track it
    self._track_trackable("name2", var)  # We track variable with name `name2`
""",
        """CODE.class WrappedDoubleDenseLayer(variable_scope_shim.VariableScopeWrapperLayer):

  def __init__(self, units, *args, **kwargs):
    super().__init__(*args, **kwargs)
    self.units = units

  def forward_pass(self, inputs, training=None):
    out = tf.compat.v1.layers.dense(
        inputs, self.units, name="dense_one",
        kernel_initializer=init_ops.ones_initializer(),
        kernel_regularizer="l2")
    with variable_scope.variable_scope("nested_scope"):
      out = tf.compat.v1.layers.dense(
          out, self.units, name="dense_two",
          kernel_initializer=init_ops.ones_initializer(),
          kernel_regularizer="l2")
    return out


class WrappedDoubleDenseLayer(variable_scope_shim.VariableScopeWrapperLayer):

  def __init__(self, units, *args, **kwargs):
    super().__init__(*args, **kwargs)
    self.units = units

  def forward_pass(self, inputs, training=None):
    out = inputs
    with tf.compat.v1.variable_scope("dense_one"):
      # The weights are created with a `regularizer`,
      # so the layer should track their regularization losses
      kernel = tf.compat.v1.get_variable(
          shape=[out.shape[-1], self.units],
          regularizer=regularizers.L2(),
          initializer=init_ops.ones_initializer(),
          name="kernel")
      bias = tf.compat.v1.get_variable(
          shape=[self.units,],
          initializer=init_ops.zeros_initializer(),
          name="bias")
      out = tf.compat.v1.math.matmul(out, kernel)
      out = tf.compat.v1.nn.bias_add(out, bias)
    with tf.compat.v1.variable_scope("nested_scope"):
      with tf.compat.v1.variable_scope("dense_two"):
        kernel = tf.compat.v1.get_variable(
            shape=[out.shape[-1], self.units],
            regularizer=regularizers.L2(),
            initializer=init_ops.ones_initializer(),
            name="kernel")
        bias = tf.compat.v1.get_variable(
            shape=[self.units,],
            initializer=init_ops.zeros_initializer(),
            name="bias")
        out = tf.compat.v1.math.matmul(out, kernel)
        out = tf.compat.v1.nn.bias_add(out, bias)
    return out
""",
        """CODE.csv_logger = CSVLogger('training.log')
model.fit(X_train, Y_train, callbacks=[csv_logger])
""",
        """CODE.data = tf.constant(np.arange(10).reshape(5, 2) * 10, dtype=tf.float32)
print(data)


layer = tf.keras.layers.LayerNormalization(axis=1)
output = layer(data)
print(output)


mean_i = sum(x_i[j] for j in range(k)) / k
var_i = sum((x_i[j] - mean_i) ** 2 for j in range(k)) / k


x_i_normalized = (x_i - mean_i) / sqrt(var_i + epsilon)


output_i = x_i_normalized * gamma + beta
""",
        """CODE.def _hash_file(fpath, algorithm='auto', chunk_size=65536):
    import hashlib

    if algorithm == 'auto':
        hasher = hashlib.md5()
    elif algorithm == 'sha256':
        hasher = hashlib.sha256()
    elif algorithm == 'md5':
        hasher = hashlib.md5()
    else:
        raise ValueError("Invalid algorithm. Use 'auto', 'sha256', or 'md5'.")

    with open(fpath, 'rb') as file:
        for chunk in iter(lambda: file.read(chunk_size), b''):
            hasher.update(chunk)

    return hasher.hexdigest()
""",
        """CODE.def accuracy(y_true, y_pred):
  return tf.cast(tf.math.equal(y_true, y_pred), tf.float32)

accuracy_metric = tf.keras.metrics.MeanMetricWrapper(fn=accuracy)

keras_model.compile(..., metrics=accuracy_metric)
""",
        """CODE.def decayed_learning_rate(step):
  return initial_learning_rate * decay_rate ^ (step / decay_steps)


initial_learning_rate = 0.1
lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(
    initial_learning_rate,
    decay_steps=100000,
    decay_rate=0.96,
    staircase=True)

model.compile(optimizer=tf.keras.optimizers.SGD(learning_rate=lr_schedule),
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])

model.fit(data, labels, epochs=5)
""",
        """CODE.def decayed_learning_rate(step):
  return initial_learning_rate / (1 + decay_rate * step / decay_step)


def decayed_learning_rate(step):
  return initial_learning_rate / (1 + decay_rate * floor(step / decay_step))


...
initial_learning_rate = 0.1
decay_steps = 1.0
decay_rate = 0.5
learning_rate_fn = keras.optimizers.schedules.InverseTimeDecay(
  initial_learning_rate, decay_steps, decay_rate)

model.compile(optimizer=tf.keras.optimizers.SGD(
                  learning_rate=learning_rate_fn),
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])

model.fit(data, labels, epochs=5)
""",
        """CODE.def decayed_learning_rate(step):
  step = min(step, decay_steps)
  cosine_decay = 0.5 * (1 + cos(pi * step / decay_steps))
  decayed = (1 - alpha) * cosine_decay + alpha
  return initial_learning_rate * decayed


decay_steps = 1000
lr_decayed_fn = tf.keras.optimizers.schedules.CosineDecay(
    initial_learning_rate, decay_steps)
""",
        """CODE.def decayed_learning_rate(step):
  step = min(step, decay_steps)
  linear_decay = (decay_steps - step) / decay_steps
  cosine_decay = 0.5 * (
      1 + cos(pi * 2 * num_periods * step / decay_steps))
  decayed = (alpha + linear_decay) * cosine_decay + beta
  return initial_learning_rate * decayed


decay_steps = 1000
lr_decayed_fn = (
  tf.keras.experimental.LinearCosineDecay(
    initial_learning_rate, decay_steps))
""",
        """CODE.def decayed_learning_rate(step):
  step = min(step, decay_steps)
  linear_decay = (decay_steps - step) / decay_steps)
  cosine_decay = 0.5 * (
      1 + cos(pi * 2 * num_periods * step / decay_steps))
  decayed = (alpha + linear_decay + eps_t) * cosine_decay + beta
  return initial_learning_rate * decayed


decay_steps = 1000
lr_decayed_fn = (
  tf.keras.experimental.NoisyLinearCosineDecay(
    initial_learning_rate, decay_steps))
""",
        """CODE.def deserialize(config, custom_objects=None):
   return deserialize_keras_object(
     identifier,
     module_objects=globals(),
     custom_objects=custom_objects,
     name="MyObjectType",
   )
""",
        """CODE.def from_config(cls, config, custom_objects=None):
    if 'my_custom_object_name' in config:
        config['hidden_cls'] = tf.keras.utils.get_registered_object(
            config['my_custom_object_name'], custom_objects=custom_objects)""",
        """CODE.def my_op(a):
  with tf.name_scope("MyOp") as scope:
    a = tf.convert_to_tensor(a, name="a")
    # Define some computation that uses `a`.
    return foo_op(..., name=scope)""",
        """CODE.def scheduler(epoch, lr):
    if epoch < 10:
        return lr
    else:
        return lr * tf.math.exp(-0.1)

model = tf.keras.models.Sequential([tf.keras.layers.Dense(10)])
model.compile(tf.keras.optimizers.SGD(), loss='mse')
round(model.optimizer.lr.numpy(), 5)

callback = tf.keras.callbacks.LearningRateScheduler(scheduler)
history = model.fit(np.arange(100).reshape(5, 20), np.zeros(5),
                    epochs=15, callbacks=[callback], verbose=0)
round(model.optimizer.lr.numpy(), 5)""",
        """CODE.ds1 = tf.data.Dataset.from_tensor_slices([1, 2, 3])
ds2 = tf.data.Dataset.from_tensor_slices([4, 5, 6])
ds_zipped_tuple = tf.data.Dataset.zip((ds1, ds2))
ds_unzipped_tuple = _unzip_dataset(ds_zipped_tuple)
ds_zipped_dict = tf.data.Dataset.zip({'ds1': ds1, 'ds2': ds2})
ds_unzipped_dict = _unzip_dataset(ds_zipped_dict)""",
        """CODE.file_pattern = 'f.batch{batch:02d}epoch{epoch:02d}.h5'
test_dir = self.get_temp_dir()
path_pattern = os.path.join(test_dir, file_pattern)
file_paths = [
    os.path.join(test_dir, file_name) for file_name in
    ['f.batch03epoch02.h5', 'f.batch02epoch02.h5', 'f.batch01epoch01.h5']
]
for file_path in file_paths:
  # Write something to each of the files
self.assertEqual(
    _get_most_recently_modified_file_matching_pattern(path_pattern),
    file_paths[-1])
""",
        """CODE.first_decay_steps = 1000
lr_decayed_fn = (
  tf.keras.optimizers.schedules.CosineDecayRestarts(
      initial_learning_rate,
      first_decay_steps))
""",
        """CODE.foo = tf.constant([-10, -5, 0.0, 5, 10], dtype = tf.float32)
tf.keras.activations.relu(foo).numpy()
tf.keras.activations.relu(foo, alpha=0.5).numpy()
tf.keras.activations.relu(foo, max_value=5).numpy()
tf.keras.activations.relu(foo, threshold=5).numpy()
""",
        """CODE.for _ in range(100):
  # Without `clear_session()`, each iteration of this loop will
  # slightly increase the size of the global state managed by Keras
  model = tf.keras.Sequential([tf.keras.layers.Dense(10) for _ in range(10)])

for _ in range(100):
  # With `clear_session()` called at the beginning,
  # Keras starts with a blank state at each iteration
  # and memory consumption is constant over time.
  tf.keras.backend.clear_session()
  model = tf.keras.Sequential([tf.keras.layers.Dense(10) for _ in range(10)])

import tensorflow as tf
layers = [tf.keras.layers.Dense(10) for _ in range(10)]
new_layer = tf.keras.layers.Dense(10)
print(new_layer.name)
dense_10
tf.keras.backend.set_learning_phase(1)
print(tf.keras.backend.learning_phase())
1
tf.keras.backend.clear_session()
new_layer = tf.keras.layers.Dense(10)
print(new_layer.name)
dense
""",
        """CODE.from PIL import Image
img = np.random.random(size=(100, 100, 3))
pil_img = tf.keras.preprocessing.image.array_to_img(img)
""",
        """CODE.from PIL import Image
import numpy as np
import tensorflow as tf

img_data = np.random.random(size=(100, 100, 3))
img = tf.keras.preprocessing.image.array_to_img(img_data)
array = tf.keras.preprocessing.image.img_to_array(img)
""",
        """CODE.from keras.models import load_model

model.save('my_model.h5')  # creates a HDF5 file 'my_model.h5'
del model  # deletes the existing model

# returns a compiled model
# identical to the previous one
model = load_model('my_model.h5')
""",
        """CODE.from keras.preprocessing.sequence import TimeseriesGenerator
import numpy as np
data = np.array([[i] for i in range(50)])
targets = np.array([[i] for i in range(50)])
data_gen = TimeseriesGenerator(data, targets,
                               length=10, sampling_rate=2,
                               batch_size=2)
assert len(data_gen) == 20
batch_0 = data_gen[0]
x, y = batch_0
assert np.array_equal(x,
                      np.array([[[0], [2], [4], [6], [8]],
                                [[1], [3], [5], [7], [9]]]))
assert np.array_equal(y,
                      np.array([[10], [11]]))
""",
        """CODE.from skimage.io import imread
from skimage.transform import resize
import numpy as np
import math

class CIFAR10Sequence(Sequence):

    def __init__(self, x_set, y_set, batch_size):
        self.x, self.y = x_set, y_set
        self.batch_size = batch_size

    def __len__(self):
        return math.ceil(len(self.x) / self.batch_size)

    def __getitem__(self, idx):
        batch_x = self.x[idx * self.batch_size:(idx + 1) * self.batch_size]
        batch_y = self.y[idx * self.batch_size:(idx + 1) * self.batch_size]

        return np.array([
            resize(imread(file_name), (200, 200))
               for file_name in batch_x]), np.array(batch_y)
""",
        """CODE.from tensorflow.keras import backend as K
kvar = K.variable(np.random.random((2,3)))
kvar_zeros = K.zeros_like(kvar)
K.eval(kvar_zeros)
# array([[ 0.,  0.,  0.], [ 0.,  0.,  0.]], dtype=float32)
""",
        """CODE.get_custom_objects().clear()
get_custom_objects()['MyObject'] = MyObject
""",
        """CODE.global_step = min(global_step, decay_steps)
cosine_decay = 0.5 * (1 + cos(pi * global_step / decay_steps))
decayed = (1 - alpha) * cosine_decay + alpha
decayed_learning_rate = learning_rate * decayed


decay_steps = 1000
lr_decayed = cosine_decay(learning_rate, global_step, decay_steps)
""",
        """CODE.global_step = min(global_step, decay_steps)
decayed_learning_rate = (learning_rate - end_learning_rate) * (1 - global_step / decay_steps) ** power + end_learning_rate


decay_steps = decay_steps * ceil(global_step / decay_steps)
decayed_learning_rate = (learning_rate - end_learning_rate) * (1 - global_step / decay_steps) ** power + end_learning_rate


global_step = tf.Variable(0, trainable=False)
starter_learning_rate = 0.1
end_learning_rate = 0.01
decay_steps = 10000
learning_rate = tf.compat.v1.train.polynomial_decay(starter_learning_rate, global_step, decay_steps, end_learning_rate, power=0.5)
learning_step = (tf.compat.v1.train.GradientDescentOptimizer(learning_rate).minimize(...my loss..., global_step=global_step))
""",
        """CODE.global_step = min(global_step, decay_steps)
linear_decay = (decay_steps - global_step) / decay_steps)
cosine_decay = 0.5 * (
    1 + cos(pi * 2 * num_periods * global_step / decay_steps))
decayed = (alpha + linear_decay + eps_t) * cosine_decay + beta
decayed_learning_rate = learning_rate * decayed

decay_steps = 1000
lr_decayed = noisy_linear_cosine_decay(
  learning_rate, global_step, decay_steps)
""",
        """CODE.global_step = min(global_step, decay_steps)
linear_decay = (decay_steps - global_step) / decay_steps)
cosine_decay = 0.5 * (
    1 + cos(pi * 2 * num_periods * global_step / decay_steps))
decayed = (alpha + linear_decay) * cosine_decay + beta
decayed_learning_rate = learning_rate * decayed


decay_steps = 1000
lr_decayed = linear_cosine_decay(learning_rate, global_step, decay_steps)
""",
        """CODE.global_step = tf.Variable(0, trainable=False)
boundaries = [100000, 110000]
values = [1.0, 0.5, 0.1]
learning_rate = tf.compat.v1.train.piecewise_constant(global_step, boundaries, values)
""",
        """CODE.global_step = tf.Variable(0, trainable=False)
starter_learning_rate = 0.1
learning_rate = tf.compat.v1.train.exponential_decay(starter_learning_rate, global_step, 100000, 0.96, staircase=True)
learning_step = (tf.compat.v1.train.GradientDescentOptimizer(learning_rate).minimize(...my loss..., global_step=global_step))
""",
        """CODE.grads = tape.gradient(loss, vars)
grads = tf.distribute.get_replica_context().all_reduce('sum', grads)
# Processing aggregated gradients.
optimizer.apply_gradients(zip(grads, vars),
    experimental_aggregate_gradients=False)
""",
        """CODE.h = tf.keras.losses.CategoricalHinge()
h(y_true, y_pred).numpy()

h(y_true, y_pred, sample_weight=[1, 0]).numpy()

h = tf.keras.losses.CategoricalHinge(
    reduction=tf.keras.losses.Reduction.SUM)
h(y_true, y_pred).numpy()

h = tf.keras.losses.CategoricalHinge(
    reduction=tf.keras.losses.Reduction.NONE)
h(y_true, y_pred).numpy()

model.compile(optimizer='sgd', loss=tf.keras.losses.CategoricalHinge())
""",
        """CODE.image = tf.keras.preprocessing.image.load_img(image_path)
input_arr = tf.keras.preprocessing.image.img_to_array(image)
input_arr = np.array([input_arr])  # Convert single image to a batch.
predictions = model.predict(input_arr)
""",
        """CODE.import keras

input1 = keras.layers.Input(shape=(16,))
x1 = keras.layers.Dense(8, activation='relu')(input1)
input2 = keras.layers.Input(shape=(32,))
x2 = keras.layers.Dense(8, activation='relu')(input2)
# Equivalent to subtracted = keras.layers.subtract([x1, x2])
subtracted = keras.layers.Subtract()([x1, x2])

out = keras.layers.Dense(4)(subtracted)
model = keras.models.Model(inputs=[input1, input2], outputs=out)
""",
        """CODE.import tensorflow as tf

# Behavior of some cells or feature columns may depend on whether we are in
# training or inference mode, e.g. applying dropout.
training = True
rating = tf.feature_column.sequence_numeric_column('rating')
watches = tf.feature_column.sequence_categorical_column_with_identity(
    'watches', num_buckets=1000)
watches_embedding = tf.feature_column.embedding_column(watches,
                                            dimension=10)
columns = [rating, watches_embedding]

features = {
 'rating': tf.sparse.from_dense([[1.0,1.1, 0, 0, 0],
                                             [2.0,2.1,2.2, 2.3, 2.5]]),
 'watches': tf.sparse.from_dense([[2, 85, 0, 0, 0],[33,78, 2, 73, 1]])
}

sequence_input_layer = tf.keras.experimental.SequenceFeatures(columns)
sequence_input, sequence_length = sequence_input_layer(
   features, training=training)
sequence_length_mask = tf.sequence_mask(sequence_length)
hidden_size = 32
rnn_cell = tf.keras.layers.SimpleRNNCell(hidden_size)
rnn_layer = tf.keras.layers.RNN(rnn_cell)
outputs, state = rnn_layer(sequence_input, mask=sequence_length_mask)
""",
        """CODE.import tensorflow as tf

# Create a tf.keras model.
model = tf.keras.Sequential()
model.add(tf.keras.layers.Dense(1, input_shape=[10]))
model.summary()

# Save the tf.keras model in the SavedModel format.
path = '/tmp/simple_keras_model'
tf.keras.experimental.export_saved_model(model, path)

# Load the saved keras model back.
new_model = tf.keras.experimental.load_from_saved_model(path)
new_model.summary()
""",
        """CODE.import tensorflow as tf

class ExampleRandomNormal(tf.keras.initializers.Initializer):

  def __init__(self, mean, stddev):
    self.mean = mean
    self.stddev = stddev

  def __call__(self, shape, dtype=None, **kwargs):
    return tf.random.normal(
        shape, mean=self.mean, stddev=self.stddev, dtype=dtype)

  def get_config(self):  # To support serialization
    return {"mean": self.mean, "stddev": self.stddev}
""",
        """CODE.import tensorflow as tf

inputs = tf.keras.Input(shape=(3,))
x = tf.keras.layers.Dense(4, activation=tf.nn.relu)(inputs)
outputs = tf.keras.layers.Dense(5, activation=tf.nn.softmax)(x)
model = tf.keras.Model(inputs=inputs, outputs=outputs)


import tensorflow as tf

class MyModel(tf.keras.Model):

  def __init__(self):
    super(MyModel, self).__init__()
    self.dense1 = tf.keras.layers.Dense(4, activation=tf.nn.relu)
    self.dense2 = tf.keras.layers.Dense(5, activation=tf.nn.softmax)

  def call(self, inputs):
    x = self.dense1(inputs)
    return self.dense2(x)


import tensorflow as tf

class MyModel(tf.keras.Model):

  def __init__(self):
    super(MyModel, self).__init__()
    self.dense1 = tf.keras.layers.Dense(4, activation=tf.nn.relu)
    self.dense2 = tf.keras.layers.Dense(5, activation=tf.nn.softmax)
    self.dropout = tf.keras.layers.Dropout(0.5)

  def call(self, inputs, training=False):
    x = self.dense1(inputs)
    if training:
      x = self.dropout(x, training=training)
    return self.dense2(x)
""",
        """CODE.import tensorflow as tf
model = tf.keras.Sequential()
model.add(tf.keras.layers.Conv2D(32, (3, 3), activation='elu', input_shape=(28, 28, 1))
model.add(tf.keras.layers.MaxPooling2D((2, 2))
model.add(tf.keras.layers.Conv2D(64, (3, 3), activation='elu'))
model.add(tf.keras.layers.MaxPooling2D((2, 2))
model.add(tf.keras.layers.Conv2D(64, (3, 3), activation='elu'))
""",
        """CODE.initializer = RandomUniform(-1, 1)
config = initializer.get_config()
initializer = RandomUniform.from_config(config)
""",
        """CODE.initializer = tf.keras.initializers.Constant(3.)
values = initializer(shape=(2, 2))
initializer = tf.keras.initializers.Constant(3.)
layer = tf.keras.layers.Dense(3, kernel_initializer=initializer)""",
        """CODE.initializer = tf.keras.initializers.GlorotNormal()
values = initializer(shape=(2, 2))

initializer = tf.keras.initializers.GlorotNormal()
layer = tf.keras.layers.Dense(3, kernel_initializer=initializer)""",
        """CODE.initializer = tf.keras.initializers.GlorotUniform()
values = initializer(shape=(2, 2))

initializer = tf.keras.initializers.GlorotUniform()
layer = tf.keras.layers.Dense(3, kernel_initializer=initializer)""",
        """CODE.initializer = tf.keras.initializers.HeNormal()
values = initializer(shape=(2, 2))

initializer = tf.keras.initializers.HeNormal()
layer = tf.keras.layers.Dense(3, kernel_initializer=initializer)
""",
        """CODE.initializer = tf.keras.initializers.HeUniform()
values = initializer(shape=(2, 2))

initializer = tf.keras.initializers.HeUniform()
layer = tf.keras.layers.Dense(3, kernel_initializer=initializer)""",
        """CODE.initializer = tf.keras.initializers.LecunNormal()
values = initializer(shape=(2, 2))
initializer = tf.keras.initializers.LecunNormal()
layer = tf.keras.layers.Dense(3, kernel_initializer=initializer)""",
        """CODE.initializer = tf.keras.initializers.LecunUniform()
values = initializer(shape=(2, 2))

initializer = tf.keras.initializers.LecunUniform()
layer = tf.keras.layers.Dense(3, kernel_initializer=initializer)""",
        """CODE.initializer = tf.keras.initializers.Ones()
values = initializer(shape=(2, 2))
initializer = tf.keras.initializers.Ones()
layer = tf.keras.layers.Dense(3, kernel_initializer=initializer)""",
        """CODE.initializer = tf.keras.initializers.RandomNormal(mean=0., stddev=1.)
values = initializer(shape=(2, 2))
initializer = tf.keras.initializers.RandomNormal(mean=0., stddev=1.)
layer = tf.keras.layers.Dense(3, kernel_initializer=initializer)""",
        """CODE.initializer = tf.keras.initializers.RandomUniform(minval=0., maxval=1.)
values = initializer(shape=(2, 2))
initializer = tf.keras.initializers.RandomUniform(minval=0., maxval=1.)
layer = tf.keras.layers.Dense(3, kernel_initializer=initializer)""",
        """CODE.initializer = tf.keras.initializers.TruncatedNormal(mean=0., stddev=1.)
values = initializer(shape=(2, 2))
initializer = tf.keras.initializers.TruncatedNormal(mean=0., stddev=1.)
layer = tf.keras.layers.Dense(3, kernel_initializer=initializer)""",
        """CODE.initializer = tf.keras.initializers.VarianceScaling(
    scale=0.1, mode='fan_in', distribution='uniform')

values = initializer(shape=(2, 2))

initializer = tf.keras.initializers.VarianceScaling(
    scale=0.1, mode='fan_in', distribution='uniform')

layer = tf.keras.layers.Dense(3, kernel_initializer=initializer)""",
        """CODE.initializer = tf.keras.initializers.Zeros()
values = initializer(shape=(2, 2))
initializer = tf.keras.initializers.Zeros()
layer = tf.keras.layers.Dense(3, kernel_initializer=initializer)""",
        """CODE.inp = np.asarray([1., 2., 1.])
layer = tf.keras.layers.Softmax()
layer(inp).numpy()
array([0.21194157, 0.5761169 , 0.21194157], dtype=float32)
mask = np.asarray([True, False, True], dtype=bool)
layer(inp, mask).numpy()
array([0.5, 0. , 0.5], dtype=float32)""",
        """CODE.inp_1 = ['a', 'b', 'c']
inp_2 = ['d', 'e', 'f']
layer = tf.keras.layers.experimental.preprocessing.CategoryCrossing()
layer([inp_1, inp_2])

inp_1 = ['a', 'b', 'c']
inp_2 = ['d', 'e', 'f']
layer = tf.keras.layers.experimental.preprocessing.CategoryCrossing(
   separator='-')
layer([inp_1, inp_2])""",
        """CODE.input = tf.keras.Input(shape=(100,), dtype='int32', name='input')
x = tf.keras.layers.Embedding(
    output_dim=512, input_dim=10000, input_length=100)(input)
x = tf.keras.layers.LSTM(32)(x)
x = tf.keras.layers.Dense(64, activation='relu')(x)
x = tf.keras.layers.Dense(64, activation='relu')(x)
x = tf.keras.layers.Dense(64, activation='relu')(x)
output = tf.keras.layers.Dense(1, activation='sigmoid', name='output')(x)
model = tf.keras.Model(inputs=[input], outputs=[output])
dot_img_file = '/tmp/model_1.png'
tf.keras.utils.plot_model(model, to_file=dot_img_file, show_shapes=True)
""",
        """CODE.input = tf.keras.backend.ones(shape=(1,3))
print(input)
<tf.Variable 'Variable:0' shape=(1, 3) dtype=float32,
numpy=array([[1., 1., 1.]], dtype=float32)>
cast_input = tf.keras.backend.cast(input, dtype='float64')
print(cast_input)
tf.Tensor([[1. 1. 1.]], shape=(1, 3), dtype=float64)""",
        """CODE.input = tf.keras.backend.placeholder(shape=(2, 4, 5))
tf.keras.backend.int_shape(input)
(2, 4, 5)
val = np.array([[1, 2], [3, 4]])
kvar = tf.keras.backend.variable(value=val)
tf.keras.backend.int_shape(kvar)
(2, 2)""",
        """CODE.input = tf.keras.backend.placeholder(shape=(2, 4, 5))
val = np.array([[1, 2], [3, 4]])
kvar = tf.keras.backend.variable(value=val)
tf.keras.backend.ndim(input)
3
tf.keras.backend.ndim(kvar)
2""",
        """CODE.input = tf.keras.layers.Input(shape=(3,))
d = tf.keras.layers.Dense(2)
output = d(input)
d.add_metric(tf.reduce_max(output), name='max')
d.add_metric(tf.reduce_min(output), name='min')
[m.name for m in d.metrics]
['max', 'min']""",
        """CODE.input_data = data[:-10]
targets = data[10:]
dataset = tf.keras.preprocessing.timeseries_dataset_from_array(
    input_data, targets, sequence_length=10)
for batch in dataset:
  inputs, targets = batch
  assert np.array_equal(inputs[0], data[:10])  # First sequence: steps [0-9]
  assert np.array_equal(targets[0], data[10])  # Corresponding target: step 10
  break


X = np.arange(100)
Y = X*2

sample_length = 20
input_dataset = tf.keras.preprocessing.timeseries_dataset_from_array(
  X, None, sequence_length=sample_length, sequence_stride=sample_length)
target_dataset = tf.keras.preprocessing.timeseries_dataset_from_array(
  Y, None, sequence_length=sample_length, sequence_stride=sample_length)

for batch in zip(input_dataset, target_dataset):
  inputs, targets = batch
  assert np.array_equal(inputs[0], X[:sample_length])

  # second sample equals output timestamps 20-40
  assert np.array_equal(targets[1], Y[sample_length:2*sample_length])
  break
""",
        """CODE.input_img = np.random.random((32, 224, 224, 3))
layer = tf.keras.layers.RandomZoom(.5, .2)
out_img = layer(input_img)
out_img.shape
""",
        """CODE.input_ph = tf.keras.backend.placeholder(shape=(2, 4, 5))
input_ph
""",
        """CODE.input_shape = (1, 1, 2, 2)
x = np.arange(np.prod(input_shape)).reshape(input_shape)
print(x)
[[[[0 1]
   [2 3]]]]
y = tf.keras.layers.ZeroPadding2D(padding=1)(x)
print(y)
tf.Tensor(
  [[[[0 0]
     [0 0]
     [0 0]
     [0 0]]
    [[0 0]
     [0 1]
     [2 3]
     [0 0]]
    [[0 0]
     [0 0]
     [0 0]
     [0 0]]]], shape=(1, 3, 4, 2), dtype=int64)""",
        """CODE.input_shape = (1, 1, 2, 2, 3)
x = np.arange(np.prod(input_shape)).reshape(input_shape)
y = tf.keras.layers.ZeroPadding3D(padding=2)(x)
print(y.shape)
(1, 5, 6, 6, 3)""",
        """CODE.input_shape = (2, 1, 2, 1, 3)
x = tf.constant(1, shape=input_shape)
y = tf.keras.layers.UpSampling3D(size=2)(x)
print(y.shape)
(2, 2, 4, 2, 3)""",
        """CODE.input_shape = (2, 2, 1, 3)
x = np.arange(np.prod(input_shape)).reshape(input_shape)
print(x)
[[[[ 0  1  2]]
  [[ 3  4  5]]]
 [[[ 6  7  8]]
  [[ 9 10 11]]]]
y = tf.keras.layers.UpSampling2D(size=(1, 2))(x)
print(y)
tf.Tensor(
  [[[[ 0  1  2]
     [ 0  1  2]]
    [[ 3  4  5]
     [ 3  4  5]]]
   [[[ 6  7  8]
     [ 6  7  8]]
    [[ 9 10 11]
     [ 9 10 11]]]], shape=(2, 2, 2, 3), dtype=int64)""",
        """CODE.input_shape = (2, 2, 3)
x = np.arange(np.prod(input_shape)).reshape(input_shape)
print(x)
[[[ 0  1  2]
  [ 3  4  5]]
 [[ 6  7  8]
  [ 9 10 11]]]
y = tf.keras.layers.UpSampling1D(size=2)(x)
print(y)
tf.Tensor(
  [[[ 0  1  2]
    [ 0  1  2]
    [ 3  4  5]
    [ 3  4  5]]
   [[ 6  7  8]
    [ 6  7  8]
    [ 9 10 11]
    [ 9 10 11]]], shape=(2, 4, 3), dtype=int64)""",
        """CODE.input_shape = (2, 2, 3)
x = np.arange(np.prod(input_shape)).reshape(input_shape)
print(x)
[[[ 0  1  2]
  [ 3  4  5]]
 [[ 6  7  8]
  [ 9 10 11]]]
y = tf.keras.layers.ZeroPadding1D(padding=2)(x)
print(y)
tf.Tensor(
  [[[ 0  0  0]
    [ 0  0  0]
    [ 0 1 2]
    [ 3 4 5]
    [ 0 0 0]
    [ 0 0 0]]
   [[ 0 0 0]
    [ 0 0 0]
    [ 6 7 8]
    [ 9 10 11]
    [ 0 0 0]
    [ 0 0 0]]], shape=(2, 6, 3), dtype=int64)""",
        """CODE.input_shape = (2, 28, 28, 10, 3)
x = np.arange(np.prod(input_shape)).reshape(input_shape)
y = tf.keras.layers.Cropping3D(cropping=(2, 4, 2))(x)
print(y.shape)
(2, 24, 20, 6, 3)""",
        """CODE.input_shape = (2, 28, 28, 3)
x = np.arange(np.prod(input_shape)).reshape(input_shape)
y = tf.keras.layers.Cropping2D(cropping=((2, 2), (4, 4)))(x)
print(y.shape)
(2, 24, 20, 3)""",
        """CODE.input_shape = (2, 3, 2)
x = np.arange(np.prod(input_shape)).reshape(input_shape)
print(x)
[[[ 0  1]
  [ 2  3]
  [ 4  5]]
 [[ 6  7]
  [ 8  9]
  [10 11]]]
y = tf.keras.layers.Cropping1D(cropping=1)(x)
print(y)
tf.Tensor(
  [[[2 3]]
   [[8 9]]], shape=(2, 1, 2), dtype=int64)""",
        """CODE.input_shape = (2, 3, 4)
x = tf.random.normal(input_shape)
y = tf.keras.layers.GlobalAveragePooling1D()(x)
print(y.shape)
(2, 4)""",
        """CODE.input_shape = (2, 3, 4)
x1 = tf.random.normal(input_shape)
x2 = tf.random.normal(input_shape)
y = tf.keras.layers.Add()([x1, x2])
print(y.shape)

input1 = tf.keras.layers.Input(shape=(16,))
x1 = tf.keras.layers.Dense(8, activation='relu')(input1)
input2 = tf.keras.layers.Input(shape=(32,))
x2 = tf.keras.layers.Dense(8, activation='relu')(input2)
added = tf.keras.layers.Add()([x1, x2])
out = tf.keras.layers.Dense(4)(added)
model = tf.keras.models.Model(inputs=[input1, input2], outputs=out)""",
        """CODE.input_shape = (2, 3, 4)
x1 = tf.random.normal(input_shape)
x2 = tf.random.normal(input_shape)
y = tf.keras.layers.add([x1, x2])
print(y.shape)

input1 = tf.keras.layers.Input(shape=(16,))
x1 = tf.keras.layers.Dense(8, activation='relu')(input1)
input2 = tf.keras.layers.Input(shape=(32,))
x2 = tf.keras.layers.Dense(8, activation='relu')(input2)
added = tf.keras.layers.add([x1, x2])
out = tf.keras.layers.Dense(4)(added)
model = tf.keras.models.Model(inputs=[input1, input2], outputs=out)""",
        """CODE.input_shape = (2, 4, 5, 3)
x = tf.random.normal(input_shape)
y = tf.keras.layers.GlobalAveragePooling2D()(x)
print(y.shape)
(2, 3)""",
        """CODE.input_shape = (4, 10, 128)
x = tf.random.normal(input_shape)
y = tf.keras.layers.Conv1D(
32, 3, activation='relu',input_shape=input_shape[1:])(x)
print(y.shape)

input_shape = (4, 7, 10, 128)
x = tf.random.normal(input_shape)
y = tf.keras.layers.Conv1D(
32, 3, activation='relu', input_shape=input_shape[2:])(x)
print(y.shape)
""",
        """CODE.input_shape = (4, 28, 28, 3)
x = tf.random.normal(input_shape)
y = tf.keras.layers.Conv2D(
2, 3, activation='relu', input_shape=input_shape[1:])(x)
print(y.shape)
(4, 26, 26, 2)

input_shape = (4, 28, 28, 3)
x = tf.random.normal(input_shape)
y = tf.keras.layers.Conv2D(
2, 3, activation='relu', dilation_rate=2, input_shape=input_shape[1:])(x)
print(y.shape)
(4, 24, 24, 2)

input_shape = (4, 28, 28, 3)
x = tf.random.normal(input_shape)
y = tf.keras.layers.Conv2D(
2, 3, activation='relu', padding="same", input_shape=input_shape[1:])(x)
print(y.shape)
(4, 28, 28, 2)

input_shape = (4, 7, 28, 28, 3)
x = tf.random.normal(input_shape)
y = tf.keras.layers.Conv2D(
2, 3, activation='relu', input_shape=input_shape[2:])(x)
print(y.shape)
(4, 7, 26, 26, 2)
""",
        """CODE.input_shape =(4, 28, 28, 28, 1)
x = tf.random.normal(input_shape)
y = tf.keras.layers.Conv3D(
2, 3, activation='relu', input_shape=input_shape[1:])(x)
print(y.shape)
(4, 26, 26, 26, 2)

input_shape = (4, 7, 28, 28, 28, 1)
x = tf.random.normal(input_shape)
y = tf.keras.layers.Conv3D(
2, 3, activation='relu', input_shape=input_shape[2:])(x)
print(y.shape)
(4, 7, 26, 26, 26, 2)
""",
        """CODE.inputs = tf.keras.Input(shape=(10, 128, 128, 3)
conv_2d_layer = tf.keras.layers.Conv2D(64, (3, 3))
outputs = tf.keras.layers.TimeDistributed(conv_2d_layer)(inputs)
outputs.shape
TensorShape([None, 10, 126, 126, 64])""",
        """CODE.inputs = tf.keras.Input(shape=(depth, height, width, input_channels))
layer = tf.keras.layers.AveragePooling3D(pool_size=3)
outputs = layer(inputs)  # Shape: (batch_size, 10, 10, 10, 3)
""",
        """CODE.inputs = tf.keras.Input(shape=(depth, height, width, input_channels))
layer = tf.keras.layers.MaxPooling3D(pool_size=3)
outputs = layer(inputs)  # Shape: (batch_size, 10, 10, 10, 3)
""",
        """CODE.inputs = tf.keras.layers.Input(shape=(3,))
outputs = tf.keras.layers.Dense(2)(inputs)
model = tf.keras.models.Model(inputs=inputs, outputs=outputs)
model.compile(optimizer="Adam", loss="mse", metrics=["mae"])

x = np.random.random((2, 3))
y = np.random.randint(0, 2, (2, 2))
_ = model.fit(x, y, verbose=0)
assert all(float(m.result()) for m in model.metrics)

model.reset_metrics()
assert all(float(m.result()) == 0 for m in model.metrics)""",
        """CODE.inputs = tf.keras.layers.Input(shape=(3,))
outputs = tf.keras.layers.Dense(2)(inputs)
model = tf.keras.models.Model(inputs=inputs, outputs=outputs)
model.compile(optimizer="Adam", loss="mse", metrics=["mae"])
model.metrics_names
[]
x = np.random.random((2, 3))
y = np.random.randint(0, 2, (2, 2))
model.fit(x, y)
model.metrics_names
['loss', 'mae']
inputs = tf.keras.layers.Input(shape=(3,))
d = tf.keras.layers.Dense(2, name='out')
output_1 = d(inputs)
output_2 = d(inputs)
model = tf.keras.models.Model(inputs=inputs, outputs=[output_1, output_2])
model.compile(optimizer="Adam", loss="mse", metrics=["mae", "acc"])
model.fit(x, (y, y))
model.metrics_names
['loss', 'out_loss', 'out_1_loss', 'out_mae', 'out_acc', 'out_1_mae', 'out_1_acc']""",
        """CODE.inputs = tf.random.normal(shape=(32, 10))
outputs = tf.keras.activations.softmax(inputs)
tf.reduce_sum(outputs[0, :])  # Each sample in the batch now sums to 1
<tf.Tensor: shape=(), dtype=float32, numpy=1.0000001>

layer = tf.keras.layers.Dense(32, activation=tf.keras.activations.softmax)
""",
        """CODE.inputs = {'x2': tf.keras.Input(shape=(5,)),
...           'x1': tf.keras.Input(shape=(1,))}
norm_layer = tf.keras.layers.experimental.preprocessing.Normalization()
y = norm_layer(inputs['x2'])
y, z = tf.keras.layers.Lambda(lambda x: (x, x))(inputs['x1'])
outputs = [inputs['x1'], [y, z]]
stage = FunctionalPreprocessingStage(inputs, outputs)""",
        """CODE.keras_model = tf.keras.Model(...)
keras_model.compile(...)

estimator = tf.keras.estimator.model_to_estimator(keras_model)

def input_fn():
  return dataset_ops.Dataset.from_tensors(
      ({'features': features, 'sample_weights': sample_weights},
       targets))

estimator.train(input_fn, steps=1)


inputs = {'a': tf.keras.Input(..., name='a'),
          'b': tf.keras.Input(..., name='b')}
outputs = {'c': tf.keras.layers.Dense(..., name='c')(inputs['a']),
           'd': tf.keras.layers.Dense(..., name='d')(inputs['b'])}
keras_model = tf.keras.Model(inputs, outputs)
keras_model.compile(...)
export_outputs = {'c': tf.estimator.export.RegressionOutput,
                  'd': tf.estimator.export.ClassificationOutput}

estimator = tf.keras.estimator.model_to_estimator(
    keras_model, export_outputs=export_outputs)

def input_fn():
  return dataset_ops.Dataset.from_tensors(
      ({'features': features, 'sample_weights': sample_weights},
       targets))

estimator.train(input_fn, steps=1)
""",
        """CODE.kl = tf.keras.losses.KLDivergence()
kl(y_true, y_pred).numpy()
0.458

kl(y_true, y_pred, sample_weight=[0.8, 0.2]).numpy()
0.366

kl = tf.keras.losses.KLDivergence(
    reduction=tf.keras.losses.Reduction.SUM)
kl(y_true, y_pred).numpy()
0.916

kl = tf.keras.losses.KLDivergence(
    reduction=tf.keras.losses.Reduction.NONE)
kl(y_true, y_pred).numpy()
array([0.916, -3.08e-06], dtype=float32)
""",
        """CODE.kvar = tf.keras.backend.eye(3)
tf.keras.backend.eval(kvar)
array([[1.,  0.,  0.],
       [0.,  1.,  0.],
       [0.,  0.,  1.]], dtype=float32)""",
        """CODE.kvar = tf.keras.backend.ones((3,4))
tf.keras.backend.eval(kvar)""",
        "CODE.kvar = tf.keras.backend.random_uniform_variable(shape=(2,3), low=0.0, high=1.0)",
        """CODE.kvar = tf.keras.backend.variable(np.array([[1, 2], [3, 4]]),
                                 dtype='float32')
tf.keras.backend.eval(kvar)""",
        """CODE.kvar = tf.keras.backend.variable(np.random.random((2,3)))
kvar_ones = tf.keras.backend.ones_like(kvar)
tf.keras.backend.eval(kvar_ones)""",
        """CODE.kvar = tf.keras.backend.zeros((3,4))
A = tf.constant([1,2,3])
kvar2 = tf.keras.backend.zeros(A.shape) # [0., 0., 0.]
kvar3 = tf.keras.backend.zeros(A.shape,dtype=tf.int32)
kvar4 = tf.keras.backend.zeros([2,3])""",
        """CODE.layer = tf.keras.layers.Activation('relu')
output = layer([-3.0, -1.0, 0.0, 2.0])
list(output.numpy())
[0.0, 0.0, 0.0, 2.0]
layer = tf.keras.layers.Activation(tf.nn.relu)
output = layer([-3.0, -1.0, 0.0, 2.0])
list(output.numpy())
[0.0, 0.0, 0.0, 2.0]""",
        """CODE.layer = tf.keras.layers.CategoryEncoding(
          num_tokens=4, output_mode="one_hot")
layer([3, 2, 0, 1])


layer = tf.keras.layers.CategoryEncoding(
          num_tokens=4, output_mode="multi_hot")
layer([[0, 1], [0, 0], [1, 2], [3, 1]])


layer = tf.keras.layers.CategoryEncoding(
          num_tokens=4, output_mode="count")
count_weights = np.array([[.1, .2], [.1, .1], [.2, .3], [.4, .2]])
layer([[0, 1], [0, 0], [1, 2], [3, 1]], count_weights=count_weights)
""",
        """CODE.layer = tf.keras.layers.Dense(
    5, input_dim=5,
    kernel_initializer='ones',
    kernel_regularizer=tf.keras.regularizers.L1(0.01),
    activity_regularizer=tf.keras.regularizers.L2(0.01))
tensor = tf.ones(shape=(5, 5)) * 2.0
out = layer(tensor)
tf.math.reduce_sum(layer.losses)
tf.keras.regularizers.L1(0.3)
tf.keras.regularizers.L2(0.1)
tf.keras.regularizers.L1L2(l1=0.01, l2=0.01)
regularizer = tf.keras.regularizers.L2(2.)
tensor = tf.ones(shape=(5, 5))
regularizer(tensor)
@tf.keras.utils.register_keras_serializable(package='Custom', name='l1')
def l1_reg(weight_matrix):
   return 0.01 * tf.math.reduce_sum(tf.math.abs(weight_matrix))
layer = tf.keras.layers.Dense(5, input_dim=5,
    kernel_initializer='ones', kernel_regularizer=l1_reg)
tensor = tf.ones(shape=(5, 5))
out = layer(tensor)
layer.losses
@tf.keras.utils.register_keras_serializable(package='Custom', name='l2')
class L2Regularizer(tf.keras.regularizers.Regularizer):
  def __init__(self, l2=0.):  # pylint: disable=redefined-outer-name
    self.l2 = l2

  def __call__(self, x):
    return self.l2 * tf.math.reduce_sum(tf.math.square(x))

  def get_config(self):
    return {'l2': float(self.l2)}
layer = tf.keras.layers.Dense(
  5, input_dim=5, kernel_initializer='ones',
  kernel_regularizer=L2Regularizer(l2=0.5))
tensor = tf.ones(shape=(5, 5))
out = layer(tensor)
layer.losses
""",
        """CODE.layer = tf.keras.layers.LeakyReLU()
output = layer([-3.0, -1.0, 0.0, 2.0])
list(output.numpy())
[-0.9, -0.3, 0.0, 2.0]
layer = tf.keras.layers.LeakyReLU(alpha=0.1)
output = layer([-3.0, -1.0, 0.0, 2.0])
list(output.numpy())
[-0.3, -0.1, 0.0, 2.0]
""",
        """CODE.layer = tf.keras.layers.experimental.preprocessing.Normalization(
...     axis=None)
layer.adapt([0, 2])
model = tf.keras.Sequential(layer)
model.predict([0, 1, 2])
array([-1.,  0.,  1.], dtype=float32)
layer.adapt([-1, 1])
model.compile() # This is needed to re-compile model.predict!
model.predict([0, 1, 2])
array([0., 1., 2.], dtype=float32)

layer = tf.keras.layers.experimental.preprocessing.Normalization(
...     axis=None)
layer.adapt([0, 2])
input_ds = tf.data.Dataset.range(3)
normalized_ds = input_ds.map(layer)
list(normalized_ds.as_numpy_iterator())
[array([-1.], dtype=float32),
 array([0.], dtype=float32),
 array([1.], dtype=float32)]
layer.adapt([-1, 1])
normalized_ds = input_ds.map(layer) # Re-map over the input dataset.
list(normalized_ds.as_numpy_iterator())
[array([0.], dtype=float32),
 array([1.], dtype=float32),
 array([2.], dtype=float32)""",
        """CODE.layer_a = tf.keras.layers.Dense(1,
  kernel_initializer=tf.constant_initializer(1.))
a_out = layer_a(tf.convert_to_tensor([[1., 2., 3.]]))
layer_a.get_weights()
[array([[1.],
       [1.],
       [1.]], dtype=float32), array([0.], dtype=float32)]
layer_b = tf.keras.layers.Dense(1,
  kernel_initializer=tf.constant_initializer(2.))
b_out = layer_b(tf.convert_to_tensor([[10., 20., 30.]]))
layer_b.get_weights()
[array([[2.],
       [2.],
       [2.]], dtype=float32), array([0.], dtype=float32)]
layer_b.set_weights(layer_a.get_weights())
layer_b.get_weights()
[array([[1.],
       [1.],
       [1.]], dtype=float32), array([0.], dtype=float32)]""",
        """CODE.layer_a = tf.keras.layers.Dense(1,
  kernel_initializer=tf.constant_initializer(1.))
a_out = layer_a(tf.convert_to_tensor([[1., 2., 3.]]))
layer_a.get_weights()
[array([[1.],
   [1.],
   [1.]], dtype=float32), array([0.], dtype=float32)]
layer_b = tf.keras.layers.Dense(1,
  kernel_initializer=tf.constant_initializer(2.))
b_out = layer_b(tf.convert_to_tensor([[10., 20., 30.]]))
layer_b.get_weights()
[array([[2.],
   [2.],
   [2.]], dtype=float32), array([0.], dtype=float32)]
layer_b.set_weights(layer_a.get_weights())
layer_b.get_weights()
[array([[1.],
   [1.],
   [1.]], dtype=float32), array([0.], dtype=float32)]""",
        """CODE.layer_a = tf.keras.layers.Dense(1,
  kernel_initializer=tf.constant_initializer(1.))
a_out = layer_a(tf.convert_to_tensor([[1., 2., 3.]]))
layer_a.get_weights()
layer_b = tf.keras.layers.Dense(1,
  kernel_initializer=tf.constant_initializer(2.))
b_out = layer_b(tf.convert_to_tensor([[10., 20., 30.]]))
layer_b.get_weights()
layer_b.set_weights(layer_a.get_weights())
layer_b.get_weights()""",
        """CODE.layer_a = tf.keras.layers.Dense(1,
...   kernel_initializer=tf.constant_initializer(1.))
a_out = layer_a(tf.convert_to_tensor([[1., 2., 3.]]))
layer_a.get_weights()
[array([[1.],
       [1.],
       [1.]], dtype=float32), array([0.], dtype=float32)]
layer_b = tf.keras.layers.Dense(1,
...   kernel_initializer=tf.constant_initializer(2.))
b_out = layer_b(tf.convert_to_tensor([[10., 20., 30.]]))
layer_b.get_weights()
[array([[2.],
       [2.],
       [2.]], dtype=float32), array([0.], dtype=float32)]
layer_b.set_weights(layer_a.get_weights())
layer_b.get_weights()
[array([[1.],
       [1.],
       [1.]], dtype=float32), array([0.], dtype=float32)]""",
        """CODE.learning_rate = 0.1
decay_steps = 1.0
decay_rate = 0.5
global_step = tf.Variable(0, trainable=False)
learning_rate = tf.compat.v1.train.inverse_time_decay(learning_rate, global_step, decay_steps, decay_rate)
learning_step = (
    tf.compat.v1.train.GradientDescentOptimizer(learning_rate)
    .minimize(...my loss..., global_step=global_step)
)
""",
        """CODE.learning_rate = 0.1
decay_steps = 5
k = 0.5
learning_rate = tf.compat.v1.train.natural_exp_decay(learning_rate, global_step, decay_steps, k)

learning_step = (
    tf.compat.v1.train.GradientDescentOptimizer(learning_rate)
    .minimize(...my loss..., global_step=global_step)
)
""",
        """CODE.linear_model = LinearModel()
dnn_model = keras.Sequential([keras.layers.Dense(units=64),
                             keras.layers.Dense(units=1)])
combined_model = WideDeepModel(linear_model, dnn_model)
combined_model.compile(optimizer=['sgd', 'adam'], 'mse', ['mse'])
# define dnn_inputs and linear_inputs as separate numpy arrays or
# a single numpy array if dnn_inputs is same as linear_inputs.
combined_model.fit([linear_inputs, dnn_inputs], y, epochs)
# or define a single `tf.data.Dataset` that contains a single tensor or
# separate tensors for dnn_inputs and linear_inputs.
dataset = tf.data.Dataset.from_tensors(([linear_inputs, dnn_inputs], y))
combined_model.fit(dataset, epochs)


linear_model = LinearModel()
linear_model.compile('adagrad', 'mse')
linear_model.fit(linear_inputs, y, epochs)
dnn_model = keras.Sequential([keras.layers.Dense(units=1)])
dnn_model.compile('rmsprop', 'mse')
dnn_model.fit(dnn_inputs, y, epochs)
combined_model = WideDeepModel(linear_model, dnn_model)
combined_model.compile(optimizer=['sgd', 'adam'], 'mse', ['mse'])
combined_model.fit([linear_inputs, dnn_inputs], y, epochs)
""",
        """CODE.logcosh = log((exp(x) + exp(-x))/2)


m = tf.keras.metrics.LogCoshError()
m.update_state([[0, 1], [0, 0]], [[1, 1], [0, 0]])
m.result().numpy()


m.reset_state()
m.update_state([[0, 1], [0, 0]], [[1, 1], [0, 0]],
               sample_weight=[1, 0])
m.result().numpy()


model.compile(optimizer='sgd',
              loss='mse',
              metrics=[tf.keras.metrics.LogCoshError()])
""",
        """CODE.logcosh = log((exp(x) + exp(-x))/2)

y_true = [[0., 1.], [0., 0.]]
y_pred = [[1., 1.], [0., 0.]]
l = tf.keras.losses.LogCosh()
l(y_true, y_pred).numpy()

l(y_true, y_pred, sample_weight=[0.8, 0.2]).numpy()

l = tf.keras.losses.LogCosh(
    reduction=tf.keras.losses.Reduction.SUM)
l(y_true, y_pred).numpy()

l = tf.keras.losses.LogCosh(
    reduction=tf.keras.losses.Reduction.NONE)
l(y_true, y_pred).numpy()

model.compile(optimizer='sgd', loss=tf.keras.losses.LogCosh())
""",
        """CODE.loss = -sum(l2_norm(y_true) * l2_norm(y_pred))


cosine_loss = tf.keras.losses.CosineSimilarity(axis=1)


cosine_loss(y_true, y_pred).numpy()


cosine_loss(y_true, y_pred, sample_weight=[0.8, 0.2]).numpy()


cosine_loss = tf.keras.losses.CosineSimilarity(axis=1,
    reduction=tf.keras.losses.Reduction.SUM)


cosine_loss(y_true, y_pred).numpy()


cosine_loss = tf.keras.losses.CosineSimilarity(axis=1,
    reduction=tf.keras.losses.Reduction.NONE)


cosine_loss(y_true, y_pred).numpy()


model.compile(optimizer='sgd', loss=tf.keras.losses.CosineSimilarity(axis=1))
""",
        """CODE.loss = 0.5 * x^2                  if |x| <= d
loss = 0.5 * d^2 + d * (|x| - d)  if |x| > d


model.compile(optimizer='sgd', loss=tf.keras.losses.Huber())
""",
        """CODE.loss = abs(y_true - y_pred)
mae = tf.keras.losses.MeanAbsoluteError()
mae(y_true, y_pred).numpy()
mae(y_true, y_pred, sample_weight=[0.7, 0.3]).numpy()
mae = tf.keras.losses.MeanAbsoluteError(reduction=tf.keras.losses.Reduction.SUM)
mae(y_true, y_pred).numpy()
mae = tf.keras.losses.MeanAbsoluteError(reduction=tf.keras.losses.Reduction.NONE)
mae(y_true, y_pred).numpy()
model.compile(optimizer='sgd', loss=tf.keras.losses.MeanAbsoluteError())""",
        """CODE.loss = abs(y_true - y_pred)
mae = tf.keras.losses.MeanAbsoluteError()
mae(y_true, y_pred).numpy()
mae(y_true, y_pred, sample_weight=[0.7, 0.3]).numpy()
mae = tf.keras.losses.MeanAbsoluteError(reduction=tf.keras.losses.Reduction.SUM)
mae(y_true, y_pred).numpy()
mae = tf.keras.losses.MeanAbsoluteError(reduction=tf.keras.losses.Reduction.NONE)
mae(y_true, y_pred).numpy()
model.compile(optimizer='sgd', loss=tf.keras.losses.MeanAbsoluteError())
""",
        """CODE.loss = maximum(neg - pos + 1, 0)

neg=maximum((1-y_true)*y_pred) and pos=sum(y_true*y_pred)

y_true = [[0, 1], [0, 0]]
y_pred = [[0.6, 0.4], [0.4, 0.6]]
h = tf.keras.losses.CategoricalHinge()
h(y_true, y_pred).numpy()

h(y_true, y_pred, sample_weight=[1, 0]).numpy()

h = tf.keras.losses.CategoricalHinge(
    reduction=tf.keras.losses.Reduction.SUM)
h(y_true, y_pred).numpy()

h = tf.keras.losses.CategoricalHinge(
    reduction=tf.keras.losses.Reduction.NONE)
h(y_true, y_pred).numpy()

model.compile(optimizer='sgd', loss=tf.keras.losses.CategoricalHinge())
""",
        """CODE.loss = square(log(y_true + 1.) - log(y_pred + 1.))

y_true = [[0., 1.], [0., 0.]]
y_pred = [[1., 1.], [1., 0.]]
msle = tf.keras.losses.MeanSquaredLogarithmicError()
msle(y_true, y_pred).numpy()

msle(y_true, y_pred, sample_weight=[0.7, 0.3]).numpy()

msle = tf.keras.losses.MeanSquaredLogarithmicError(
    reduction=tf.keras.losses.Reduction.SUM)
msle(y_true, y_pred).numpy()

msle = tf.keras.losses.MeanSquaredLogarithmicError(
    reduction=tf.keras.losses.Reduction.NONE)
msle(y_true, y_pred).numpy()
array([0.240, 0.240], dtype=float32)

model.compile(optimizer='sgd',
              loss=tf.keras.losses.MeanSquaredLogarithmicError())
""",
        """CODE.lr_schedule = keras.optimizers.schedules.ExponentialDecay(
    initial_learning_rate=1e-2,
    decay_steps=10000,
    decay_rate=0.9)
optimizer = keras.optimizers.SGD(learning_rate=lr_schedule)


class MyLRSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):

  def __init__(self, initial_learning_rate):
    self.initial_learning_rate = initial_learning_rate

  def __call__(self, step):
     return self.initial_learning_rate / (step + 1)

optimizer = tf.keras.optimizers.SGD(learning_rate=MyLRSchedule(0.1))
""",
        """CODE.m = tf.keras.metrics.Accuracy()
m.update_state([[1], [2], [3], [4]], [[0], [2], [3], [4]])
m.result().numpy()
0.75

m.reset_state()
m.update_state([[1], [2], [3], [4]], [[0], [2], [3], [4]],
               sample_weight=[1, 1, 0, 0])
m.result().numpy()
0.5

model.compile(optimizer='sgd',
              loss='mse',
              metrics=[tf.keras.metrics.Accuracy()])
""",
        """CODE.m = tf.keras.metrics.BinaryAccuracy()
m.update_state([[1], [1], [0], [0]], [[0.98], [1], [0], [0.6]])
m.result().numpy()

m.reset_state()
m.update_state([[1], [1], [0], [0]], [[0.98], [1], [0], [0.6]],
               sample_weight=[1, 0, 0, 1])
m.result().numpy()

model.compile(optimizer='sgd',
              loss='mse',
              metrics=[tf.keras.metrics.BinaryAccuracy()])
""",
        """CODE.m = tf.keras.metrics.CategoricalAccuracy()
m.update_state([[0, 0, 1], [0, 1, 0]], [[0.1, 0.9, 0.8],
...                 [0.05, 0.95, 0]])
m.result().numpy()
0.5

m.reset_state()
m.update_state([[0, 0, 1], [0, 1, 0]], [[0.1, 0.9, 0.8],
...                 [0.05, 0.95, 0]],
...                sample_weight=[0.7, 0.3])
m.result().numpy()
0.3

model.compile(
  optimizer='sgd',
  loss='mse',
  metrics=[tf.keras.metrics.CategoricalAccuracy()])
""",
        """CODE.m = tf.keras.metrics.FalseNegatives()
m.update_state([0, 1, 1, 1], [0, 1, 0, 0])
m.result().numpy()

m.reset_state()
m.update_state([0, 1, 1, 1], [0, 1, 0, 0], sample_weight=[0, 0, 1, 0])
m.result().numpy()

model.compile(optimizer='sgd',
              loss='mse',
              metrics=[tf.keras.metrics.FalseNegatives()])
""",
        """CODE.m = tf.keras.metrics.FalsePositives()
m.update_state([0, 1, 0, 0], [0, 0, 1, 1])
m.result().numpy()
2.0

m.reset_state()
m.update_state([0, 1, 0, 0], [0, 0, 1, 1], sample_weight=[0, 0, 1, 0])
m.result().numpy()
1.0

model.compile(optimizer='sgd',
              loss='mse',
              metrics=[tf.keras.metrics.FalsePositives()])
""",
        """CODE.m = tf.keras.metrics.Mean()
m.update_state([1, 3, 5, 7])
m.result().numpy()
4.0
m.reset_state()
m.update_state([1, 3, 5, 7], sample_weight=[1, 1, 0, 0])
m.result().numpy()
2.0
model.add_metric(tf.keras.metrics.Mean(name='mean_1')(outputs))
model.compile(optimizer='sgd', loss='mse')
""",
        """CODE.m = tf.keras.metrics.MeanIoU(num_classes=2)
m.update_state([0, 0, 1, 1], [0, 1, 0, 1])
m.result().numpy()
0.33333334

m.reset_state()
m.update_state([0, 0, 1, 1], [0, 1, 0, 1],
...                sample_weight=[0.3, 0.3, 0.3, 0.1])
m.result().numpy()
0.23809525
""",
        """CODE.m = tf.keras.metrics.MeanRelativeError(normalizer=[1, 3, 2, 3])
m.update_state([1, 3, 2, 3], [2, 4, 6, 8])

model.compile(
  optimizer='sgd',
  loss='mse',
  metrics=[tf.keras.metrics.MeanRelativeError(normalizer=[1, 3])])
""",
        """CODE.m = tf.keras.metrics.PrecisionAtRecall(0.5)
m.update_state([0, 0, 0, 1, 1], [0, 0.3, 0.8, 0.3, 0.8])
m.result().numpy()
0.5

m.reset_state()
m.update_state([0, 0, 0, 1, 1], [0, 0.3, 0.8, 0.3, 0.8],
...                sample_weight=[2, 2, 2, 1, 1])
m.result().numpy()
0.33333333

model.compile(
    optimizer='sgd',
    loss='mse',
    metrics=[tf.keras.metrics.PrecisionAtRecall(recall=0.8)])
""",
        """CODE.m = tf.keras.metrics.RootMeanSquaredError()
m.update_state([[0, 1], [0, 0]], [[1, 1], [0, 0]])
m.result().numpy()
m.reset_state()
m.update_state([[0, 1], [0, 0]], [[1, 1], [0, 0]],
               sample_weight=[1, 0])
m.result().numpy()""",
        """CODE.m = tf.keras.metrics.SensitivityAtSpecificity(0.5)
m.update_state([0, 0, 0, 1, 1], [0, 0.3, 0.8, 0.3, 0.8])
m.result().numpy()
0.5

m.reset_state()
m.update_state([0, 0, 0, 1, 1], [0, 0.3, 0.8, 0.3, 0.8],
...                sample_weight=[1, 1, 2, 2, 1])
m.result().numpy()
0.333333

model.compile(
    optimizer='sgd',
    loss='mse',
    metrics=[tf.keras.metrics.SensitivityAtSpecificity()])
""",
        """CODE.m = tf.keras.metrics.SpecificityAtSensitivity(0.5)
m.update_state([0, 0, 0, 1, 1], [0, 0.3, 0.8, 0.3, 0.8])
m.result().numpy()
0.66666667

m.reset_state()
m.update_state([0, 0, 0, 1, 1], [0, 0.3, 0.8, 0.3, 0.8],
...                sample_weight=[1, 1, 2, 2, 2])
m.result().numpy()
0.5

model.compile(
    optimizer='sgd',
    loss='mse',
    metrics=[tf.keras.metrics.SpecificityAtSensitivity()])
""",
        """CODE.m = tf.keras.metrics.Sum()
m.update_state([1, 3, 5, 7])
m.result().numpy()
16.0

model.add_metric(tf.keras.metrics.Sum(name='sum_1')(outputs))
model.compile(optimizer='sgd', loss='mse')
""",
        """CODE.m = tf.keras.metrics.TopKCategoricalAccuracy(k=1)
m.update_state([[0, 0, 1], [0, 1, 0]],
               [[0.1, 0.9, 0.8], [0.05, 0.95, 0]])
m.result().numpy()
0.5

m.reset_state()
m.update_state([[0, 0, 1], [0, 1, 0]],
               [[0.1, 0.9, 0.8], [0.05, 0.95, 0]],
               sample_weight=[0.7, 0.3])
m.result().numpy()
0.3
""",
        """CODE.m = tf.keras.metrics.TrueNegatives()
m.update_state([0, 1, 0, 0], [1, 1, 0, 0])
m.result().numpy()
2.0

m.reset_state()
m.update_state([0, 1, 0, 0], [1, 1, 0, 0], sample_weight=[0, 0, 1, 0])
m.result().numpy()
1.0
model.compile(optimizer='sgd',
              loss='mse',
              metrics=[tf.keras.metrics.TrueNegatives()])
""",
        """CODE.m = tf.keras.metrics.TruePositives()
m.update_state([0, 1, 1, 1], [1, 0, 1, 1])
m.result().numpy()
2.0

m.reset_state()
m.update_state([0, 1, 1, 1], [1, 0, 1, 1], sample_weight=[0, 0, 1, 0])
m.result().numpy()
1.0

model.compile(optimizer='sgd',
              loss='mse',
              metrics=[tf.keras.metrics.TruePositives()])
""",
        """CODE.mape = tf.keras.losses.MeanAbsolutePercentageError()
mape(y_true, y_pred).numpy()

mape(y_true, y_pred, sample_weight=[0.7, 0.3]).numpy()

mape = tf.keras.losses.MeanAbsolutePercentageError(
    reduction=tf.keras.losses.Reduction.SUM)
mape(y_true, y_pred).numpy()

mape = tf.keras.losses.MeanAbsolutePercentageError(
    reduction=tf.keras.losses.Reduction.NONE)
mape(y_true, y_pred).numpy()

model.compile(optimizer='sgd',
              loss=tf.keras.losses.MeanAbsolutePercentageError())
""",
        """CODE.mape = tf.keras.losses.MeanAbsolutePercentageError()
mape(y_true, y_pred).numpy()
50.

mape(y_true, y_pred, sample_weight=[0.7, 0.3]).numpy()
20.

mape = tf.keras.losses.MeanAbsolutePercentageError(
    reduction=tf.keras.losses.Reduction.SUM)
mape(y_true, y_pred).numpy()
100.

mape = tf.keras.losses.MeanAbsolutePercentageError(
    reduction=tf.keras.losses.Reduction.NONE)
mape(y_true, y_pred).numpy()
array([25., 75.], dtype=float32)
""",
        """CODE.model = LinearModel()
model.compile(optimizer='sgd', loss='mse')
model.fit(x, y, epochs=epochs)


model = LinearModel()
opt = tf.keras.optimizers.Adam()
loss_fn = tf.keras.losses.MeanSquaredError()
with tf.GradientTape() as tape:
  output = model(sparse_input)
  loss = tf.reduce_mean(loss_fn(target, output))
grads = tape.gradient(loss, model.weights)
opt.apply_gradients(zip(grads, model.weights))
""",
        """CODE.model = Sequential()
model.add(Bidirectional(LSTM(10, return_sequences=True), input_shape=(5, 10)))
model.add(Bidirectional(LSTM(10)))
model.add(Dense(5))
model.add(Activation('softmax'))
model.compile(loss='categorical_crossentropy', optimizer='rmsprop')

# With custom backward layer
model = Sequential()
forward_layer = LSTM(10, return_sequences=True)
backward_layer = LSTM(10, activation='relu', return_sequences=True,
                      go_backwards=True)
model.add(Bidirectional(forward_layer, backward_layer=backward_layer,
                        input_shape=(5, 10)))
model.add(Dense(5))
model.add(Activation('softmax'))
model.compile(loss='categorical_crossentropy', optimizer='rmsprop')
""",
        """CODE.model = Sequential()
model.add(Dense(32, input_dim=32))
model.add(RepeatVector(3))
""",
        """CODE.model = Sequential()
model.add(LocallyConnected1D(64, 3, input_shape=(10, 32)))
model.add(LocallyConnected1D(32, 3))
""",
        """CODE.model = Sequential()
model.add(LocallyConnected2D(64, (3, 3), input_shape=(32, 32, 3)))
model.add(LocallyConnected2D(32, (3, 3)))
""",
        """CODE.model = Sequential()
model.add(Permute((2, 1), input_shape=(10, 64)))
# now: model.output_shape == (None, 64, 10)
# note: `None` is the batch dimension
""",
        """CODE.model = keras.Sequential([
  keras.Input(shape=(784,)),
  RandomFourierFeatures(
      output_dim=4096,
      scale=10.,
      kernel_initializer='gaussian'),
  layers.Dense(units=10, activation='softmax'),
])
model.compile(
    optimizer='adam',
    loss='categorical_crossentropy',
    metrics=['categorical_accuracy']
)


model = keras.Sequential([
  keras.Input(shape=(784,)),
  RandomFourierFeatures(
      output_dim=4096,
      scale=10.,
      kernel_initializer='gaussian'),
  layers.Dense(units=10),
])
model.compile(
    optimizer='adam',
    loss='hinge',
    metrics=['categorical_accuracy']
)
""",
        """CODE.model = tf.keras.Model()
model.arr1 = []  # Creates a ListWrapper object
with no_automatic_dependency_tracking_scope(model):
  model.arr2 = []  # Creates a regular, untracked python list
""",
        """CODE.model = tf.keras.Model(...)

@tf.function
def serve(*args, **kwargs):
  outputs = model(*args, **kwargs)
  # Apply postprocessing steps, or add additional outputs.
  ...
  return outputs

# arg_specs is `[tf.TensorSpec(...), ...]`. kwarg_specs, in this example, is
# an empty dict since functional models do not use keyword arguments.
arg_specs, kwarg_specs = model.save_spec()

model.save(path, signatures={
  'serving_default': serve.get_concrete_function(*arg_specs, **kwarg_specs)
})
""",
        """CODE.model = tf.keras.Sequential()
model.add(tf.keras.layers.Conv2D(64, 3, 3, input_shape=(3, 32, 32)))
model.output_shape
(None, 1, 10, 64)

model.add(Flatten())
model.output_shape
(None, 640)
""",
        """CODE.model = tf.keras.Sequential()
model.add(tf.keras.layers.Conv2D(64, 3, 3, input_shape=(3, 32, 32)))
model.output_shape
model.add(Flatten())
model.output_shape
""",
        """CODE.model = tf.keras.Sequential()
model.add(tf.keras.layers.Dense(8, input_shape=(16,)))
model.add(tf.keras.layers.Dense(4))

model = tf.keras.Sequential()
model.add(tf.keras.Input(shape=(16,)))
model.add(tf.keras.layers.Dense(8))

model = tf.keras.Sequential()
model.add(tf.keras.layers.Dense(8))
model.add(tf.keras.layers.Dense(4))

model = tf.keras.Sequential()
model.add(tf.keras.layers.Dense(8, input_shape=(16,)))
model.add(tf.keras.layers.Dense(4))
len(model.weights)

model = tf.keras.Sequential()
model.add(tf.keras.layers.Dense(8))
model.add(tf.keras.layers.Dense(4))
model.build((None, 16))
len(model.weights)

model = tf.keras.Sequential()
model.add(tf.keras.layers.Dense(8))
model.add(tf.keras.layers.Dense(1))
model.compile(optimizer='sgd', loss='mse')
model.fit(x, y, batch_size=32, epochs=10)
""",
        """CODE.model = tf.keras.Sequential()
model.add(tf.keras.layers.Reshape((3, 4), input_shape=(12,)))
model.add(tf.keras.layers.Reshape((6, 2)))
model.add(tf.keras.layers.Reshape((-1, 2, 2)))""",
        """CODE.model = tf.keras.Sequential([
    tf.keras.layers.Dense(5, input_shape=(3,)),
    tf.keras.layers.Softmax()])
config = model.to_json()
loaded_model = tf.keras.models.model_from_json(config)""",
        """CODE.model = tf.keras.Sequential([
    tf.keras.layers.Dense(5, input_shape=(3,)),
    tf.keras.layers.Softmax()])
model.save('/tmp/model')
loaded_model = tf.keras.models.load_model('/tmp/model')
x = tf.random.uniform((10, 3))
assert np.allclose(model.predict(x), loaded_model.predict(x))""",
        """CODE.model = tf.keras.Sequential([
  tf.keras.layers.InputLayer(input_shape=(4,)),
  tf.keras.layers.Dense(8)])
model.compile(tf.optimizers.RMSprop(0.001), loss='mse')
model.fit(np.zeros((10, 4)),
          np.ones((10, 8)))

model = tf.keras.Sequential([
  tf.keras.layers.Dense(8, input_shape=(4,))])
model.compile(tf.optimizers.RMSprop(0.001), loss='mse')
model.fit(np.zeros((10, 4)),
          np.ones((10, 8)))
""",
        """CODE.model = tf.keras.Sequential([tf.keras.layers.Dense(10)])
model.compile(tf.keras.optimizers.SGD(), loss="mse")

def dataset_fn(input_context):
  global_batch_size = 64
  batch_size = input_context.get_per_replica_batch_size(global_batch_size)
  dataset = tf.data.Dataset.from_tensors(([1.], [1.])).repeat()
  dataset = dataset.shard(
      input_context.num_input_pipelines, input_context.input_pipeline_id)
  dataset = dataset.batch(batch_size)
  dataset = dataset.prefetch(2)
  return dataset

input_options = tf.distribute.InputOptions(
    experimental_fetch_to_device=True,
    experimental_per_replica_buffer_size=2)
model.fit(tf.keras.utils.experimental.DatasetCreator(
    dataset_fn, input_options=input_options), epochs=10, steps_per_epoch=10)

strategy = tf.distribute.experimental.ParameterServerStrategy(
    cluster_resolver)
with strategy.scope():
  model = tf.keras.Sequential([tf.keras.layers.Dense(10)])
model.compile(tf.keras.optimizers.SGD(), loss="mse")
""",
        """CODE.model = tf.keras.models.Sequential(...)
model.compile(metrics=tf.keras.metrics.SparseCategoricalAccuracy(
    name="eval_metrics"))
data = tf.data.Dataset.from_tensor_slices(...)

SidecarEvaluator(
    model=model,
    data=data,
    checkpoint_dir='/tmp/checkpoint_dir',  # dir for training-saved checkpoint
    steps=None,  # Eval until dataset is exhausted
    max_evaluations=None,  # The evaluation needs to be stopped manually
    callbacks=[tf.keras.callbacks.TensorBoard(log_dir='/tmp/log_dir')]
).start()


checkpoint_dir = ...  # Same `checkpoint_dir` supplied to `SidecarEvaluator`.
checkpoint = tf.train.Checkpoint(model=model, optimizer=optimizer)
checkpoint_manager = tf.train.CheckpointManager(
    checkpoint, checkpoint_dir=..., max_to_keep=...)
checkpoint_manager.save()

checkpoint_dir = ...  # Same `checkpoint_dir` supplied to `SidecarEvaluator`.
model_checkpoint = tf.keras.callbacks.ModelCheckpoint(
    filepath=os.path.join(checkpoint_dir, 'ckpt-{epoch}'),
    save_weights_only=True)
model.fit(dataset, epochs, callbacks=[model_checkpoint])
""",
        """CODE.model.add(Lambda(lambda x: x ** 2))
model.add(Lambda(antirectifier))
""",
        """CODE.model.add(tf.keras.layers.experimental.SyncBatchNormalization())
""",
        """CODE.model.compile(
    optimizer='sgd',
    loss='mse',
    metrics=[tf.keras.metrics.RecallAtPrecision(precision=0.8)])
""",
        """CODE.model.compile(
  loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),
  ....
)

y_true = [0, 1, 0, 0]
y_pred = [-18.6, 0.51, 2.94, -12.8]
bce = tf.keras.losses.BinaryCrossentropy(from_logits=True)
bce(y_true, y_pred).numpy()

y_true = [[0, 1], [0, 0]]
y_pred = [[-18.6, 0.51], [2.94, -12.8]]
bce = tf.keras.losses.BinaryCrossentropy(from_logits=True)
bce(y_true, y_pred).numpy()
bce(y_true, y_pred, sample_weight=[0.8, 0.2]).numpy()
bce = tf.keras.losses.BinaryCrossentropy(from_logits=True,
    reduction=tf.keras.losses.Reduction.SUM)
bce(y_true, y_pred).numpy()
bce = tf.keras.losses.BinaryCrossentropy(from_logits=True,
    reduction=tf.keras.losses.Reduction.NONE)
bce(y_true, y_pred).numpy()
""",
        """CODE.model.compile(
  loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),
  ....
)

y_true = [0, 1, 0, 0]
y_pred = [-18.6, 0.51, 2.94, -12.8]
bce = tf.keras.losses.BinaryCrossentropy(from_logits=True)
bce(y_true, y_pred).numpy()

y_true = [[0, 1], [0, 0]]
y_pred = [[-18.6, 0.51], [2.94, -12.8]]
bce = tf.keras.losses.BinaryCrossentropy(from_logits=True)
bce(y_true, y_pred).numpy()
bce(y_true, y_pred, sample_weight=[0.8, 0.2]).numpy()
bce = tf.keras.losses.BinaryCrossentropy(from_logits=True,
    reduction=tf.keras.losses.Reduction.SUM)
bce(y_true, y_pred).numpy()
bce = tf.keras.losses.BinaryCrossentropy(from_logits=True,
    reduction=tf.keras.losses.Reduction.NONE)
bce(y_true, y_pred).numpy()

tf.keras.losses.BinaryCrossentropy()
y_pred = [0.6, 0.3, 0.2, 0.8]
""",
        """CODE.model.compile(
  optimizer='sgd',
  loss='mse',
  metrics=[tf.keras.metrics.SparseTopKCategoricalAccuracy()])
""",
        """CODE.model.compile(loss=..., optimizer=...,
              metrics=['accuracy'])

EPOCHS = 10
checkpoint_filepath = '/tmp/checkpoint'
model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(
    filepath=checkpoint_filepath,
    save_weights_only=True,
    monitor='val_accuracy',
    mode='max',
    save_best_only=True)

model.fit(epochs=EPOCHS, callbacks=[model_checkpoint_callback])

model.load_weights(checkpoint_filepath)
""",
        """CODE.model.compile(optimizer=tf.keras.optimizer.Adam(learning_rate=1e-3),
              loss=tf.keras.losses.BinaryCrossentropy(),
              metrics=[tf.keras.metrics.BinaryAccuracy(),
                       tf.keras.metrics.FalseNegatives()])
""",
        """CODE.mse = tf.keras.losses.MeanSquaredError()
mse(y_true, y_pred).numpy()
0.5

mse(y_true, y_pred, sample_weight=[0.7, 0.3]).numpy()
0.25

mse = tf.keras.losses.MeanSquaredError(reduction=tf.keras.losses.Reduction.SUM)
mse(y_true, y_pred).numpy()
1.0

mse = tf.keras.losses.MeanSquaredError(reduction=tf.keras.losses.Reduction.NONE)
mse(y_true, y_pred).numpy()
array([0.5, 0.5], dtype=float32)
""",
        """CODE.mse = tf.keras.losses.MeanSquaredError()
mse(y_true, y_pred).numpy()
0.5

mse(y_true, y_pred, sample_weight=[0.7, 0.3]).numpy()
0.25

mse = tf.keras.losses.MeanSquaredError(reduction=tf.keras.losses.Reduction.SUM)
mse(y_true, y_pred).numpy()
1.0

mse = tf.keras.losses.MeanSquaredError(reduction=tf.keras.losses.Reduction.NONE)
mse(y_true, y_pred).numpy()
array([0.5, 0.5], dtype=float32)

model.compile(optimizer='sgd', loss=tf.keras.losses.MeanSquaredError())
""",
        """CODE.np_var = np.array([1, 2])
tf.keras.backend.is_keras_tensor(np_var)
keras_var = tf.keras.backend.variable(np_var)
tf.keras.backend.is_keras_tensor(keras_var)
keras_placeholder = tf.keras.backend.placeholder(shape=(2, 4, 5))
tf.keras.backend.is_keras_tensor(keras_placeholder)
keras_input = tf.keras.layers.Input([10])
tf.keras.backend.is_keras_tensor(keras_input)
keras_layer_output = tf.keras.layers.Dense(10)(keras_input)
tf.keras.backend.is_keras_tensor(keras_layer_output)""",
        """CODE.num_units = [128, 64]
cells = [BasicLSTMCell(num_units=n) for n in num_units]
stacked_rnn_cell = MultiRNNCell(cells)""",
        """CODE.opt = tf.keras.optimizers.RMSprop()
m = tf.keras.models.Sequential([tf.keras.layers.Dense(10)])
m.compile(opt, loss='mse')
data = np.arange(100).reshape(5, 20)
labels = np.zeros(5)
print('Training'); results = m.fit(data, labels)
len(opt.get_weights())""",
        """CODE.opt = tf.keras.optimizers.RMSprop()
m = tf.keras.models.Sequential([tf.keras.layers.Dense(10)])
m.compile(opt, loss='mse')
data = np.arange(100).reshape(5, 20)
labels = np.zeros(5)
print('Training'); results = m.fit(data, labels)
new_weights = [np.array(10), np.ones([20, 10]), np.zeros([10])]
opt.set_weights(new_weights)
opt.iterations""",
        """CODE.path_to_downloaded_file = tf.keras.utils.get_file(
    "flower_photos",
    "https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz",
    untar=True)
""",
        """CODE.price = tf.feature_column.numeric_column('price')
keywords_embedded = tf.feature_column.embedding_column(
    tf.feature_column.categorical_column_with_hash_bucket("keywords", 10K),
    dimension=16)
columns = [price, keywords_embedded, ...]
partitioner = tf.compat.v1.fixed_size_partitioner(num_shards=4)
feature_layer = tf.compat.v1.keras.layers.DenseFeatures(
    feature_columns=columns, partitioner=partitioner)

features = tf.io.parse_example(
    ..., features=tf.feature_column.make_parse_example_spec(columns))
dense_tensor = feature_layer(features)
for units in [128, 64, 32]:
  dense_tensor = tf.compat.v1.keras.layers.Dense(
                     units, activation='relu')(dense_tensor)
prediction = tf.compat.v1.keras.layers.Dense(1)(dense_tensor)
""",
        """CODE.query_input = tf.keras.Input(shape=(None,), dtype='int32')
value_input = tf.keras.Input(shape=(None,), dtype='int32')

token_embedding = tf.keras.layers.Embedding(max_tokens, dimension)
query_embeddings = token_embedding(query_input)
value_embeddings = token_embedding(value_input)

cnn_layer = tf.keras.layers.Conv1D(
    filters=100,
    kernel_size=4,
    padding='same')
query_seq_encoding = cnn_layer(query_embeddings)
value_seq_encoding = cnn_layer(value_embeddings)

query_value_attention_seq = tf.keras.layers.AdditiveAttention()(
    [query_seq_encoding, value_seq_encoding])

query_encoding = tf.keras.layers.GlobalAveragePooling1D()(
    query_seq_encoding)
query_value_attention = tf.keras.layers.GlobalAveragePooling1D()(
    query_value_attention_seq)

input_layer = tf.keras.layers.Concatenate()(
    [query_encoding, query_value_attention])
""",
        """CODE.random_normal_tensor = tf.keras.backend.random_normal(shape=(2,3),
mean=0.0, stddev=1.0)""",
        """CODE.reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2,
                              patience=5, min_lr=0.001)
model.fit(X_train, Y_train, callbacks=[reduce_lr])
""",
        """CODE.rnn = tf.keras.layers.RNN(tf.keras.layers.SimpleRNNCell(4))

output = rnn(inputs)  # The output has shape `[32, 4]`.

rnn = tf.keras.layers.RNN(
    tf.keras.layers.SimpleRNNCell(4),
    return_sequences=True,
    return_state=True)

# whole_sequence_output has shape `[32, 10, 4]`.
# final_state has shape `[32, 4]`.
whole_sequence_output, final_state = rnn(inputs)
""",
        """CODE.sample_text = 'This is a sample sentence.'
tf.keras.preprocessing.text.text_to_word_sequence(sample_text)""",
        """CODE.samples, timesteps, features = 32, 10, 8
inputs = np.random.random([samples, timesteps, features]).astype(np.float32)
inputs[:, 3, :] = 0.
inputs[:, 5, :] = 0.

model = tf.keras.models.Sequential()
model.add(tf.keras.layers.Masking(mask_value=0.,
                                  input_shape=(timesteps, features)))
model.add(tf.keras.layers.LSTM(32))

output = model(inputs)
# The time step 3 and 5 will be skipped from LSTM calculation.
""",
        """CODE.scce = tf.keras.losses.SparseCategoricalCrossentropy()
scce(y_true, y_pred).numpy()
1.177
scce(y_true, y_pred, sample_weight=tf.constant([0.3, 0.7])).numpy()
0.814
scce = tf.keras.losses.SparseCategoricalCrossentropy(
...     reduction=tf.keras.losses.Reduction.SUM)
scce(y_true, y_pred).numpy()
2.354
scce = tf.keras.losses.SparseCategoricalCrossentropy(
...     reduction=tf.keras.losses.Reduction.NONE)
scce(y_true, y_pred).numpy()
array([0.0513, 2.303], dtype=float32)""",
        """CODE.sequence = [[1], [2, 3], [4, 5, 6]]
tf.keras.preprocessing.sequence.pad_sequences(sequence)

tf.keras.preprocessing.sequence.pad_sequences(sequence, value=-1)

tf.keras.preprocessing.sequence.pad_sequences(sequence, padding='post')

tf.keras.preprocessing.sequence.pad_sequences(sequence, maxlen=2)
""",
        """CODE.set_keras_style()

model_1 = RNNModel(name="model_1")
model_2 = RNNModel(name="model_2")

# model_1 and model_2 are guaranteed to create their own variables.
output_1, next_state_1 = model_1(input, state)
output_2, next_state_2 = model_2(input, state)

assert len(model_1.weights) > 0
assert len(model_2.weights) > 0
assert(model_1.weights != model_2.weights)
""",
        """CODE.simple_rnn = tf.keras.layers.SimpleRNN(4)

output = simple_rnn(inputs)  # The output has shape `[32, 4]`.

simple_rnn = tf.keras.layers.SimpleRNN(
    4, return_sequences=True, return_state=True)

# whole_sequence_output has shape `[32, 10, 4]`.
# final_state has shape `[32, 4]`.
whole_sequence_output, final_state = simple_rnn(inputs)
""",
        """CODE.size = (200, 200)
ds = ds.map(lambda img: tf.image.resize(img, size))


size = (200, 200)
ds = ds.map(lambda img: smart_resize(img, size))
""",
        """CODE.step = tf.Variable(0, trainable=False)
boundaries = [100000, 110000]
values = [1.0, 0.5, 0.1]
learning_rate_fn = keras.optimizers.schedules.PiecewiseConstantDecay(
    boundaries, values)

# Later, whenever we perform an optimization step, we pass in the step.
learning_rate = learning_rate_fn(step)
""",
        """CODE.tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir="./logs")
model.fit(x_train, y_train, epochs=2, callbacks=[tensorboard_callback])
# Then run the tensorboard command to view the visualizations.


class MyModel(tf.keras.Model):

  def build(self, _):
    self.dense = tf.keras.layers.Dense(10)

  def call(self, x):
    outputs = self.dense(x)
    tf.summary.histogram('outputs', outputs)
    return outputs

model = MyModel()
model.compile('sgd', 'mse')

# Make sure to set `update_freq=N` to log a batch-level summary every N batches.
# In addition to any `tf.summary` contained in `Model.call`, metrics added in
# `Model.compile` will be logged every N batches.
tb_callback = tf.keras.callbacks.TensorBoard('./logs', update_freq=1)
model.fit(x_train, y_train, callbacks=[tb_callback])


def my_summary(x):
  tf.summary.histogram('x', x)
  return x

inputs = tf.keras.Input(10)
x = tf.keras.layers.Dense(10)(inputs)
outputs = tf.keras.layers.Lambda(my_summary)(x)
model = tf.keras.Model(inputs, outputs)
model.compile('sgd', 'mse')

# Make sure to set `update_freq=N` to log a batch-level summary every N batches.
# In addition to any `tf.summary` contained in `Model.call`, metrics added in
# `Model.compile` will be logged every N batches.
tb_callback = tf.keras.callbacks.TensorBoard('./logs', update_freq=1)
model.fit(x_train, y_train, callbacks=[tb_callback])


# Profile a single batch, e.g. the 5th batch.
tensorboard_callback = tf.keras.callbacks.TensorBoard(
    log_dir='./logs', profile_batch=5)
model.fit(x_train, y_train, epochs=2, callbacks=[tensorboard_callback])

# Profile a range of batches, e.g. from 10 to 20.
tensorboard_callback = tf.keras.callbacks.TensorBoard(
    log_dir='./logs', profile_batch=(10,20))
model.fit(x_train, y_train, epochs=2, callbacks=[tensorboard_callback])
""",
        """CODE.tf.constant([[1, 2], [3, 4]])
tf.keras.backend.repeat(b, n=2)""",
        """CODE.tf.keras.activations.deserialize('linear')
tf.keras.activations.deserialize('sigmoid')
tf.keras.activations.deserialize('abcd')""",
        """CODE.tf.keras.activations.get('softmax')
 <function softmax at 0x1222a3d90>
tf.keras.activations.get(tf.keras.activations.softmax)
 <function softmax at 0x1222a3d90>
tf.keras.activations.get(None)
 <function linear at 0x1239596a8>
tf.keras.activations.get(abs)
 <built-in function abs>
tf.keras.activations.get('abcd')
Traceback (most recent call last):
...
ValueError: Unknown activation function:abcd""",
        """CODE.tf.keras.activations.get('softmax')
tf.keras.activations.get(tf.keras.activations.softmax)
tf.keras.activations.get(None)
tf.keras.activations.get(abs)
tf.keras.activations.get('abcd')""",
        """CODE.tf.keras.activations.get('softmax')
tf.keras.activations.get(tf.keras.activations.softmax)
tf.keras.activations.get(None)
tf.keras.activations.get(abs)
tf.keras.activations.get('abcd')
""",
        """CODE.tf.keras.activations.serialize(tf.keras.activations.tanh)
tf.keras.activations.serialize(tf.keras.activations.sigmoid)
tf.keras.activations.serialize('abcd')""",
        "CODE.tf.keras.backend.arange(start=0, stop=10, step=1.5)",
        """CODE.tf.keras.backend.dtype(tf.keras.backend.placeholder(shape=(2,4,5)))
tf.keras.backend.dtype(tf.keras.backend.placeholder(shape=(2,4,5), dtype='float32'))
tf.keras.backend.dtype(tf.keras.backend.placeholder(shape=(2,4,5), dtype='float64'))
kvar = tf.keras.backend.variable(np.array([[1, 2], [3, 4]]))
tf.keras.backend.dtype(kvar)
kvar = tf.keras.backend.variable(np.array([[1, 2], [3, 4]]), dtype='float32')
tf.keras.backend.dtype(kvar)""",
        "CODE.tf.keras.backend.epsilon()",
        "CODE.tf.keras.backend.flatten(b)",
        "CODE.tf.keras.backend.floatx()",
        "CODE.tf.keras.backend.image_data_format()",
        "CODE.tf.keras.backend.repeat_elements(b, rep=2, axis=0)",
        """CODE.tf.keras.backend.set_epsilon(1e-5)
tf.keras.backend.set_epsilon(1e-7)""",
        """CODE.tf.keras.backend.set_floatx('float16')
tf.keras.backend.set_floatx('float32')
tf.keras.backend.set_floatx('float64')""",
        """CODE.tf.keras.backend.set_image_data_format('channels_first')
tf.keras.backend.set_image_data_format('channels_last')""",
        """CODE.tf.keras.mixed_precision.global_policy()
tf.keras.layers.Dense(10).dtype_policy""",
        """CODE.tf.random.set_seed(0)
layer = tf.keras.layers.Dropout(.2, input_shape=(2,))
data = np.arange(10).reshape(5, 2).astype(np.float32)
print(data)
[[0. 1.]
 [2. 3.]
 [4. 5.]
 [6. 7.]
 [8. 9.]]
outputs = layer(data, training=True)
print(outputs)
tf.Tensor(
[[ 0.    1.25]
 [ 2.5   3.75]
 [ 5.    6.25]
 [ 7.5   8.75]
 [10.    0.  ]], shape=(5, 2), dtype=float32)""",
        """CODE.val = np.array([[1, 2], [3, 4]])
kvar = tf.keras.backend.variable(value=val)
tf.keras.backend.shape(kvar)
input = tf.keras.backend.placeholder(shape=(2, 4, 5))
tf.keras.backend.shape(input)""",
        """CODE.val = np.array([[1, 2], [3, 4]])
kvar = tf.keras.backend.variable(value=val, dtype='float64', name='example_var')
tf.keras.backend.dtype(kvar)
print(kvar)""",
        """CODE.var = tf.keras.backend.variable([[1, 2, 3], [4, 5, 6])
tf.keras.backend.eval(var)
var_transposed = tf.keras.backend.transpose(var)
tf.keras.backend.eval(var_transposed)
input = tf.keras.backend.placeholder((2, 3))
input_transposed = tf.keras.backend.transpose(input)""",
        """CODE.var = tf.keras.backend.variable([[1, 2, 3], [4, 5, 6]])
tf.keras.backend.eval(var)
array([[1., 2., 3.],
       [4., 5., 6.]], dtype=float32)
var_gathered = tf.keras.backend.gather(var, [0])
tf.keras.backend.eval(var_gathered)
array([[1., 2., 3.]], dtype=float32)
var_gathered = tf.keras.backend.gather(var, [1])
tf.keras.backend.eval(var_gathered)
array([[4., 5., 6.]], dtype=float32)
var_gathered = tf.keras.backend.gather(var, [0,1,0])
tf.keras.backend.eval(var_gathered)
array([[1., 2., 3.],
       [4., 5., 6.],
       [1., 2., 3.]], dtype=float32)""",
        """CODE.x = Input(shape=(32,))
y = Dense(16, activation='softmax')(x)
model = Model(x, y)


x = Input(shape=(32,))
y = tf.square(x)
model = Model(x, y)


x = Input(type_spec=tf.RaggedTensorSpec(shape=[None, None], dtype=tf.float32, ragged_rank=1))
y = x.values
model = Model(x, y)
""",
        """CODE.x = [1]
y = [1]

class Reference:
    def __init__(self, obj):
        self.obj = obj

    def __eq__(self, other):
        return self.obj is other.obj

x_ref1 = Reference(x)
x_ref2 = Reference(x)
y_ref2 = Reference(y)

print(x_ref1 == x_ref2)
print(x_ref1 == y)
""",
        """CODE.x = np.arange(10).reshape(1, 5, 2)
print(x)
[[[0 1]
  [2 3]
  [4 5]
  [6 7]
  [8 9]]]
y = np.arange(10, 20).reshape(1, 2, 5)
print(y)
[[[10 11 12 13 14]
  [15 16 17 18 19]]]
tf.keras.layers.Dot(axes=(1, 2))([x, y])
<tf.Tensor: shape=(1, 2, 2), dtype=int64, numpy=
array([[[260, 360],
        [320, 445]]])>

x1 = tf.keras.layers.Dense(8)(np.arange(10).reshape(5, 2))
x2 = tf.keras.layers.Dense(8)(np.arange(10, 20).reshape(5, 2))
dotted = tf.keras.layers.Dot(axes=1)([x1, x2])
dotted.shape
TensorShape([5, 1])""",
        """CODE.x = np.arange(20).reshape(2, 2, 5)
print(x)
y = np.arange(20, 30).reshape(2, 1, 5)
print(y)
tf.keras.layers.Concatenate(axis=1)([x, y])

x1 = tf.keras.layers.Dense(8)(np.arange(10).reshape(5, 2))
x2 = tf.keras.layers.Dense(8)(np.arange(10, 20).reshape(5, 2))
concatted = tf.keras.layers.Concatenate()([x1, x2])
concatted.shape""",
        """CODE.x = tf.Variable(0.0)
momentum=0.9
x = x * momentum + value * (1 - momentum)
moving_average_update(x, value = 2.0, momentum=momentum).numpy()
x.numpy()
num_updates = 1.0
x_zdb = x/(1 - momentum**num_updates)
x_zdb.numpy()
""",
        """CODE.x = tf.Variable([[1, 2], [3, 4]])
y = tf.Variable([[2, 1], [0, -1]])
m = tf.keras.backend.maximum(x, y)
m
<tf.Tensor: shape=(2, 2), dtype=int32, numpy=
array([[2, 2],
       [3, 4]], dtype=int32)>""",
        """CODE.x = tf.constant([-3.0, -1.0, 0.0, 1.0, 3.0], dtype=tf.float32)
y = tf.keras.activations.gelu(x)
y.numpy()
array([-0.00404951, -0.15865529,  0.        ,  0.8413447 ,  2.9959507 ],
    dtype=float32)
y = tf.keras.activations.gelu(x, approximate=True)
y.numpy()
array([-0.00363752, -0.15880796,  0.        ,  0.841192  ,  2.9963627 ],
    dtype=float32)
""",
        """CODE.x = tf.constant([1., 2., 3., 4., 5.])
x = tf.reshape(x, [1, 5, 1])
avg_pool_1d = tf.keras.layers.AveragePooling1D(pool_size=2,
...    strides=1, padding='valid')
avg_pool_1d(x)
<tf.Tensor: shape=(1, 4, 1), dtype=float32, numpy=
array([[[1.5],
        [2.5],
        [3.5],
        [4.5]]], dtype=float32)>
x = tf.constant([1., 2., 3., 4., 5.])
x = tf.reshape(x, [1, 5, 1])
avg_pool_1d = tf.keras.layers.AveragePooling1D(pool_size=2,
...    strides=2, padding='valid')
avg_pool_1d(x)
<tf.Tensor: shape=(1, 2, 1), dtype=float32, numpy=
array([[[1.5],
        [3.5]]], dtype=float32)>
x = tf.constant([1., 2., 3., 4., 5.])
x = tf.reshape(x, [1, 5, 1])
avg_pool_1d = tf.keras.layers.AveragePooling1D(pool_size=2,
...    strides=1, padding='same')
avg_pool_1d(x)
<tf.Tensor: shape=(1, 5, 1), dtype=float32, numpy=
array([[[1.5],
        [2.5],
        [3.5],
        [4.5],
        [5.]]], dtype=float32)>""",
        """CODE.x = tf.constant([1., 2., 3., 4., 5.])
x = tf.reshape(x, [1, 5, 1])
max_pool_1d = tf.keras.layers.MaxPooling1D(pool_size=2,
...    strides=1, padding='valid')
max_pool_1d(x)
<tf.Tensor: shape=(1, 4, 1), dtype=float32, numpy=
array([[[2.],
        [3.],
        [4.],
        [5.]]], dtype=float32)>
x = tf.constant([1., 2., 3., 4., 5.])
x = tf.reshape(x, [1, 5, 1])
max_pool_1d = tf.keras.layers.MaxPooling1D(pool_size=2,
...    strides=2, padding='valid')
max_pool_1d(x)
<tf.Tensor: shape=(1, 2, 1), dtype=float32, numpy=
array([[[2.],
        [4.]]], dtype=float32)>
x = tf.constant([1., 2., 3., 4., 5.])
x = tf.reshape(x, [1, 5, 1])
max_pool_1d = tf.keras.layers.MaxPooling1D(pool_size=2,
...    strides=1, padding='same')
max_pool_1d(x)
<tf.Tensor: shape=(1, 5, 1), dtype=float32, numpy=
array([[[2.],
        [3.],
        [4.],
        [5.],
        [5.]]], dtype=float32)>""",
        """CODE.x = tf.constant([1., 2., 3., 4., 5.])
x = tf.reshape(x, [1, 5, 1])
x
<tf.Tensor: shape=(1, 5, 1), dtype=float32, numpy=
  array([[[1.],
          [2.],
          [3.],
          [4.],
          [5.]], dtype=float32)>
avg_pool_1d = tf.keras.layers.AveragePooling1D(pool_size=2,
...    strides=1, padding='valid')
avg_pool_1d(x)
<tf.Tensor: shape=(1, 4, 1), dtype=float32, numpy=
array([[[1.5],
        [2.5],
        [3.5],
        [4.5]]], dtype=float32)>
x = tf.constant([1., 2., 3., 4., 5.])
x = tf.reshape(x, [1, 5, 1])
x
<tf.Tensor: shape=(1, 5, 1), dtype=float32, numpy=
  array([[[1.],
          [2.],
          [3.],
          [4.],
          [5.]], dtype=float32)>
avg_pool_1d = tf.keras.layers.AveragePooling1D(pool_size=2,
...    strides=2, padding='valid')
avg_pool_1d(x)
<tf.Tensor: shape=(1, 2, 1), dtype=float32, numpy=
array([[[1.5],
        [3.5]]], dtype=float32)>
x = tf.constant([1., 2., 3., 4., 5.])
x = tf.reshape(x, [1, 5, 1])
x
<tf.Tensor: shape=(1, 5, 1), dtype=float32, numpy=
  array([[[1.],
          [2.],
          [3.],
          [4.],
          [5.]], dtype=float32)>
avg_pool_1d = tf.keras.layers.AveragePooling1D(pool_size=2,
...    strides=1, padding='same')
avg_pool_1d(x)
<tf.Tensor: shape=(1, 5, 1), dtype=float32, numpy=
array([[[1.5],
        [2.5],
        [3.5],
        [4.5],
        [5.]]], dtype=float32)>)""",
        """CODE.x = tf.constant([[1., 2., 3.],
...                  [4., 5., 6.],
...                  [7., 8., 9.]])
x = tf.reshape(x, [1, 3, 3, 1])
avg_pool_2d = tf.keras.layers.AveragePooling2D(pool_size=(2, 2),
...    strides=(1, 1), padding='valid')
avg_pool_2d(x)
<tf.Tensor: shape=(1, 2, 2, 1), dtype=float32, numpy=
  array([[[[3.],
           [4.]],
          [[6.],
           [7.]]]], dtype=float32)>
x = tf.constant([[1., 2., 3., 4.],
...                  [5., 6., 7., 8.],
...                  [9., 10., 11., 12.]])
x = tf.reshape(x, [1, 3, 4, 1])
avg_pool_2d = tf.keras.layers.AveragePooling2D(pool_size=(2, 2),
...    strides=(2, 2), padding='valid')
avg_pool_2d(x)
<tf.Tensor: shape=(1, 1, 2, 1), dtype=float32, numpy=
  array([[[[3.5],
           [5.5]]]], dtype=float32)>
x = tf.constant([[1., 2., 3.],
...                  [4., 5., 6.],
...                  [7., 8., 9.]])
x = tf.reshape(x, [1, 3, 3, 1])
avg_pool_2d = tf.keras.layers.AveragePooling2D(pool_size=(2, 2),
...    strides=(1, 1), padding='same')
avg_pool_2d(x)
<tf.Tensor: shape=(1, 3, 3, 1), dtype=float32, numpy=
  array([[[[3.],
           [4.],
           [4.5]],
          [[6.],
           [7.],
           [7.5]],
          [[7.5],
           [8.5],
           [9.]]]], dtype=float32)>""",
        """CODE.x = tf.constant([[1., 2., 3.],
...                  [4., 5., 6.],
...                  [7., 8., 9.]])
x = tf.reshape(x, [1, 3, 3, 1])
max_pool_2d = tf.keras.layers.MaxPooling2D(pool_size=(2, 2),
...    strides=(1, 1), padding='valid')
max_pool_2d(x)
<tf.Tensor: shape=(1, 2, 2, 1), dtype=float32, numpy=
  array([[[[5.],
           [6.]],
          [[8.],
           [9.]]]], dtype=float32)>
x = tf.constant([[1., 2., 3., 4.],
...                  [5., 6., 7., 8.],
...                  [9., 10., 11., 12.]])
x = tf.reshape(x, [1, 3, 4, 1])
max_pool_2d = tf.keras.layers.MaxPooling2D(pool_size=(2, 2),
...    strides=(2, 2), padding='valid')
max_pool_2d(x)
<tf.Tensor: shape=(1, 1, 2, 1), dtype=float32, numpy=
  array([[[[6.],
           [8.]]]], dtype=float32)>
input_image = tf.constant([[[[1.], [1.], [2.], [4.]],
...                            [[2.], [2.], [3.], [2.]],
...                            [[4.], [1.], [1.], [1.]],
...                            [[2.], [2.], [1.], [4.]]])
output = tf.constant([[[[1], [0]],
...                       [[0], [1]]]])
model = tf.keras.models.Sequential()
model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2),
...    input_shape=(4, 4, 1)))
model.compile('adam', 'mean_squared_error')
model.predict(input_image, steps=1)
array([[[[2.],
         [4.]],
        [[4.],
         [4.]]]], dtype=float32)>
x = tf.constant([[1., 2., 3.],
...                  [4., 5., 6.],
...                  [7., 8., 9.]])
x = tf.reshape(x, [1, 3, 3, 1])
max_pool_2d = tf.keras.layers.MaxPooling2D(pool_size=(2, 2),
...    strides=(1, 1), padding='same')
max_pool_2d(x)
<tf.Tensor: shape=(1, 3, 3, 1), dtype=float32, numpy=
  array([[[[5.],
           [6.],
           [6.]],
          [[8.],
           [9.],
           [9.]],
          [[8.],
           [9.],
           [9.]]]], dtype=float32)>""",
        """CODE.x = tf.constant([[1., 2., 3.], [4., 5., 6.], [7., 8., 9.]])
x = tf.reshape(x, [3, 3, 1])
max_pool_1d = tf.keras.layers.GlobalMaxPooling1D()
max_pool_1d(x)""",
        """CODE.x = tf.constant([[1.0, 2.0], [3.0, 4.0]])
tf.keras.backend.print_tensor(x)""",
        """CODE.x = tf.keras.backend.placeholder(shape=(2, 3))
y = tf.keras.backend.placeholder(shape=(3, 4))
xy = tf.keras.backend.dot(x, y)

x = tf.keras.backend.placeholder(shape=(32, 28, 3))
y = tf.keras.backend.placeholder(shape=(3, 4))
xy = tf.keras.backend.dot(x, y)

x = tf.keras.backend.random_uniform_variable(shape=(2, 3), low=0, high=1)
y = tf.keras.backend.ones((4, 3, 5))
xy = tf.keras.backend.dot(x, y)
tf.keras.backend.int_shape(xy)""",
        """CODE.x = tf.ones((10, 1))
data = tf.keras.utils.pack_x_y_sample_weight(x)
y = tf.ones((10, 1))
data = tf.keras.utils.pack_x_y_sample_weight(x, y)
x, y = data""",
        """CODE.x1 = np.arange(3.0)
x2 = np.arange(3.0)
tf.keras.layers.multiply([x1, x2])

input1 = tf.keras.layers.Input(shape=(16,))
x1 = tf.keras.layers.Dense(8, activation='relu')(input1)
input2 = tf.keras.layers.Input(shape=(32,))
x2 = tf.keras.layers.Dense(8, activation='relu')(input2)
out = tf.keras.layers.multiply([x1,x2])
out = tf.keras.layers.Dense(4)(out)
model = tf.keras.models.Model(inputs=[input1, input2], outputs=out)""",
        """CODE.x1 = np.ones((2, 2))
x2 = np.zeros((2, 2))
y = tf.keras.layers.Average()([x1, x2])
y.numpy().tolist()
[[0.5, 0.5], [0.5, 0.5]]
input1 = tf.keras.layers.Input(shape=(16,))
x1 = tf.keras.layers.Dense(8, activation='relu')(input1)
input2 = tf.keras.layers.Input(shape=(32,))
x2 = tf.keras.layers.Dense(8, activation='relu')(input2)
avg = tf.keras.layers.Average()([x1, x2])
out = tf.keras.layers.Dense(4)(avg)
model = tf.keras.models.Model(inputs=[input1, input2], outputs=out)""",
        """CODE.x1 = tf.keras.layers.Dense(8)(np.arange(10).reshape(5, 2)
x2 = tf.keras.layers.Dense(8)(np.arange(10, 20).reshape(5, 2)
maxed = tf.keras.layers.Maximum()([x1, x2])
maxed.shape""",
        """CODE.x1 = tf.keras.layers.Dense(8)(np.arange(10).reshape(5, 2)
x2 = tf.keras.layers.Dense(8)(np.arange(10, 20).reshape(5, 2)
minned = tf.keras.layers.Minimum()([x1, x2)
minned.shape""",
        """CODE.x1 = tf.keras.layers.Dense(8)(np.arange(10).reshape(5, 2)
x2 = tf.keras.layers.Dense(8)(np.arange(10, 20).reshape(5, 2)
multiplied = tf.keras.layers.Multiply()([x1, x2])""",
        """CODE.x_batch = tf.keras.backend.ones(shape=(2, 3, 4, 5))
x_batch_flatten = batch_flatten(x_batch)
tf.keras.backend.int_shape(x_batch_flatten)
(2, 60)""",
        """CODE.x_ref = Reference(x)
print(x is x_ref.deref())
""",
        """CODE.y_true = [1, 2]
y_pred = [[0.05, 0.95, 0], [0.1, 0.8, 0.1]]
# Using 'auto'/'sum_over_batch_size' reduction type.
scce = tf.keras.losses.SparseCategoricalCrossentropy()
scce(y_true, y_pred).numpy()
1.177

# Calling with 'sample_weight'.
scce(y_true, y_pred, sample_weight=tf.constant([0.3, 0.7])).numpy()
0.814

# Using 'sum' reduction type.
scce = tf.keras.losses.SparseCategoricalCrossentropy(
...     reduction=tf.keras.losses.Reduction.SUM)
scce(y_true, y_pred).numpy()
2.354

# Using 'none' reduction type.
scce = tf.keras.losses.SparseCategoricalCrossentropy(
...     reduction=tf.keras.losses.Reduction.NONE)
scce(y_true, y_pred).numpy()
array([0.0513, 2.303], dtype=float32)""",
        """CODE.y_true = [2, 1]
y_pred = [[0.1, 0.9, 0.8], [0.05, 0.95, 0]]
m = tf.keras.metrics.sparse_categorical_accuracy(y_true, y_pred)
assert m.shape == (2,)
m.numpy()
array([0., 1.], dtype=float32)""",
        """CODE.y_true = [2, 1]
y_pred = [[0.1, 0.9, 0.8], [0.05, 0.95, 0]]
m = tf.keras.metrics.sparse_top_k_categorical_accuracy(
...     y_true, y_pred, k=3)
assert m.shape == (2,)
m.numpy()
array([1., 1.], dtype=float32)""",
        """CODE.y_true = [[0, 0, 1], [0, 1, 0]]
y_pred = [[0.1, 0.9, 0.8], [0.05, 0.95, 0]]
m = tf.keras.metrics.categorical_accuracy(y_true, y_pred)
assert m.shape == (2,)
m.numpy()
array([0., 1.], dtype=float32)""",
        """CODE.y_true = [[0, 0, 1], [0, 1, 0]]
y_pred = [[0.1, 0.9, 0.8], [0.05, 0.95, 0]]
m = tf.keras.metrics.top_k_categorical_accuracy(y_true, y_pred, k=3)
assert m.shape == (2,)
m.numpy()
array([1., 1.], dtype=float32)""",
        """CODE.y_true = [[0, 1, 0], [0, 0, 1]]
y_pred = [[0.05, 0.95, 0], [0.1, 0.8, 0.1]]
# Using 'auto'/'sum_over_batch_size' reduction type.
cce = tf.keras.losses.CategoricalCrossentropy()
cce(y_true, y_pred).numpy()
1.177

# Calling with 'sample_weight'.
cce(y_true, y_pred, sample_weight=tf.constant([0.3, 0.7])).numpy()
0.814

# Using 'sum' reduction type.
cce = tf.keras.losses.CategoricalCrossentropy(
...     reduction=tf.keras.losses.Reduction.SUM)
cce(y_true, y_pred).numpy()
2.354

# Using 'none' reduction type.
cce = tf.keras.losses.CategoricalCrossentropy(
...     reduction=tf.keras.losses.Reduction.NONE)
cce(y_true, y_pred).numpy()
array([0.0513, 2.303], dtype=float32)""",
        """CODE.y_true = [[0., 1.], [0., 0.]]
y_pred = [[0.6, 0.4], [0.4, 0.6]]
h = tf.keras.losses.Hinge()
h(y_true, y_pred).numpy()
1.3

h(y_true, y_pred, sample_weight=[1, 0]).numpy()
0.55

h = tf.keras.losses.Hinge(
    reduction=tf.keras.losses.Reduction.SUM)
h(y_true, y_pred).numpy()
2.6

h = tf.keras.losses.Hinge(
    reduction=tf.keras.losses.Reduction.NONE)
h(y_true, y_pred).numpy()
array([1.1, 1.5], dtype=float32)
""",
        """CODE.y_true = [[0., 1.], [0., 0.]]
y_pred = [[0.6, 0.4], [0.4, 0.6]]
h = tf.keras.losses.Hinge()
h(y_true, y_pred).numpy()
1.3

h(y_true, y_pred, sample_weight=[1, 0]).numpy()
0.55

h = tf.keras.losses.Hinge(
...     reduction=tf.keras.losses.Reduction.SUM)
h(y_true, y_pred).numpy()
2.6

h = tf.keras.losses.Hinge(
...     reduction=tf.keras.losses.Reduction.NONE)
h(y_true, y_pred).numpy()
array([1.1, 1.5], dtype=float32)
""",
        """CODE.y_true = [[0., 1.], [0., 0.]]
y_pred = [[1., 1.], [0., 0.]]
p = tf.keras.losses.Poisson()
p(y_true, y_pred).numpy()
0.5

p(y_true, y_pred, sample_weight=[0.8, 0.2]).numpy()
0.4

p = tf.keras.losses.Poisson(
    reduction=tf.keras.losses.Reduction.SUM)
p(y_true, y_pred).numpy()
0.999

p = tf.keras.losses.Poisson(
    reduction=tf.keras.losses.Reduction.NONE)
p(y_true, y_pred).numpy()
array([0.999, 0.], dtype=float32)
""",
        """CODE.y_true = [[0., 1.], [0., 0.]]
y_pred = [[1., 1.], [0., 0.]]
p = tf.keras.losses.Poisson()
p(y_true, y_pred).numpy()
p(y_true, y_pred, sample_weight=[0.8, 0.2]).numpy()
p = tf.keras.losses.Poisson(reduction=tf.keras.losses.Reduction.SUM)
p(y_true, y_pred).numpy()
p = tf.keras.losses.Poisson(reduction=tf.keras.losses.Reduction.NONE)
p(y_true, y_pred).numpy()
model.compile(optimizer='sgd', loss=tf.keras.losses.Poisson())
""",
        """CODE.y_true = [[0., 1.], [0., 0.]]
y_pred = [[1., 1.], [1., 0.]]
msle = tf.keras.losses.MeanSquaredLogarithmicError()
msle(y_true, y_pred).numpy()
msle(y_true, y_pred, sample_weight=[0.7, 0.3]).numpy()
msle = tf.keras.losses.MeanSquaredLogarithmicError(
    reduction=tf.keras.losses.Reduction.SUM)
msle(y_true, y_pred).numpy()
msle = tf.keras.losses.MeanSquaredLogarithmicError(
    reduction=tf.keras.losses.Reduction.NONE)
msle(y_true, y_pred).numpy()
model.compile(optimizer='sgd',
              loss=tf.keras.losses.MeanSquaredLogarithmicError())
""",
        """CODE.y_true = [[0., 1.], [1., 1.], [1., 1.]]
y_pred = [[1., 0.], [1., 1.], [-1., -1.]]
loss = tf.keras.losses.cosine_similarity(y_true, y_pred, axis=1)
loss.numpy()
array([-0., -0.999, 0.999], dtype=float32)""",
        """CODE.y_true = [[1], [1], [0], [0]]
y_pred = [[1], [1], [0], [0]]
m = tf.keras.metrics.binary_accuracy(y_true, y_pred)
assert m.shape == (4,)
m.numpy()
array([1., 1., 1., 1.], dtype=float32)""",
        """CODE.y_true = np.random.choice([-1, 1], size=(2, 3))
y_pred = np.random.random(size=(2, 3))
loss = tf.keras.losses.hinge(y_true, y_pred)
assert loss.shape == (2,)
assert np.array_equal(
...     loss.numpy(),
...     np.mean(np.maximum(1. - y_true * y_pred, 0.), axis=-1))""",
        """CODE.y_true = np.random.choice([-1, 1], size=(2, 3))
y_pred = np.random.random(size=(2, 3))
loss = tf.keras.losses.squared_hinge(y_true, y_pred)
assert loss.shape == (2,)
assert np.array_equal(
...     loss.numpy(),
...     np.mean(np.square(np.maximum(1. - y_true * y_pred, 0.)), axis=-1))""",
        """CODE.y_true = np.random.randint(0, 2, size=(2, 3))
y_pred = np.random.random(size=(2, 3))
loss = tf.keras.losses.mean_absolute_error(y_true, y_pred)
assert loss.shape == (2,)
assert np.array_equal(
    loss.numpy(), np.mean(np.abs(y_true - y_pred), axis=-1))
""",
        """CODE.y_true = np.random.randint(0, 2, size=(2, 3))
y_pred = np.random.random(size=(2, 3))
loss = tf.keras.losses.mean_squared_error(y_true, y_pred)
assert loss.shape == (2,)
assert np.array_equal(
    loss.numpy(), np.mean(np.square(y_true - y_pred), axis=-1))
""",
        """CODE.y_true = np.random.randint(0, 2, size=(2, 3))
y_pred = np.random.random(size=(2, 3))
loss = tf.keras.losses.mean_squared_logarithmic_error(y_true, y_pred)
assert loss.shape == (2,)
y_true = np.maximum(y_true, 1e-7)
y_pred = np.maximum(y_pred, 1e-7)
assert np.allclose(
...     loss.numpy(),
...     np.mean(
...         np.square(np.log(y_true + 1.) - np.log(y_pred + 1.)), axis=-1))""",
        """CODE.y_true = np.random.randint(0, 2, size=(2, 3))
y_pred = np.random.random(size=(2, 3))
loss = tf.keras.losses.poisson(y_true, y_pred)
assert loss.shape == (2,)
y_pred = y_pred + 1e-7
assert np.allclose(
...     loss.numpy(), np.mean(y_pred - y_true * np.log(y_pred), axis=-1),
...     atol=1e-5)""",
        """CODE.y_true = np.random.randint(0, 3, size=(2,))
y_true = tf.keras.utils.to_categorical(y_true, num_classes=3)
y_pred = np.random.random(size=(2, 3))
loss = tf.keras.losses.categorical_hinge(y_true, y_pred)
assert loss.shape == (2,)
pos = np.sum(y_true * y_pred, axis=-1)
neg = np.amax((1. - y_true) * y_pred, axis=-1)
assert np.array_equal(loss.numpy(), np.maximum(0., neg - pos + 1.))""",
        """CODE.y_true = np.random.random(size=(2, 3))
y_true = np.maximum(y_true, 1e-7)  # Prevent division by zero
y_pred = np.random.random(size=(2, 3))
loss = tf.keras.losses.mean_absolute_percentage_error(y_true, y_pred)
assert loss.shape == (2,)
assert np.array_equal(
...     loss.numpy(),
...     100. * np.mean(np.abs((y_true - y_pred) / y_true), axis=-1))""" .

<DEPENDENCY.keras==2.7.0> <CONTAINS> """CODE.@tf.compat.v1.keras.utils.track_tf1_style_variables
def call(self, inputs):
    with tf.compat.v1.variable_scope("double_dense_layer"):
        out = tf.compat.v1.layers.dense(
            inputs, self.units, name="dense_one",
            kernel_initializer=tf.compat.v1.random_normal_initializer,
            kernel_regularizer="l2")
        out = tf.compat.v1.layers.dense(
            out, self.units, name="dense_two",
            kernel_initializer=tf.compat.v1.random_normal_initializer(),
            kernel_regularizer="l2")
    return out


@tf.compat.v1.keras.utils.track_tf1_style_variables
def call(self, inputs):
    out = inputs
    with tf.compat.v1.variable_scope("double_dense_layer"):
        with tf.compat.v1.variable_scope("dense_one"):
            kernel = tf.compat.v1.get_variable(
                shape=[out.shape[-1], self.units],
                regularizer=regularizers.L2(),
                initializer=init_ops.ones_initializer(),
                name="kernel")
            bias = tf.compat.v1.get_variable(
                shape=[self.units,],
                initializer=init_ops.zeros_initializer(),
                name="bias")
            out = tf.compat.v1.math.matmul(out, kernel)
            out = tf.compat.v1.nn.bias_add(out, bias)
        with tf.compat.v1.variable_scope("dense_two"):
            kernel = tf.compat.v1.get_variable(
                shape=[out.shape[-1], self.units],
                regularizer=regularizers.L2(),
                initializer=init_ops.ones_initializer(),
                name="kernel")
            bias = tf.compat.v1.get_variable(
                shape=[self.units,],
                initializer=init_ops.zeros_initializer(),
                name="bias")
            out = tf.compat.v1.math.matmul(out, kernel)
            out = tf.compat.v1.nn.bias_add(out, bias)
    return out

@tf.compat.v1.keras.utils.track_tf1_style_variables
def call(self, inputs):
    with tf.compat.v1.variable_scope("double_dense_layer"):
        out = tf.compat.v1.layers.dense(
            inputs, self.units, name="dense_one",
            kernel_initializer=tf.compat.v1.random_normal_initializer,
            kernel_regularizer="l2")
        out = tf.compat.v1.layers.dense(
            out, self.units, name="dense_two",
            kernel_initializer=tf.compat.v1.random_normal_initializer(),
            kernel_regularizer="l2")
    return out

@tf.compat.v1.keras.utils.track_tf1_style_variables
def call(self, inputs):
    out = inputs
    with tf.compat.v1.variable_scope("double_dense_layer"):
        with tf.compat.v1.variable_scope("dense_one"):
            kernel = tf.compat.v1.get_variable(
                shape=[out.shape[-1], self.units],
                regularizer=regularizers.L2(),
                initializer=init_ops.ones_initializer(),
                name="kernel")
            bias = tf.compat.v1.get_variable(
                shape=[self.units,],
                initializer=init_ops.zeros_initializer(),
                name="bias")
            out = tf.compat.v1.math.matmul(out, kernel)
            out = tf.compat.v1.nn.bias_add(out, bias)
        with tf.compat.v1.variable_scope("dense_two"):
            kernel = tf.compat.v1.get_variable(
                shape=[out.shape[-1], self.units],
                regularizer=regularizers.L2(),
                initializer=init_ops.ones_initializer(),
                name="kernel")
            bias = tf.compat.v1.get_variable(
                shape=[self.units,],
                initializer=init_ops.zeros_initializer(),
                name="bias")
            out = tf.compat.v1.math.matmul(out, kernel)
            out = tf.compat.v1.nn.bias_add(out, bias)
    return out

@tf.compat.v1.keras.utils.track_tf1_style_variables
def call(self, inputs):
    with tf.compat.v1.variable_scope("double_dense_layer"):
        out = tf.compat.v1.layers.dense(
            inputs, self.units, name="dense_one",
            kernel_initializer=tf.compat.v1.random_normal_initializer,
            kernel_regularizer="l2")
        out = tf.compat.v1.layers.dense(
            out, self.units, name="dense_two",
            kernel_initializer=tf.compat.v1.random_normal_initializer(),
            kernel_regularizer="l2")
    return out

@tf.compat.v1.keras.utils.track_tf1_style_variables
def call(self, inputs):
    out = inputs
    with tf.compat.v1.variable_scope("double_dense_layer"):
        with tf.compat.v1.variable_scope("dense_one"):
            kernel = tf.compat.v1.get_variable(
                shape=[out.shape[-1], self.units],
                regularizer=regularizers.L2(),
                initializer=init_ops.ones_initializer(),
                name="kernel")
            bias = tf.compat.v1.get_variable(
                shape=[self.units,],
                initializer=init_ops.zeros_initializer(),
                name="bias")
            out = tf.compat.v1.math.matmul(out, kernel)
            out = tf.compat.v1.nn.bias_add(out, bias)
        with tf.compat.v1.variable_scope("dense_two"):
            kernel = tf.compat.v1.get_variable(
                shape=[out.shape[-1], self.units],
                regularizer=regularizers.L2(),
                initializer=init_ops.ones_initializer(),
                name="kernel")
            bias = tf.compat.v1.get_variable(
                shape=[self.units,],
                initializer=init_ops.zeros_initializer(),
                name="bias")
            out = tf.compat.v1.math.matmul(out, kernel)
            out = tf.compat.v1.nn.bias_add(out, bias)
    return out

@tf.compat.v1.keras.utils.track_tf1_style_variables
def call(self, inputs):
    with tf.compat.v1.variable_scope("double_dense_layer"):
        out = tf.compat.v1.layers.dense(
            inputs, self.units, name="dense_one",
            kernel_initializer=tf.compat.v1.random_normal_initializer,
            kernel_regularizer="l2")
        out = tf.compat.v1.layers.dense(
            out, self.units, name="dense_two",
            kernel_initializer=tf.compat.v1.random_normal_initializer(),
            kernel_regularizer="l2")
    return out

@tf.compat.v1.keras.utils.track_tf1_style_variables
def call(self, inputs):
    out = inputs
    with tf.compat.v1.variable_scope("double_dense_layer"):
        with tf.compat.v1.variable_scope("dense_one"):
            kernel = tf.compat.v1.get_variable(
                shape=[out.shape[-1], self.units],
                regularizer=regularizers.L2(),
                initializer=init_ops.ones_initializer(),
                name="kernel")
            bias = tf.compat.v1.get_variable(
                shape=[self.units,],
                initializer=init_ops.zeros_initializer(),
                name="bias")
            out = tf.compat.v1.math.matmul(out, kernel)
            out = tf.compat.v1.nn.bias_add(out, bias)
        with tf.compat.v1.variable_scope("dense_two"):
            kernel = tf.compat.v1.get_variable(
                shape=[out.shape[-1], self.units],
                regularizer=regularizers.L2(),
                initializer=init_ops.ones_initializer(),
                name="kernel")
            bias = tf.compat.v1.get_variable(
                shape=[self.units,],
                initializer=init_ops.zeros_initializer(),
                name="bias")
            out = tf.compat.v1.math.matmul(out, kernel)
            out = tf.compat.v1.nn.bias_add(out, bias)
    return out

@tf.compat.v1.keras.utils.track_tf1_style_variables
def call(self, inputs):
    with tf.compat.v1.variable_scope("double_dense_layer"):
        out = tf.compat.v1.layers.dense(
            inputs, self.units, name="dense_one",
            kernel_initializer=tf.compat.v1.random_normal_initializer,
            kernel_regularizer="l2")
        out = tf.compat.v1.layers.dense(
            out, self.units, name="dense_two",
            kernel_initializer=tf.compat.v1.random_normal_initializer(),
            kernel_regularizer="l2")
    return out

@tf.compat.v1.keras.utils.track_tf1_style_variables
def call(self, inputs):
    out = inputs
    with tf.compat.v1.variable_scope("double_dense_layer"):
        with tf.compat.v1.variable_scope("dense_one"):
            kernel = tf.compat.v1.get_variable(
                shape=[out.shape[-1], self.units],
                regularizer=regularizers.L2(),
                initializer=init_ops.ones_initializer(),
                name="kernel")
            bias = tf.compat.v1.get_variable(
                shape=[self.units,],
                initializer=init_ops.zeros_initializer(),
                name="bias")
            out = tf.compat.v1.math.matmul(out, kernel)
            out = tf.compat.v1.nn.bias_add(out, bias)
        with tf.compat.v1.variable_scope("dense_two"):
            kernel = tf.compat.v1.get_variable(
                shape=[out.shape[-1], self.units],
                regularizer=regularizers.L2(),
                initializer=init_ops.ones_initializer(),
                name="kernel")
            bias = tf.compat.v1.get_variable(
                shape=[self.units,],
                initializer=init_ops.zeros_initializer(),
                name="bias")
            out = tf.compat.v1.math.matmul(out, kernel)
            out = tf.compat.v1.nn.bias_add(out, bias)
    return out

@tf.compat.v1.keras.utils.track_tf1_style_variables
def call(self, inputs):
    with tf.compat.v1.variable_scope("double_dense_layer"):
        out = tf.compat.v1.layers.dense(
            inputs, self.units, name="dense_one",
            kernel_initializer=tf.compat.v1.random_normal_initializer,
            kernel_regularizer="l2")
        out = tf.compat.v1.layers.dense(
            out, self.units, name="dense_two",
            kernel_initializer=tf.compat.v1.random""",
        """CODE.class WrappedDoubleDenseLayer(variable_scope_shim.VariableScopeLayer):

  def __init__(self, units, *args, **kwargs):
    super().__init__(*args, **kwargs)
    self.units = units

  def forward_pass(self, inputs):
    with variable_scope.variable_scope("double_dense_layer"):
      out = tf.compat.v1.layers.dense(
          inputs, self.units, name="dense_one",
          kernel_initializer=tf.compat.v1.random_normal_initializer,
          kernel_regularizer="l2")
      out = tf.compat.v1.layers.dense(
          out, self.units, name="dense_two",
          kernel_initializer=tf.compat.v1.random_normal_initializer(),
          kernel_regularizer="l2")
    return out

# Create a layer that can be used as a standard keras layer
layer = WrappedDoubleDenseLayer(10)

# call the layer on inputs
layer(...)

# Variables created/used within the scope will be tracked by the layer
layer.weights
layer.trainable_variables

# Regularization losses will be captured in layer.losses after a call,
# just like any other Keras layer
reg_losses = layer.losses


class WrappedDoubleDenseLayer(variable_scope_shim.VariableScopeLayer):

  def __init__(self, units, *args, **kwargs):
    super().__init__(*args, **kwargs)
    self.units = units

  def forward_pass(self, inputs):
    out = inputs
    with tf.compat.v1.variable_scope("double_dense_layer"):
      with tf.compat.v1.variable_scope("dense_one"):
        # The weights are created with a `regularizer`,
        # so the layer should track their regularization losses
        kernel = tf.compat.v1.get_variable(
            shape=[out.shape[-1], self.units],
            regularizer=regularizers.L2(),
            initializer=init_ops.ones_initializer(),
            name="kernel")
        bias = tf.compat.v1.get_variable(
            shape=[self.units,],
            initializer=init_ops.zeros_initializer(),
            name="bias")
        out = tf.compat.v1.math.matmul(out, kernel)
        out = tf.compat.v1.nn.bias_add(out, bias)
      with tf.compat.v1.variable_scope("dense_two"):
        kernel = tf.compat.v1.get_variable(
            shape=[out.shape[-1], self.units],
            regularizer=regularizers.L2(),
            initializer=init_ops.ones_initializer(),
            name="kernel")
        bias = tf.compat.v1.get_variable(
            shape=[self.units,],
            initializer=init_ops.zeros_initializer(),
            name="bias")
        out = tf.compat.v1.math.matmul(out, kernel)
        out = tf.compat.v1.nn.bias_add(out, bias)
    return out

# Create a layer that can be used as a standard keras layer
layer = WrappedDoubleDenseLayer(10)

# call the layer on inputs
layer(...)

# Variables created/used within the scope will be tracked by the layer
layer.weights
layer.trainable_variables

# Regularization losses will be captured in layer.losses after a call,
# just like any other Keras layer
reg_losses = layer.losses
""",
        """CODE.m1 = tf.keras.metrics.Accuracy()
_ = m1.update_state([[1], [2]], [[0], [2]])

m2 = tf.keras.metrics.Accuracy()
_ = m2.update_state([[3], [4]], [[3], [4]])

m2.merge_state([m1])
m2.result().numpy()
0.75""",
        """CODE.strategy = utils.get_strategy()
with strategy.scope():
  model = tf.keras.Sequential([tf.keras.layers.Dense(10)])

model.compile(...)
train_ds, test_ds = ...
model.fit(train_ds, validation_data=test_ds, epochs=10)
""",
        """CODE.tf.keras.mixed_precision.set_global_policy('mixed_float16')
tf.keras.mixed_precision.global_policy()
tf.keras.layers.Dense(10).dtype_policy
tf.keras.layers.Dense(10, dtype='float64').dtype_policy
tf.keras.mixed_precision.set_global_policy('float32')""" .

<DEPENDENCY.keras==2.8.0> <CONTAINS> """CODE.    import tensorflow as tf
    from keras.applications import Xception
    from keras.utils import multi_gpu_model
    import numpy as np

    num_samples = 1000
    height = 224
    width = 224
    num_classes = 1000

    # Instantiate the base model (or "template" model).
    # We recommend doing this with under a CPU device scope,
    # so that the model's weights are hosted on CPU memory.
    # Otherwise they may end up hosted on a GPU, which would
    # complicate weight sharing.
    with tf.device('/cpu:0'):
        model = Xception(weights=None,
                         input_shape=(height, width, 3),
                         classes=num_classes)

    # Replicates the model on 8 GPUs.
    # This assumes that your machine has 8 available GPUs.
    parallel_model = multi_gpu_model(model, gpus=8)
    parallel_model.compile(loss='categorical_crossentropy',
                           optimizer='rmsprop')

    # Generate dummy data.
    x = np.random.random((num_samples, height, width, 3))
    y = np.random.random((num_samples, num_classes))

    # This `fit` call will be distributed on 8 GPUs.
    # Since the batch size is 256, each GPU will process 32 samples.
    parallel_model.fit(x, y, epochs=20, batch_size=256)

    # Save model via the template model (which shares the same weights):
    model.save('my_model.h5')


     ..
     # Not needed to change the device scope for model definition:
     model = Xception(weights=None, ..)

     try:
         model = multi_gpu_model(model, cpu_relocation=True)
         print("Training using multiple GPUs..")
     except:
         print("Training using single GPU or CPU..")

     model.compile(..)
     ..


     ..
     # Not needed to change the device scope for model definition:
     model = Xception(weights=None, ..)

     try:
         model = multi_gpu_model(model, cpu_merge=False)
         print("Training using multiple GPUs..")
     except:
         print("Training using single GPU or CPU..")
     model.compile(..)
     ..
""",
        """CODE.# Create 2 PeepholeLSTMCells
peephole_lstm_cells = [PeepholeLSTMCell(size) for size in [128, 256]]
# Create a layer composed sequentially of the peephole LSTM cells.
layer = RNN(peephole_lstm_cells)
input = keras.Input((timesteps, input_dim))
output = layer(input)
""",
        """CODE.model.compile(
  loss=tf.keras.losses.BinaryFocalCrossentropy(gamma=2.0, from_logits=True),
  ....
)


# Example 1: (batch_size = 1, number of samples = 4)
y_true = [0, 1, 0, 0]
y_pred = [-18.6, 0.51, 2.94, -12.8]
loss = tf.keras.losses.BinaryFocalCrossentropy(gamma=2, from_logits=True)
loss(y_true, y_pred).numpy()
0.691

# Example 2: (batch_size = 2, number of samples = 4)
y_true = [[0, 1], [0, 0]]
y_pred = [[-18.6, 0.51], [2.94, -12.8]]
# Using default 'auto'/'sum_over_batch_size' reduction type.
loss = tf.keras.losses.BinaryFocalCrossentropy(gamma=3, from_logits=True)
loss(y_true, y_pred).numpy()
0.647

# Using 'sample_weight' attribute
loss(y_true, y_pred, sample_weight=[0.8, 0.2]).numpy()
0.133

# Using 'sum' reduction` type.
loss = tf.keras.losses.BinaryFocalCrossentropy(gamma=4, from_logits=True,
...     reduction=tf.keras.losses.Reduction.SUM)
loss(y_true, y_pred).numpy()
1.222

# Using 'none' reduction type.
loss = tf.keras.losses.BinaryFocalCrossentropy(gamma=5, from_logits=True,
...     reduction=tf.keras.losses.Reduction.NONE)
loss(y_true, y_pred).numpy()
array([0.0017 1.1561], dtype=float32)
""" .

<DEPENDENCY.keras==2.9.0> <CONTAINS> """CODE.@allow_initializer_layout
def __init__(self, units,
             kernel_initializer='zeros',
             bias_initializer='zeros',
             **kwargs):
   super().__init__(**kwargs)""",
        """CODE.@inject_mesh
def __init__(self, name='accuracy', dtype=None):
    super().__init__(**kwargs)

acc = Accuracy(mesh=mesh)
assert acc._mesh == mesh""",
        """CODE.class SubclassLayer(BaseImageAugmentationLayer):
  def __init__(self):
    super().__init__()
    self.auto_vectorize = False


class RandomContrast(BaseImageAugmentationLayer):

  def __init__(self, factor=(0.5, 1.5), **kwargs):
    super().__init__(**kwargs)
    self._factor = factor

  def augment_image(self, image, transformation=None):
    random_factor = tf.random.uniform([], self._factor[0], self._factor[1])
    mean = tf.math.reduced_mean(inputs, axis=-1, keep_dim=True)
    return (inputs - mean) * random_factor + mean
""",
        """CODE.data = tf.constant(np.arange(6).reshape(2, 3), dtype=tf.float32)
normalized_data = tf.keras.layers.UnitNormalization()(data)
print(tf.reduce_sum(normalized_data[0, :] ** 2).numpy())
""",
        """CODE.layout_map = layout_map_lib.LayoutMap(mesh=self.mesh)
layout_map['d1.kernel'] = layout_1
layout_map['d1.bias'] = layout_2
layout_map['d2.kernel'] = layout_3
layout_map['d2.bias'] = layout_4

## Subclassed model
class SubclassModel(tf.keras.Model):

  def __init__(self, name=None):
    super().__init__(name=name)
    self.d1 = tf.keras.layers.Dense(1000)
    self.d2 = tf.keras.layers.Dense(1000)

  def call(self, inputs):
    x = self.d1(inputs)
    return self.d2(x)

with layout_map_scope(layout_map):
  model = SubclassModel()
# Triggering the creation of weights within or outside of the scope works
inputs = tf.zeros((10, 10))
results = model(inputs)

model.d1.kernel.layout == layout_1
model.d1.bias.layout == layout_2
model.d2.kernel.layout == layout_3
model.d2.bias.layout == layout_4

## Functional model
with layout_map_scope(layout_map):
  inputs = tf.keras.Input((10,), batch_size=10)
  x = tf.keras.layers.Dense(20, name='d1')(inputs)
  output = tf.keras.layers.Dense(30, name='d2')(x)

  model = tf.keras.Model(inputs, output)

d1 = model.layers[1]
d2 = model.layers[2]

d1.kernel.layout == layout_1
d1.bias.layout == layout_2
d1.kernel.layout == layout_3
d1.bias.layout == layout_4

## Sequential model
with layout_map_scope(layout_map):
  model = tf.keras.Sequential([
      tf.keras.layers.Dense(20, name='d1', input_shape=(10,)),
      tf.keras.layers.Dense(30, name='d2')
  ])

d1 = model.layers[0]
d2 = model.layers[1]

d1.kernel.layout == layout_1
d1.bias.layout == layout_2
d1.kernel.layout == layout_3
d1.bias.layout == layout_4
""",
        """CODE.map = LayoutMap(mesh=None)
map['.*dense.*kernel'] = layout_2d
map['.*dense.*bias'] = layout_1d
map['.*conv2d.*kernel'] = layout_4d
map['.*conv2d.*bias'] = layout_1d

layout_1 = map['dense_1.kernel']    #   layout_1 == layout_2d
layout_2 = map['dense_1.bias']      #   layout_2 == layout_1d
layout_3 = map['dense_2.kernel']    #   layout_3 == layout_2d
layout_4 = map['dense_2.bias']      #   layout_4 == layout_1d
layout_5 = map['my_model/conv2d_123/kernel']    #   layout_5 == layout_4d
layout_6 = map['my_model/conv2d_123/bias']      #   layout_6 == layout_1d
""",
        """CODE.random_bright = tf.keras.layers.RandomBrightness(factor=0.2)

# An image with shape [2, 2, 3]
image = [[[1, 2, 3], [4 ,5 ,6]], [[7, 8, 9], [10, 11, 12]]]

# Assume we randomly select the factor to be 0.1, then it will apply
# 0.1 * 255 to all the channel
output = random_bright(image, training=True)

# output will be int64 with 25.5 added to each channel and round down.
tf.Tensor([[[26.5, 27.5, 28.5]
            [29.5, 30.5, 31.5]]
           [[32.5, 33.5, 34.5]
            [35.5, 36.5, 37.5]]],
          shape=(2, 2, 3), dtype=int64)
""",
        """CODE.text_parts, floats = _FloatExtractor()("Text 1.0 Text")
text_parts
['Text ', ' Text']
floats
array([1.])""" .

<DEPENDENCY.keras==3.0.0> <CONTAINS> """CODE.devices = list_devices()    # Assume there are 8 devices.

# Create a mesh with 2 devices for data parallelism and 4 devices for
# model parallelism.
device_mesh = DeviceMesh(shape=(2, 4), axis_names=('batch', 'model'),
                         devices=devices)
# Create a layout map that shard the `Dense` layer and `Conv2D`
# layer variables on the last dimension.
# Based on the `device_mesh`, this means the variables
# will be split across 4 devices. Any other variable that doesn't
# match any key in the layout map will be fully replicated.
layout_map = LayoutMap(device_mesh)
layout_map['dense.*kernel'] = (None, 'model')
layout_map['dense.*bias'] = ('model',)
layout_map['conv2d.*kernel'] = (None, None, None, 'model')
layout_map['conv2d.*bias'] = ('model',)

distribution = ModelParallel(device_mesh=device_mesh,
                             layout_map=layout_map,
                             batch_dim_name='batch')
# Set the global distribution, or via `with distribution.scope():`
set_distribution(distribution)

model = model_creation()
model.compile()
model.fit(data)



# With only the shape change for the device mesh, the variables will be
# sharded across 8 devices instead of 4, which further reduces the memory
# footprint of variables on each of the device.
device_mesh = DeviceMesh(shape=(1, 8), axis_names=('batch', 'model'),
                         devices=devices)



model = create_model()
for v in model.variables:
    print(v.path)
""",
        "CODE.hash_file('/path/to/file.zip')",
        """CODE.model.export("path/to/artifact")
reloaded_layer = TFSMLayer("path/to/artifact")
outputs = reloaded_layer(inputs)
""" .

<DEPENDENCY.keras==3.0.3> <CONTAINS> """CODE.# Remember to set `use_ema=True` in the optimizer
optimizer = SGD(use_ema=True)
model.compile(optimizer=optimizer, loss=..., metrics=...)

# Metrics will be computed with EMA weights
model.fit(X_train, Y_train, callbacks=[SwapEMAWeights()])

# If you want to save model checkpoint with EMA weights, you can set
# `swap_on_epoch=True` and place ModelCheckpoint after SwapEMAWeights.
model.fit(
    X_train,
    Y_train,
    callbacks=[SwapEMAWeights(swap_on_epoch=True), ModelCheckpoint(...)]
)
""" .

<DEPENDENCY.librosa==0.10.0> <CONTAINS> """CODE.# Compare 3-limit tuning to Pythagorean tuning and 12-TET
librosa.plimit_intervals(primes=[3], bins_per_octave=12)
# Pythagorean intervals:
librosa.pythagorean_intervals(bins_per_octave=12)
# 12-TET intervals:
2**(np.arange(12)/12)

# Create a 7-bin, 5-limit interval set
librosa.plimit_intervals(primes=[3, 5], bins_per_octave=7)

# The same example, but now in factored form
librosa.plimit_intervals(primes=[3, 5], bins_per_octave=7, return_factors=True)
""",
        """CODE.harmonics = np.arange(1, 13)
f0_harm = librosa.f0_harmonics(S, freqs=freqs, f0=f0, harmonics=harmonics)
import matplotlib.pyplot as plt
fig, ax =plt.subplots(nrows=2, sharex=True)
librosa.display.specshow(librosa.amplitude_to_db(S, ref=np.max),
...                          x_axis='time', y_axis='log', ax=ax[0])
times = librosa.times_like(f0)
for h in harmonics:
...     ax[0].plot(times, h * f0, label=f"{h}*f0")
ax[0].legend(ncols=4, loc='lower right')
ax[0].label_outer()
librosa.display.specshow(librosa.amplitude_to_db(f0_harm, ref=np.max),
...                          x_axis='time', ax=ax[1])
ax[1].set_yticks(harmonics-1)
ax[1].set_yticklabels(harmonics)
ax[1].set(ylabel='Harmonics')""",
        """CODE.import librosa
y, sr = librosa.load(librosa.ex('trumpet'))
n_bins = 36
chroma_cq = librosa.feature.chroma_cqt(y=y, sr=sr, n_chroma=n_bins)
chroma_vq = librosa.feature.chroma_vqt(y=y, sr=sr, intervals='ji5', bins_per_octave=n_bins)
import matplotlib.pyplot as plt
fig, ax = plt.subplots(nrows=2, sharex=True)
librosa.display.specshow(chroma_cq, y_axis='chroma', x_axis='time', ax=ax[0], bins_per_octave=n_bins)
ax[0].set(ylabel='chroma_cqt')
ax[0].label_outer()
img = librosa.display.specshow(chroma_vq, y_axis='chroma_fjs', x_axis='time', ax=ax[1], bins_per_octave=n_bins, intervals='ji5')
ax[1].set(ylabel='chroma_vqt')
fig.colorbar(img, ax=ax)
""",
        """CODE.import matplotlib.pyplot as plt
import numpy as np
import librosa.display.ChromaFJSFormatter

values = np.arange(12)
fig, ax = plt.subplots()
ax.plot(values)
ax.yaxis.set_major_formatter(librosa.display.ChromaFJSFormatter(intervals="ji5", bins_per_octave=12))
ax.set(ylabel='Pitch class')""",
        """CODE.librosa.interval_frequencies(24, fmin=55, intervals="pythagorean", bins_per_octave=12)
librosa.interval_frequencies(24, fmin=55, intervals="ji5", bins_per_octave=12)
intervals = [1, 4/3, 3/2]
librosa.interval_frequencies(9, fmin=55, intervals=intervals)""",
        """CODE.librosa.interval_to_fjs(3/2, unison='C')
librosa.interval_to_fjs(4/3, unison='F')
librosa.interval_to_fjs(5/4, unison='A')
librosa.interval_to_fjs(6/5, unison='A')
librosa.interval_to_fjs(25/14, unison='F#')
librosa.interval_to_fjs(25/14, unison='F#', unicode=False)""",
        """CODE.librosa.pythagorean_intervals(bins_per_octave=12)
librosa.pythagorean_intervals(bins_per_octave=7, sort=False)
librosa.pythagorean_intervals(bins_per_octave=7, sort=False, return_factors=True)""",
        """CODE.librosa.util.abs2(3 + 4j)
25.0
librosa.util.abs2((0.5j)**np.arange(8))
array([1.000e+00, 2.500e-01, 6.250e-02, 1.562e-02, 3.906e-03, 9.766e-04,
   2.441e-04, 6.104e-05])""" .

<DEPENDENCY.librosa==0.2.0> <CONTAINS> """CODE.CQT             = librosa.cqt(y, sr, fmin=55, fmax=440)
freqs           = librosa.cqt_frequencies(CQT.shape[0], fmin=55)
percept_CQT     = librosa.feature.perceptual_weighting(CQT, freqs, ref_power=CQT.max())
""",
        """CODE.CQT = librosa.cqt(y, sr)
chroma_map = librosa.filters.cq_to_chroma(CQT.shape[0])
chromagram = chroma_map.dot(CQT)
""",
        """CODE.D = librosa.stft(y)
S, P = librosa.magphase(D)
D == S * P""",
        """CODE.R = librosa.feature.recurrence_matrix(mfccs)
S = librosa.feature.structure_feature(R)
R_hat = librosa.feature.structure_feature(S, inverse=True)""",
        """CODE.S           = librosa.melspectrogram(y, sr)
dct_filters = librosa.filters.dct(13, S.shape[0])
mfcc        = dct_filters.dot(librosa.logamplitude(S))
""",
        """CODE.S = np.abs(librosa.stft(y))
components, activations = librosa.decompose.decompose(S, n_components=32)


T = sklearn.decomposition.DictionaryLearning(n_components=32)
components, activations = librosa.decompose.decompose(S, transformer=T)
""",
        """CODE.basis   = librosa.filters.constant_q(22050)
CQT     = librosa.cqt(y, sr, basis=basis)

basis   = librosa.filters.constant_q(22050, window=np.hanning)

basis   = librosa.filters.constant_q(22050, resolution=2)""",
        """CODE.chroma_fb   = librosa.filters.chroma(22050, 4096)
chroma_fbq  = librosa.filters.chroma(22050, 4096, n_chroma=24)
chroma_fb   = librosa.filters.chroma(22050, 4096, ctroct=5, octwidth=2)""",
        """CODE.freqs   = librosa.mel_frequencies(20)
librosa.A_weighting(freqs)""",
        "CODE.librosa.cqt_frequencies(24, fmin=librosa.midi_to_hz(librosa.note_to_midi('C2')))",
        """CODE.librosa.feature.estimate_tuning(freqs)
librosa.feature.estimate_tuning(pitches)""",
        """CODE.librosa.fft_frequencies(sr=22050, n_fft=16)
array([     0.   ,   1378.125,   2756.25 ,   4134.375,   5512.5  ,
         6890.625,   8268.75 ,   9646.875,  11025.   ])""",
        """CODE.librosa.note_to_midi('C')
librosa.note_to_midi('C#3')
librosa.note_to_midi('f4')
librosa.note_to_midi('Bb-1')
librosa.note_to_midi('A!8')""",
        """CODE.librosa.octs_to_hz(1)
librosa.octs_to_hz([-2, -1, 0, 1, 2])""",
        "CODE.librosa.peak_pick(x, 3, 3, 5, 5, 0.5, 10)",
        """CODE.librosa.stft(y, n_fft=2048, hop_length=512)
D_fast  = librosa.phase_vocoder(D, 2.0, hop_length=512)
y_fast  = librosa.istft(D_fast, hop_length=512)
D_slow  = librosa.phase_vocoder(D, 1./3, hop_length=512)
y_slow  = librosa.istft(D_slow, hop_length=512)""",
        "CODE.librosa.time_to_frames(np.arange(0, 1, 0.1), sr=22050, hop_length=512)",
        """CODE.logfs_fb = librosa.filters.logfrequency(22050, 4096)
logfs_fb = librosa.filters.logfrequency(22050, 4096, fmin=110, fmax=880)
logfs_fb = librosa.filters.logfrequency(22050, 4096, spread=0.05)
logfs_fb = librosa.filters.logfrequency(22050, 4096, spread=0.5)""",
        """CODE.mfcc    = librosa.feature.mfcc(y=y, sr=sr)
R       = librosa.segment.recurrence_matrix(mfcc)

R       = librosa.segment.recurrence_matrix(mfcc, k=5)

R       = librosa.segment.recurrence_matrix(mfcc, width=7)

R       = librosa.segment.recurrence_matrix(mfcc, metric='cosine')

R       = librosa.segment.recurrence_matrix(mfcc, sym=True)
""",
        """CODE.mfccs       = librosa.feature.mfcc(y=y, sr=sr)
delta_mfcc  = librosa.feature.delta(mfccs)
delta2_mfcc = librosa.feature.delta(mfccs, order=2)""",
        """CODE.onset_frames    = librosa.onset.onset_detect(y=y, sr=sr, hop_length=64)
onset_times     = librosa.frames_to_time(onset_frames, sr, hop_length=64)

onsets          = librosa.onset.onset_strength(y, sr)
onset_frames    = librosa.onset.onset_detect(onset_envelope=onsets, sr=sr)
""",
        """CODE.tempo, beats = librosa.beat.beat_track(y, sr=sr, hop_length=64)
librosa.output.frames_csv('beat_times.csv', frames, sr=sr, hop_length=64)""",
        """CODE.y, sr = librosa.load('file.wav')
C = librosa.cqt(y, sr)

C = librosa.cqt(y, sr, fmin=librosa.midi_to_hz(36), fmax=librosa.midi_to_hz(96))

basis = librosa.filters.constant_q(sr, ...)
C = librosa.cqt(y, sr, basis=basis)""",
        """CODE.y, sr = librosa.load('file.wav')
frequencies, D = librosa.ifgram(y, sr=sr)
""" .

<DEPENDENCY.librosa==0.2.1> <CONTAINS> """CODE.
boundaries = librosa.segment.agglomerative(data, k=10)
boundary_times = librosa.frames_to_time(boundaries, sr=sr, hop_length=hop_length)
time_start, time_end = boundaries[:-1], boundaries[1:]
labels = ['Segment #%03d' % i for i in range(len(time_start))]
librosa.output.annotation('segments.csv', time_start, time_end, annotations=annotations)
""",
        """CODE.# Load a file
y, sr = librosa.load('file.mp3')
# Extract 2048-sample frames from y with a hop of 64
y_frames = librosa.util.frame(y, frame_length=2048, hop_length=64)
""",
        """CODE.W, H = librosa.decompose.decompose(S)
W_sort = librosa.util.axis_sort(W)
W_sort = librosa.util.axis_sort(W, value=np.argmin)
W_sort_rows = librosa.util.axis_sort(W, axis=0)
W_sort, idx = librosa.util.axis_sort(W, index=True)
H_sort = H[index, :]
""",
        """CODE.tempo, beats = librosa.beat.beat_track(y, sr=sr, hop_length=64)
times = librosa.frames_to_time(beats, sr=sr, hop_length=64)
librosa.output.times_csv('beat_times.csv', times)
""" .

<DEPENDENCY.librosa==0.3.0> <CONTAINS> """CODE.freqs = librosa.cqt_frequencies(24, 55, tuning=0.25)
librosa.feature.pitch_tuning(freqs)
0.25

pitches, magnitudes, stft = librosa.feature.ifptrack(y, sr)
pitches = pitches[magnitudes > np.median(magnitudes)]
librosa.feature.pitch_tuning(pitches)
""",
        "CODE.librosa.load(librosa.util.example_audio_file())",
        """CODE.y, sr = librosa.load('file.mp3')
y_fast = librosa.effects.time_stretch(y, 2.0)
y_slow = librosa.effects.time_stretch(y, 0.5)""",
        """CODE.y, sr = librosa.load('file.mp3')
y_harmonic = librosa.effects.harmonic(y)""",
        """CODE.y, sr = librosa.load('file.mp3')
y_percussive = librosa.effects.percussive(y)""",
        """CODE.y_third = librosa.effects.pitch_shift(y, sr, n_steps=4)
y_tritone = librosa.effects.pitch_shift(y, sr, n_steps=-6)
y_three_qt = librosa.effects.pitch_shift(y, sr, n_steps=3, bins_per_octave=24)
""" .

<DEPENDENCY.librosa==0.3.1> <CONTAINS> """CODE.C = librosa.chromagram(y, sr)

S = np.abs(librosa.stft(y, n_fft=4096))
C = librosa.chromagram(S=S)
""",
        """CODE.y, sr = librosa.load(librosa.util.example_audio())
d = librosa.get_duration(y=y, sr=sr)
d


S = librosa.stft(y)
d = librosa.get_duration(S=S, sr=sr)


S_left = librosa.stft(y, center=False)
d = librosa.get_duration(S=S_left, sr=sr)
""" .

<DEPENDENCY.librosa==0.4.0> <CONTAINS> """CODE.# Get the frequency of a note
librosa.note_to_hz('C')
array([ 16.352])
# Or multiple notes
librosa.note_to_hz(['A3', 'A4', 'A5'])
array([ 220.,  440.,  880.])
# Or notes with tuning deviations
librosa.note_to_hz('C2-32', round_midi=False)
array([ 64.209])
""",
        """CODE.data_tl = librosa.segment.recurrence_to_lag(data)
data_filtered_tl = function(data_tl)
data_filtered = librosa.segment.lag_to_recurrence(data_filtered_tl)

diagonal_median = librosa.segment.timelag_filter(median_filter)
rec_filtered = diagonal_median(rec, size=(1, 5), mode='mirror')""",
        """CODE.frames = np.arange(0, 1000.0, 50)
frames
array([   0.,   50.,  100.,  150.,  200.,  250.,  300.,  350.,
        400.,  450.,  500.,  550.,  600.,  650.,  700.,  750.,
        800.,  850.,  900.,  950.])
librosa.util.fix_frames(frames, x_max=250)
array([  0,  50, 100, 150, 200, 250])
librosa.util.fix_frames(frames, x_max=2500)
array([   0,   50,  100,  150,  200,  250,  300,  350,  400,
        450,  500,  550,  600,  650,  700,  750,  800,  850,
        900,  950, 2500])
librosa.util.fix_frames(frames, x_max=2500, pad=False)
array([  0,  50, 100, 150, 200, 250, 300, 350, 400, 450, 500,
       550, 600, 650, 700, 750, 800, 850,900,950])
frames = np.arange(200,500,33)
frames
array([200,233,266,299,332,365,398,431,464,497])
librosa.util.fix_frames(frames)
array([0,200,233,266,299,332,365,398,431,464,497])
librosa.util.fix_frames(frames,x_max=500)
array([0,200,233,266,299,332,365,398,431,464,497,
       500])
""",
        """CODE.import librosa
import numpy as np

# Get the frame numbers for every 256 samples
librosa.samples_to_frames(np.arange(0, 22050, 256))
""",
        """CODE.import numpy as np
import librosa

def subsegment(data, frames, n_segments):
    boundaries = []
    # Your code here
    return np.array(boundaries)

# Example usage
y, sr = librosa.load(librosa.util.example_audio_file(), duration=15)
tempo, beats = librosa.beat.beat_track(y=y, sr=sr, hop_length=512)
cqt = librosa.cqt(y, sr=sr, hop_length=512)
subseg = subsegment(cqt, beats, n_segments=2)
print(subseg)""",
        """CODE.intervals = [(0, 100), (200, 300), (400, 500)]
y_remix = librosa.effects.remix(y, intervals[::-1])""",
        "CODE.librosa.samples_to_time(np.arange(0, 22050, 512))",
        "CODE.librosa.time_to_samples(np.arange(0, 1, 0.1), sr=22050)",
        """CODE.librosa.util.valid_audio(y)
librosa.util.valid_audio(y, mono=False)""",
        """CODE.line = librosa.feature.poly_features(S=S, sr=sr)
line
array([[ -2.406e-08,  -5.051e-06, ...,  -1.103e-08,  -5.651e-09],
       [  3.445e-04,   3.834e-02, ...,   2.661e-04,   2.239e-04]])

quad = librosa.feature.poly_features(S=S, order=2)
quad
array([[  6.276e-12,   2.010e-09, ...,   1.493e-12,   1.000e-13],
       [ -9.325e-08,  -2.721e-05, ...,  -2.749e-08,  -6.754e-09],
       [  4.715e-04,   7.902e-02, ...,   2.963e-04,   2.259e-04]])

import matplotlib.pyplot as plt
plt.figure()
plt.subplot(3, 1, 1)
librosa.display.specshow(line)
plt.colorbar()
plt.title('Line coefficients')
plt.subplot(3, 1, 2)
librosa.display.specshow(quad)
plt.colorbar()
plt.title('Quadratic coefficients')
plt.subplot(3, 1, 3)
librosa.display.specshow(librosa.logamplitude(S**2, ref_power=np.max),
...                          y_axis='log', x_axis='time')
plt.title('log Power spectrogram')
plt.colorbar(format='%+2.0f dB')
plt.tight_layout()""",
        """CODE.s_from = np.arange(0, 100, 7)
s_to = np.arange(0, 100, 10)
idx = librosa.util.match_events(s_from, s_to)
zip(s_from, s_to[idx])
""",
        """CODE.spec_bw = librosa.feature.spectral_bandwidth(y=y, sr=sr)
spec_bw
array([[ 3379.878,  1429.486, ...,  3235.214,  3080.148]])

S, phase = librosa.magphase(librosa.stft(y=y))
librosa.feature.spectral_bandwidth(S=S)
array([[ 3379.878,  1429.486, ...,  3235.214,  3080.148]])

if_gram, D = librosa.ifgram(y)
librosa.feature.spectral_bandwidth(S=np.abs(D), freq=if_gram)
array([[ 3380.011,  1429.11 , ...,  3235.22 ,  3080.148]])

import matplotlib.pyplot as plt
plt.figure()
plt.subplot(2, 1, 1)
plt.semilogy(spec_bw.T, label='Spectral bandwidth')
plt.ylabel('Hz')
plt.xticks([])
plt.xlim([0, spec_bw.shape[-1]])
plt.legend()
plt.subplot(2, 1, 2)
librosa.display.specshow(librosa.logamplitude(S**2, ref_power=np.max),
...                          y_axis='log', x_axis='time')
plt.title('log Power spectrogram')
plt.tight_layout()""",
        """CODE.x = scipy.signal.hann(32)
x_sparse = librosa.util.sparsify_rows(x, quantile=0.01)
x_sparse.todense()
x_sparse = librosa.util.sparsify_rows(x, quantile=0.1)
x_sparse.todense()
""",
        """CODE.y = np.sin(np.linspace(0, 4 * 2 * np.pi, 20))
z = librosa.zero_crossings(y)
np.vstack([y, z]).T
np.nonzero(z)""",
        """CODE.y, sr = librosa.load(librosa.util.example_audio_file())
S = np.abs(librosa.stft(y))
contrast = librosa.feature.spectral_contrast(S=S, sr=sr)
import matplotlib.pyplot as plt
plt.figure()
plt.subplot(2, 1, 1)
librosa.display.specshow(librosa.logamplitude(S ** 2, ref_power=np.max), y_axis='log')
plt.colorbar(format='%+2.0f dB')
plt.title('Power spectrogram')
plt.subplot(2, 1, 2)
librosa.display.specshow(contrast, x_axis='time')
plt.colorbar()
plt.ylabel('Frequency bands')
plt.title('Spectral contrast')
plt.tight_layout()""",
        """CODE.y, sr = librosa.load(librosa.util.example_audio_file())
cent = librosa.feature.spectral_centroid(y=y, sr=sr)
cent
array([[ 4382.894,   626.588, ...,  5037.07 ,  5413.398]])

S, phase = librosa.magphase(librosa.stft(y=y))
librosa.feature.spectral_centroid(S=S)
array([[ 4382.894,   626.588, ...,  5037.07 ,  5413.398]])

y, sr = librosa.load(librosa.util.example_audio_file())
if_gram, D = librosa.ifgram(y)
librosa.feature.spectral_centroid(S=np.abs(D), freq=if_gram)

import matplotlib.pyplot as plt
plt.figure()
plt.subplot(2, 1, 1)
plt.semilogy(cent.T, label='Spectral centroid')
plt.ylabel('Hz')
plt.xticks([])
plt.xlim([0, cent.shape[-1]])
plt.legend()
plt.subplot(2, 1, 2)
librosa.display.specshow(librosa.logamplitude(S**2, ref_power=np.max),
...                          y_axis='log', x_axis='time')
plt.title('log Power spectrogram')
plt.tight_layout()""",
        """CODE.y, sr = librosa.load(librosa.util.example_audio_file())
librosa.feature.chroma_stft(y=y, sr=sr)
array([[ 0.974,  0.881, ...,  0.925,  1.   ],
       [ 1.   ,  0.841, ...,  0.882,  0.878],
       ...,
       [ 0.658,  0.985, ...,  0.878,  0.764],
       [ 0.969,  0.92 , ...,  0.974,  0.915]])

S = np.abs(librosa.stft(y, n_fft=4096))
chroma = librosa.feature.chroma_stft(S=S, sr=sr)
chroma
array([[ 0.685,  0.477, ...,  0.961,  0.986],
       [ 0.674,  0.452, ...,  0.952,  0.926],
       ...,
       [ 0.844,  0.575, ...,  0.934,  0.869],
       [ 0.793,  0.663, ...,  0.964,  0.972]])

import matplotlib.pyplot as plt
librosa.display.specshow(chroma, y_axis='chroma', x_axis='time')
plt.colorbar()
plt.title('Chromagram')
plt.tight_layout()""",
        """CODE.y, sr = librosa.load(librosa.util.example_audio_file())
librosa.feature.rmse(y=y)

S, phase = librosa.magphase(librosa.stft(y))
rms = librosa.feature.rmse(S=S)

import matplotlib.pyplot as plt
plt.figure()
plt.subplot(2, 1, 1)
plt.semilogy(rms.T, label='RMS Energy')
plt.xticks([])
plt.xlim([0, rms.shape[-1]])
plt.legend(loc='best')
plt.subplot(2, 1, 2)
librosa.display.specshow(librosa.logamplitude(S**2, ref_power=np.max),
...                          y_axis='log', x_axis='time')
plt.title('log Power spectrogram')
plt.tight_layout()""",
        """CODE.y, sr = librosa.load(librosa.util.example_audio_file())
y = librosa.effects.harmonic(y)
tonnetz = librosa.feature.tonnetz(y=y, sr=sr)
tonnetz
array([[-0.073, -0.053, ..., -0.054, -0.073],
       [ 0.001,  0.001, ..., -0.054, -0.062],
       ...,
       [ 0.039,  0.034, ...,  0.044,  0.064],
       [ 0.005,  0.002, ...,  0.011,  0.017]])

import matplotlib.pyplot as plt
plt.subplot(2, 1, 1)
librosa.display.specshow(tonnetz, y_axis='tonnetz')
plt.colorbar()
plt.title('Tonal Centroids (Tonnetz)')
plt.subplot(2, 1, 2)
librosa.display.specshow(librosa.feature.chroma_cqt(y, sr=sr),
...                          y_axis='chroma', x_axis='time')
plt.colorbar()
plt.title('Chroma')
plt.tight_layout()""",
        """CODE.y, sr = librosa.load(librosa.util.example_audio_file(), mono=False)
y.shape
(2, 1355168)
y_mono = librosa.to_mono(y)
y_mono.shape""" .

<DEPENDENCY.librosa==0.4.1> <CONTAINS> """CODE.librosa.util.index_to_slice(np.arange(20, 100, 15))
librosa.util.index_to_slice(np.arange(20, 100, 15),
                            idx_min=0, idx_max=100)
librosa.util.index_to_slice(np.arange(20, 100, 15),
                            idx_min=0, idx_max=100, step=5)""",
        "CODE.onset_subbands = librosa.onset.onset_strength_multi(y=y, sr=sr, channels=[0, 32, 64, 96, 128])",
        """CODE.y, sr = librosa.load(librosa.util.example_audio_file())
hop_length = 512
oenv = librosa.onset.onset_strength(y=y, sr=sr, hop_length=hop_length)
tempogram = librosa.feature.tempogram(onset_envelope=oenv, sr=sr,
...                                       hop_length=hop_length)
ac_global = librosa.autocorrelate(oenv, max_size=tempogram.shape[0])
ac_global = librosa.util.normalize(ac_global)
tempo = librosa.beat.estimate_tempo(oenv, sr=sr, hop_length=hop_length)
import matplotlib.pyplot as plt
plt.figure(figsize=(8, 6))
plt.subplot(3, 1, 1)
plt.plot(oenv, label='Onset strength')
plt.xticks([])
plt.legend(frameon=True)
plt.axis('tight')
plt.subplot(3, 1, 2)
librosa.display.specshow(tempogram[:100], sr=sr, hop_length=hop_length,
...                          x_axis='time', y_axis='tempo',
...                          tmin=tempo/4, tmax=2*tempo, n_yticks=4)
plt.subplot(3, 1, 3)
x = np.linspace(0, tempogram.shape[0] * float(hop_length) / sr, num=tempogram.shape[0])
plt.plot(x, np.mean(tempogram, axis=1), label='Mean local autocorrelation')
plt.plot(x, ac_global, '--', alpha=0.75, label='Global autocorrelation')
plt.xlabel('Lag (seconds)')
plt.axis('tight')
plt.legend(frameon=True)
plt.tight_layout()""",
        """CODE.y_beats = librosa.clicks(frames=beats, sr=sr)
y_beats = librosa.clicks(frames=beats, sr=sr, length=len(y))
times = librosa.frames_to_time(beats, sr=sr)
y_beat_times = librosa.clicks(times=times, sr=sr)
y_beat_times880 = librosa.clicks(times=times, sr=sr, click_freq=880, click_duration=0.5)""" .

<DEPENDENCY.librosa==0.4.2> <CONTAINS> """CODE.librosa.display.frequency_ticks()
librosa.display.frequency_ticks(locations, frequencies)
librosa.display.frequency_ticks(frequencies, freq_fmt='Hz')
librosa.display.frequency_ticks(frequencies, axis='y')""" .

<DEPENDENCY.librosa==0.4.3> <CONTAINS> """CODE.# Generate a random sparse binary matrix
X = scipy.sparse.lil_matrix(np.random.randint(0, 2, size=(5,5)))
X_roll = roll_sparse(X, 2, axis=0)  # Roll by 2 on the first axis
X_dense_r = roll_sparse(X.toarray(), 2, axis=0)  # Equivalent dense roll
np.allclose(X_roll, X_dense_r.toarray())
""",
        """CODE.import sklearn.pipeline
MS = librosa.util.FeatureExtractor(librosa.feature.melspectrogram,
                                   sr=22050, n_fft=2048,
                                   n_mels=128, fmax=8000)
LA = librosa.util.FeatureExtractor(librosa.logamplitude,
                                   ref_power=np.max)
Features = sklearn.pipeline.Pipeline([('MelSpectrogram', MS),
                                      ('LogAmplitude', LA)])
y, sr = librosa.load(librosa.util.example_audio_file())
F = Features.transform([y])""",
        """CODE.librosa.display.specshow(S)
librosa.display.frequency_ticks()
librosa.display.frequency_ticks(locations, frequencies)
librosa.display.frequency_ticks(frequencies, freq_fmt='Hz')
librosa.display.frequency_ticks(frequencies, axis='y')""",
        """CODE.y, sr = librosa.load(librosa.util.example_audio_file(),
...                      offset=10, duration=15)
chroma_cens = librosa.feature.chroma_cens(y=y, sr=sr)
chroma_cq = librosa.feature.chroma_cqt(y=y, sr=sr)

import matplotlib.pyplot as plt
plt.figure()
plt.subplot(2,1,1)
librosa.display.specshow(chroma_cq, y_axis='chroma')
plt.title('chroma_cq')
plt.colorbar()
plt.subplot(2,1,2)
librosa.display.specshow(chroma_cens, y_axis='chroma', x_axis='time')
plt.title('chroma_cens')
plt.colorbar()
plt.tight_layout()""" .

<DEPENDENCY.librosa==0.5.0> <CONTAINS> """CODE.# Estimate a static tempo
y, sr = librosa.load(librosa.util.example_audio_file())
onset_env = librosa.onset.onset_strength(y, sr=sr)
tempo = librosa.beat.tempo(onset_envelope=onset_env, sr=sr)
tempo
array([129.199])

# Or a dynamic tempo
dtempo = librosa.beat.tempo(onset_envelope=onset_env, sr=sr,
                            aggregate=None)
dtempo
array([ 143.555,  143.555,  143.555, ...,  161.499,  161.499,
    172.266])

import matplotlib.pyplot as plt
# Convert to scalar
tempo = np.asscalar(tempo)
# Compute 2-second windowed autocorrelation
hop_length = 512
ac = librosa.autocorrelate(onset_env, 2 * sr // hop_length)
freqs = librosa.tempo_frequencies(len(ac), sr=sr,
                                hop_length=hop_length)
# Plot on a BPM axis.  We skip the first (0-lag) bin.
plt.figure(figsize=(8,4))
plt.semilogx(freqs[1:], librosa.util.normalize(ac)[1:],
            label='Onset autocorrelation', basex=2)
plt.axvline(tempo, 0, 1, color='r', alpha=0.75, linestyle='--',
            label='Tempo: {:.2f} BPM'.format(tempo))
plt.xlabel('Tempo (BPM)')
plt.grid()
plt.title('Static tempo estimation')
plt.legend(frameon=True)
plt.axis('tight')

plt.figure()
tg = librosa.feature.tempogram(onset_envelope=onset_env, sr=sr,
                            hop_length=hop_length)
librosa.display.specshow(tg, x_axis='time', y_axis='tempo')
plt.plot(librosa.frames_to_time(np.arange(len(dtempo))), dtempo,
        color='w', linewidth=1.5, label='Tempo estimate')
plt.title('Dynamic tempo estimation')
plt.legend(frameon=True, framealpha=0.75)
""",
        """CODE.# Trim leading and trailing silence from an audio signal.

import numpy as np
import librosa

def trim_audio_signal(y, top_db=60, ref=np.max, frame_length=2048, hop_length=512):
    y_trimmed, index = librosa.effects.trim(y, top_db=top_db, ref=ref, frame_length=frame_length, hop_length=hop_length)
    return y_trimmed, index

# Load some audio
y, sr = librosa.load(librosa.util.example_audio_file())
# Trim the beginning and ending silence
yt, index = trim_audio_signal(y)
# Print the durations
print(librosa.get_duration(y), librosa.get_duration(yt))
""",
        """CODE.S = np.abs(librosa.stft(y))
librosa.power_to_db(S**2)
array([[-33.293, -27.32 , ..., -33.293, -33.293],
       [-33.293, -25.723, ..., -33.293, -33.293],
       ...,
       [-33.293, -33.293, ..., -33.293, -33.293],
       [-33.293, -33.293, ..., -33.293, -33.293]], dtype=float32)
librosa.power_to_db(S**2, ref=np.max)
array([[-80.   , -74.027, ..., -80.   , -80.   ],
       [-80.   , -72.431, ..., -80.   , -80.   ],
       ...,
       [-80.   , -80.   , ..., -80.   , -80.   ],
       [-80.   , -80.   , ..., -80.   , -80.   ]], dtype=float32)
librosa.power_to_db(S**2, ref=np.median)
array([[-0.189,  5.784, ..., -0.189, -0.189],
       [-0.189,  7.381, ..., -0.189, -0.189],
       ...,
       [-0.189, -0.189, ..., -0.189, -0.189],
       [-0.189, -0.189, ..., -0.189, -0.189]], dtype=float32)
import matplotlib.pyplot as plt
plt.figure()
plt.subplot(2, 1, 1)
librosa.display.specshow(S**2, sr=sr, y_axis='log')
plt.colorbar()
plt.title('Power spectrogram')
plt.subplot(2, 1, 2)
librosa.display.specshow(librosa.power_to_db(S**2, ref=np.max),
...                          sr=sr, y_axis='log', x_axis='time')
plt.colorbar(format='%+2.0f dB')
plt.title('Log-Power spectrogram')
plt.tight_layout()""",
        """CODE.h_range = [1, 2, 3, 4, 5]
f_tempo = librosa.tempo_frequencies(len(tempi), sr=sr)
t_harmonics = librosa.interp_harmonics(tempi, f_tempo, h_range)
print(t_harmonics.shape)

import matplotlib.pyplot as plt
plt.figure()
librosa.display.specshow(t_harmonics, x_axis='tempo', sr=sr)
plt.yticks(0.5 + np.arange(len(h_range)),
...            ['{:.3g}'.format(_) for _ in h_range])
plt.ylabel('Harmonic')
plt.xlabel('Tempo (BPM)')
plt.tight_layout()

h_range = [1./3, 1./2, 1, 2, 3, 4]
S = np.abs(librosa.stft(y))
fft_freqs = librosa.fft_frequencies(sr=sr)
S_harm = librosa.interp_harmonics(S, fft_freqs, h_range, axis=0)
print(S_harm.shape)

plt.figure()
for i, _sh in enumerate(S_harm, 1):
...     plt.subplot(3, 2, i)
...     librosa.display.specshow(librosa.amplitude_to_db(_sh,
...                                                      ref=S.max()),
...                              sr=sr, y_axis='log')
...     plt.title('h={:.3g}'.format(h_range[i-1]))
...     plt.yticks([])
plt.tight_layout()""",
        """CODE.import matplotlib.pyplot as plt
import numpy as np
import librosa.display

times = np.arange(30)
values = np.random.randn(len(times))
plt.figure()
ax = plt.gca()
ax.plot(times, values)
ax.xaxis.set_major_formatter(librosa.display.TimeFormatter())
ax.set_xlabel('Time')

times = np.arange(60)
values = np.random.randn(len(times))
plt.figure()
ax = plt.gca()
ax.plot(times, values)
ax.xaxis.set_major_formatter(librosa.display.TimeFormatter(lag=True))
ax.set_xlabel('Lag')""",
        """CODE.import matplotlib.pyplot as plt
values = np.arange(12)
plt.figure()
ax = plt.gca()
ax.plot(values)
ax.yaxis.set_major_formatter(librosa.display.ChromaFormatter())
ax.set_ylabel('Pitch class')""",
        """CODE.import numpy as np
import librosa

S = np.abs(librosa.stft(y))
freqs = librosa.core.fft_frequencies(sr)
harms = [1, 2, 3, 4]
weights = [1.0, 0.5, 0.33, 0.25]
S_sal = librosa.salience(S, freqs, harms, weights, fill_value=0)
print(S_sal.shape)
import matplotlib.pyplot as plt
plt.figure()
librosa.display.specshow(librosa.amplitude_to_db(S_sal, ref=np.max), sr=sr, y_axis='log', x_axis='time')
plt.colorbar()
plt.title('Salience spectrogram')
plt.tight_layout()""",
        """CODE.import numpy as np
import matplotlib.pyplot as plt
y, sr = librosa.load(librosa.util.example_audio_file(), offset=10, duration=15)
X = librosa.feature.chroma_cens(y=y, sr=sr)
noise = np.random.rand(X.shape[0], 200)
Y = np.concatenate((noise, noise, X, noise), axis=1)
D, wp = librosa.dtw(X, Y, subseq=True)
plt.subplot(2, 1, 1)
librosa.display.specshow(D, x_axis='frames', y_axis='frames')
plt.title('Database excerpt')
plt.plot(wp[:, 1], wp[:, 0], label='Optimal path', color='y')
plt.legend()
plt.subplot(2, 1, 2)
plt.plot(D[-1, :] / wp.shape[0])
plt.xlim([0, Y.shape[1]])
plt.ylim([0, 2])
plt.title('Matching cost function')
plt.tight_layout()""",
        """CODE.y, sr = librosa.load(librosa.util.example_audio_file(),
...                      offset=30, duration=2.0)
oenv = librosa.onset.onset_strength(y=y, sr=sr)
# Detect events without backtracking
onset_raw = librosa.onset.onset_detect(onset_envelope=oenv,
...                                        backtrack=False)
# Backtrack the events using the onset envelope
onset_bt = librosa.onset.onset_backtrack(onset_raw, oenv)
# Backtrack the events using the RMS energy
rmse = librosa.feature.rmse(S=np.abs(librosa.stft(y=y)))
onset_bt_rmse = librosa.onset.onset_backtrack(onset_raw, rmse[0])

# Plot the results
import matplotlib.pyplot as plt
plt.figure()
plt.subplot(2,1,1)
plt.plot(oenv, label='Onset strength')
plt.vlines(onset_raw, 0, oenv.max(), label='Raw onsets')
plt.vlines(onset_bt, 0, oenv.max(), label='Backtracked', color='r')
plt.legend(frameon=True, framealpha=0.75)
plt.subplot(2,1,2)
plt.plot(rmse[0], label='RMSE')
plt.vlines(onset_bt_rmse, 0, rmse.max(), label='Backtracked (RMSE)', color='r')
plt.legend(frameon=True, framealpha=0.75)""" .

<DEPENDENCY.librosa==0.6.2> <CONTAINS> """CODE.y, sr = librosa.load(librosa.util.example_audio_file())
librosa.feature.rmse(y=y)

S, phase = librosa.magphase(librosa.stft(y))
rms = librosa.feature.rmse(S=S)

S = librosa.magphase(librosa.stft(y, window=np.ones, center=False))[0]
librosa.feature.rmse(S=S)""" .

<DEPENDENCY.librosa==0.6.3> <CONTAINS> """CODE.n_fft = 2048
dct_filters = librosa.filters.dct(13, 1 + n_fft // 2)
dct_filters
array([[ 0.031,  0.031, ...,  0.031,  0.031],
       [ 0.044,  0.044, ..., -0.044, -0.044],
       ...,
       [ 0.044,  0.044, ..., -0.044, -0.044],
       [ 0.044,  0.044, ...,  0.044,  0.044]])

import matplotlib.pyplot as plt
plt.figure()
librosa.display.specshow(dct_filters, x_axis='linear')
plt.ylabel('DCT function')
plt.title('DCT filter bank')
plt.colorbar()
plt.tight_layout()""",
        """CODE.y, sr = librosa.load(librosa.util.example_audio_file())
librosa.feature.rms(y=y)

S, phase = librosa.magphase(librosa.stft(y))
rms = librosa.feature.rms(S=S)

import matplotlib.pyplot as plt
plt.figure()
plt.subplot(2, 1, 1)
plt.semilogy(rms.T, label='RMS Energy')
plt.xticks([])
plt.xlim([0, rms.shape[-1]])
plt.legend(loc='best')
plt.subplot(2, 1, 2)
librosa.display.specshow(librosa.amplitude_to_db(S, ref=np.max),
...                          y_axis='log', x_axis='time')
plt.title('log Power spectrogram')
plt.tight_layout()

S = librosa.magphase(librosa.stft(y, window=np.ones, center=False))[0]
librosa.feature.rms(S=S)""" .

<DEPENDENCY.librosa==0.7.0> <CONTAINS> """CODE.S = np.abs(librosa.stft(y))
mel_spec = librosa.feature.melspectrogram(S=S, sr=sr)
S_inv = librosa.feature.inverse.mel_to_stft(mel_spec, sr=sr)
import matplotlib.pyplot as plt
plt.figure()
plt.subplot(2,1,1)
librosa.display.specshow(librosa.amplitude_to_db(S, ref=np.max, top_db=None),
...                          y_axis='log', x_axis='time')
plt.colorbar()
plt.title('Original STFT')
plt.subplot(2,1,2)
librosa.display.specshow(librosa.amplitude_to_db(np.abs(S_inv - S),
...                                                  ref=S.max(), top_db=None),
...                          vmax=0, y_axis='log', x_axis='time', cmap='magma')
plt.title('Residual error (dB)')
plt.colorbar()
plt.tight_layout()
plt.show()""",
        """CODE.import numpy as np
import matplotlib.pyplot as plt

x = 2 * np.pi * np.linspace(0, 1, num=64, endpoint=False)
y = np.cos(x)
grad = np.gradient(y)
cyclic_grad = librosa.util.cyclic_gradient(y)
true_grad = -np.sin(x) * 2 * np.pi / len(x)
plt.plot(x, true_grad, label='True gradient', linewidth=5, alpha=0.35)
plt.plot(x, cyclic_grad, label='cyclic_gradient')
plt.plot(x, grad, label='np.gradient', linestyle=':')
plt.legend()
plt.xlim([0, np.pi/16])
plt.ylim([-0.025, 0.025])
plt.show()""",
        """CODE.import numpy as np
import matplotlib.pyplot as plt
import librosa

y, sr = librosa.load(librosa.util.example_audio_file(), offset=10, duration=30)
chroma = librosa.feature.chroma_cqt(y=y, sr=sr)
chroma_stack = librosa.feature.stack_memory(chroma, n_steps=3)
rec = librosa.segment.recurrence_matrix(chroma_stack, width=43, mode='affinity', metric='cosine')
L_score, L_path = librosa.sequence.rqa(rec, np.inf, np.inf, knight_moves=False)
plt.figure(figsize=(10, 4))
plt.subplot(1,2,1)
librosa.display.specshow(rec, x_axis='frames', y_axis='frames')
plt.title('Recurrence matrix')
plt.colorbar()
plt.subplot(1,2,2)
librosa.display.specshow(L_score, x_axis='frames', y_axis='frames')
plt.title('Alignment score matrix')
plt.colorbar()
plt.plot(L_path[:, 1], L_path[:, 0], label='Optimal path', color='c')
plt.legend()
plt.show()

score, path = librosa.sequence.rqa(rec, 5, 10)
plt.figure(figsize=(10, 4))
plt.subplot(1,2,1)
librosa.display.specshow(rec, x_axis='frames', y_axis='frames')
plt.title('Recurrence matrix')
plt.colorbar()
plt.subplot(1,2,2)
librosa.display.specshow(score, x_axis='frames', y_axis='frames')
plt.title('Alignment score matrix')
plt.plot(path[:, 1], path[:, 0], label='Optimal path', color='c')
plt.colorbar()
plt.legend()
plt.show()""",
        """CODE.import pyfftw
librosa.set_fftlib(pyfftw.interfaces.numpy_fft)
librosa.set_fftlib()""",
        "CODE.librosa.blocks_to_time(n, block_length=16, hop_length=512, sr=sr)",
        "CODE.librosa.fourier_tempo_frequencies(384)",
        """CODE.path = librosa.util.example_audio_file()
librosa.get_samplerate(path)
44100""",
        """CODE.stream librosa.stream(filename,
                      block_length=256,
                      frame_length=4096,
                      hop_length=1024)
for y_block in stream:
    D_block = librosa.stft(y_block, center=False)

stream = librosa.stream(filename,
                        block_length=256,
                        frame_length=2048,
                        hop_length=2048)
for y_block in stream:
    m_block = librosa.feature.melspectrogram(y_block, sr=sr,
                                             n_fft=2048,
                                             hop_length=2048,
                                             center=False)""",
        """CODE.y, sr = librosa.load(librosa.util.example_audio_file())
hop_length = 512
oenv = librosa.onset.onset_strength(y=y, sr=sr, hop_length=hop_length)
tempogram = librosa.feature.fourier_tempogram(onset_envelope=oenv, sr=sr,
...                                               hop_length=hop_length)
ac_tempogram = librosa.feature.tempogram(onset_envelope=oenv, sr=sr,
...                                          hop_length=hop_length, norm=None)""",
        """CODE.y, sr = librosa.load(librosa.util.example_audio_file(), offset=30,
...                      duration=10)
librosa.lpc(y, 16)

import matplotlib.pyplot as plt
import scipy
y, sr = librosa.load(librosa.util.example_audio_file(), offset=30,
...                      duration=0.020)
a = librosa.lpc(y, 2)
y_hat = scipy.signal.lfilter([0] + -1*a[1:], [1], y)
plt.figure()
plt.plot(y)
plt.plot(y_hat, linestyle='--')
plt.legend(['y', 'y_hat'])
plt.title('LP Model Forward Prediction')
plt.show()""",
        """CODE.y, sr = librosa.load(librosa.util.example_audio_file(), offset=30, duration=10)
S = np.abs(librosa.stft(y, n_fft=2048))
M = librosa.feature.melspectrogram(S=S, sr=sr, power=1)
mel_basis = librosa.filters.mel(sr, n_fft=2048, n_mels=M.shape[0])
S_recover = librosa.util.nnls(mel_basis, M)
import matplotlib.pyplot as plt
plt.figure()
plt.subplot(3,1,1)
librosa.display.specshow(librosa.amplitude_to_db(S, ref=np.max), y_axis='log')
plt.colorbar()
plt.title('Original spectrogram (1025 bins)')
plt.subplot(3,1,2)
librosa.display.specshow(librosa.amplitude_to_db(M, ref=np.max), y_axis='mel')
plt.title('Mel spectrogram (128 bins)')
plt.colorbar()
plt.subplot(3,1,3)
librosa.display.specshow(librosa.amplitude_to_db(S_recover, ref=np.max), y_axis='log')
plt.colorbar()
plt.title('Reconstructed spectrogram (1025 bins)')
plt.tight_layout()
plt.show()""" .

<DEPENDENCY.librosa==0.7.1> <CONTAINS> """CODE.E = np.eye(3)
librosa.util.shear(E, factor=-1, axis=-1)
array([[1., 1., 1.],
       [0., 0., 0.],
       [0., 0., 0.]])
librosa.util.shear(E, factor=-1, axis=0)
array([[1., 0., 0.],
       [1., 0., 0.],
       [1., 0., 0.]])
librosa.util.shear(E, factor=1, axis=-1)
array([[1., 0., 0.],
       [0., 0., 1.],
       [0., 1., 0.]])""",
        """CODE.y, sr = librosa.load(librosa.util.example_audio_file())
times, S = librosa.core.spectrum.__reassign_times(y, sr=sr)
times
array([[ 0.077,  0.079,  ..., 61.362, 61.388],
       [ 0.078,  0.077,  ..., 61.366, 61.538],
       [ 0.088,  0.08 ,  ..., 61.358, 61.399],
       ...,
       [ 0.078,  0.077,  ..., 61.378, 61.372],
       [ 0.082,  0.077,  ..., 61.371, 61.38 ],
       [ 0.075,  0.076,  ..., 61.374, 61.385]])
""",
        """CODE.y_filt = librosa.effects.preemphasis(y)
y_filt_1, zf = librosa.effects.preemphasis(y[:1000], return_zf=True)
y_filt_2, zf = librosa.effects.preemphasis(y[1000:], zi=zf, return_zf=True)""",
        """CODE.y_left = np.ones(5)
y_right = -np.ones(5)
y_stereo = librosa.util.stack([y_left, y_right], axis=0)
y_stereo
array([[ 1.,  1.,  1.,  1.,  1.],
       [-1., -1., -1., -1., -1.]])
y_stereo.flags

y_stereo = librosa.util.stack([y_left, y_right], axis=-1)
y_stereo
array([[ 1., -1.],
       [ 1., -1.],
       [ 1., -1.],
       [ 1., -1.],
       [ 1., -1.]])
y_stereo.flags
""" .

<DEPENDENCY.librosa==0.7.2> <CONTAINS> """CODE.# Generate a random sparse binary matrix
X = scipy.sparse.lil_matrix(np.random.randint(0, 2, size=(5,5)))
X_roll = roll_sparse(X, 2, axis=0)  # Roll by 2 on the first axis
X_dense_r = roll_sparse(X.toarray(), 2, axis=0)  # Equivalent dense roll
np.allclose(X_roll, X_dense_r.toarray())
""",
        "CODE.librosa.output.times_csv('beat_times.csv', beats)",
        """CODE.x = np.linspace(-1, 1, num=16)
x
array([-1.        , -0.86666667, -0.73333333, -0.6       , -0.46666667,
       -0.33333333, -0.2       , -0.06666667,  0.06666667,  0.2       ,
        0.33333333,  0.46666667,  0.6       ,  0.73333333,  0.86666667,
        1.        ])
y = librosa.mu_compress(x, quantize=False)
y
array([-1.        , -0.97430198, -0.94432361, -0.90834832, -0.86336132,
       -0.80328309, -0.71255496, -0.52124063,  0.52124063,  0.71255496,
        0.80328309,  0.86336132,  0.90834832,  0.94432361,  0.97430198,
        1.        ])
y = librosa.mu_compress(x, quantize=True)
y
array([-128, -124, -120, -116, -110, -102,  -91,  -66,   66,   91,  102,
       110,  116,  120,  124,  127])
y = librosa.mu_compress(x, mu=15, quantize=True)
y
array([-8, -7, -7, -6, -6, -5, -4, -2,  2,  4,  5,  6,  6,  7,  7,  7])""" .

<DEPENDENCY.librosa==0.8.0> <CONTAINS> """CODE.# `C:maj` will use all sharps
librosa.key_to_notes('C:maj')
['C', 'Câ¯', 'D', 'Dâ¯', 'E', 'F', 'Fâ¯', 'G', 'Gâ¯', 'A', 'Aâ¯', 'B']

# `A:min` has the same notes
librosa.key_to_notes('A:min')
['C', 'Câ¯', 'D', 'Dâ¯', 'E', 'F', 'Fâ¯', 'G', 'Gâ¯', 'A', 'Aâ¯', 'B']

# `Aâ¯:min` will use sharps, but spell note 0 (`C`) as `Bâ¯`
librosa.key_to_notes('A#:min')
['Bâ¯', 'Câ¯', 'D', 'Dâ¯', 'E', 'Eâ¯', 'Fâ¯', 'G', 'Gâ¯', 'A', 'Aâ¯', 'B']

# `Gâ¯:maj` will use a double-sharp to spell note 7 (`G`) as `Fðª`:
librosa.key_to_notes('G#:maj')
['Bâ¯', 'Câ¯', 'D', 'Dâ¯', 'E', 'Eâ¯', 'Fâ¯', 'Fðª', 'Gâ¯', 'A', 'Aâ¯', 'B']

# `Fâ­:min` will use double-flats
librosa.key_to_notes('Fb:min')
['Dð«', 'Dâ­', 'Eð«', 'Eâ­', 'Fâ­', 'F', 'Gâ­', 'Að«', 'Aâ­', 'Bð«', 'Bâ­', 'Câ­']
""",
        """CODE.import matplotlib.pyplot as plt
freqs = librosa.cqt_frequencies(108, librosa.note_to_hz('C1'))
weightings = 'ABCDZ'
weights = librosa.multi_frequency_weighting(freqs, weightings)
fig, ax = plt.subplots()
for label, w in zip(weightings, weights):
    ax.plot(freqs, w, label=label)
ax.set(xlabel='Frequency (Hz)', ylabel='Weighting (log10)',
       title='Weightings of CQT frequencies')
ax.legend()""",
        """CODE.import matplotlib.pyplot as plt
freqs = librosa.cqt_frequencies(108, librosa.note_to_hz('C1'))
weights = librosa.frequency_weighting(freqs, 'A')
fig, ax = plt.subplots()
ax.plot(freqs, weights)
ax.set(xlabel='Frequency (Hz)', ylabel='Weighting (log10)',
       title='A-Weighting of CQT frequencies')""",
        """CODE.import os
os.environ['LIBROSA_DATA_DIR'] = '/path/to/store/data'
import librosa
y, sr = librosa.load(librosa.example('brahms'))
y, sr = librosa.load(librosa.example('vibeace', hq=True))""",
        """CODE.librosa.hz_to_svara_c([261/2, 261, 261*2], Sa=261, mela='kanakangi')
['SÌ£', 'S', 'SÌ']

freqs = librosa.cqt_frequencies(12, fmin=261)
librosa.hz_to_svara_c(freqs, Sa=freqs[0], mela=36)
['S', 'Râ', 'Râ', 'Râ', 'Gâ', 'Mâ', 'Mâ', 'P', 'Dâ', 'Dâ', 'Dâ', 'Nâ']
""",
        """CODE.librosa.key_to_degrees('C:maj')
array([ 0,  2,  4,  5,  7,  9, 11])
librosa.key_to_degrees('C#:maj')
array([ 1,  3,  5,  6,  8, 10,  0])
librosa.key_to_degrees('A:min')
array([ 9, 11,  0,  2,  4,  5,  7])""",
        """CODE.librosa.mela_to_degrees(1)
librosa.mela_to_degrees('kanakangi')""",
        """CODE.librosa.mela_to_svara(1)
librosa.mela_to_svara(19)
librosa.mela_to_svara(31)
librosa.mela_to_svara(34)
librosa.mela_to_svara(36)
librosa.mela_to_svara('chalanatta')""",
        """CODE.librosa.midi_svara_h([60, 61, 62], Sa=60)
['S', 'r', 'R']
librosa.midi_to_svara_h([60, 61, 62], Sa=67)
['mÌ£', 'MÌ£', 'PÌ£']
librosa.midi_to_svara_h([60, 61, 62], Sa=67, unicode=False)
['m,', 'M,', 'P,']
librosa.midi_to_svara_h([72, 73, 74], Sa=60, abbr=False)
['SÌa', 'rÌe', 'RÌe']""",
        """CODE.librosa.thaat_to_degrees('bilaval')
librosa.thaat_to_degrees('todi')""",
        """CODE.librosa.util.dtype_r2c(np.float32)
librosa.util.dtype_r2c(np.int16)
librosa.util.dtype_r2c(np.complex128)""",
        """CODE.x = np.array([1, 0, 1, 2, -1, 0, -2, 1])
librosa.util.localmin(x)
array([False,  True, False, False,  True, False,  True, False])

x = np.array([[1,0,1], [2, -1, 0], [2, 1, 3]])
librosa.util.localmin(x, axis=0)
array([[False, False, False],
       [False,  True,  True],
       [False, False, False]])

librosa.util.localmin(x, axis=1)
array([[False,  True, False],
       [False,  True, False],
       [False,  True, False]])
""" .

<DEPENDENCY.librosa==0.9.0> <CONTAINS> """CODE.x = np.vander(np.arange(5))
librosa.util.count_unique(x, axis=0)
librosa.util.count_unique(x, axis=-1)""",
        """CODE.x = np.vander(np.arange(5))
x
array([[  0,   0,   0,   0,   1],
   [  1,   1,   1,   1,   1],
   [ 16,   8,   4,   2,   1],
   [ 81,  27,   9,   3,   1],
   [256,  64,  16,   4,   1]])
# Check uniqueness along rows
librosa.util.is_unique(x, axis=0)
array([ True,  True,  True,  True, False])
# Check uniqueness along columns
librosa.util.is_unique(x, axis=-1)
array([False, False,  True,  True,  True])""" .

<DEPENDENCY.nbformat==5.2.0> <CONTAINS> """CODE.s = Struct(a=10,b=30)
s2 = s.copy()
type(s2) is Struct""" .

<DEPENDENCY.optax==0.0.9> <CONTAINS> """CODE.import optax
import jax
import jax.numpy as jnp

def map_nested_fn(fn):
    '''Recursively apply `fn` to the key-value pairs of a nested dict'''
    def map_fn(nested_dict):
        return {k: (map_fn(v) if isinstance(v, dict) else fn(k, v))
                for k, v in nested_dict.items()}
    return map_fn

params = {'linear_1': {'w': jnp.zeros((5, 6)), 'b': jnp.zeros(5)},
          'linear_2': {'w': jnp.zeros((6, 1)), 'b': jnp.zeros(1)}}
gradients = jax.tree_map(jnp.ones_like, params)  # dummy gradients

label_fn = map_nested_fn(lambda k, _: k)
tx = optax.multi_transform({'w': optax.adam(1.0), 'b': optax.sgd(1.0)},
                           label_fn)
state = tx.init(params)
updates, new_state = tx.update(gradients, state, params)
new_params = optax.apply_updates(params, updates)""" .

<DEPENDENCY.optax==0.1.5> <CONTAINS> """CODE.opt_specs = optax.tree_map_params(
    opt,
    lambda _, spec: spec,
    state,
    specs,
    transform_non_params=lambda _: None,
)""" .

<DEPENDENCY.optax==0.1.8> <CONTAINS> """CODE.optax.tree_utils.tree_vdot(
  {a: jnp.array([1, 2]), b: jnp.array([1, 2])},
  {a: jnp.array([-1, -1]), b: jnp.array([1, 1])},
)""" .

<DEPENDENCY.packaging==22.0> <CONTAINS> "CODE.Specifier(\"==1.2.3\").operator",
        "CODE.Specifier(\"==1.2.3\").version",
        """CODE.Version("1.2.3").base_version
Version("1.2.3+abc").base_version
Version("1!1.2.3+abc.dev1").base_version""",
        """CODE.Version("1.2.3").is_devrelease
Version("1.2.3.dev1").is_devrelease""",
        """CODE.Version("1.2.3").is_postrelease
Version("1.2.3.post1").is_postrelease""",
        """CODE.Version("1.2.3").is_prerelease
Version("1.2.3a1").is_prerelease
Version("1.2.3b1").is_prerelease
Version("1.2.3rc1").is_prerelease
Version("1.2.3dev1").is_prerelease""",
        """CODE.Version("1.2.3").local
None
Version("1.2.3+abc").local
'abc'""",
        "CODE.Version(\"1.2.3\").major",
        """CODE.Version("1.2.3").micro
3
Version("1").micro""",
        """CODE.Version("1.2.3").minor
Version("1").minor""",
        """CODE.Version("1.2.3").post
Version("1.2.3.post1").post""",
        """CODE.Version("1.2.3").pre
Version("1.2.3a1").pre
Version("1.2.3b1").pre
Version("1.2.3rc1").pre""",
        """CODE.Version("1.2.3").public
Version("1.2.3+abc").public
Version("1.2.3+abc.dev1").public""",
        """CODE.Version("1.2.3").release
Version("2.0.0").release
Version("1!2.0.0.post0").release""",
        "CODE.Version(\"1.2.3.dev1\").dev",
        """CODE.Version("2.0.0").epoch
Version("1!2.0.0").epoch""",
        """CODE.list(Specifier(">=1.2.3").filter(["1.2", "1.3", "1.5a1"]))
['1.3']
list(Specifier(">=1.2.3").filter(["1.2", "1.2.3", "1.3", Version("1.4")]))
['1.2.3', '1.3', <Version('1.4')>]
list(Specifier(">=1.2.3").filter(["1.2", "1.5a1"]))
['1.5a1']
list(Specifier(">=1.2.3").filter(["1.3", "1.5a1"], prereleases=True))
['1.3', '1.5a1']
list(Specifier(">=1.2.3", prereleases=True).filter(["1.3", "1.5a1"]))
['1.3', '1.5a1']
""",
        """CODE.v1 = Version("1.0a5")
v2 = Version("1.0")
v1
<Version('1.0a5')>
v2
<Version('1.0')>
v1 < v2
True
v1 == v2
False
v1 > v2
False
v1 >= v2
False
v1 <= v2
True""" .

<DEPENDENCY.paddlepaddle-gpu==1.6.3> <CONTAINS> """CODE.fc = FC("fc", 2, num_flatten_dims=2)
out = fc(input=[data_1, data_2])

from paddle.fluid.dygraph.base import to_variable
import paddle.fluid as fluid
from paddle.fluid.dygraph import FC
import numpy as np

data = np.random.uniform(-1, 1, [30, 10, 32]).astype('float32')
with fluid.dygraph.guard():
    fc = FC("fc", 64, num_flatten_dims=2)
    data = to_variable(data)
    conv = fc(data)""",
        """CODE.import paddle.fluid as fluid

with fluid.dygraph.guard():
    adam = fluid.optimizer.Adam(0.001)

    state_dict = adam.state_dict()
    fluid.save_optimizer(state_dict, "opt_adam")

    fluid.load_optimizer("opt_adam")""" .

<DEPENDENCY.paddlepaddle-gpu==1.7.0> <CONTAINS> """CODE.@skip_check_grad_ci(reason="For inference, check_grad is not required.")
class TestInference(OpTest):""",
        """CODE.from paddle.fluid.incubate.fleet.utils.fleet_util import FleetUtil
fleet_util = FleetUtil()
program_path = "./program.pbtxt"
is_text = True
output_dir = "/tmp/"
fleet_util.parse_program_proto(program_path, is_text, output_dir)""",
        """CODE.import numpy as np
import paddle.fluid as fluid

dict_size = 20
data_t = fluid.layers.data(name='word', shape=[1], dtype='int64', lod_level=1)
padding_idx = np.random.randint(1, 10)
out = fluid.contrib.fused_embedding_seq_pool(
    input=data_t,
    size=[dict_size, 32],
    param_attr='w',
    padding_idx=padding_idx,
    is_sparse=False)""",
        """CODE.import paddle.fluid as fluid

prog = fluid.default_main_program()
rlt = fluid.layers.data("fake_data", shape=[1,1], dtype='float32')
debug_str = prog.to_string(throw_on_error=True, with_details=False)
print(debug_str)""",
        """CODE.import paddle.fluid as fluid

with fluid.dygraph.guard():
    fc1 = fluid.Linear(10, 3)
    fc2 = fluid.Linear(3, 10, bias_attr=False)
    model = fluid.dygraph.Sequential(fc1, fc2)
    for name, param in model.named_parameters():
        print(name, param)""",
        """CODE.import paddle.fluid as fluid

with fluid.dygraph.guard():
    fc1 = fluid.Linear(10, 3)
    fc2 = fluid.Linear(3, 10, bias_attr=False)
    model = fluid.dygraph.Sequential(fc1, fc2)
    for prefix, layer in model.named_sublayers():
        print(prefix, layer)""",
        """CODE.import paddle.fluid as fluid
a = fluid.data(name='a', shape=[-1, 1], dtype='float32')
b = fluid.data(name='b', shape=[-1, 1], dtype='float32')
c = a * b
out = fluid.layers.cond(a < b, lambda: a + c, lambda: b * b)
""",
        """CODE.import paddle.fluid as fluid
boxes = fluid.data(name='bboxes', shape=[None, 81, 8], dtype='float32')
scores = fluid.data(name='scores', shape=[None, 1, 81], dtype='float32')
out = fluid.layers.locality_aware_nms(bboxes=boxes, scores=scores, score_threshold=0.5, nms_top_k=400, nms_threshold=0.3, keep_top_k=200, normalized=False)""",
        """CODE.import paddle.fluid as fluid
data = fluid.data(name="img", shape=[64, 784])
w = fluid.layers.create_parameter(shape=[784, 200], dtype='float32', name='fc_w')
b = fluid.layers.create_parameter(shape=[200], dtype='float32', name='fc_b')
list_para  = fluid.io.get_program_parameter(  fluid.default_main_program() )""",
        """CODE.import paddle.fluid as fluid
data = fluid.data(name="img", shape=[64, 784])
w = fluid.layers.create_parameter(shape=[784, 200], dtype='float32', name='fc_w')
b = fluid.layers.create_parameter(shape=[200], dtype='float32', name='fc_b')
list_para  = fluid.io.get_program_persistable_vars(  fluid.default_main_program() )""",
        """CODE.import paddle.fluid as fluid
from paddle.fluid.dygraph import Linear, to_variable, TracedLayer
import numpy as np

class ExampleLayer(fluid.dygraph.Layer):
    def __init__(self):
        super(ExampleLayer, self).__init__()
        self._fc = Linear(3, 10)

    def forward(self, input):
        return self._fc(input)

save_dirname = './saved_infer_model'
in_np = np.random.random([2, 3]).astype('float32')

with fluid.dygraph.guard():
    layer = ExampleLayer()
    in_var = to_variable(in_np)
    out_dygraph, static_layer = TracedLayer.trace(layer, inputs=[in_var])
    static_layer.save_inference_model(save_dirname, feed=[0], fetch=[0])

place = fluid.CPUPlace()
exe = fluid.Executor(place)
program, feed_vars, fetch_vars = fluid.io.load_inference_model(save_dirname,
                                        exe)

fetch, = exe.run(program, feed={feed_vars[0]: in_np}, fetch_list=fetch_vars)
print(fetch.shape) # (2, 10)""",
        """CODE.import paddle.fluid as fluid
from paddle.fluid.dygraph import Linear, to_variable, TracedLayer
import numpy as np

class ExampleLayer(fluid.dygraph.Layer):
    def __init__(self):
        super(ExampleLayer, self).__init__()
        self._fc = Linear(3, 10)

    def forward(self, input):
        return self._fc(input)

with fluid.dygraph.guard():
    layer = ExampleLayer()
    in_np = np.random.random([2, 3]).astype('float32')
    in_var = to_variable(in_np)

    out_dygraph, static_layer = TracedLayer.trace(layer, inputs=[in_var])

    build_strategy = fluid.BuildStrategy()
    build_strategy.enable_inplace = True

    exec_strategy = fluid.ExecutionStrategy()
    exec_strategy.num_threads = 2

    static_layer.set_strategy(build_strategy=build_strategy, exec_strategy=exec_strategy)
    out_static_graph = static_layer([in_var])""",
        """CODE.import paddle.fluid as fluid
from paddle.fluid.dygraph import Linear, to_variable, TracedLayer
import numpy as np

class ExampleLayer(fluid.dygraph.Layer):
    def __init__(self):
        super(ExampleLayer, self).__init__()
        self._fc = Linear(3, 10)

    def forward(self, input):
        return self._fc(input)

with fluid.dygraph.guard():
    layer = ExampleLayer()
    in_np = np.random.random([2, 3]).astype('float32')
    in_var = to_variable(in_np)
    out_dygraph, static_layer = TracedLayer.trace(layer, inputs=[in_var])

    # run the static graph model using Executor inside
    out_static_graph = static_layer([in_var])

    print(len(out_static_graph)) # 1
    print(out_static_graph[0].shape) # (2, 10)

    # save the static graph model for inference
    static_layer.save_inference_model(dirname='./saved_infer_model')""",
        """CODE.import paddle.fluid as fluid
import numpy as np

# example1: LearningRateDecay is not used, return value is all the same
with fluid.dygraph.guard():
    emb = fluid.dygraph.Embedding([10, 10])
    adam = fluid.optimizer.Adam(0.001, parameter_list = emb.parameters())
    lr = adam.current_step_lr()
    print(lr) # 0.001

# example2: PiecewiseDecay is used, return the step learning rate
with fluid.dygraph.guard():
    inp = np.random.uniform(-0.1, 0.1, [10, 10]).astype("float32")
    linear = fluid.dygraph.nn.Linear(10, 10)
    inp = fluid.dygraph.to_variable(inp)
    out = linear(inp)
    loss = fluid.layers.reduce_mean(out)

    bd = [2, 4, 6, 8]
    value = [0.2, 0.4, 0.6, 0.8, 1.0]
    adam = fluid.optimizer.Adam(fluid.dygraph.PiecewiseDecay(bd, value, 0),
                           parameter_list=linear.parameters())

    # first step: learning rate is 0.2
    np.allclose(adam.current_step_lr(), 0.2, rtol=1e-06, atol=0.0) # True

    # learning rate for different steps
    ret = [0.2, 0.2, 0.4, 0.4, 0.6, 0.6, 0.8, 0.8, 1.0, 1.0, 1.0, 1.0]
    for i in range(12):
        adam.minimize(loss)
        lr = adam.current_step_lr()
        np.allclose(lr, ret[i], rtol=1e-06, atol=0.0) # True""",
        """CODE.import paddle.fluid as fluid
import numpy as np

class MyLayer(fluid.Layer):
    def __init__(self):
        super(MyLayer, self).__init__()
        self.linears = fluid.dygraph.LayerList(
            [fluid.dygraph.Linear(10, 10) for i in range(10)])

    def forward(self, x):
        # LayerList can act as an iterable, or be indexed using ints
        for i, l in enumerate(self.linears):
            x = self.linears[i // 2](x) + l(x)
        return x""",
        """CODE.import paddle.fluid as fluid
import numpy as np

class MyLayer(fluid.Layer):
    def __init__(self, num_stacked_param):
        super(MyLayer, self).__init__()
        # create ParameterList with iterable Parameters
        self.params = fluid.dygraph.ParameterList(
            [fluid.layers.create_parameter(
                shape=[2, 2], dtype='float32')] * num_stacked_param)

    def forward(self, x):
        for i, p in enumerate(self.params):
            tmp = self._helper.create_variable_for_type_inference('float32')
            self._helper.append_op(
                type="mul",
                inputs={"X": x,
                        "Y": p},
                outputs={"Out": tmp},
                attrs={"x_num_col_dims": 1,
                       "y_num_col_dims": 1})
            x = tmp
        return x

data_np = np.random.uniform(-1, 1, [5, 2]).astype('float32')
with fluid.dygraph.guard():
    x = fluid.dygraph.to_variable(data_np)
    num_stacked_param = 4
    model = MyLayer(num_stacked_param)
    print(len(model.params))  # 4
    res = model(x)
    print(res.shape)  # [5, 2]

    replaced_param = fluid.layers.create_parameter(shape=[2, 3], dtype='float32')
    model.params[num_stacked_param - 1] = replaced_param  # replace last param
    res = model(x)
    print(res.shape)  # [5, 3]
    model.params.append(fluid.layers.create_parameter(shape=[3, 4], dtype='float32'))  # append param
    print(len(model.params))  # 5
    res = model(x)
    print(res.shape)  # [5, 4]""",
        """CODE.import paddle.fluid as fluid
import numpy as np

data = np.random.uniform(-1, 1, [30, 10]).astype('float32')
with fluid.dygraph.guard():
    data = fluid.dygraph.to_variable(data)
    # create Sequential with iterable Layers
    model1 = fluid.dygraph.Sequential(
        fluid.Linear(10, 1), fluid.Linear(1, 2)
    )
    model1[0]  # access the first layer
    res1 = model1(data)  # sequential execution

    # create Sequential with name Layer pairs
    model2 = fluid.dygraph.Sequential(
        ('l1', fluid.Linear(10, 2)),
        ('l2', fluid.Linear(2, 3))
    )
    model2['l1']  # access l1 layer
    model2.add_sublayer('l3', fluid.Linear(3, 3))  # add sublayer
    res2 = model2(data)  # sequential execution""",
        """CODE.import paddle.fluid as fluid
import numpy as np

with fluid.dygraph.guard():
    value = np.arange(26).reshape(2, 13).astype("float32")
    a = fluid.dygraph.to_variable(value)
    linear = fluid.Linear(13, 5, dtype="float32")
    # This can be any optimizer supported by dygraph.
    adam = fluid.optimizer.Adam(learning_rate = 0.01,
                                parameter_list = linear.parameters())
    out = linear(a)
    out.backward()
    adam.minimize(out)
    adam.clear_gradients()""",
        """CODE.import paddle.fluid as fluid
import paddle.fluid.layers as layers

def fn_1():
    return layers.fill_constant(shape=[1, 2], dtype='float32', value=1)

def fn_2():
    return layers.fill_constant(shape=[2, 2], dtype='int32', value=2)

def fn_3():
    return layers.fill_constant(shape=[3], dtype='int32', value=3)

main_program = fluid.default_startup_program()
startup_program = fluid.default_main_program()
with fluid.program_guard(main_program, startup_program):
    index_1 = layers.fill_constant(shape=[1], dtype='int32', value=1)
    index_2 = layers.fill_constant(shape=[1], dtype='int32', value=2)

    out_1 = layers.switch_case(
        branch_index=index_1,
        branch_fns={1: fn_1, 2: fn_2},
        default=fn_3)

    out_2 = layers.switch_case(
        branch_index=index_2,
        branch_fns=[(1, fn_1), (2, fn_2)],
        default=fn_3)

    # Argument default is None and no index matches. fn_3 will be called because of the max index 7.
    out_3 = layers.switch_case(
        branch_index=index_2,
        branch_fns=[(0, fn_1), (4, fn_2), (7, fn_3)])

exe = fluid.Executor(fluid.CPUPlace())
res_1, res_2, res_3 = exe.run(main_program,
                              fetch_list=[out_1, out_2, out_3])
print(res_1)  # [[1. 1.]]
print(res_2)  # [[2 2] [2 2]]
print(res_3)  # [3 3 3]
""",
        """CODE.import paddle.fluid as fluid
import paddle.fluid.layers as layers

def fn_1():
    return layers.fill_constant(shape=[1, 2], dtype='float32', value=1)

def fn_2():
    return layers.fill_constant(shape=[2, 2], dtype='int32', value=2)

def fn_3():
    return layers.fill_constant(shape=[3], dtype='int32', value=3)

main_program = fluid.default_startup_program()
startup_program = fluid.default_main_program()
with fluid.program_guard(main_program, startup_program):
    x = layers.fill_constant(shape=[1], dtype='float32', value=0.3)
    y = layers.fill_constant(shape=[1], dtype='float32', value=0.1)
    z = layers.fill_constant(shape=[1], dtype='float32', value=0.2)

    pred_1 = layers.less_than(z, x)  # true: 0.2 < 0.3
    pred_2 = layers.less_than(x, y)  # false: 0.3 < 0.1
    pred_3 = layers.equal(x, y)      # false: 0.3 == 0.1

    # Call fn_1 because pred_1 is True
    out_1 = layers.case(
        pred_fn_pairs=[(pred_1, fn_1), (pred_2, fn_2)], default=fn_3)

    # Argument default is None and no pred in pred_fn_pairs is True. fn_3 will be called.
    # because fn_3 is the last callable in pred_fn_pairs.
    out_2 = layers.case(pred_fn_pairs=[(pred_2, fn_2), (pred_3, fn_3)])

exe = fluid.Executor(fluid.CPUPlace())
res_1, res_2 = exe.run(main_program, fetch_list=[out_1, out_2])
print(res_1)  # [[1. 1.]]
print(res_2)  # [3 3 3]""",
        """CODE.import paddle.fluid as fluid
import paddle.fluid.layers as layers
trg_emb = fluid.data(name="trg_emb",
                     shape=[None, None, 128],
                     dtype="float32")

trg_embeder = lambda x: fluid.embedding(
    x, size=[10000, 128], param_attr=fluid.ParamAttr(name="trg_embedding"))
output_layer = lambda x: layers.fc(x,
                                size=10000,
                                num_flatten_dims=len(x.shape) - 1,
                                param_attr=fluid.ParamAttr(name=
                                                        "output_w"),
                                bias_attr=False)
helper = layers.GreedyEmbeddingHelper(trg_embeder, start_tokens=0, end_token=1)
decoder_cell = layers.GRUCell(hidden_size=128)
decoder = layers.BasicDecoder(decoder_cell, helper, output_fn=output_layer)
outputs = layers.dynamic_decode(
    decoder=decoder, inits=decoder_cell.get_initial_states(encoder_output))""",
        """CODE.import paddle.fluid as fluid
import paddle.fluid.layers as layers
trg_emb = fluid.data(name="trg_emb",
                     shape=[None, None, 128],
                     dtype="float32")

trg_embeder = lambda x: fluid.embedding(
    x, size=[10000, 128], param_attr=fluid.ParamAttr(name="trg_embedding"))
output_layer = lambda x: layers.fc(x,
                                size=10000,
                                num_flatten_dims=len(x.shape) - 1,
                                param_attr=fluid.ParamAttr(name=
                                                        "output_w"),
                                bias_attr=False)
helper = layers.SampleEmbeddingHelper(trg_embeder, start_tokens=0, end_token=1)
decoder_cell = layers.GRUCell(hidden_size=128)
decoder = layers.BasicDecoder(decoder_cell, helper, output_fn=output_layer)
outputs = layers.dynamic_decode(
    decoder=decoder, inits=decoder_cell.get_initial_states(encoder_output))""",
        """CODE.import paddle.fluid as fluid
import paddle.fluid.layers as layers
trg_emb = fluid.data(name="trg_emb",
                     shape=[None, None, 128],
                     dtype="float32")
trg_seq_length = fluid.data(name="trg_seq_length",
                            shape=[None],
                            dtype="int64")
helper = layers.TrainingHelper(trg_emb, trg_seq_length)
decoder_cell = layers.GRUCell(hidden_size=128)
decoder = layers.BasicDecoder(decoder_cell, helper)
outputs = layers.dynamic_decode(
    decoder,
    inits=decoder_cell.get_initial_states(trg_emb),
    is_test=False)""",
        """CODE.import paddle.fluid as fluid
with fluid.dygraph.guard():
    strategy=dygraph.parallel.prepare_context()
    emb = fluid.dygraph.Embedding([10, 10])
    emb = dygraph.parallel.DataParallel(emb, strategy)

    state_dict = emb.state_dict()
    fluid.save_dygraph( state_dict, "paddle_dy")

    para_state_dict, _ = fluid.load_dygraph( "paddle_dy")

    emb.set_dict( para_state_dict )""",
        """CODE.import paddle.fluid as fluid
with fluid.dygraph.guard():
    strategy=dygraph.parallel.prepare_context()
    emb = fluid.dygraph.Embedding([10, 10])
    emb = dygraph.parallel.DataParallel(emb, strategy)

    state_dict = emb.state_dict()
    fluid.save_dygraph( state_dict, "paddle_dy")

    para_state_dict, _ = fluid.load_dygraph( "paddle_dy")

    emb.load_dict( para_state_dict )""",
        """CODE.import paddle.fluid as fluid
x = fluid.data(name="x", shape=[-1, 23, 48], dtype='float32')
print(x.grad_name) # output is "x@GRAD\"""",
        """CODE.import paddle.fluid as fluid
x = fluid.layers.data(name="x", shape=[-1, 4])
out = fluid.contrib.layers.shuffle_batch(x)""",
        "CODE.linear.clear_gradients()" .

<DEPENDENCY.paddlepaddle-gpu==1.7.1> <CONTAINS> """CODE.import paddle.fluid as fluid

program = fluid.default_main_program()
data = fluid.data(name='x', shape=[None, 13], dtype='float32')
hidden = fluid.layers.fc(input=data, size=10)
loss = fluid.layers.mean(hidden)
fluid.optimizer.SGD(learning_rate=0.01).minimize(loss)

for param in program.all_parameters():
    print(param)""" .

<DEPENDENCY.paddlepaddle-gpu==1.7.2> <CONTAINS> """CODE._save_pserver_vars_by_notify(executor=exe,
                dirname=param_path, lookup_table=table_name,
                ps_endpoint_list=ps_endpoints)""",
        """CODE.config = fluid.CheckpointConfig("./checkpoints")
trainer = fluid.Trainer(train_func=train_program,
                        place=place,
                        optimizer_func=optimizer_func,
                        checkpoint_config=config)
trainer.train(...)""",
        """CODE.def inference_program():
    x = fluid.layers.data(name='x', shape=[13], dtype='float32')
    y_predict = fluid.layers.fc(input=x, size=1, act=None)
    return y_predict

place = fluid.CPUPlace()
inferencer = fluid.Inferencer(
    infer_func=inference_program, param_path="/tmp/model", place=place)""",
        """CODE.def mlp(image, layer_sizes=[200, 100], activation="relu", num_classes=10):
    hidden = image
    for layer_size in layer_sizes:
        hidden = fluid.layers.fc(input=hidden, size=layer_size, act=activation)
    return fluid.layers.fc(input=hidden, size=num_classes, act="softmax")

def train_mnist_mlp():
    img = fluid.layers.data(name='image', shape=[784])
    label = fluid.layers.data(name='label', shape=[1], dtype='int64')
    prediction = mlp(img)
    return fluid.layers.mean(fluid.layers.cross_entropy(prediction, label))

def optimizer():
    return fluid.optimizer.Adam()

trainer = Trainer(train_func=train_mnist_mlp,
                  optimizer_func=optimizer,
                  place=fluid.CUDAPlace(0),
                  parallel=True)

def train_callback(event):
    if isinstance(event, fluid.EndStepEvent):
        print "Epoch ID", event.epoch, "Step ID", event.step, "AvgLoss", event.metrics[0]
    elif isinstance(event, fluid.EndEpochEvent):
        trainer.save_params("./model_{0}".format(event.epoch))

trainer.train(num_epochs=100, event_handler=train_callback)""",
        """CODE.import numpy as np
import paddle
import paddle.fluid as fluid

from paddle.fluid.dygraph.base import to_variable
from paddle.fluid.dygraph.nn import Linear

from paddle.fluid.clip import GradClipByValue, GradClipByNorm, GradClipByGlobalNorm

from paddle.fluid.optimizer import SGDOptimizer

with fluid.dygraph.guard():
    norm_clip = GradClipByNorm( 5.0 )
    sgd = SGDOptimizer(learning_rate=1.0)

    init_value = np.random.uniform( -1, 1, (10, 10)).astype('float32')

    linear = Linear( 10, 10)

    out = linear( to_variable(init_value) )

    loss = fluid.layers.reduce_mean( out )

    loss.backward()
    sgd.minimize(loss, grad_clip = norm_clip)""",
        """CODE.import numpy as np
import paddle
import paddle.fluid as fluid

from paddle.fluid.dygraph.base import to_variable
from paddle.fluid.dygraph.nn import Linear

from paddle.fluid.clip import GradClipByValue, GradClipByNorm, GradClipByGlobalNorm

from paddle.fluid.optimizer import SGDOptimizer

with fluid.dygraph.guard():
    value_clip = GradClipByValue( -1.0, 1.0 )
    sgd = SGDOptimizer(learning_rate=1.0)

    init_value = np.random.uniform( -1, 1, (10, 10)).astype('float32')

    linear = Linear( 10, 10)

    out = linear( to_variable(init_value) )

    loss = fluid.layers.reduce_mean( out )

    loss.backward()
    sgd.minimize(loss, grad_clip = value_clip)""",
        """CODE.import numpy as np
import paddle
import paddle.fluid as fluid

from paddle.fluid.dygraph.base import to_variable
from paddle.fluid.dygraph.nn import Linear

from paddle.fluid.dygraph_grad_clip import GradClipByValue, GradClipByNorm, GradClipByGlobalNorm

from paddle.fluid.optimizer import SGDOptimizer

with fluid.dygraph.guard():
    gloabl_norm_clip = GradClipByGlobalNorm( 5.0 )
    sgd = SGDOptimizer(learning_rate=1.0)

    init_value = np.random.uniform( -1, 1, (10, 10)).astype('float32')

    linear = Linear( 10, 10)

    out = linear( to_variable(init_value) )

    loss = fluid.layers.reduce_mean( out )

    loss.backward()
    sgd.minimize(loss, grad_clip = gloabl_norm_clip)""",
        """CODE.import paddle.fluid as fluid
if fluid.initializer.force_init_on_cpu():
    step = fluid.layers.create_global_var(
        shape=[2,3], value=1.0, dtype='float32')""",
        """CODE.import paddle.fluid as fluid
import numpy as np

x = fluid.data(name='x', shape=[-1, 5], dtype='float64')
index = fluid.data(name='index', shape=[-1, 3], dtype='int32')
output = fluid.contrib.layers.index_sample(x=x, index=index)""",
        """CODE.import paddle.fluid as fluid
import numpy as np
# create x value
x_shape = (2, 5)
x_type = "float64"
x_np = np.random.random(x_shape).astype(x_type)
# create index value
index_shape = (2, 3)
index_type = "int32"
index_np = np.random.randint(low=0,
                             high=x_shape[1],
                             size=index_shape).astype(index_type)
x = fluid.data(name='x', shape=[-1, 5], dtype='float64')
index = fluid.data(name='index', shape=[-1, 3], dtype='int32')
output = fluid.contrib.layers.index_sample(x=x, index=index)""",
        """CODE.import paddle.fluid as fluid
import numpy as np
x = fluid.data(name="x", shape=[None, 1], dtype="int32", lod_level=1)
travel_list = [[1, 3], [1, 4], [2, 5], [2, 6]] # leaf node's travel path, shape(leaf_node_num, layer_num)
layer_list_flat = [[1], [2], [3], [4], [5], [6]] # shape(node_nums, 1)

neg_samples_num_list = [0, 0] # negative sample nums = 0
layer_node_num_list = [2, 4] #two layer (exclude root node)
leaf_node_num = 4

travel_array = np.array(travel_list)
layer_array = np.array(layer_list_flat)

sample, label, mask = fluid.contrib.layers.tdm_sampler(
    x,
    neg_samples_num_list,
    layer_node_num_list,
    leaf_node_num,
    tree_travel_attr=fluid.ParamAttr(
        initializer=fluid.initializer.NumpyArrayInitializer(
            travel_array)),
    tree_layer_attr=fluid.ParamAttr(
        initializer=fluid.initializer.NumpyArrayInitializer(
            layer_array)),
    output_positive=True,
    output_list=True,
    seed=0,
    tree_dtype='int32')

place = fluid.CPUPlace()
exe = fluid.Executor(place)
exe.run(fluid.default_startup_program())
xx = np.array([[0],[1]]).reshape((2,1)).astype("int32")

exe.run(feed={"x":xx})""",
        """CODE.import paddle.fluid as fluid
import numpy as np
x = fluid.data(name="x", shape=[None, 1], dtype="int32", lod_level=1)
tree_info = [[0,0,0,1,2],
             [0,1,0,3,4],[0,1,0,5,6],
             [0,2,1,0,0],[1,2,1,0,0],[2,2,2,0,0],[3,2,2,0,0]]
tree_info_np = np.array(tree_info)
tree_info_np = np.reshape(tree_info_np, (7,5))
node_nums = 7
child_nums = 2
child, leaf_mask  = fluid.contrib.layers.tdm_child(x, node_nums, child_nums,
                        param_attr=fluid.ParamAttr(
                            initializer=fluid.initializer.NumpyArrayInitializer(
                                                                    tree_info_np)))
place = fluid.CPUPlace()
exe = fluid.Executor(place)
exe.run(fluid.default_startup_program())
xx = np.array([[2],[3]]).reshape((2,1)).astype("int32")
child_res, leaf_mask_res = exe.run(feed={"x":xx}, fetch_list=[child, leaf_mask])""",
        """CODE.import paddle.fluid as fluid
with fluid.initializer.init_on_cpu():
    step = fluid.layers.create_global_var(
        shape=[2,3], value=1.0, dtype='float32')""",
        """CODE.param_path = "./checkpoint/"
serial = 7
trainer_id = 2
trainer_args = ["epoch_id", "step_id"]

_load_trainer_args(checkpoint_dir=param_path, serial=serial,
trainer_id=trainer_id, trainer_args=trainer_args)""",
        """CODE.tensor_x = numpy.random.uniform(0, 10, [batch_size, 13]).astype("float32")
results = inferencer.infer({'x': tensor_x})""" .

<DEPENDENCY.paddlepaddle-gpu==1.8.0> <CONTAINS> "CODE.fleet.clear_one_table(0)",
        "CODE.fleet.save_model(\"afs:/user/path/\")",
        """CODE.hidden2 = fluid.layers.inplace_abn(input=hidden1)
hidden3 = fluid.layers.inplace_abn(input=hidden2, act='leaky_relu', act_alpha=0.2)""",
        """CODE.import numpy as np
from paddle import fluid
import paddle.fluid.dygraph as dg

mse_loss = fluid.dygraph.MSELoss()
input = fluid.data(name="input", shape=[1])
label = fluid.data(name="label", shape=[1])
place = fluid.CPUPlace()
input_data = np.array([1.5]).astype("float32")
label_data = np.array([1.7]).astype("float32")

# declarative mode
output = mse_loss(input,label)
exe = fluid.Executor(place)
exe.run(fluid.default_startup_program())
output_data = exe.run(
    fluid.default_main_program(),
    feed={"input":input_data, "label":label_data},
    fetch_list=[output],
    return_numpy=True)
print(output_data)

# imperative mode
with dg.guard(place) as g:
    input = dg.to_variable(input_data)
    label = dg.to_variable(label_data)
    output = mse_loss(input, label)
    print(output.numpy())
    # [0.04000002]
""",
        """CODE.import numpy as np
import paddle
import paddle.fluid as fluid

input = fluid.data(name='input', shape=[2, 2], dtype='float32')
x = fluid.data(name='x', shape=[2, 2], dtype='float32')
y = fluid.data(name='y', shape=[2, 2], dtype='float32')
out = fluid.layers.addmm( input=input, x=x, y=y, alpha=5.0, beta=0.5 )

data_x = np.ones((2, 2)).astype(np.float32)
data_y = np.ones((2, 2)).astype(np.float32)
data_input = np.ones((2, 2)).astype(np.float32)

place =  fluid.CUDAPlace(0) if fluid.core.is_compiled_with_cuda() else fluid.CPUPlace()
exe = fluid.Executor(place)
results = exe.run(fluid.default_main_program(),
                  fetch_list=[out], feed={"input": data_input, 'x': data_x, "y": data_y})
print( np.array(results[0]) )""",
        """CODE.import numpy as np
import paddle.fluid as fluid

BATCH_SIZE = 32
BATCH_NUM = 20
SAVE_DIRNAME = "fc.inference.model"

def random_batch_reader():
    def _get_random_images_and_labels(image_shape, label_shape):
        image = np.random.random(size=image_shape).astype('float32')
        label = np.random.random(size=label_shape).astype('int64')
        return image, label

    def __reader__():
        for _ in range(BATCH_NUM):
            batch_image, batch_label = _get_random_images_and_labels(
                [BATCH_SIZE, 784], [BATCH_SIZE, 1])
            yield batch_image, batch_label

    return __reader__

def train_and_save_static_model(place):
    img = fluid.data(name='img', shape=[None, 784], dtype='float32')
    label = fluid.data(name='label', shape=[None, 1], dtype='int64')

    pred = fluid.layers.fc(input=img, size=10, act='softmax')

    loss = fluid.layers.cross_entropy(input=pred, label=label)
    avg_loss = fluid.layers.mean(loss)

    optimizer = fluid.optimizer.SGD(learning_rate=0.001)
    optimizer.minimize(avg_loss)

    exe = fluid.Executor(place)
    exe.run(fluid.default_startup_program())

    loader = fluid.io.DataLoader.from_generator(
        feed_list=[img, label], capacity=5, iterable=True)
    loader.set_batch_generator(random_batch_reader(), places=place)

    for data in loader():
        exe.run(
            fluid.default_main_program(),
            feed=data,
            fetch_list=[avg_loss])

    # save model by fluid.io.save_inference_model
    fluid.io.save_inference_model(
        SAVE_DIRNAME, ["img"], [pred], exe)


# Step 1. train and save inference model in static graph mode
place = fluid.CPUPlace()
train_and_save_static_model(place)

# Step 2. load inference model in dygraph and fine-tune
with fluid.dygraph.guard(place):
    fc = fluid.dygraph.static_runner.StaticModelRunner(SAVE_DIRNAME)

    sgd = fluid.optimizer.SGD(learning_rate=0.001,
                            parameter_list=fc.parameters())

    train_loader = fluid.io.DataLoader.from_generator(capacity=5)
    train_loader.set_batch_generator(
        random_batch_reader(), places=place)

    for data in train_loader():
        img = data[0]
        label = data[1]
        label.stop_gradient = True

        cost = fc(inputs=img)

        loss = fluid.layers.cross_entropy(cost, label)
        avg_loss = fluid.layers.mean(loss)

        avg_loss.backward()
        sgd.minimize(avg_loss)""",
        """CODE.import numpy as np
import paddle.fluid as fluid

data = np.array([[1.0, 2.0, 3.0],
                 [4.0, 5.0, 6.0],
                 [7.0, 8.0, 9.0]])
with fluid.dygraph.guard():
    x = fluid.dygraph.to_variable(data)
    out_z1 = fluid.layers.roll(x, shifts=1)
    print(out_z1.numpy())
    #[[9. 1. 2.]
    # [3. 4. 5.]
    # [6. 7. 8.]]
    out_z2 = fluid.layers.roll(x, shifts=1, dims=0)
    print(out_z2.numpy())
    #[[7. 8. 9.]
    # [1. 2. 3.]
    # [4. 5. 6.]]""",
        """CODE.import numpy as np
import paddle.fluid as fluid

data = np.array([[2, 3], [4, 5]]).astype('float32')
with fluid.dygraph.guard():
    l0 = fluid.Linear(2, 2)
    l1 = fluid.Linear(2, 2)
    with fluid.dygraph.no_grad():
        tmp = l1.weight * 2
    x = fluid.dygraph.to_variable(data)
    y = l0(x) + tmp
    o = l1(y)
    o.backward()
    print(tmp.gradient() is None)
    print(l0.weight.gradient() is None)

@fluid.dygraph.no_grad
def test_layer():
    with fluid.dygraph.guard():
        inp = np.ones([3, 1024], dtype='float32')
        t = fluid.dygraph.base.to_variable(inp)
        linear1 = fluid.Linear(1024, 4, bias_attr=False)
        linear2 = fluid.Linear(4, 4)
        ret = linear1(t)
        dy_ret = linear2(ret)

test_layer()""",
        """CODE.import numpy as np
import paddle.fluid as fluid
import paddle.fluid.dygraph as dygraph
from paddle.fluid.optimizer import AdamOptimizer
from paddle.fluid.dygraph.nn import Linear
from paddle.fluid.dygraph.base import to_variable

place = fluid.CUDAPlace(fluid.dygraph.ParallelEnv().dev_id)
with fluid.dygraph.guard(place=place):

    # prepare the data parallel context
    strategy=dygraph.prepare_context()

    linear = Linear(1, 10, act="softmax")
    adam = fluid.optimizer.AdamOptimizer()

    # make the module become the data parallelism module
    linear = dygraph.DataParallel(linear, strategy)

    x_data = np.random.random(size=[10, 1]).astype(np.float32)
    data = to_variable(x_data)

    hidden = linear(data)
    avg_loss = fluid.layers.mean(hidden)

    # scale the loss according to the number of trainers.
    avg_loss = linear.scale_loss(avg_loss)

    avg_loss.backward()

    # collect the gradients of trainers.
    linear.apply_collective_grads()

    adam.minimize(avg_loss)
    linear.clear_gradients()""",
        """CODE.import paddle
from paddle import fluid
import paddle.fluid.dygraph as dg

a = np.array([[1.0+1.0j, 2.0+1.0j], [3.0+1.0j, 4.0+1.0j]])
b = np.array([[5.0+2.0j, 6.0+2.0j], [7.0+2.0j, 8.0+2.0j]])

place = fluid.CPUPlace()
with dg.guard(place):
    x = dg.to_variable(a)
    y = dg.to_variable(b)
    out = paddle.complex.kron(x, y)
    print(out.numpy())""",
        """CODE.import paddle
import paddle.fluid as fluid
import numpy as np

data = np.array([[1.0, 2.0, 3.0, 4.0],
                 [5.0, 6.0, 7.0, 8.0],
                 [9.0, 10.0, 11.0, 12.0]])
data_index = np.array([0, 1, 1]).astype('int32')

with fluid.dygraph.guard():
    x = fluid.dygraph.to_variable(data)
    index = fluid.dygraph.to_variable(data_index)
    out_z1 = fluid.layers.index_select(x, index)
    print(out_z1.numpy())
    #[[1. 2. 3. 4.]
    # [5. 6. 7. 8.]
    # [5. 6. 7. 8.]]
    out_z2 = fluid.layers.index_select(x, index, dim=1)
    print(out_z2.numpy())
    #[[ 1.  2.  2.]
    # [ 5.  6.  6.]
    # [ 9. 10. 10.]]""",
        """CODE.import paddle
import paddle.fluid as fluid
import numpy as np

data1 = np.array([[1.0, 0.0, 0.0],
                  [0.0, 2.0, 0.0],
                  [0.0, 0.0, 3.0]])
data2 = np.array([0.0, 1.0, 0.0, 3.0])
data3 = np.array([0.0, 0.0, 0.0])
with fluid.dygraph.guard():
    x1 = fluid.dygraph.to_variable(data1)
    x2 = fluid.dygraph.to_variable(data2)
    x3 = fluid.dygraph.to_variable(data3)
    out_z1 = fluid.layers.nonzero(x1)
    print(out_z1.numpy())
    #[[0 0]
    # [1 1]
    # [2 2]]
    out_z1_tuple = fluid.layers.nonzero(x1, as_tuple=True)
    for out in out_z1_tuple:
        print(out.numpy())
    #[[0]
    # [1]
    # [2]]
    #[[0]
    # [1]
    # [2]]
    out_z2 = fluid.layers.nonzero(x2)
    print(out_z2.numpy())
    #[[1]
    # [3]]
    out_z2_tuple = fluid.layers.nonzero(x2, as_tuple=True)
    for out in out_z2_tuple:
        print(out.numpy())
    #[[1]
    # [3]]
    out_z3 = fluid.layers.nonzero(x3)
    print(out_z3.numpy())
    #[]
    out_z3_tuple = fluid.layers.nonzero(x3, as_tuple=True)
    for out in out_z3_tuple:
        print(out.numpy())
    #[]""",
        """CODE.import paddle
import paddle.fluid as fluid
import numpy as np

data_x = np.array([[1.0, 1.0, 1.0],
                   [2.0, 2.0, 2.0],
                   [3.0, 3.0, 3.0]])
data_y = np.array([[1.0, 1.0, 1.0],
                   [1.0, 1.0, 1.0],
                   [1.0, 1.0, 1.0]])

with fluid.dygraph.guard():
    x = fluid.dygraph.to_variable(data_x)
    y = fluid.dygraph.to_variable(data_y)
    out_z1 = fluid.layers.cross(x, y)
    print(out_z1.numpy())
    #[[-1. -1. -1.]
    # [ 2.  2.  2.]
    # [-1. -1. -1.]]
    out_z2 = fluid.layers.cross(x, y, dim=1)
    print(out_z2.numpy())
    #[[0. 0. 0.]
    # [0. 0. 0.]
    # [0. 0. 0.]]""",
        """CODE.import paddle
import paddle.fluid as fluid
import numpy as np

in1 = np.array([[1.2,3.5],
                [4.5,6.4]]).astype('float32')
with fluid.dygraph.guard():
    x1 = fluid.dygraph.to_variable(in1)
    out1 = fluid.layers.clamp(x1, min=3.5, max=5.0)
    out2 = fluid.layers.clamp(x1, min=2.5)
    print(out1.numpy())
    # [[3.5, 3.5]
    # [4.5, 5.0]]
    print(out2.numpy())
    # [[2.5, 3.5]
    # [[4.5, 6.4]""",
        """CODE.import paddle
import paddle.fluid as fluid
import numpy as np

with fluid.dygraph.guard():
    x = fluid.dygraph.to_variable(np.array([[3, 3],[3, 3]]).astype(np.float32))
    y = fluid.dygraph.to_variable(np.array([[3, 3],[3, 1]]).astype(np.float32))
    out = fluid.layers.dist(x, y, 0)
    print(out.numpy()) # out = [1.]

    out = fluid.layers.dist(x, y, 2)
    print(out.numpy()) # out = [2.]

    out = fluid.layers.dist(x, y, float("inf"))
    print(out.numpy()) # out = [2.]

    out = fluid.layers.dist(x, y, float("-inf"))
    print(out.numpy()) # out = [0.]""",
        """CODE.import paddle
import paddle.fluid as fluid
import numpy as np

with fluid.dygraph.guard():
  np_x = np.random.uniform(0.1, 1, [10]).astype(np.float32)
  x = fluid.dygraph.to_variable(np_x)
  print(fluid.layers.logsumexp(x).numpy())

import paddle
import paddle.fluid as fluid
import numpy as np

with fluid.dygraph.guard():
    np_x = np.random.uniform(0.1, 1, [2, 3, 4]).astype(np.float32)
    x = fluid.dygraph.to_variable(np_x)
    print(fluid.layers.logsumexp(x, dim=1).numpy())
    print(fluid.layers.logsumexp(x, dim=[0, 2]).numpy())""",
        """CODE.import paddle
import paddle.fluid as fluid
import numpy as np

with fluid.dygraph.guard():
  x = fluid.dygraph.to_variable(np.random.uniform(0.1, 1, [10]).astype(np.float32))
  y = fluid.dygraph.to_variable(np.random.uniform(1, 3, [10]).astype(np.float32))
  z = fluid.layers.dot(x, y)
  print(z.numpy())""",
        """CODE.import paddle
import paddle.fluid as fluid
import numpy as np
# Graph Organizing
x = fluid.data(name="x", shape=[2,1], dtype="float32")
res = fluid.layers.log1p(x)
# Create an executor using CPU as an example
exe = fluid.Executor(fluid.CPUPlace())
# Execute
x_i = np.array([[0], [1]]).astype(np.float32)
res_val, = exe.run(fluid.default_main_program(), feed={'x':x_i}, fetch_list=[res])
print(res_val) # [[0.], [0.6931472]]""",
        """CODE.import paddle
import paddle.fluid as fluid
import numpy as np
input = fluid.data(name="input", shape=[None,3,6,10])
output = fluid.layers.interpolate(input=input,out_shape=[12,12])
place = fluid.CPUPlace()
exe = fluid.Executor(place)
exe.run(fluid.default_startup_program())

input_data = np.random.rand(2,3,6,10).astype("float32")
output_data = exe.run(fluid.default_main_program(),
    feed={"input":input_data},
    fetch_list=[output],
    return_numpy=True)

print(output_data[0].shape)""",
        """CODE.import paddle
import paddle.fluid as fluid
import numpy as np
input = fluid.data(name='input', dtype='float32', shape=[2, 3])
output = fluid.layers.full_like(input, 2.0)
exe = fluid.Executor(fluid.CPUPlace())
exe.run(fluid.default_startup_program())
img=np.array([[1, 2, 3], [4, 5, 6]]).astype(np.float32)
res = exe.run(fluid.default_main_program(), feed={'input':img}, fetch_list=[output])
print(res) # [array([[2., 2., 2.], [2., 2., 2.]], dtype=float32)]""",
        """CODE.import paddle
import paddle.fluid as fluid
input = fluid.data(name='input', dtype='float32', shape=[3, 4])
tensor1 = fluid.data(name='tenosr1', dtype='float32', shape=[1, 4])
tensor2 = fluid.data(name='tensor2', dtype='float32', shape=[3, 4])
data = fluid.layers.addcmul(input, tensor1, tensor2, value=1.0)""",
        """CODE.import paddle
import paddle.fluid as fluid
x = fluid.data(name='x', shape=[2, 3], dtype='float32')
x_transposed = fluid.layers.t(x)
print x_transposed.shape""",
        """CODE.import paddle
import paddle.fluid as fluid
x = fluid.layers.data(name='x', shape=[10, 3, 4], dtype='float32')
y = fluid.layers.data(name='y', shape=[10, 4, 5], dtype='float32')
out = fluid.layers.bmm(x, y)

input1 = np.array([[[1.0, 1.0, 1.0],[2.0, 2.0, 2.0]],[[3.0, 3.0, 3.0],[4.0, 4.0, 4.0]]])
input2 = np.array([[[1.0, 1.0],[2.0, 2.0],[3.0, 3.0]],[[4.0, 4.0],[5.0, 5.0],[6.0, 6.0]]])
with fluid.dygraph.guard():
    x = fluid.dygraph.to_variable(input1)
    y = fluid.dygraph.to_variable(input2)
    out = fluid.layers.bmm(x, y)
    out_np = out.numpy()""",
        """CODE.import paddle
import paddle.fluid.dygraph as dg
import numpy as np

case1 = np.random.randn(3, 10, 10).astype('float64') + 1j * np.random.randn(3, 10, 10).astype('float64')

with dg.guard():
    case1 = dg.to_variable(case1)
    data1 = paddle.complex.trace(case1, offset=1, dim1=1, dim2=2) # data1.shape = [3]""",
        """CODE.import paddle
import paddle.fluid as fluid
import numpy as np
x = fluid.data(name='x', shape=[100], dtype='int32')
y = fluid.data(name='y', shape=[200], dtype='int32')
input_1 = np.random.randint(0, 100, [100, ]).astype('int32')
input_2 = np.random.randint(0, 100, [200, ]).astype('int32')
exe = fluid.Executor(place=fluid.CPUPlace())
grid_x, grid_y = fluid.layers.meshgrid([x, y])
res_1, res_2 = exe.run(fluid.default_main_program(),
                         feed={'x': input_1,
                               'y': input_2},
                         fetch_list=[grid_x, grid_y])

#the shape of res_1 is (100, 200)
#the shape of res_2 is (100, 200)

#example 2: in dygraph mode
import paddle
import paddle.fluid as fluid
import numpy as np
input_3 = np.random.randint(0, 100, [100, ]).astype('int32')
input_4 = np.random.randint(0, 100, [200, ]).astype('int32')
with fluid.dygraph.guard():
    tensor_3 = fluid.dygraph.to_variable(input_3)
    tensor_4 = fluid.dygraph.to_variable(input_4)
    grid_x, grid_y = fluid.layers.meshgrid([tensor_3, tensor_4])
#the shape of grid_x is (100, 200)
#the shape of grid_y is (100, 200)    """,
        """CODE.import paddle.fluid as fluid

# example 1:
# attr shape is a list which doesn't contain tensor Variable.
result_1 = fluid.layers.randint(low=-5, high=5, shape=[3, 4], dtype="int64")

# example 2:
# attr shape is a list which contains tensor Variable.
dim_1 = fluid.layers.fill_constant([1],"int64",3)
dim_2 = fluid.layers.fill_constant([1],"int32",5)
result_2 = fluid.layers.randint(low=-5, high=5, shape=[dim_1, dim_2], dtype="int32")

# example 3:
# attr shape is a Variable, the data type must be int64 or int32.
var_shape = fluid.data(name='var_shape', shape=[2], dtype="int64")
result_3 = fluid.layers.randint(low=-5, high=5, shape=var_shape, dtype="int32")
var_shape_int32 = fluid.data(name='var_shape_int32', shape=[2], dtype="int32")
result_4 = fluid.layers.randint(low=-5, high=5, shape=var_shape_int32, dtype="int64")

# example 4:
# Input only one parameter
# low=0, high=10, shape=[1], dtype='int64'
result_4 = fluid.layers.randint(10)""",
        """CODE.import paddle.fluid as fluid

data = fluid.layers.randn([2, 4])
place = fluid.CPUPlace()
exe = fluid.Executor(place)
res, = exe.run(fluid.default_main_program(), feed={}, fetch_list=[data])
print(res)

import paddle.fluid as fluid
import paddle.fluid.dygraph as dg

place = fluid.CPUPlace()
with dg.guard(place) as g:
    x = fluid.layers.randn([2, 4])
    x_np = x.numpy()
    print(x_np)""",
        """CODE.import paddle.fluid as fluid

data = np.arange(1, 13, dtype="int64").reshape(3,-1)
x = fluid.data(shape=(-1, 4), dtype='int64', name='x')
exe = fluid.Executor(fluid.CPUPlace())

tril = fluid.layers.tril(x)
tril_out, = exe.run(fluid.default_main_program(), feed={"x": data},
    fetch_list=[tril], return_numpy=True)

tril = fluid.layers.tril(x, diagonal=2)
tril_out, = exe.run(fluid.default_main_program(), feed={"x": data},
    fetch_list=[tril], return_numpy=True)

tril = fluid.layers.tril(x, diagonal=-1)
tril_out, = exe.run(fluid.default_main_program(), feed={"x": data},
    fetch_list=[tril], return_numpy=True)""",
        """CODE.import paddle.fluid as fluid

data1 = fluid.layers.full(shape=[2,1], fill_value=0, dtype='int64') # data1=[[0],[0]]
data2 = fluid.layers.full(shape=[2,1], fill_value=5, dtype='int64', device='gpu') # data2=[[5],[5]]

# attr shape is a list which contains Variable Tensor.
positive_2 = fluid.layers.fill_constant([1], "int32", 2)
data3 = fluid.layers.full(shape=[1, positive_2], dtype='float32', fill_value=1.5) # data3=[1.5, 1.5]

# attr shape is an Variable Tensor.
shape = fluid.layers.fill_constant([1,2], "int32", 2) # shape=[2,2]
data4 = fluid.layers.full(shape=shape, dtype='bool', fill_value=True) # data4=[[True,True],[True,True]]

# attr value is an Variable Tensor.
val = fluid.layers.fill_constant([1], "float32", 2.0) # val=[2.0]
data5 = fluid.layers.full(shape=[2,1], fill_value=val, dtype='float32') #data5=[[2.0],[2.0]]""",
        """CODE.import paddle.fluid as fluid

flags = ['FLAGS_eager_delete_tensor_gb', 'FLAGS_check_nan_inf']
res = fluid.get_flags(flags)
print(res)""",
        """CODE.import paddle.fluid as fluid

fluid.enable_dygraph()  # Now we are in dygragh mode
print(fluid.dygraph.enabled())  # True
fluid.disable_dygraph()
print(fluid.dygraph.enabled())  # False""",
        """CODE.import paddle.fluid as fluid

fluid.enable_dygraph()  # Now we are in dygragh mode
print(fluid.in_dygraph_mode())  # True
fluid.disable_dygraph()
print(fluid.in_dygraph_mode())  # False""",
        """CODE.import paddle.fluid as fluid

num = 6
is_use_gpu = False

data_1 = fluid.layers.randperm(num)
fluid.layers.Print(data_1)

data_2 = fluid.layers.randperm(num, dtype="int32", seed=1)
fluid.layers.Print(data_2)

data_3 = fluid.layers.randperm(num, stop_gradient=False, device="cpu")
fluid.layers.Print(data_3)

fluid.layers.randperm(num, out=data_3)
fluid.layers.Print(data_3)

place = fluid.CUDAPlace(0) if is_use_gpu else fluid.CPUPlace()
exe = fluid.Executor(place)
exe.run(fluid.default_startup_program())
exe.run()""",
        """CODE.import paddle.fluid as fluid

support_gpu = fluid.is_compiled_with_cuda()
place = fluid.CPUPlace()
if support_gpu:
    place = fluid.CUDAPlace(0)

# if GPU is supported, the three OPs below will be automatically assigned to CUDAPlace(0)
data1 = fluid.layers.fill_constant(shape=[1, 3, 8, 8], value=0.5, dtype='float32')
data2 = fluid.layers.fill_constant(shape=[1, 3, 5, 5], value=0.5, dtype='float32')
shape = fluid.layers.shape(data2)

with fluid.device_guard("cpu"):
    # Ops created here will be placed on CPUPlace
    shape = fluid.layers.slice(shape, axes=[0], starts=[0], ends=[4])
with fluid.device_guard('gpu'):
    # if GPU is supported, OPs created here will be placed on CUDAPlace(0), otherwise on CPUPlace
    out = fluid.layers.crop_tensor(data1, shape=shape)

exe = fluid.Executor(place)
exe.run(fluid.default_startup_program())
result = exe.run(fetch_list=[out])""",
        """CODE.import paddle.fluid as fluid

with fluid.dygraph.guard():
    linears = fluid.dygraph.LayerList([fluid.dygraph.Linear(10, 10) for i in range(10)])
    another = fluid.dygraph.Linear(10, 10)
    linears.insert(3, another)
    print(linears[3] is another)  # True""",
        """CODE.import paddle.fluid as fluid

with fluid.dygraph.guard():
    linears = fluid.dygraph.LayerList([fluid.dygraph.Linear(10, 10) for i in range(10)])
    another_list = fluid.dygraph.LayerList([fluid.dygraph.Linear(10, 10) for i in range(5)])
    linears.extend(another_list)
    print(len(linears))  # 15
    print(another_list[0] is linears[10])  # True""",
        """CODE.import paddle.fluid as fluid
data = fluid.layers.arange(0, 10, 2, 'int32')

import paddle.fluid as fluid
with fluid.dygraph.guard():
    x = fluid.layers.arange(0, 6, 2)
    # x: [0, 2, 4]
    # x dtype: float32""",
        """CODE.import paddle.fluid as fluid
data = fluid.layers.data(name='sequence', shape=[1], dtype='int64', lod_level=1)
emb = fluid.layers.nn._pull_sparse(
    input=data, size=11, table_id=0, accessor_class="DownpourCtrAccessor")""",
        """CODE.import paddle.fluid as fluid
data = fluid.layers.data(name='sequence', shape=[1], dtype='int64', lod_level=1)
emb = fluid.layers.nn._pull_sparse_v2(
    input=data, size=11, table_id=0, accessor_class="DownpourCtrAccessor")""",
        """CODE.import paddle.fluid as fluid
dataset = fluid.DatasetFactory().create_dataset("InMemoryDataset")
dataset.set_enable_pv_merge(True)""",
        """CODE.import paddle.fluid as fluid
dataset = fluid.DatasetFactory().create_dataset("InMemoryDataset")
dataset.set_merge_by_sid(True)""",
        """CODE.import paddle.fluid as fluid
dataset = fluid.DatasetFactory().create_dataset("InMemoryDataset")
dataset.set_parse_logkey(True)""",
        """CODE.import paddle.fluid as fluid
dataset = fluid.DatasetFactory().create_dataset("InMemoryDataset")
filelist = ["a.txt", "b.txt"]
dataset.set_filelist(filelist)
dataset.load_into_memory()
dataset.preprocess_instance()""",
        """CODE.import paddle.fluid as fluid
dataset = fluid.DatasetFactory().create_dataset("InMemoryDataset")
filelist = ["a.txt", "b.txt"]
dataset.set_filelist(filelist)
dataset.load_into_memory()
dataset.preprocess_instance()
exe.train_from_dataset(dataset)
dataset.postprocess_instance()""",
        """CODE.import paddle.fluid as fluid
dataset = fluid.DatasetFactory().create_dataset("InMemoryDataset")
filelist = ["a.txt", "b.txt"]
dataset.set_filelist(filelist)
dataset.load_into_memory()
dataset.set_current_phase(1)""",
        """CODE.import paddle.fluid as fluid
dataset = fluid.DatasetFactory().create_dataset("InMemoryDataset")
filelist = ["a.txt", "b.txt"]
dataset.set_filelist(filelist)
dataset.load_into_memory()
print dataset.get_pv_data_size()""",
        """CODE.import paddle.fluid as fluid
dataset = fluid.DatasetFactory().create_dataset()
dataset.set_download_cmd("./read_from_afs")""",
        """CODE.import paddle.fluid as fluid
dataset = fluid.DatasetFactory().create_dataset()
dataset.set_rank_offset("rank_offset")""",
        """CODE.import paddle.fluid as fluid
env = fluid.dygraph.ParallelEnv()
print("The current endpoint are %s" % env.current_endpoint)
# The current endpoint are 127.0.0.1:6170""",
        """CODE.import paddle.fluid as fluid
env = fluid.dygraph.ParallelEnv()
print("The device id are %d" % env.dev_id)""",
        """CODE.import paddle.fluid as fluid
env = fluid.dygraph.ParallelEnv()
print("The local rank is %d" % env.local_rank)""",
        """CODE.import paddle.fluid as fluid
env = fluid.dygraph.ParallelEnv()
print("The nranks is %d" % env.nranks)""",
        """CODE.import paddle.fluid as fluid
env = fluid.dygraph.ParallelEnv()
print("The trainer endpoints are %s" % env.trainer_endpoints)""",
        """CODE.import paddle.fluid as fluid
fluid.dygraph.ProgramTranslator()
fluid.dygraph.ProgramTranslator.get_instance()""",
        """CODE.import paddle.fluid as fluid
fluid.set_flags({'FLAGS_eager_delete_tensor_gb': 1.0})""",
        """CODE.import paddle.fluid as fluid
from paddle.fluid.dygraph.base import to_variable
import numpy as np

x = np.random.random(size=(3, 10, 3, 7)).astype('float32')
with fluid.dygraph.guard():
    x = to_variable(x)
    m = fluid.dygraph.Dropout(p=0.5)
    droped_train = m(x)
    # switch to eval mode
    m.eval()
    droped_eval = m(x)""",
        """CODE.import paddle.fluid as fluid
from paddle.fluid.dygraph.base import to_variable
import numpy as np
import paddle

# x's shape is [1, 3, 1, 2]
x = np.array([[[[1.0, 8.0]], [[10.0, 5.0]], [[4.0, 6.0]]]]).astype('float32')
with fluid.dygraph.guard():
    x = to_variable(x)
    instanceNorm = paddle.nn.InstanceNorm(3)
    ret = instanceNorm(x)
    # ret's shape is [1, 3, 1, 2]; value is [-1 1 0.999999 -0.999999 -0.999995 0.999995]
    print(ret)""",
        """CODE.import paddle.fluid as fluid
from paddle.fluid.dygraph.dygraph_to_static import convert_call

def dyfunc(x):
    if fluid.layers.mean(x) < 0:
        x_v = x - 1
    else:
        x_v = x + 1

    return x_v

new_func = convert_call(dyfunc)
x = fluid.layers.fill_constant(shape=[3, 3], value=0, dtype='float64')
x_v = new_func(x)
exe = fluid.Executor(fluid.CPUPlace())
out = exe.run(fetch_list=[x_v])
print(out[0])
# [[1. 1. 1.]
#  [1. 1. 1.]
#  [1. 1. 1.]]""",
        """CODE.import paddle.fluid as fluid
import numpy as np

# the forward_post_hook change the input of the layer: input = input * 2
def forward_pre_hook(layer, input):
    # user can use layer and input for information statistis tasks

    # change the input
    input_return = (input[0] * 2)
    return input_return

with fluid.dygraph.guard():
    linear = fluid.Linear(13, 5, dtype="float32")

    # register the hook
    forward_pre_hook_handle = linear.register_forward_pre_hook(forward_pre_hook)

    value0 = np.arange(26).reshape(2, 13).astype("float32")
    in0 = fluid.dygraph.to_variable(value0)
    out0 = linear(in0)

    # remove the hook
    forward_pre_hook_handle.remove()

    value1 = value0 * 2
    in1 = fluid.dygraph.to_variable(value1)
    out1 = linear(in1)

    # hook change the linear's input to input * 2, so out0 is equal to out1.
    assert (out0.numpy() == out1.numpy()).any()""",
        """CODE.import paddle.fluid as fluid
import numpy as np

# the forward_post_hook change the output of the layer: output = output * 2
def forward_post_hook(layer, input, output):
    # user can use layer, input and output for information statistis tasks

    # change the output
    return output * 2

with fluid.dygraph.guard():
    linear = fluid.Linear(13, 5, dtype="float32")

    # register the hook
    forward_post_hook_handle = linear.register_forward_post_hook(forward_post_hook)

    value1 = np.arange(26).reshape(2, 13).astype("float32")
    in1 = fluid.dygraph.to_variable(value1)

    out0 = linear(in1)

    # remove the hook
    forward_post_hook_handle.remove()

    out1 = linear(in1)

    # hook change the linear's output to output * 2, so out0 is equal to out1 * 2.
    assert (out0.numpy() == (out1.numpy()) * 2).any()
""",
        """CODE.import paddle.fluid as fluid
import numpy as np

@fluid.dygraph.jit.declarative
def func(x):
    x = fluid.dygraph.to_variable(x)
    if fluid.layers.mean(x) > 0:
        x_v = x - 1
    else:
        x_v = x + 1
    return x_v

prog_trans = fluid.dygraph.ProgramTranslator()
prog_trans.enable(False)

x = np.ones([1, 2])
# The declarative is disabled so the func is run in dygraph
with fluid.dygraph.guard():
    print(func(x).numpy()) # [[2. 2.]]""",
        """CODE.import paddle.fluid as fluid
import numpy as np

data = np.arange(1, 13, dtype="int64").reshape(3,-1)
x = fluid.data(shape=(-1, 4), dtype='int64', name='x')
exe = fluid.Executor(fluid.CPUPlace())

triu = fluid.layers.triu(x)
triu_out, = exe.run(fluid.default_main_program(), feed={"x": data},
    fetch_list=[triu], return_numpy=True)

triu = fluid.layers.triu(x, diagonal=2)
triu_out, = exe.run(fluid.default_main_program(), feed={"x": data},
    fetch_list=[triu], return_numpy=True)

triu = fluid.layers.triu(x, diagonal=-1)
triu_out, = exe.run(fluid.default_main_program(), feed={"x": data},
    fetch_list=[triu], return_numpy=True)""",
        """CODE.import paddle.fluid as fluid
import numpy as np

data = np.array([[[-2.0, 3.0, -4.0, 5.0],
                [3.0, -4.0, 5.0, -6.0],
                [-7.0, -8.0, 8.0, 9.0]],
               [[1.0, -2.0, -3.0, 4.0],
                [-5.0, 6.0, 7.0, -8.0],
                [6.0, 7.0, 8.0, 9.0]]]).astype('float32')
with fluid.dygraph.guard():
    data = fluid.dygraph.to_variable(data)
    res = fluid.layers.log_softmax(data, -1)""",
        """CODE.import paddle.fluid as fluid
import numpy as np

def func(x):
    x = fluid.dygraph.to_variable(x)
    if fluid.layers.mean(x) > 0:
        x_v = x - 1
    else:
        x_v = x + 1
    return x_v

prog_trans = fluid.dygraph.ProgramTranslator()

code = prog_trans.get_code(func)
print(type(code)) # <class 'str'>""",
        """CODE.import paddle.fluid as fluid
import numpy as np

def func(x):
    x = fluid.dygraph.to_variable(x)
    if fluid.layers.mean(x) > 0:
        x_v = x - 1
    else:
        x_v = x + 1
    return x_v

prog_trans = fluid.dygraph.ProgramTranslator()

static_func = prog_trans.get_func(func)
print(callable(static_func)) # True""",
        """CODE.import paddle.fluid as fluid
import numpy as np

def func(x):
    x = fluid.dygraph.to_variable(x)
    if fluid.layers.mean(x) > 0:
        x_v = x - 1
    else:
        x_v = x + 1
    return x_v

prog_trans = fluid.dygraph.ProgramTranslator()

x = np.ones([1, 2])
main_prog, start_prog, inputs, outputs = prog_trans.get_program(func, x)
print([i.name for i in inputs])
# ['x_0'] the feed input variable name representing x
print([o.name for o in outputs])
# ['_generated_var_4'] the fetch output variable name representing x_v""",
        """CODE.import paddle.fluid as fluid
import numpy as np

def func(x):
    x = fluid.dygraph.to_variable(x)
    if fluid.layers.mean(x) > 0:
        x_v = x - 1
    else:
        x_v = x + 1
    return x_v

prog_trans = fluid.dygraph.ProgramTranslator()

x = np.ones([1, 2])
x_v = prog_trans.get_output(func, x)
print(x_v.numpy()) # [[0. 0.]]
""",
        """CODE.import paddle.fluid as fluid
import numpy as np

from paddle.fluid.dygraph.nn import Linear

@fluid.dygraph.declarative
def linear_func(x):
    x = fluid.dygraph.to_variable(x)
    linear = Linear(32, 1)
    y = linear(x)
    z = linear(x)
    return y, z

prog_trans = fluid.dygraph.ProgramTranslator()

adam = fluid.optimizer.AdamOptimizer(learning_rate=0.001)
prog_trans.set_optimizer(adam,index_of_loss=1) # minimize on 'z'

for i in range(10):
    y, z_loss = linear_func(np.ones(32).astype('float32'))
    print(z_loss.numpy())""",
        """CODE.import paddle.fluid as fluid
import numpy as np

from paddle.fluid.dygraph.nn import Linear

@fluid.dygraph.declarative
def linear_func(x):
    x = fluid.dygraph.to_variable(x)
    linear = Linear(32, 1)
    y = linear(x)
    z = linear(x)
    return y, z

prog_trans = fluid.dygraph.ProgramTranslator()

adam = fluid.optimizer.AdamOptimizer(learning_rate=0.001)
prog_trans.set_optimizer(adam,index_of_loss=1) # minimize on 'z'

for i in range(10):
    y, z_loss = linear_func(np.ones(32).astype('float32'))
    print(z_loss.numpy())

# Save inference model.
# Note that fetch=[0] means we set 'y' as the inference output.
prog_trans.save_inference_model("./dy2stat_infer_model", fetch=[0])""",
        """CODE.import paddle.fluid as fluid
import numpy as np

input = fluid.data(name="input", shape=[None, 2], dtype="float32")
rank_offset = fluid.data(name="rank_offset", shape=[None, 7], dtype="int32")
out = fluid.contrib.layers.rank_attention(input=input,
                                          rank_offset=rank_offset,
                                          rank_param_shape=[18,3],
                                          rank_param_attr=
                                            fluid.ParamAttr(learning_rate=1.0,
                                                          name="ubm_rank_param.w_0",
                                                          initializer=
                                                          fluid.initializer.Xavier(uniform=False)),
                                          max_rank=3)""",
        """CODE.import paddle.fluid as fluid
import numpy as np

use_cuda = fluid.core.is_compiled_with_cuda()

a = fluid.data(name="a", shape=[2], dtype='float32')
b = fluid.data(name="b", shape=[2], dtype='float32')

result = fluid.layers.allclose(a, b, rtol=1e-05, atol=1e-08,
                          equal_nan=False, name="ignore_nan")
result_nan = fluid.layers.allclose(a, b, rtol=1e-05, atol=1e-08,
                              equal_nan=True, name="equal_nan")

place = fluid.CUDAPlace(0) if use_cuda else fluid.CPUPlace()
exe = fluid.Executor(place)
exe.run(fluid.default_startup_program())

x = np.array([10000., 1e-07]).astype("float32")
y = np.array([10000.1, 1e-08]).astype("float32")
result_v, result_nan_v = exe.run(
    feed={'a': x, 'b': y},
    fetch_list=[result, result_nan])
print(result_v, result_nan_v)

x = np.array([10000., 1e-08]).astype("float32")
y = np.array([10000.1, 1e-09]).astype("float32")
result_v, result_nan_v = exe.run(
    feed={'a': x, 'b': y},
    fetch_list=[result, result_nan])
print(result_v, result_nan_v)

x = np.array([1.0, float('nan')]).astype("float32")
y = np.array([1.0, float('nan')]).astype("float32")
result_v, result_nan_v = exe.run(
    feed={'a': x, 'b': y},
    fetch_list=[result, result_nan])
print(result_v, result_nan_v)""",
        """CODE.import paddle.fluid as fluid
import numpy as np
input = fluid.data(name="input", shape=[1])
label = fluid.data(name="label", shape=[1])
l1_loss = fluid.dygraph.L1Loss(reduction='mean')
output = l1_loss(input,label)

import paddle.fluid.dygraph as dg
with dg.guard(place) as g:
    input = dg.to_variable(input_data)
    label = dg.to_variable(label_data)
    l1_loss = fluid.dygraph.L1Loss(reduction='mean')
    output = l1_loss(input,label)
    print(output.numpy())  # [0.2]""",
        """CODE.import paddle.fluid as fluid
import numpy as np
input = fluid.data(name="input", shape=[3, 1], dtype='float32')
label = fluid.data(name="label", shape=[3, 1], dtype='float32')
bce_loss = fluid.dygraph.BCELoss()
output = bce_loss(input, label)
place = fluid.CPUPlace()
exe = fluid.Executor(place)
exe.run(fluid.default_startup_program())

input_data = np.array([0.5, 0.6, 0.7]).astype("float32")
label_data = np.array([1.0, 0.0, 1.0]).astype("float32")
output_data = exe.run(fluid.default_main_program(),
        feed={"input":input_data, "label":label_data},
        fetch_list=[output],
        return_numpy=True)

print(output_data)  # [array([0.65537095], dtype=float32)]

import paddle.fluid.dygraph as dg
with dg.guard(place) as g:
    input = dg.to_variable(input_data)
    label = dg.to_variable(label_data)
    output = bce_loss(input, label)
    print(output.numpy())  # [0.65537095]""",
        """CODE.import paddle.fluid as fluid
import numpy as np
label = fluid.layers.assign(np.array([3, 3], dtype="int32"))
limit = fluid.layers.assign(np.array([3, 2], dtype="int32"))
out1 = fluid.layers.elementwise_equal(x=label, y=limit) #out1=[True, False]""",
        """CODE.import paddle.fluid as fluid
import paddle.fluid.dygraph as dg
import numpy as np

diag_embed = np.random.randn(2, 3).astype('float32')
with dg.guard():
    data1 = fluid.layers.diag_embed(diag_embed)
    data1.numpy()

    data2 = fluid.layers.diag_embed(diag_embed, offset=-1, dim1=0, dim2=2)
    data2.numpy()

    data3 = fluid.layers.diag_embed(diag_embed, offset=1, dim1=0, dim2=2)
    data3.numpy()""",
        """CODE.import paddle.fluid as fluid
prog = fluid.default_main_program()
print(prog.random_seed)
## 0
## the default random seed is 0

prog.global_seed(102)
prog1 = fluid.default_main_program()
print(prog1.random_seed)
## 102
## the random seed is 102""",
        """CODE.import paddle.fluid as fluid
prog_trans = fluid.dygraph.ProgramTranslator()
prog_cache = prog_trans.get_program_cache()""",
        """CODE.import paddle.fluid as fluid
x = fluid.data(name="x", shape=[None,3], dtype="float32")
y = fluid.data(name="y", shape=[None,3], dtype="float32")
concat = fluid.contrib.layers.partial_concat(
    [x, y], start_index=0, length=2)""",
        """CODE.import paddle.fluid.layers as layers
import paddle.fluid as fluid
import numpy as np
x = fluid.data(name="x", shape=[None, 3], dtype="float32")
y = fluid.data(name="y", shape=[None, 3], dtype="float32")
sum = layers.partial_sum([x,y], start_index=0, length=2)
place = fluid.CPUPlace()
exe = fluid.Executor(place)
xx = np.array([1,2,3,4,5,6]).reshape((2,3)).astype("float32")
yy = np.array([6,5,4,4,5,6]).reshape((2,3)).astype("float32")
out = exe.run(feed={"x":xx, "y":yy}, fetch_list=[sum])""",
        """CODE.with fleet_embedding(click_name=label.name):
    emb = fluid.layers.embedding(
        input=var,
        size=[-1, 11],
        is_sparse=True,
        is_distributed=True,
        param_attr=fluid.ParamAttr(name="embedding"))""" .

<DEPENDENCY.paddlepaddle-gpu==1.8.1> <CONTAINS> """CODE.import paddle.fluid as fluid

fluid.enable_imperative()  # Now we are in imperative mode
x = fluid.layers.ones( (2, 2), "float32")
y = fluid.layers.zeros( (2, 2), "float32")
z = x + y
print( z.numpy() )   #[[1, 1], [1, 1]]""",
        """CODE.import paddle.fluid as fluid

fluid.enable_imperative()  # Now we are in imperative mode
x = fluid.layers.ones( (2, 2), "float32")
y = fluid.layers.zeros( (2, 2), "float32")
z = x + y
print( z.numpy() )   #[[1, 1], [1, 1]]
fluid.disable_imperative() # Now we are in declarative mode""" .

<DEPENDENCY.paddlepaddle-gpu==1.8.2> <CONTAINS> """CODE.import paddle.fluid as fluid
import numpy as np

with fluid.dygraph.guard():
    x = np.random.uniform(-1, 1, [10, 10]).astype("float32")
    linear = fluid.dygraph.Linear(10, 10)
    input = fluid.dygraph.to_variable(x)

    reduce_lr = fluid.dygraph.ReduceLROnPlateau(
                            learning_rate = 1.0,
                            decay_rate = 0.5,
                            patience = 5,
                            verbose = True,
                            cooldown = 3)
    adam = fluid.optimizer.Adam(
        learning_rate = reduce_lr,
        parameter_list = linear.parameters())

    for epoch in range(10):
        total_loss = 0
        for bath_id in range(5):
            out = linear(input)
            loss = fluid.layers.reduce_mean(out)
            total_loss += loss
            adam.minimize(loss)

        avg_loss = total_loss/5

        # adjust learning rate according to avg_loss
        reduce_lr.step(avg_loss)
        lr = adam.current_step_lr()
        print("current avg_loss is %s, current lr is %s" % (avg_loss.numpy()[0], lr))""" .

<DEPENDENCY.paddlepaddle-gpu==1.8.3> <CONTAINS> """CODE.import numpy as np
from paddle.incubate.hapi.vision.transforms import BrightnessTransform
transform = BrightnessTransform(0.4)
fake_img = np.random.rand(500, 500, 3).astype('float32')
fake_img = transform(fake_img)
print(fake_img.shape)""",
        """CODE.import numpy as np
from paddle.incubate.hapi.vision.transforms import ColorJitter
transform = ColorJitter(0.4)
fake_img = np.random.rand(500, 500, 3).astype('float32')
fake_img = transform(fake_img)
print(fake_img.shape)""",
        """CODE.import numpy as np
from paddle.incubate.hapi.vision.transforms import ContrastTransform
transform = ContrastTransform(0.4)
fake_img = np.random.rand(500, 500, 3).astype('float32')
fake_img = transform(fake_img)
print(fake_img.shape)""",
        """CODE.import numpy as np
from paddle.incubate.hapi.vision.transforms import HueTransform
transform = HueTransform(0.4)
fake_img = np.random.rand(500, 500, 3).astype('float32')
fake_img = transform(fake_img)
print(fake_img.shape)""",
        """CODE.import numpy as np
from paddle.incubate.hapi.vision.transforms import SaturationTransform
transform = SaturationTransform(0.4)
fake_img = np.random.rand(500, 500, 3).astype('float32')

fake_img = transform(fake_img)
print(fake_img.shape)""",
        """CODE.import paddle.fluid as fluid
boxes = fluid.data(name='bboxes', shape=[None,81, 4],
                          dtype='float32', lod_level=1)
scores = fluid.data(name='scores', shape=[None,81],
                          dtype='float32', lod_level=1)
out = fluid.layers.matrix_nms(bboxes=boxes,
                              scores=scores,
                              background_label=0,
                              score_threshold=0.5,
                              post_threshold=0.1,
                              nms_top_k=400,
                              keep_top_k=200,
                              normalized=False)""",
        """CODE.import paddle.fluid as fluid
import numpy as np

DATATYPE='float32'

x_data = np.array([i for i in range(1,5)]).reshape([1,1,4]).astype(DATATYPE)

x = fluid.data(name="x", shape=[None,1,4], dtype=DATATYPE)
y = fluid.layers.mish(x)

place = fluid.CPUPlace()
# place = fluid.CUDAPlace(0)
exe = fluid.Executor(place)
out, = exe.run(feed={'x':x_data}, fetch_list=[y.name])
print(out)  # [[0.66666667, 1.66666667, 3., 4.]]
""",
        """CODE.import paddle.fluid as fluid
import numpy as np

data = np.array([[1.0, 2.0, 3.0, 4.0],
                    [5.0, 6.0, 7.0, 8.0],
                    [9.0, 10.0, 11.0, 12.0]]).astype('float32')

data_index = np.array([[0, 1, 2],
                        [1, 2, 3],
                        [0, 0, 0]]).astype('int32')

target_data = np.array([[100, 200, 300, 400],
                        [500, 600, 700, 800],
                        [900, 1000, 1100, 1200]]).astype('int32')

with fluid.dygraph.guard():
    x = fluid.dygraph.to_variable(data)
    index = fluid.dygraph.to_variable(data_index)
    target = fluid.dygraph.to_variable(target_data)

    out_z1 = fluid.layers.index_sample(x, index)
    print(out_z1.numpy())
    #[[1. 2. 3.]
    # [6. 7. 8.]
    # [9. 9. 9.]]

    # Use the index of the maximum value by topk op
    # get the value of the element of the corresponding index in other tensors
    top_value, top_index = fluid.layers.topk(x, k=2)
    out_z2 = fluid.layers.index_sample(target, top_index)
    print(top_value.numpy())
    #[[ 4.  3.]
    # [ 8.  7.]
    # [12. 11.]]

    print(top_index.numpy())
    #[[3 2]
    # [3 2]
    # [3 2]]

    print(out_z2.numpy())
    #[[ 400  300]
    # [ 800  700]
    # [1200 1100]]""",
        """CODE.learning_rate = 0.5
lr_lambda = lambda epoch: 0.95 ** epoch

learning_rate = 0.5
learning_rate = 0.475
learning_rate = 0.45125

import paddle.fluid as fluid
import numpy as np
with fluid.dygraph.guard():
    x = np.random.uniform(-1, 1, [10, 10]).astype("float32")
    linear = fluid.dygraph.Linear(10, 10)
    input = fluid.dygraph.to_variable(x)
    scheduler = fluid.dygraph.LambdaDecay(0.5, lr_lambda=lambda x: 0.95**x)
    adam = fluid.optimizer.Adam(learning_rate = scheduler, parameter_list = linear.parameters())

    for epoch in range(6):
        for batch_id in range(5):
            out = linear(input)
            loss = fluid.layers.reduce_mean(out)
            adam.minimize(loss)
        scheduler.epoch()

        print("epoch:%d, current lr is %f" .format(epoch, adam.current_step_lr()))
        # epoch:0, current lr is 0.5
        # epoch:1, current lr is 0.475
        # epoch:2, current lr is 0.45125""" .

<DEPENDENCY.paddlepaddle-gpu==1.8.5> <CONTAINS> """CODE.import paddle.fluid as fluid
dataset = fluid.DatasetFactory().create_dataset("InMemoryDataset")
dataset.set_fleet_send_batch_size(800)""",
        """CODE.import paddle.fluid as fluid
dataset = fluid.DatasetFactory().create_dataset("InMemoryDataset")
dataset.set_fleet_send_sleep_seconds(2)""",
        """CODE.import paddle.fluid as fluid
dataset = fluid.DatasetFactory().create_dataset("InMemoryDataset")
dataset.set_merge_by_lineid()""",
        """CODE.import paddle.fluid as fluid
dataset = fluid.DatasetFactory().create_dataset("InMemoryDataset")
dataset.set_parse_content(True)""",
        """CODE.import paddle.fluid as fluid
dataset = fluid.DatasetFactory().create_dataset("InMemoryDataset")
dataset.set_parse_ins_id(True)""",
        """CODE.import paddle.fluid as fluid
dataset = fluid.DatasetFactory().create_dataset("InMemoryDataset")
dataset.set_queue_num(12)""",
        """CODE.import paddle.fluid as fluid
dataset = fluid.DatasetFactory().create_dataset()
dataset.set_batch_size(128)""",
        """CODE.import paddle.fluid as fluid
dataset = fluid.DatasetFactory().create_dataset()
dataset.set_download_cmd("./read_from_afs")""",
        """CODE.import paddle.fluid as fluid
dataset = fluid.DatasetFactory().create_dataset()
dataset.set_hdfs_config("my_fs_name", "my_fs_ugi")""",
        """CODE.import paddle.fluid as fluid
dataset = fluid.DatasetFactory().create_dataset()
dataset.set_pipe_command("python my_script.py")""",
        """CODE.import paddle.fluid as fluid
dataset = fluid.DatasetFactory().create_dataset()
dataset.set_thread(12)""",
        """CODE.import paddle.fluid as fluid
dataset = fluid.DatasetFactory().create_dataset()
dataset.set_use_var([data, label])""",
        """CODE.import paddle.fluid as fluid
dataset = fluid.DatasetFactory().create_dataset()
print(dataset.desc())""",
        """CODE.import paddle.fluid as fluid
env = fluid.dygraph.ParallelEnv()
print("The device id are %d" % env.dev_id)""" .

<DEPENDENCY.paddlepaddle-gpu==2.0.0> <CONTAINS> """CODE.        from paddle.distributed.fleet.utils import LocalFS
        client = LocalFS()
        client.touch("test_rename_src")
        print(client.is_exists("test_rename_src")) # True
        client.rename("test_rename_src", "test_rename_dst")
        print(client.is_exists("test_rename_src")) # False
        print(client.is_exists("test_rename_dst")) # True
        client.delete("test_rename_dst")""",
        """CODE.from paddle.distributed.fleet.utils import LocalFS

client = LocalFS()
subdirs, files = client.ls_dir("./")""",
        """CODE.from paddle.distributed.fleet.utils import LocalFS
client = LocalFS()
client.mkdirs("test_is_dir")
print(client.is_dir("test_is_file")) # True
client.delete("test_is_dir")""",
        """CODE.from paddle.distributed.fleet.utils import LocalFS
client = LocalFS()
client.mkdirs("test_localFS_mkdirs")
client.delete("test_localFS_mkdirs")""",
        """CODE.from paddle.distributed.fleet.utils import LocalFS
client = LocalFS()
client.mkdirs("test_mkdirs")
client.delete("test_mkdirs")""",
        """CODE.from paddle.distributed.fleet.utils import LocalFS
client = LocalFS()
client.touch("test_is_file")
print(client.is_file("test_is_file")) # True
client.delete("test_is_file")""",
        """CODE.from paddle.distributed.fleet.utils import LocalFS
client = LocalFS()
client.touch("test_mv_src")
client.mv("test_mv_src", "test_mv_dst")
client.delete("test_mv_dst")""",
        """CODE.from paddle.distributed.fleet.utils import LocalFS
client = LocalFS()
client.touch("test_touch")
client.delete("test_touch")""",
        """CODE.from paddle.distributed.fleet.utils import LocalFS
client = LocalFS()
ret = local_fs.is_exist("test_is_exist")""",
        """CODE.from paddle.distributed.fleet.utils import LocalFS
client = LocalFS()
subdirs = client.list_dirs("./")""",
        """CODE.from paddle.distributed.fleet.utils import LocalFS
client = LocalFS()
subdirs, files = client.ls_dir("./")""",
        """CODE.from paddle.io import Dataset, Sampler

class RandomDataset(Dataset):
    def __init__(self, num_samples):
        self.num_samples = num_samples

    def __getitem__(self, idx):
        image = np.random.random([784]).astype('float32')
        label = np.random.randint(0, 9, (1, )).astype('int64')
        return image, label

    def __len__(self):
        return self.num_samples

class MySampler(Sampler):
    def __init__(self, data_source):
        self.data_source = data_source

    def __iter__(self):
        return iter(range(len(self.data_source)))

    def __len__(self):
        return len(self.data_source)

sampler = MySampler(data_source=RandomDataset(100))

for index in sampler:
    print(index)
""",
        """CODE.from paddle.regularizer import L2Decay
linear = paddle.nn.Linear(10, 10)
momentum = paddle.optimizer.Momentum(
    learning_rate=0.1,
    parameters=linear.parameters(),
    weight_decay=L2Decay(0.0001))
""",
        """CODE.import numpy as np
from paddle.io import Dataset

# define a random dataset
class RandomDataset(Dataset):
    def __init__(self, num_samples):
        self.num_samples = num_samples

    def __iter__(self):
        for i in range(self.num_samples):
            image = np.random.random([784]).astype('float32')
            label = np.random.randint(0, 9, (1, )).astype('int64')
            yield image, label

dataset = RandomDataset(10)
for img, lbl in dataset:
    print(img, lbl)

import math
import numpy as np
import paddle.fluid as fluid
from paddle.io import IterableDataset, DataLoader, get_worker_info

class SplitedIterableDataset(IterableDataset):
    def __init__(self, start, end):
        self.start = start
        self.end = end

    def __iter__(self):
        worker_info = get_worker_info()
        if worker_info is None:
            iter_start = self.start
            iter_end = self.end
        else:
            per_worker = int(
                math.ceil((self.end - self.start) / float(
                    worker_info.num_workers)))
            worker_id = worker_info.id
            iter_start = self.start + worker_id * per_worker
            iter_end = min(iter_start + per_worker, self.end)

        for i in range(iter_start, iter_end):
            yield np.array([i])

place = fluid.CPUPlace()
with fluid.dygraph.guard(place):
    dataset = SplitedIterableDataset(start=2, end=9)
    dataloader = DataLoader(
        dataset,
        places=place,
        num_workers=2,
        batch_size=1,
        drop_last=True)

    print(list(dataloader))
    # outputs: [2, 5, 3, 6, 4, 7]

import math
import numpy as np
import paddle.fluid as fluid
from paddle.io import IterableDataset, DataLoader, get_worker_info

class RangeIterableDataset(IterableDataset):
    def __init__(self, start, end):
        self.start = start
        self.end = end

    def __iter__(self):
        for i in range(self.start, self.end):
            yield np.array([i])

place = fluid.CPUPlace()
with fluid.dygraph.guard(place):
    dataset = RangeIterableDataset(start=2, end=9)

    def worker_init_fn(worker_id):
        worker_info = get_worker_info()

        dataset = worker_info.dataset
        start = dataset.start
        end = dataset.end
        num_per_worker = int(
            math.ceil((end - start) / float(worker_info.num_workers)))

        worker_id = worker_info.id
        dataset.start = start + worker_id * num_per_worker
        dataset.end = min(dataset.start + num_per_worker, end)

    dataloader = DataLoader(
        dataset,
        places=place,
        num_workers=2,
        batch_size=1,
        drop_last=True,
        worker_init_fn=worker_init_fn)

    print(list(dataloader))
    # outputs: [2, 5, 3, 6, 4, 7]""",
        """CODE.import numpy as np
import paddle
from paddle.distributed import ReduceOp
from paddle.distributed import init_parallel_env

paddle.set_device('gpu:%d'%paddle.distributed.ParallelEnv().dev_id)
init_parallel_env()
if paddle.distributed.ParallelEnv().local_rank == 0:
    np_data = np.array([[4, 5, 6], [4, 5, 6]])
else:
    np_data = np.array([[1, 2, 3], [1, 2, 3]])
data = paddle.to_tensor(np_data)
paddle.distributed.all_reduce(data, op=ReduceOp.SUM)
out = data.numpy()""",
        """CODE.import numpy as np
import paddle
from paddle.io import Dataset, ComposeDataset

# define a random dataset
class RandomDataset(Dataset):
    def __init__(self, num_samples):
        self.num_samples = num_samples

    def __getitem__(self, idx):
        image = np.random.random([32]).astype('float32')
        label = np.random.randint(0, 9, (1, )).astype('int64')
        return image, label

    def __len__(self):
        return self.num_samples

dataset = ComposeDataset([RandomDataset(10), RandomDataset(10)])
for i in range(len(dataset)):
    image1, label1, image2, label2 = dataset[i]
    print(image1)
    print(label1)
    print(image2)
    print(label2)""",
        """CODE.import numpy as np
import paddle
from paddle.io import IterableDataset, ChainDataset

# define a random dataset
class RandomDataset(IterableDataset):
    def __init__(self, num_samples):
        self.num_samples = num_samples

    def __iter__(self):
        for i in range(10):
            image = np.random.random([32]).astype('float32')
            label = np.random.randint(0, 9, (1, )).astype('int64')
            yield image, label

dataset = ChainDataset([RandomDataset(10), RandomDataset(10)])
for image, label in iter(dataset):
    print(image, label)""",
        """CODE.import numpy as np
import paddle
from paddle.io import TensorDataset

paddle.disable_static()

input_np = np.random.random([2, 3, 4]).astype('float32')
input = paddle.to_tensor(input_np)
label_np = np.random.random([2, 1]).astype('int32')
label = paddle.to_tensor(label_np)

dataset = TensorDataset([input, label])

for i in range(len(dataset)):
    input, label = dataset[i]
    print(input, label)""",
        """CODE.import numpy as np
import paddle
import paddle.nn as nn
import import paddle.optimizer as opt

BATCH_SIZE = 16
BATCH_NUM = 4
EPOCH_NUM = 4

IMAGE_SIZE = 784
CLASS_NUM = 10

# define a random dataset
class RandomDataset(paddle.io.Dataset):
    def __init__(self, num_samples):
        self.num_samples = num_samples

    def __getitem__(self, idx):
        image = np.random.random([IMAGE_SIZE]).astype('float32')
        label = np.random.randint(0, CLASS_NUM - 1, (1, )).astype('int64')
        return image, label

    def __len__(self):
        return self.num_samples

class LinearNet(nn.Layer):
    def __init__(self):
        super(LinearNet, self).__init__()
        self._linear = nn.Linear(IMAGE_SIZE, CLASS_NUM)

    @paddle.jit.to_static
    def forward(self, x):
        return self._linear(x)

def train(layer, loader, loss_fn, opt):
    for epoch_id in range(EPOCH_NUM):
        for batch_id, (image, label) in enumerate(loader()):
            out = layer(image)
            loss = loss_fn(out, label)
            loss.backward()
            opt.step()
            opt.clear_grad()
            print("Epoch {} batch {}: loss = {}".format(
                epoch_id, batch_id, np.mean(loss.numpy())))

# create network
layer = LinearNet()
loss_fn = nn.CrossEntropyLoss()
adam = opt.Adam(learning_rate=0.001, parameters=layer.parameters())

# create data loader
dataset = RandomDataset(BATCH_NUM * BATCH_SIZE)
loader = paddle.io.DataLoader(dataset,
    batch_size=BATCH_SIZE,
    shuffle=True,
    drop_last=True,
    num_workers=2)

# train
train(layer, loader, loss_fn, adam)

# save
model_path = "linear.example.model"
paddle.jit.save(layer, model_path)

# load
translated_layer = paddle.jit.load(model_path)

# get program
program = translated_layer.program()""",
        """CODE.import numpy as np
import paddle.fluid as fluid

data = np.random.uniform(-1, 1, [10, 3, 32, 32]).astype('float32')
with fluid.dygraph.guard():
    model = fluid.dygraph.Conv2D(3, 2, 3)
    optimizer = fluid.optimizer.SGDOptimizer(
            learning_rate=0.01, parameter_list=model.parameters())
    scaler = fluid.dygraph.AmpScaler(init_loss_scaling=1024)
    data = fluid.dygraph.to_variable(data)
    with fluid.dygraph.amp_guard():
        conv = model(data)
        loss = fluid.layers.reduce_mean(conv)
        scaled = scaler.scale(loss)
        scaled.backward()
        scaler.minimize(optimizer, scaled)""",
        """CODE.import paddle

model = paddle.nn.Conv2D(3, 2, 3, bias_attr=True)
optimizer = paddle.optimizer.SGD(learning_rate=0.01, parameters=model.parameters())
scaler = paddle.amp.GradScaler(init_loss_scaling=1024)
data = paddle.rand([10, 3, 32, 32])

with paddle.amp.auto_cast():
    conv = model(data)
    loss = paddle.mean(conv)

scaled = scaler.scale(loss)  # scale the loss
scaled.backward()            # do backward
scaler.minimize(optimizer, scaled)  # update parameters""",
        """CODE.import paddle

x = paddle.uniform([10, 10], min=-1.0, max=1.0, dtype='float32')
linear = paddle.nn.Linear(in_features=10, out_features=10,
                          weight_attr=paddle.ParamAttr(need_clip=True),
                          bias_attr=paddle.ParamAttr(need_clip=False))
out = linear(x)
loss = paddle.mean(out)
loss.backward()

clip = paddle.nn.ClipGradByGlobalNorm(clip_norm=1.0)
sdg = paddle.optimizer.SGD(learning_rate=0.1, parameters=linear.parameters(), grad_clip=clip)
sdg.step()""",
        """CODE.import paddle

x = paddle.uniform([10, 10], min=-1.0, max=1.0, dtype='float32')
linear = paddle.nn.Linear(in_features=10, out_features=10,
                          weight_attr=paddle.ParamAttr(need_clip=True),
                          bias_attr=paddle.ParamAttr(need_clip=False))
out = linear(x)
loss = paddle.mean(out)
loss.backward()

clip = paddle.nn.ClipGradByNorm(clip_norm=1.0)
sdg = paddle.optimizer.SGD(learning_rate=0.1, parameters=linear.parameters(), grad_clip=clip)
sdg.step()""",
        """CODE.import paddle

x = paddle.uniform([10, 10], min=-1.0, max=1.0, dtype='float32')
linear = paddle.nn.Linear(in_features=10, out_features=10,
                          weight_attr=paddle.ParamAttr(need_clip=True),
                          bias_attr=paddle.ParamAttr(need_clip=False))
out = linear(x)
loss = paddle.mean(out)
loss.backward()

clip = paddle.nn.ClipGradByValue(min=-1, max=1)
sdg = paddle.optimizer.SGD(learning_rate=0.1, parameters=linear.parameters(), grad_clip=clip)
sdg.step()""",
        """CODE.import paddle
dataset = paddle.distributed.fleet.DatasetBase()
dataset._set_batch_size(128)""",
        """CODE.import paddle
dataset = paddle.distributed.fleet.DatasetBase()
dataset._set_download_cmd("./read_from_afs")""",
        """CODE.import paddle
dataset = paddle.distributed.fleet.DatasetBase()
dataset._set_hdfs_config("my_fs_name", "my_fs_ugi")""",
        """CODE.import paddle
dataset = paddle.distributed.fleet.DatasetBase()
dataset._set_thread(12)""",
        """CODE.import paddle
dataset = paddle.distributed.fleet.DatasetBase()
dataset._set_use_var([data, label])""",
        """CODE.import paddle
dataset = paddle.distributed.fleet.DatasetBase()
print(dataset._desc())""",
        """CODE.import paddle
dataset = paddle.distributed.fleet.dataset.DatasetBase()
dataset._set_pipe_command("python my_script.py")""",
        """CODE.import paddle
from paddle.jit import to_static
from paddle.static import InputSpec

paddle.disable_static()

def foo(x, y):
    z = x + y
    return z

decorated_foo = to_static(foo, input_spec=[InputSpec([10], name='x'), InputSpec([10], name='y')])
print(decorated_foo.concrete_program)

decorated_foo = to_static(foo)
out_foo = decorated_foo(paddle.rand([10]), paddle.rand([10]))
print(decorated_foo.concrete_program)""",
        """CODE.import paddle
from paddle.regularizer import L1Decay
import numpy as np
linear = paddle.nn.Linear(10, 10)
inp = paddle.rand(shape=[10, 10], dtype="float32")
out = linear(inp)
loss = paddle.mean(out)
beta1 = paddle.to_tensor([0.9], dtype="float32")
beta2 = paddle.to_tensor([0.99], dtype="float32")
momentum = paddle.optimizer.Momentum(
    learning_rate=0.1,
    parameters=linear.parameters(),
    weight_decay=L1Decay(0.0001))
back = out.backward()
momentum.step()
momentum.clear_grad()

my_conv2d = Conv2D(
        in_channels=10,
        out_channels=10,
        kernel_size=1,
        stride=1,
        padding=0,
        weight_attr=ParamAttr(regularizer=L2Decay(coeff=0.01)),
        bias_attr=False)""",
        """CODE.import paddle
import numpy as np

inp_np = np.ones([5, 2, 3, 4]).astype('float32')
inp_np = paddle.to_tensor(inp_np)
flatten = paddle.nn.Flatten(start_axis=1, stop_axis=2)
flatten_res = flatten(inp_np)""",
        """CODE.import paddle
paddle.enable_static()
dataset = paddle.distributed.InMemoryDataset()
dataset._set_fleet_send_batch_size(800)""",
        """CODE.import paddle
paddle.enable_static()
dataset = paddle.distributed.InMemoryDataset()
dataset._set_fleet_send_sleep_seconds(2)""",
        """CODE.import paddle
paddle.enable_static()
dataset = paddle.distributed.InMemoryDataset()
dataset._set_merge_by_lineid()""",
        """CODE.import paddle
paddle.enable_static()
dataset = paddle.distributed.InMemoryDataset()
dataset._set_parse_content(True)""",
        """CODE.import paddle
paddle.enable_static()
dataset = paddle.distributed.InMemoryDataset()
dataset._set_parse_ins_id(True)""",
        """CODE.import paddle
paddle.enable_static()
dataset = paddle.distributed.InMemoryDataset()
dataset._set_queue_num(12)""",
        """CODE.import paddle.distributed as dist
env = dist.ParallelEnv()
print("The device id are %d" % env.device_id)""",
        """CODE.import paddle.distributed.fleet as fleet
from paddle.distributed.fleet import PaddleCloudRoleMaker
import sys
import numpy as np
import os

os.environ["PADDLE_WITH_GLOO"] = "2"

def train():
    role = PaddleCloudRoleMaker(
        is_collective=False,
        init_gloo=True,
        path="./tmp_gloo")
    fleet.init(role)

    if fleet.is_server():
        input = [1, 2]
        output = fleet.util.all_reduce(input, "sum", "server")
        print(output)
        # [2, 4]
    elif fleet.is_worker():
        input = np.array([3, 4])
        output = fleet.util.all_reduce(input, "sum", "worker")
        print(output)
        # [6, 8]
    output = fleet.util.all_reduce(input, "sum", "all")
    print(output)
    # [8, 12]
if __name__ == "__main__":
    train()""",
        """CODE.import paddle.distributed.fleet as fleet
from paddle.distributed.fleet import PaddleCloudRoleMaker
import sys
import os

os.environ["PADDLE_WITH_GLOO"] = "2"

def train():
    role = PaddleCloudRoleMaker(
        is_collective=False,
        init_gloo=True,
        path="./tmp_gloo")
    fleet.init(role)

    if fleet.is_server():
        fleet.util.barrier("server")
        print("all server arrive here")
    elif fleet.is_worker():
        fleet.util.barrier("worker")
        print("all server arrive here")
    fleet.util.barrier("all")
    print("all servers and workers arrive here")

if __name__ == "__main__":
    train()""",
        """CODE.import paddle.distributed.fleet as fleet
from paddle.distributed.fleet import PaddleCloudRoleMaker
import sys
import os

os.environ["PADDLE_WITH_GLOO"] = "2"

def train():
    role = PaddleCloudRoleMaker(
        is_collective=False,
        init_gloo=True,
        path="./tmp_gloo")
    fleet.init(role)

    if fleet.is_server():
        input = fleet.server_index()
        output = fleet.util.all_gather(input, "server")
        print(output)
        # output = [0, 1]
    elif fleet.is_worker():
        input = fleet.worker_index()
        output = fleet.util.all_gather(input, "worker")
        # output = [0, 1]
        print(output)
    output = fleet.util.all_gather(input, "all")
    print(output)
    # output = [0, 1, 0, 1]

if __name__ == "__main__":
    train()""",
        """CODE.import paddle.distributed.fleet as fleet
from paddle.distributed.fleet import UserDefinedRoleMaker

role = UserDefinedRoleMaker(
    is_collective=False,
    init_gloo=False,
    current_id=0,
    role=fleet.Role.WORKER,
    worker_endpoints=["127.0.0.1:6003", "127.0.0.1:6004"],
    server_endpoints=["127.0.0.1:6001", "127.0.0.1:6002"])
fleet.init(role)

files = fleet.util.get_file_shard(["file1", "file2", "file3"])
print(files)""",
        """CODE.import paddle.distributed.fleet as fleet
from paddle.distributed.fleet import UserDefinedRoleMaker

role = UserDefinedRoleMaker(
    is_collective=False,
    init_gloo=False,
    current_id=0,
    role=fleet.Role.WORKER,
    worker_endpoints=["127.0.0.1:6003", "127.0.0.1:6004"],
    server_endpoints=["127.0.0.1:6001", "127.0.0.1:6002"])
fleet.init(role)

fleet.util.print_on_rank("I'm worker 0", 0)""",
        """CODE.import paddle.fluid as fluid
import numpy as np

def gen_data(batch_size):
    return {"x": np.random.random(size=(batch_size, 32)).astype('float32'),
            "y": np.random.random(size=(batch_size, 1)).astype('int64')}

def mlp(input_x, input_y, hid_dim=128, label_dim=2):
    fc_1 = fluid.layers.fc(input=input_x, size=hid_dim)
    prediction = fluid.layers.fc(input=[fc_1], size=label_dim, act='softmax')
    cost = fluid.layers.cross_entropy(input=prediction, label=input_y)
    sum_cost = fluid.layers.reduce_mean(cost)
    return sum_cost, fc_1, prediction

input_x = fluid.layers.data(name="x", shape=[32], dtype='float32')
input_y = fluid.layers.data(name="y", shape=[1], dtype='int64')
cost, fc_1, pred = mlp(input_x, input_y)
sgd = fluid.optimizer.Adam(learning_rate=0.01)
sgd = fluid.optimizer.GradientMergeOptimizer(sgd, k_steps=4, avg=True)
sgd.minimize(cost)

place = fluid.CPUPlace()
exe = fluid.Executor(place)
exe.run(fluid.default_startup_program())

for i in range(10):
    cost_val = exe.run(feed=gen_data(32),
               program=fluid.default_main_program(),
               fetch_list=[cost.name])
    print("step=%d, cost=%f" % (i, cost_val[0]))""" .

<DEPENDENCY.paddlepaddle-gpu==2.0.2> <CONTAINS> """CODE.import paddle
import paddle.fluid as fluid

train_reader = paddle.batch(paddle.dataset.mnist.train(),
            batch_size=32,drop_last=True)
train_reader = fluid.contrib.reader.distributed_batch_reader(
            train_reader)""",
        """CODE.import paddle.fluid as fluid
fluid.load_op_library('custom_op.so')""" .

<DEPENDENCY.paddlepaddle-gpu==2.1.0> <CONTAINS> """CODE.(m1, m2) = prefetch(m1@offload, m2@offload)
(m1out, m2out, pout) = adam(m1, m2, p)
(m1@offload, m2@offload) = memcpy(m1, m2)""",
        """CODE... code-block:: python
    import paddle
    paddle.enable_static()
    with paddle.static.amp.bf16_guard():
        paddle.static.amp.AutoMixedPrecisionListsBF16(custom_fp32_list={'lstm'})""",
        """CODE.import cv2
import paddle

fake_img = (np.random.random(
            (400, 300, 3)) * 255).astype('uint8')

cv2.imwrite('fake.jpg', fake_img)

img_bytes = paddle.vision.ops.read_file('fake.jpg')
img = paddle.vision.ops.decode_jpeg(img_bytes)

print(img.shape)""",
        """CODE.import paddle

linear=paddle.nn.Linear(2, 2)
linear.weight
#Parameter containing:
#Tensor(shape=[2, 2], dtype=float32, place=CUDAPlace(0), stop_gradient=False,
#       [[-0.32770029,  0.38653070],
#        [ 0.46030545,  0.08158520]])

linear.to(dtype='float64')
linear.weight
#Tenor(shape=[2, 2], dtype=float64, place=CUDAPlace(0), stop_gradient=False,
#       [[-0.32770029,  0.38653070],
#        [ 0.46030545,  0.08158520]])

linear.to(device='cpu')
linear.weight
#Tensor(shape=[2, 2], dtype=float64, place=CPUPlace, stop_gradient=False,
#       [[-0.32770029,  0.38653070],
#        [ 0.46030545, 0.08158520]])
linear.to(device=paddle.CUDAPinnedPlace(), blocking=False)
linear.weight
#Tensor(shape=[2, 2], dtype=float64, place=CUDAPinnedPlace, stop_gradient=False,
#       [[-0.04989364, -0.56889004],
#        [ 0.33960250, 0.96878713]])""",
        """CODE.import paddle

paddle.distributed.init_parallel_env()
tindata = paddle.randn(shape=[2, 3])
paddle.distributed.all_reduce(tindata, use_calc_stream=True)
paddle.distributed.wait(tindata)""",
        """CODE.import paddle

paddle.hub.list('lyuwenyu/paddlehub_demo:main', source='github', force_reload=False)
""",
        """CODE.import paddle

x = paddle.to_tensor([1.0, 2.0, 3.0, 4.0])
m = paddle.nn.Silu()
out = m(x) # [ 0.731059, 1.761594, 2.857722, 3.928055 ]""",
        """CODE.import paddle
from collections import OrderedDict

sublayers = OrderedDict([
    ('conv1d', paddle.nn.Conv1D(3, 2, 3)),
    ('conv2d', paddle.nn.Conv2D(3, 2, 3)),
    ('conv3d', paddle.nn.Conv3D(4, 6, (3, 3, 3))),
])

layer_dict = paddle.nn.LayerDict(sublayers=sublayers)
for k in layer_dict.keys():
    print(k)""",
        """CODE.import paddle
from collections import OrderedDict

sublayers = OrderedDict([
    ('conv1d', paddle.nn.Conv1D(3, 2, 3)),
    ('conv2d', paddle.nn.Conv2D(3, 2, 3)),
    ('conv3d', paddle.nn.Conv3D(4, 6, (3, 3, 3))),
])

layer_dict = paddle.nn.LayerDict(sublayers=sublayers)
for k, v in layer_dict.items():
    print(k, ":", v)

#conv1d : Conv1D(3, 2, kernel_size=[3], data_format=NCL)
#conv2d : Conv2D(3, 2, kernel_size=[3, 3], data_format=NCHW)
#conv3d : Conv3D(4, 6, kernel_size=[3, 3, 3], data_format=NCDHW)""",
        """CODE.import paddle
from collections import OrderedDict

sublayers = OrderedDict([
    ('conv1d', paddle.nn.Conv1D(3, 2, 3)),
    ('conv2d', paddle.nn.Conv2D(3, 2, 3)),
    ('conv3d', paddle.nn.Conv3D(4, 6, (3, 3, 3))),
])

layer_dict = paddle.nn.LayerDict(sublayers=sublayers)
for v in layer_dict.values():
    print(v)

#Conv1D(3, 2, kernel_size=[3], data_format=NCL)
#Conv2D(3, 2, kernel_size=[3, 3], data_format=NCHW)
#Conv3D(4, 6, kernel_size=[3, 3, 3], data_format=NCDHW)""",
        """CODE.import paddle
from collections import OrderedDict

sublayers = OrderedDict([
    ('conv1d', paddle.nn.Conv1D(3, 2, 3)),
    ('conv2d', paddle.nn.Conv2D(3, 2, 3)),
    ('conv3d', paddle.nn.Conv3D(4, 6, (3, 3, 3))),
])

layer_dict = paddle.nn.LayerDict(sublayers=sublayers)
len(layer_dict)
#3

layer_dict.clear()
len(layer_dict)
#0""",
        """CODE.import paddle
from collections import OrderedDict

sublayers = OrderedDict([
    ('conv1d', paddle.nn.Conv1D(3, 2, 3)),
    ('conv2d', paddle.nn.Conv2D(3, 2, 3)),
    ('conv3d', paddle.nn.Conv3D(4, 6, (3, 3, 3))),
])

layer_dict = paddle.nn.LayerDict(sublayers=sublayers)
len(layer_dict)
#3

layer_dict.pop('conv2d')
len(layer_dict)
#2""",
        """CODE.import paddle
from collections import OrderedDict

sublayers = OrderedDict([
    ('conv1d', paddle.nn.Conv1D(3, 2, 3)),
    ('conv2d', paddle.nn.Conv2D(3, 2, 3)),
    ('conv3d', paddle.nn.Conv3D(4, 6, (3, 3, 3))),
])

new_sublayers = OrderedDict([
    ('relu', paddle.nn.ReLU()),
    ('conv2d', paddle.nn.Conv2D(4, 2, 4)),
])
layer_dict = paddle.nn.LayerDict(sublayers=sublayers)

layer_dict.update(new_sublayers)

for k, v in layer_dict.items():
    print(k, ":", v)
#conv1d : Conv1D(3, 2, kernel_size=[3], data_format=NCL)
#conv2d : Conv2D(4, 2, kernel_size=[4, 4], data_format=NCHW)
#conv3d : Conv3D(4, 6, kernel_size=[3, 3, 3], data_format=NCDHW)
#relu : ReLU()""",
        """CODE.import paddle
from paddle.autograd import PyLayer

# Inherit from PyLayer
class cus_tanh(PyLayer):
    @staticmethod
    def forward(ctx, x, func1, func2=paddle.square):
        # ctx is a context object that store some objects for backward.
        ctx.func = func2
        y = func1(x)
        # Pass tensors to backward.
        ctx.save_for_backward(y)
        return y

    @staticmethod
    # forward has only one output, so there is only one gradient in the input of backward.
    def backward(ctx, dy):
        # Get the tensors passed by forward.
        y, = ctx.saved_tensor()
        grad = dy * (1 - ctx.func(y))
        # forward has only one input, so only one gradient tensor is returned.
        return grad

data = paddle.randn([2, 3], dtype="float64")
data.stop_gradient = False
z = cus_tanh.apply(data, func1=paddle.tanh)
z.mean().backward()

print(data.grad)""",
        """CODE.import paddle
from paddle.autograd import PyLayer

class cus_tanh(PyLayer):
    @staticmethod
    def forward(ctx, x):
        # ctx is a context object that store some objects for backward.
        y = paddle.tanh(x)
        # Pass tensors to backward.
        ctx.save_for_backward(y)
        return y

    @staticmethod
    def backward(ctx, dy):
        # Get the tensors passed by forward.
        y, = ctx.saved_tensor()
        grad = dy * (1 - paddle.square(y))
        return grad""",
        """CODE.import paddle
from paddle.autograd import PyLayer

class cus_tanh(PyLayer):
    @staticmethod
    def forward(ctx, x):
        y = paddle.tanh(x)
        # Pass tensors to backward.
        ctx.save_for_backward(y)
        return y

    @staticmethod
    def backward(ctx, dy):
        # Get the tensors passed by forward.
        y, = ctx.saved_tensor()
        grad = dy * (1 - paddle.square(y))
        return grad""",
        """CODE.import paddle
from paddle.autograd import PyLayer

class cus_tanh(PyLayer):
    @staticmethod
    def forward(ctx, x, func1, func2=paddle.square):
        ctx.func = func2
        y = func1(x)
        # Pass tensors to backward.
        ctx.save_for_backward(y)
        return y

    @staticmethod
    def backward(ctx, dy):
        # Get the tensors passed by forward.
        y, = ctx.saved_tensor()
        grad = dy * (1 - ctx.func(y))
        return grad


data = paddle.randn([2, 3], dtype="float64")
data.stop_gradient = False
# run custom Layer.
z = cus_tanh.apply(data, func1=paddle.tanh)""",
        """CODE.import paddle
from paddle.distributed import init_parallel_env
init_parallel_env()
if paddle.distributed.ParallelEnv().rank == 0:
    data = paddle.to_tensor([7, 8, 9])
    paddle.distributed.send(data, dst=1)
else:
    data = paddle.to_tensor([1,2,3])
    paddle.distributed.recv(data, src=0)
out = data.numpy()""",
        """CODE.import paddle
import numpy as np
from collections import OrderedDict

sublayers = OrderedDict([
    ('conv1d', paddle.nn.Conv1D(3, 2, 3)),
    ('conv2d', paddle.nn.Conv2D(3, 2, 3)),
    ('conv3d', paddle.nn.Conv3D(4, 6, (3, 3, 3))),
])

layers_dict = paddle.nn.LayerDict(sublayers=sublayers)

l = layers_dict['conv1d']

for k in layers_dict:
    l = layers_dict[k]

len(layers_dict)
#3

del layers_dict['conv2d']
len(layers_dict)
#2

conv1d = layers_dict.pop('conv1d')
len(layers_dict)
#1

layers_dict.clear()
len(layers_dict)
#0""",
        """CODE.import paddle
import paddle.nn as nn

x = paddle.randn((100,3,224,224))
unfold = nn.Unfold(kernel_sizes=[3, 3])
result = unfold(x)
print(result)""",
        """CODE.import paddle
import paddle.nn.functional as F

x = paddle.to_tensor([1.0, 2.0, 3.0, 4.0])
out = F.silu(x) # [ 0.731059, 1.761594, 2.857722, 3.928055 ]""",
        """CODE.import paddle
import paddle.nn.functional as F
paddle.enable_static()
data = paddle.static.data(name='X', shape=[None, 1, 28, 28], dtype='float32')
conv2d = paddle.static.nn.conv2d(input=data, num_filters=6, filter_size=3)

with paddle.static.amp.bf16_guard():
    bn = paddle.static.nn.batch_norm(input=conv2d, act="relu")
    pool = F.max_pool2d(bn, kernel_size=2, stride=2)
    hidden = paddle.static.nn.fc(pool, size=10)
    loss = paddle.mean(hidden)""",
        """CODE.import paddle
import paddle.static as static

paddle.enable_static()

data = static.data(name='X', shape=[None, 1], dtype='float32')
hidden = static.nn.fc(x=data, size=10)
loss = paddle.mean(hidden)
optimizer = paddle.optimizer.Adam(learning_rate=0.001)

mp_optimizer = static.amp.decorate_bf16(optimizer=optimizer)

ops, param_grads = mp_optimizer.minimize(loss)

place = paddle.CPUPlace(0)
exe = paddle.static.Executor(place)
data = paddle.static.data(name='X', shape=[None, 1, 28, 28], dtype='float32')
conv2d = paddle.static.nn.conv2d(input=data, num_filters=6, filter_size=3)
# 1) Use bf16_guard to control the range of bf16 kernels used.
with paddle.static.amp.bf16_guard():
    bn = paddle.static.nn.batch_norm(input=conv2d, act="relu")
    pool = F.max_pool2d(bn, kernel_size=2, stride=2)
    hidden = paddle.static.nn.fc(pool, size=10)
    loss = paddle.mean(hidden)
# 2) Create the optimizer and set `multi_precision` to True.
# Setting `multi_precision` to True can avoid the poor accuracy
# or the slow convergence in a way.
optimizer = paddle.optimizer.Momentum(learning_rate=0.01, multi_precision=True)
# 3) These ops in `custom_fp32_list` will keep in the float32 computation type.
amp_list = paddle.static.amp.CustomOpLists(
    custom_fp32_list=['pool2d'])
# 4) The entry of Paddle AMP.
# Enable pure bf16 training by setting `use_pure_bf16` to True.
optimizer = paddle.static.amp.decorate_bf16(
    optimizer,
    amp_list,
    use_pure_bf16=True)
# If you don't use the default_startup_program(), you sholud pass
# your defined `startup_program` into `minimize`.
optimizer.minimize(loss)
exe.run(paddle.static.default_startup_program())
# 5) Use `amp_init` after FP32 parameters initialization(such as `exe.run(startup_program)`).
# If you want to perform the testing process, you should pass `test_program` into `amp_init`.
optimizer.amp_init(place, scope=paddle.static.global_scope())""",
        """CODE.import paddle
import paddle.static as static

paddle.enable_static()

x = static.data(name="x", shape=[10, 10], dtype='float32')
y = static.nn.fc(x, 10)
z = static.nn.fc(y, 10)

place = paddle.CPUPlace()
exe = static.Executor(place)
exe.run(static.default_startup_program())
prog = static.default_main_program()

path = "./temp/model.pdparams"
paddle.save(prog.state_dict(), path)
state_dict_load = paddle.load(path)
prog.set_state_dict(state_dict_load)""",
        """CODE.import paddle
import paddle.static as static
import numpy as np

paddle.enable_static()

x = static.data(name="x", shape=[10, 10], dtype='float32')

y = static.nn.fc(x, 10, name='fc')
place = paddle.CPUPlace()
exe = static.Executor(place)
prog = paddle.static.default_main_program()
exe.run(static.default_startup_program())
inputs = np.ones((10, 10), dtype='float32')
exe.run(prog, feed={'x': inputs}, fetch_list=[y, ])
path = 'temp/tensor_'
for var in prog.list_vars():
    if var.persistable:
        t = var.get_value()
        paddle.save(t, path+var.name+'.pdtensor')

for var in prog.list_vars():
    if var.persistable:
        t_load = paddle.load(path+var.name+'.pdtensor')
        var.set_value(t_load)""",
        """CODE.import paddle
paddle.distributed.init_parallel_env()
tindata = paddle.randn(shape=[2, 3])
gp = paddle.distributed.new_group([2,4,6])
paddle.distributed.all_reduce(tindata, group=gp, use_calc_stream=False)""",
        """CODE.import paddle
support_npu = paddle.is_compiled_with_npu()""",
        """CODE.import paddle.fluid as fluid
import paddle.fluid.profiler as profiler
import numpy as np

epoc = 8
dshape = [4, 3, 28, 28]
data = fluid.data(name='data', shape=[None, 3, 28, 28], dtype='float32')
conv = fluid.layers.conv2d(data, 20, 3, stride=[1, 1], padding=[1, 1])

place = fluid.NPUPlace(0)
exe = fluid.Executor(place)
exe.run(fluid.default_startup_program())

output_file = 'npu.txt'
with profiler.npu_profiler(output_file) as npu_prof:
    for i in range(epoc):
        input = np.random.random(dshape).astype('float32')
        exe.run(fluid.default_main_program(), feed={'data': input})""",
        """CODE.paddle.distributed.new_group([2,4,6])
paddle.distributed.get_group(gid.id)""",
        "CODE.parse_args()" .

<DEPENDENCY.paddlepaddle-gpu==2.1.1> <CONTAINS> """CODE.import paddle
support_gpu = paddle.is_compiled_with_rocm()""" .

<DEPENDENCY.paddlepaddle-gpu==2.1.2> <CONTAINS> """CODE.import paddle
paddle.enable_static()
x = paddle.static.data(name='x', shape=[3, 2, 1])
y = x.size()""" .

<DEPENDENCY.paddlepaddle-gpu==2.2.0> <CONTAINS> """CODE.import paddle
import paddle.distributed as dist

paddle.enable_static()

mesh = dist.ProcessMesh([[2, 4, 5], [0, 1, 3]])
mesh.set_placement([0, 1, 2, 3, 4, 5])""",
        """CODE.import paddle.distributed.auto_parallel as auto
from paddle.fluid.distributed_attribute import get_default_distributed_context
from paddle.distributed import fleet
from paddle.distributed.auto_parallel.partitioner import Partitioner

# create serial program with forward only
with static.program_guard(serial_main_program, serial_start_program):
    model = create_model(config)
    tokens = static.data(name="tokens", shape=[batch_size, sequence_len], dtype='int64')
    labels = static.data(name="labels", shape=[batch_size, sequence_len], dtype='int64')
    loss_mask = static.data(name="loss_mask", shape=[batch_size, sequence_len], dtype='int64')
    preds = model(tokens)
    loss = criterion(preds, labels, loss_mask)

# auto completion
auto.ProcessMesh(shape=[2, 4], process_group=[0, 1, 2, 3, 4, 5, 6, 7])
annotated_main_program = auto.complete_annotation(serial_main_program)
auto_paralle_context = get_default_distributed_context()

# distributed strategy & rank info
rank_id = paddle.distributed.get_rank()
dist_strategy = fleet.DistributedStrategy()

# create partitioner
Partitioner = Partitioner(dist_strategy, auto_paralle_context, rank_id)

# create dist program with forward only
# for distributed inference, using partitioned_main_prog from here
partitioned_main_prog, partitioned_startup_prog = Partitioner.transpile_forward(complete_train_program, start_program)

# create dist program with forward/backward/update
# for distributed training, using partitioned_main_prog from here
dist_params_grads = Partitioner.apply_backward(loss, complete_train_program, start_program, partitioned_main_prog, partitioned_startup_prog)
optimizer = paddle.fluid.optimizer.AdamOptimizer(
    learning_rate=0.00001,
    beta1=0.9,
    beta2=0.999,
    epsilon=1e-08,
    grad_clip=None)
opt_ops = Partitioner.apply_optimize(optimizer, dist_params_grads, partitioned_main_prog, partitioned_startup_prog)""" .

<DEPENDENCY.paddlepaddle-gpu==2.2.1> <CONTAINS> """CODE.import paddle

x = paddle.to_tensor([[0, 2, 3], [1, 4, 5], [2, 6, 7]], dtype="float32")
indexes = paddle.to_tensor([[0, 1], [1, 2], [2, 1], [0, 0]], dtype="int32")
src_index = indexes[:, 0]
dst_index = indexes[:, 1]
out = paddle.incubate.graph_send_recv(x, src_index, dst_index, pool_type="sum")""",
        """CODE.import paddle
import numpy as np

x = paddle.to_tensor([[1, 1, 1],
                      [0, 2, 1],
                      [0, 0,-1]], dtype="float64")
y = paddle.to_tensor([[0], [-9], [5]], dtype="float64")
out = paddle.linalg.triangular_solve(x, y, upper=True)

print(out)""" .

<DEPENDENCY.paddlepaddle-gpu==2.2.2> <CONTAINS> """CODE.import paddle

x = paddle.to_tensor([-5., 0., 5.])
m = paddle.nn.Mish()
out = m(x) # [-0.03357624, 0., 4.99955208]""" .

<DEPENDENCY.paddlepaddle-gpu==2.3.0> <CONTAINS> """CODE.import numpy as np
complete_tensor = np.array([[[1.11, 1.12, 1.13, 1.14, 1.15, 1.16]]])
rank = 2
complete_shape = [1, 1, 6]
dims_mapping = [-1, -1, 0]
process_shape = [3]
process_group = [0, 1, 2]

slice_tensor = _slice_tensor(complete_tensor, [[], [], [2, 4]], 3)

index = _get_sliced_index(rank, complete_shape, dims_mapping, process_shape, process_group)""",
        """CODE.import numpy as np
complete_tensor = np.array([[[1.11, 1.12, 1.13, 1.14, 1.15, 1.16]]])
rank = 2
complete_shape = [1, 1, 6]
dims_mapping = [-1, -1, 0]
process_shape = [3]
process_group = [0, 1, 2]

sliced_tensor_list = split(complete_tensor, [[], [], [2, 4]], 3)""",
        """CODE.import numpy as np
complete_tensors = np.arange(4).reshape([2, 2])
partitial_tensors = np.split(complete_tensors, 2, axis=0)
name = "tmp_0"
tensors_dict = {name: partitial_tensors}
strategy_1 = {
    name: {
        "process_shape": [2],
        "process_group": [0, 1],
        "dims_mapping": [0, -1]
    }
}
strategy_2 = {
    name: {
        "process_shape": [2],
        "process_group": [0, 1],
        "dims_mapping": [-1, -1]
    }
}
converter = Converter(tensors_dict, strategy_1, strategy_2)
result = converter.convert()""",
        """CODE.import numpy as np
def _get_split_indices(complete_shape, dims_mapping, process_shape, process_group):
    split_indices_list = []
    for i in range(len(complete_shape)):
        if dims_mapping[i] == -1:
            split_indices_list.append([])
        else:
            start = sum(process_shape[:process_group.index(dims_mapping[i])])
            end = start + process_shape[process_group.index(dims_mapping[i])]
            split_indices_list.append(list(range(start, end)))
    return split_indices_list

complete_tensor = np.array([[[1.11, 1.12, 1.13, 1.14, 1.15, 1.16]]])
complete_shape = [1, 1, 6]
dims_mapping = [-1, -1, 0]
process_shape = [3]
process_group = [0, 1, 2]

index = _get_split_indices(complete_shape, dims_mapping, process_shape, process_group)
print(index)""",
        """CODE.import paddle


def func(x, y):
    return paddle.matmul(x, y)


x = paddle.to_tensor([[1., 2.], [3., 4.]])
J = paddle.incubate.autograd.Jacobian(func, [x, x])
print(J[:, :])
# Tensor(shape=[4, 8], dtype=float32, place=Place(gpu:0), stop_gradient=False,
#        [[1., 3., 0., 0., 1., 0., 2., 0.],
#         [2., 4., 0., 0., 0., 1., 0., 2.],
#         [0., 0., 1., 3., 3., 0., 4., 0.],
#         [0., 0., 2., 4., 0., 3., 0., 4.]])

print(J[0, :])
# Tensor(shape=[8], dtype=float32, place=Place(gpu:0), stop_gradient=False,
#        [1., 3., 0., 0., 1., 0., 2., 0.])
print(J[:, 0])
# Tensor(shape=[4], dtype=float32, place=Place(gpu:0), stop_gradient=False,
#        [1., 2., 0., 0.]""",
        """CODE.import paddle


def reducer(x):
    return paddle.sum(x * x)


x = paddle.rand([2, 2])
h = paddle.incubate.autograd.Hessian(reducer, x)
print(h[:])
# Tensor(shape=[4, 4], dtype=float32, place=Place(gpu:0), stop_gradient=False,
#        [[2., 0., 0., 0.],
#         [0., 2., 0., 0.],
#         [0., 0., 2., 0.],
#         [0., 0., 0., 2.]])
""",
        """CODE.import paddle

# scale input
beta = paddle.distribution.Beta(alpha=0.5, beta=0.5)
print(beta.mean)
# Tensor(shape=[1], dtype=float32, place=CUDAPlace(0), stop_gradient=True,
#        [0.50000000])
print(beta.variance)
# Tensor(shape=[1], dtype=float32, place=CUDAPlace(0), stop_gradient=True,
#        [0.12500000])
print(beta.entropy())
# Tensor(shape=[1], dtype=float32, place=CUDAPlace(0), stop_gradient=True,
#        [0.12500000])

# tensor input with broadcast
beta = paddle.distribution.Beta(alpha=paddle.to_tensor([0.2, 0.4]), beta=0.6)
print(beta.mean)
# Tensor(shape=[2], dtype=float32, place=CUDAPlace(0), stop_gradient=True,
#        [0.25000000, 0.40000001])
print(beta.variance)
# Tensor(shape=[2], dtype=float32, place=CUDAPlace(0), stop_gradient=True,
#        [0.10416666, 0.12000000])
print(beta.entropy())
# Tensor(shape=[2], dtype=float32, place=CUDAPlace(0), stop_gradient=True,
#        [-1.91923141, -0.38095069])""",
        """CODE.import paddle

abs = paddle.distribution.AbsTransform()

print(abs.forward(paddle.to_tensor([-1., 0., 1.])))
# Tensor(shape=[3], dtype=float32, place=Place(gpu:0), stop_gradient=True,
#        [1., 0., 1.])

print(abs.inverse(paddle.to_tensor(1.)))
# (Tensor(shape=[1], dtype=float32, place=Place(gpu:0), stop_gradient=True,
#        [-1.]), Tensor(shape=[1], dtype=float32, place=Place(gpu:0), stop_gradient=True,
#        [1.]))

# The |dX/dY| is constant 1. So Log|dX/dY| == 0
print(abs.inverse_log_det_jacobian(paddle.to_tensor(1.)))
# (Tensor(shape=[], dtype=float32, place=Place(gpu:0), stop_gradient=True,
#        0.), Tensor(shape=[], dtype=float32, place=Place(gpu:0), stop_gradient=True,
#        0.))

#Special case handling of 0.
print(abs.inverse(paddle.to_tensor(0.))
# (Tensor(shape=[1], dtype=float32, place=Place(gpu:0), stop_gradient=True,
#        [0.]), Tensor(shape=[1], dtype=float32, place=Place(gpu:0), stop_gradient=True,
#        [0.]))
print(abs.inverse_log_det_jacobian(paddle.to_tensor(0.))
# (Tensor(shape=[], dtype=float32, place=Place(gpu:0), stop_gradient=True,
#        0.), Tensor(shape=[], dtype=float32, place=Place(gpu:0), stop_gradient=True,
#        0.))""",
        """CODE.import paddle

exp = paddle.distribution.ExpTransform()
print(exp.forward(paddle.to_tensor([1., 2., 3.]))

print(exp.inverse(paddle.to_tensor([1., 2., 3.]))

print(exp.forward_log_det_jacobian(paddle.to_tensor([1., 2., 3.]))

print(exp.inverse_log_det_jacobian(paddle.to_tensor([1., 2., 3.]))""",
        """CODE.import paddle

multinomial = paddle.distribution.Multinomial(10, paddle.to_tensor([0.2, 0.3, 0.5]))
print(multinomial.sample((2, 3)))



""",
        """CODE.import paddle

x = paddle.ones((1,2,3))
reshape_transform = paddle.distribution.ReshapeTransform((2, 3), (3, 2))
print(reshape_transform.forward_shape((1,2,3)))
print(reshape_transform.forward(x))
print(reshape_transform.inverse(reshape_transform.forward(x)))
print(reshape_transform.forward_log_det_jacobian(x))""",
        """CODE.import paddle

x = paddle.ones((2,3))
t = paddle.distribution.SigmoidTransform()
print(t.forward(x))
print(t.inverse(t.forward(x)))
print(t.forward_log_det_jacobian(x))""",
        """CODE.import paddle

x = paddle.ones((2,3))
t = paddle.distribution.SoftmaxTransform()
print(t.forward(x))
print(t.inverse(t.forward(x)))""",
        """CODE.import paddle

x = paddle.stack(
    (paddle.to_tensor([1., 2., 3.]), paddle.to_tensor([1, 2., 3.])), 1)
t = paddle.distribution.StackTransform(
    (paddle.distribution.ExpTransform(),
    paddle.distribution.PowerTransform(paddle.to_tensor(2.))),
    1
)
print(t.forward(x))
print(t.inverse(t.forward(x)))
print(t.forward_log_det_jacobian(x))""",
        """CODE.import paddle

x = paddle.to_tensor([1., 2.])
affine = paddle.distribution.AffineTransform(paddle.to_tensor(0.), paddle.to_tensor(1.))

print(affine.forward(x))
print(affine.inverse(affine.forward(x)))
print(affine.forward_log_det_jacobian(x))""",
        """CODE.import paddle

x = paddle.to_tensor([1., 2.])
power = paddle.distribution.PowerTransform(paddle.to_tensor(2.))

print(power.forward(x))
print(power.inverse(power.forward(x)))
print(power.forward_log_det_jacobian(x))""",
        """CODE.import paddle

x = paddle.to_tensor([1.,2.,3.])
t = paddle.distribution.StickBreakingTransform()
print(t.forward(x))
print(t.inverse(t.forward(x)))
print(t.forward_log_det_jacobian(x))""",
        """CODE.import paddle
from paddle.autograd import PyLayer

class cus_tanh(PyLayer):
    @staticmethod
    def forward(ctx, x):
        # ctx is a context object that store some objects for backward.
        y = paddle.tanh(x)
        # Pass tensors to backward.
        ctx.save_for_backward(y)
        return y

    @staticmethod
    def backward(ctx, dy):
        # Get the tensors passed by forward.
        y, = ctx.saved_tensor()
        grad = dy * (1 - paddle.square(y))
        return grad""",
        """CODE.import paddle
from paddle.autograd import PyLayer

class cus_tanh(PyLayer):
    @staticmethod
    def forward(ctx, x):
        y = paddle.tanh(x)
        # Pass tensors to backward.
        ctx.save_for_backward(y)
        return y

    @staticmethod
    def backward(ctx, dy):
        # Get the tensors passed by forward.
        y, = ctx.saved_tensor()
        grad = dy * (1 - paddle.square(y))
        return grad""",
        """CODE.import paddle
from paddle.autograd import PyLayer
import numpy as np

class Tanh(PyLayer):
    @staticmethod
    def forward(ctx, x):
        a = x + x
        b = x + x + x
        ctx.mark_non_differentiable(a)
        return a, b

    @staticmethod
    def backward(ctx, grad_a, grad_b):
        assert np.equal(grad_a.numpy(), paddle.zeros([1]).numpy())
        assert np.equal(grad_b.numpy(), paddle.ones([1], dtype="float64").numpy())
        return grad_b

x = paddle.ones([1], dtype="float64")
x.stop_gradient = False
a, b = Tanh.apply(x)
b.sum().backward()""",
        """CODE.import paddle
from paddle.autograd import PyLayer
import numpy as np

class Tanh(PyLayer):
    @staticmethod
    def forward(ctx, x):
        return x, x+x

    @staticmethod
    def backward(ctx, grad, grad2):
        assert np.equal(grad2.numpy(), paddle.zeros([1]).numpy())
        return grad

class Tanh2(PyLayer):
    @staticmethod
    def forward(ctx, x):
        ctx.set_materialize_grads(False)
        return x, x+x

    @staticmethod
    def backward(ctx, grad, grad2):
        assert grad2==None
        return grad

x = paddle.ones([1], dtype="float64")
x.stop_gradient = False
Tanh.apply(x)[0].backward()

x2 = paddle.ones([1], dtype="float64")
x2.stop_gradient = False
Tanh2.apply(x2)[0].backward()""",
        """CODE.import paddle
from paddle.distribution import independent

beta = paddle.distribution.Beta(paddle.to_tensor([0.5, 0.5]), paddle.to_tensor([0.5, 0.5]))
print(beta.batch_shape, beta.event_shape)
# (2,) ()
print(beta.log_prob(paddle.to_tensor(0.2)))
# Tensor(shape=[2], dtype=float32, place=Place(gpu:0), stop_gradient=True,
#        [-0.22843921, -0.22843921])
reinterpreted_beta = independent.Independent(beta, 1)
print(reinterpreted_beta.batch_shape, reinterpreted_beta.event_shape)
# () (2,)
print(reinterpreted_beta.log_prob(paddle.to_tensor([0.2,  0.2])))
# Tensor(shape=[1], dtype=float32, place=Place(gpu:0), stop_gradient=True,
#        [-0.45687842])""",
        """CODE.import paddle
import paddle.static as static

paddle.enable_static()

a = static.data(name='data', shape=[None, 1], dtype='int32')
b = a + 1
main_prog = static.default_main_program()

ipu_strategy = static.IpuStrategy()
ipu_strategy.set_graph_config(num_ipus=1, is_training=True, micro_batch_size=1)
ipu_strategy.set_pipelining_config(enable_pipelining=False, batches_per_step=1, enable_gradient_accumulation=False, accumulation_factor=1)
ipu_strategy.set_precision_config(enable_fp16=False)

ipu_compiled_program = static.IpuCompiledProgram(
    main_prog,
    ipu_strategy=ipu_strategy)""",
        """CODE.import paddle
import paddle.static as static

paddle.enable_static()

a = static.data(name='data', shape=[None, 1], dtype='int32')
b = a + 1
main_prog = static.default_main_program()

ipu_strategy = static.IpuStrategy()
ipu_strategy.set_graph_config(num_ipus=1, is_training=True, micro_batch_size=1)
ipu_strategy.set_pipelining_config(enable_pipelining=False, batches_per_step=1, enable_gradient_accumulation=False, accumulation_factor=1)
ipu_strategy.set_precision_config(enable_fp16=False)

program = static.IpuCompiledProgram(
    main_prog,
    ipu_strategy=ipu_strategy).compile([a.name], [b.name])""",
        """CODE.import paddle
import paddle.static as static

paddle.enable_static()

ipu_strategy = static.IpuStrategy()""",
        """CODE.import paddle
import paddle.static as static

paddle.enable_static()

ipu_strategy = static.IpuStrategy()
ipu_strategy.add_custom_op('paddle_relu', 'popart_relu')""",
        """CODE.import paddle
import paddle.static as static

paddle.enable_static()

ipu_strategy = static.IpuStrategy()
ipu_strategy.enable_pattern("ViewSimplifyPattern")""",
        """CODE.import paddle
import paddle.static as static

paddle.enable_static()

ipu_strategy = static.IpuStrategy()
ipu_strategy.set_pipelining_config(enable_pipelining=False,
                                    batches_per_step=1,
                                    enable_gradient_accumulation=False,
                                    accumulation_factor=1)""",
        """CODE.import paddle
import paddle.static as static

paddle.enable_static()

ipu_strategy = static.IpuStrategy()
ipu_strategy.set_precision_config(enable_fp16=False)""",
        """CODE.import paddle
import paddle.static as static

paddle.enable_static()

ipu_strategy = static.IpuStrategy()
num_ipus = ipu_strategy.get_option('num_ipus')""",
        """CODE.import paddle
import paddle.static as static

paddle.enable_static()

ipu_strategy = static.IpuStrategy()
options = {'num_ipus':1, 'enable_fp16': True}
ipu_strategy.set_options(options)""",
        """CODE.import paddle
paddle.enable_static()

sparse_feature_dim = 1024
embedding_size = 64

shows = paddle.static.data(name='show', shape=[1], dtype='int64')
clicks = paddle.static.data(name='click', shape=[1], dtype='int64')
input = paddle.static.data(name='ins', shape=[1], dtype='int64')

entry = paddle.distributed.ShowClickEntry("show", "click")

emb = paddle.static.nn.sparse_embedding(
    input=input,
    size=[sparse_feature_dim, embedding_size],
    is_test=False,
    entry=entry,
    param_attr=paddle.ParamAttr(name="SparseFeatFactors",
                               initializer=paddle.nn.initializer.Uniform()))""",
        """CODE.import paddle
tanh = paddle.distribution.TanhTransform()
x = paddle.to_tensor([[1., 2., 3.], [4., 5., 6.]])
print(tanh.forward(x))
print(tanh.inverse(tanh.forward(x)))
print(tanh.forward_log_det_jacobian(x))
print(tanh.inverse_log_det_jacobian(tanh.forward(x)))""",
        """CODE.import paddle
x = paddle.to_tensor([0., 1., 2., 3.])
chain = paddle.distribution.ChainTransform((
    paddle.distribution.AffineTransform(
        paddle.to_tensor(0.), paddle.to_tensor(1.)),
    paddle.distribution.ExpTransform()
))
print(chain.forward(x))
print(chain.inverse(chain.forward(x)))
print(chain.forward_log_det_jacobian(x))
print(chain.inverse_log_det_jacobian(chain.forward(x)))""",
        """CODE.import paddle
from paddle.distribution import transformed_distribution

d = transformed_distribution.TransformedDistribution(
    paddle.distribution.Normal(0., 1.),
    [paddle.distribution.AffineTransform(paddle.to_tensor(1.), paddle.to_tensor(2.))]
)

print(d.sample([10]))
# Tensor(shape=[10], dtype=float32, place=Place(gpu:0), stop_gradient=True,
#        [-0.10697651,  3.33609009, -0.86234951,  5.07457638,  0.75925219,
#         -4.17087793,  2.22579336, -0.93845034,  0.66054249,  1.50957513])
print(d.log_prob(paddle.to_tensor(0.5)))
# Tensor(shape=[1], dtype=float32, place=Place(gpu:0), stop_gradient=True,
#        [-1.64333570])""",
        """CODE.ipu_strategy = static.IpuStrategy()
ipu_strategy.disable_pattern("ViewSimplifyPattern")""",
        """CODE.ipu_strategy = static.IpuStrategy()
ipu_strategy.set_graph_config(num_ipus=1,
                            is_training=True,
                            micro_batch_size=1,
                            enable_manual_shard=False)""" .

<DEPENDENCY.paddlepaddle-gpu==2.4.0> <CONTAINS> "CODE.model = EfficientNet.from_pretrained('efficientnet-b0')" .

<DEPENDENCY.paddlepaddle-gpu==2.5.0> <CONTAINS> """CODE.import numpy as np
import paddle
from paddle.nn import RNNTLoss

fn = RNNTLoss(reduction='sum', fastemit_lambda=0.0)

acts = np.array([[[[0.1, 0.6, 0.1, 0.1, 0.1],
                [0.1, 0.1, 0.6, 0.1, 0.1],
                [0.1, 0.1, 0.2, 0.8, 0.1]],
                [[0.1, 0.6, 0.1, 0.1, 0.1],
                [0.1, 0.1, 0.2, 0.1, 0.1],
                [0.7, 0.1, 0.2, 0.1, 0.1]]]])
labels = [[1, 2]]

acts = paddle.to_tensor(acts, stop_gradient=False)

lengths = [acts.shape[1]] * acts.shape[0]
label_lengths = [len(l) for l in labels]
labels = paddle.to_tensor(labels, paddle.int32)
lengths = paddle.to_tensor(lengths, paddle.int32)
label_lengths = paddle.to_tensor(label_lengths, paddle.int32)

costs = fn(acts, labels, lengths, label_lengths)
print(costs)""",
        """CODE.import paddle

# use as generator

x = paddle.to_tensor([1.], stop_gradient=False)
with paddle.no_grad():
    with paddle.enable_grad():
        y = x * 2
assert(y.stop_gradient == False)
y.backward()
assert(x.grad is not None)

# use as decorator

@paddle.enable_grad()
def double(x):
    return x * 2

with paddle.no_grad():
    z = double(x)

assert(z.stop_gradient == False)""",
        """CODE.import paddle

checker_config = paddle.amp.debugging.TensorCheckerConfig(enable=True, debug_mode=paddle.amp.debugging.DebugMode.CHECK_NAN_INF)
paddle.amp.debugging.enable_tensor_checker(checker_config)

x = paddle.to_tensor([1, 0, 3], place=paddle.CPUPlace(), dtype='float32', stop_gradient=False)
y = paddle.to_tensor([0.2, 0, 0.5], place=paddle.CPUPlace(), dtype='float32')
res = paddle.pow(x, y)
paddle.autograd.backward(res, retain_graph=True)
paddle.amp.debugging.disable_tensor_checker()""",
        """CODE.import paddle

class Exp(paddle.autograd.PyLayer):
    @staticmethod
    def forward(ctx, x):
        ctx.mark_not_inplace(x)
        return x

    @staticmethod
    def backward(ctx, grad_output):
        out = grad_output.exp()
        return out

x = paddle.randn((1, 1))
x.stop_gradient = False
attn_layers = []
for idx in range(0, 2):
    attn_layers.append(Exp())

for step in range(0, 2):
    a = x
    for j in range(0,2):
        a = attn_layers[j].apply(x)
    a.backward()""",
        """CODE.import paddle

def pack_hook(x):
    print("Packing", x)
    return x.numpy()

def unpack_hook(x):
    print("UnPacking", x)
    return paddle.to_tensor(x)

a = paddle.ones([3,3])
b = paddle.ones([3,3]) * 2
a.stop_gradient = False
b.stop_gradient = False
with paddle.autograd.saved_tensors_hooks(pack_hook, unpack_hook):
    y = paddle.multiply(a, b)
y.sum().backward()

import paddle
from paddle.autograd import PyLayer

class cus_multiply(PyLayer):
    @staticmethod
    def forward(ctx, a, b):
        y = paddle.multiply(a, b)
        ctx.save_for_backward(a, b)
        return y

    @staticmethod
    def backward(ctx, dy):
        a,b = ctx.saved_tensor()
        grad_a = dy * a
        grad_b = dy * b
        return grad_a, grad_b

def pack_hook(x):
    print("Packing", x)
    return x.numpy()

def unpack_hook(x):
    print("UnPacking", x)
    return paddle.to_tensor(x)

a = paddle.ones([3,3])
b = paddle.ones([3,3]) * 2
a.stop_gradient = False
b.stop_gradient = False
with paddle.autograd.saved_tensors_hooks(pack_hook, unpack_hook):
    y = cus_multiply.apply(a, b)
y.sum().backward()""",
        """CODE.import paddle

m = paddle.distribution.Laplace(paddle.to_tensor(0.0), paddle.to_tensor(1.0)
value = paddle.to_tensor(0.1)
m.log_prob(value)""",
        """CODE.import paddle

m = paddle.distribution.Laplace(paddle.to_tensor(0.0), paddle.to_tensor(1.0))
m.entropy()""",
        """CODE.import paddle

m = paddle.distribution.Laplace(paddle.to_tensor(0.0), paddle.to_tensor(1.0))
m.sample()  # Laplace distributed with loc=0, scale=1
# Tensor(shape=[], dtype=float32, place=Place(cpu), stop_gradient=True,
#        3.68546247)""",
        """CODE.import paddle

m = paddle.distribution.Laplace(paddle.to_tensor(0.0), paddle.to_tensor(1.0))
value = paddle.to_tensor(0.1)
m.icdf(value)""",
        """CODE.import paddle

m = paddle.distribution.Laplace(paddle.to_tensor([0.0]), paddle.to_tensor([1.0]))
m.rsample((1,))  # Laplace distributed with loc=0, scale=1
# Tensor(shape=[1, 1], dtype=float32, place=Place(cpu), stop_gradient=True,
# [[0.04337667]])""",
        """CODE.import paddle

paddle.set_device('custom_cpu')
s = paddle.device.Stream()
data1 = paddle.ones(shape=[20])
data2 = paddle.ones(shape=[20])
data3 = data1 + data2
with paddle.device.stream_guard(s):
    s.wait_stream(paddle.device.default_stream())
    data4 = data1 + data3""",
        """CODE.import paddle

paddle.set_device('custom_cpu')
s = paddle.device.Stream()
s.synchronize()""",
        """CODE.import paddle

x = paddle.randn(shape=[4, 6, 8])
shape = [2, 3]
axis = 1
unflatten = paddle.nn.Unflatten(axis, shape)
res = unflatten(x)
print(res.shape)""",
        """CODE.import paddle

x = paddle.to_tensor([[1, 2, 3], [4, 5, 6]], dtype="float32")
m = paddle.incubate.nn.FusedDropout(p=0.5)

y_train = m(x)
print(y_train)

m.eval()  # switch the model to test phase
y_test = m(x)
print(y_test)""",
        """CODE.import paddle
from paddle.autograd import PyLayer

class cus_tanh(PyLayer):
    @staticmethod
    def forward(ctx, x):
        # ctx is a context object that store some objects for backward.
        y = paddle.tanh(x)
        # Pass tensors to backward.
        ctx.save_for_backward(y)
        return y

    @staticmethod
    def backward(ctx, dy):
        # Get the tensors passed by forward.
        y, = ctx.saved_tensor()
        grad = dy * (1 - paddle.square(y))
        return grad""",
        """CODE.import paddle
from paddle.autograd import PyLayer

class cus_tanh(PyLayer):
    @staticmethod
    def forward(ctx, x):
        y = paddle.tanh(x)
        # Pass tensors to backward.
        ctx.save_for_backward(y)
        return y

    @staticmethod
    def backward(ctx, dy):
        # Get the tensors passed by forward.
        y, = ctx.saved_tensor()
        grad = dy * (1 - paddle.square(y))
        return grad""",
        """CODE.import paddle
from paddle.autograd import PyLayer
import numpy as np

class Tanh(PyLayer):
    @staticmethod
    def forward(ctx, x):
        a = x + x
        b = x + x + x
        ctx.mark_non_differentiable(a)
        return a, b

    @staticmethod
    def backward(ctx, grad_a, grad_b):
        assert np.equal(grad_a.numpy(), paddle.zeros([1]).numpy())
        assert np.equal(grad_b.numpy(), paddle.ones([1], dtype="float64").numpy())
        return grad_b

x = paddle.ones([1], dtype="float64")
x.stop_gradient = False
a, b = Tanh.apply(x)
b.sum().backward()""",
        """CODE.import paddle
from paddle.autograd import PyLayer
import numpy as np

class Tanh(PyLayer):
    @staticmethod
    def forward(ctx, x):
        return x+x+x, x+x

    @staticmethod
    def backward(ctx, grad, grad2):
        assert np.equal(grad2.numpy(), paddle.zeros([1]).numpy())
        return grad

class Tanh2(PyLayer):
    @staticmethod
    def forward(ctx, x):
        ctx.set_materialize_grads(False)
        return x+x+x, x+x

    @staticmethod
    def backward(ctx, grad, grad2):
        assert grad2==None
        return grad

x = paddle.ones([1], dtype="float64")
x.stop_gradient = False
Tanh.apply(x)[0].backward()

x2 = paddle.ones([1], dtype="float64")
x2.stop_gradient = False
Tanh2.apply(x2)[0].backward()""",
        """CODE.import paddle
from paddle.distribution import Bernoulli

paddle.seed(2023)

rv = Bernoulli(paddle.full((), 0.3))
print(rv.sample([100]).shape)

rv = Bernoulli(0.3)
print(rv.rsample([100]).shape)

rv = Bernoulli(paddle.to_tensor([0.3, 0.5]))
print(rv.rsample([100]).shape)

rv = Bernoulli(paddle.to_tensor([0.3, 0.5]))
print(rv.rsample([100, 2]).shape)

rv = Bernoulli(0.3)
rsample = rv.rsample([3, ])
rsample_sigmoid = paddle.nn.functional.sigmoid(rsample)
print(rsample, rsample_sigmoid)

print(paddle.nn.functional.sigmoid(rv.rsample([1000, ], temperature=1.0)).sum())

print(paddle.nn.functional.sigmoid(rv.rsample([1000, ], temperature=0.1)).sum())""",
        """CODE.import paddle
from paddle.distribution import Bernoulli

rv = Bernoulli(0.3)
print(rv.cdf(paddle.to_tensor([1.0])))""",
        """CODE.import paddle
from paddle.distribution import Bernoulli

rv = Bernoulli(0.3)
print(rv.entropy())""",
        """CODE.import paddle
from paddle.distribution import Bernoulli

rv = Bernoulli(0.3)
print(rv.log_prob(paddle.to_tensor([1.0]))""",
        """CODE.import paddle
from paddle.distribution import Bernoulli

rv = Bernoulli(paddle.full((), 0.3))
print(rv.sample([100]).shape)

rv = Bernoulli(paddle.to_tensor(0.3))
print(rv.sample([100]).shape)

rv = Bernoulli(paddle.to_tensor([0.3, 0.5]))
print(rv.sample([100]).shape)

rv = Bernoulli(paddle.to_tensor([0.3, 0.5]))
print(rv.sample([100, 2]).shape)""",
        """CODE.import paddle
from paddle.distribution import Cauchy

# init Cauchy with float
rv = Cauchy(loc=0.1, scale=1.2)
print(rv.cdf(paddle.to_tensor(1.5)))

# broadcast to value
rv = Cauchy(loc=0.1, scale=1.2)
print(rv.cdf(paddle.to_tensor([1.5, 5.1])))

# init Cauchy with N-Dim tensor
rv = Cauchy(loc=paddle.to_tensor([0.1, 0.1]), scale=paddle.to_tensor([1.0, 2.0]))
print(rv.cdf(paddle.to_tensor([1.5, 5.1])))

# init Cauchy with N-Dim tensor with broadcast
rv = Cauchy(loc=paddle.to_tensor(0.1), scale=paddle.to_tensor([1.0, 2.0]))
print(rv.cdf(paddle.to_tensor([1.5, 5.1])))

""",
        """CODE.import paddle
from paddle.distribution import Cauchy

# init Cauchy with float
rv = Cauchy(loc=0.1, scale=1.2)
print(rv.entropy())

# init Cauchy with N-Dim tensor
rv = Cauchy(loc=paddle.to_tensor(0.1), scale=paddle.to_tensor([1.0, 2.0]))
print(rv.entropy())""",
        """CODE.import paddle
from paddle.distribution import Cauchy

# init Cauchy with float
rv = Cauchy(loc=0.1, scale=1.2)
print(rv.entropy())
# Tensor(shape=[], dtype=float32, place=Place(cpu), stop_gradient=True,
#        2.71334577)

# init Cauchy with N-Dim tensor
rv = Cauchy(loc=paddle.to_tensor(0.1), scale=paddle.to_tensor([1.0, 2.0]))
print(rv.entropy())
# Tensor(shape=[2], dtype=float32, place=Place(cpu), stop_gradient=True,
#        [2.53102422, 3.22417140])""",
        """CODE.import paddle
from paddle.distribution import Cauchy

# init Cauchy with float
rv = Cauchy(loc=0.1, scale=1.2)
print(rv.log_prob(paddle.to_tensor(1.5))

# broadcast to value
rv = Cauchy(loc=0.1, scale=1.2)
print(rv.log_prob(paddle.to_tensor([1.5, 5.1]))

# init Cauchy with N-Dim tensor
rv = Cauchy(loc=paddle.to_tensor([0.1, 0.1]), scale=paddle.to_tensor([1.0, 2.0]))
print(rv.log_prob(paddle.to_tensor([1.5, 5.1]))

# init Cauchy with N-Dim tensor with broadcast
rv = Cauchy(loc=paddle.to_tensor(0.1), scale=paddle.to_tensor([1.0, 2.0]))
print(rv.log_prob(paddle.to_tensor([1.5, 5.1]))""",
        """CODE.import paddle
from paddle.distribution import Cauchy

# init Cauchy with float
rv = Cauchy(loc=0.1, scale=1.2)
print(rv.rsample([10]).shape)

# init Cauchy with 0-Dim tensor
rv = Cauchy(loc=paddle.full((), 0.1), scale=paddle.full((), 1.2))
print(rv.rsample([10]).shape)

# init Cauchy with N-Dim tensor
rv = Cauchy(loc=paddle.to_tensor(0.1), scale=paddle.to_tensor([1.0, 2.0]))
print(rv.rsample([10]).shape)

# sample 2-Dim data
rv = Cauchy(loc=0.1, scale=1.2)
print(rv.rsample([10, 2]).shape)

rv = Cauchy(loc=paddle.to_tensor(0.1), scale=paddle.to_tensor([1.0, 2.0]))
print(rv.rsample([10, 2]).shape)""",
        """CODE.import paddle
from paddle.distribution import Cauchy

# init Cauchy with float
rv = Cauchy(loc=0.1, scale=1.2)
print(rv.sample([10]).shape)

# init Cauchy with 0-Dim tensor
rv = Cauchy(loc=paddle.full((), 0.1), scale=paddle.full((), 1.2))
print(rv.sample([10]).shape)

# init Cauchy with N-Dim tensor
rv = Cauchy(loc=paddle.to_tensor(0.1), scale=paddle.to_tensor([1.0, 2.0]))
print(rv.sample([10]).shape)

# sample 2-Dim data
rv = Cauchy(loc=0.1, scale=1.2)
print(rv.sample([10, 2]).shape)

rv = Cauchy(loc=paddle.to_tensor(0.1), scale=paddle.to_tensor([1.0, 2.0]))
print(rv.sample([10, 2]).shape)""",
        """CODE.import paddle
from paddle.distribution import Cauchy

rv = Cauchy(loc=0.1, scale=1.2)
rv_other = Cauchy(loc=paddle.to_tensor(1.2), scale=paddle.to_tensor([2.3, 3.4]))
print(rv.kl_divergence(rv_other))""",
        """CODE.import paddle
from paddle.distribution import Geometric

geom = Geometric(0.5)
geom.entropy()""",
        """CODE.import paddle
from paddle.distribution import Geometric

geom = Geometric(0.5)
geom.log_pmf(2)""",
        """CODE.import paddle
from paddle.distribution import Geometric

geom = Geometric(0.5)
geom.sample((2,2))""",
        """CODE.import paddle
from paddle.distribution import LogNormal

# Define a single scalar LogNormal distribution.
dist = LogNormal(loc=0., scale=3.)
# Define a batch of two scalar valued LogNormals.
# The underlying Normal of first has mean 1 and standard deviation 11, the underlying Normal of second 2 and 22.
dist = LogNormal(loc=[1., 2.], scale=[11., 22.])
# Get 3 samples, returning a 3 x 2 tensor.
dist.sample((3, ))

# Define a batch of two scalar valued LogNormals.
# Their underlying Normal have mean 1, but different standard deviations.
dist = LogNormal(loc=1., scale=[11., 22.])

# Complete example
value_tensor = paddle.to_tensor([0.8], dtype="float32")

lognormal_a = LogNormal([0.], [1.])
lognormal_b = LogNormal([0.5], [2.])
sample = lognormal_a.sample((2, ))
# a random tensor created by lognormal distribution with shape: [2, 1]
entropy = lognormal_a.entropy()
# [1.4189385] with shape: [1]
lp = lognormal_a.log_prob(value_tensor)
# [-0.72069150] with shape: [1]
p = lognormal_a.probs(value_tensor)
# [0.48641577] with shape: [1]
kl = lognormal_a.kl_divergence(lognormal_b)
# [0.34939718] with shape: [1]""",
        """CODE.import paddle
from paddle.distribution.gumbel import Gumbel

# Gumbel distributed with loc=0, scale=1
dist = Gumbel(paddle.full([1], 0.0), paddle.full([1], 1.0)
dist.sample([2])
value = paddle.full([1], 0.5)
dist.prob(value)
dist.log_prob(value)
dist.cdf(value)
dist.entropy()
dist.rsample([2])""",
        """CODE.import paddle
from paddle.incubate.nn.layer.fused_dropout_add import FusedDropoutAdd

x = paddle.to_tensor([[1,2,3], [4,5,6]], dtype="float32")
y = paddle.to_tensor([[1,2,3], [4,5,6]], dtype="float32")

m = FusedDropoutAdd(p=0.5)

out = m(x, y)""",
        """CODE.import paddle
from paddle.incubate.nn.layer.fused_ec_moe import FusedEcMoe

x = paddle.randn([10, 128, 1024]) # [bsz, seq_len, d_model]
gate = paddle.randn([10, 128, 8]) # [bsz, seq_len, num_experts]
moe = FusedEcMoe(1024, 4096, 8, act_type="gelu")
y = moe(x, gate)
print(y.shape) # [10, 128, 1024]""",
        """CODE.import paddle
import numpy as np
from paddle.incubate.optimizer import LBFGS

paddle.disable_static()
np.random.seed(0)
np_w = np.random.rand(1).astype(np.float32)
np_x = np.random.rand(1).astype(np.float32)

inputs = [np.random.rand(1).astype(np.float32) for i in range(10)]
# y = 2x
targets = [2 * x for x in inputs]

class Net(paddle.nn.Layer):
    def __init__(self):
        super().__init__()
        w = paddle.to_tensor(np_w)
        self.w = paddle.create_parameter(shape=w.shape, dtype=w.dtype, default_initializer=paddle.nn.initializer.Assign(w))

    def forward(self, x):
        return self.w * x

net = Net()
opt = LBFGS(learning_rate=1, max_iter=1, max_eval=None, tolerance_grad=1e-07, tolerance_change=1e-09, history_size=100, line_search_fn='strong_wolfe', parameters=net.parameters())
def train_step(inputs, targets):
    def closure():
        outputs = net(inputs)
        loss = paddle.nn.functional.mse_loss(outputs, targets)
        print('loss: ', loss.item())
        opt.clear_grad()
        loss.backward()
        return loss
    opt.step(closure)


for input, target in zip(inputs, targets):
    input = paddle.to_tensor(input)
    target = paddle.to_tensor(target)
    train_step(input, target)""",
        """CODE.import paddle
import paddle.distributed as dist
from paddle.distributed.fleet.base.strategy_group import DPGroup, MPGroup, PPGroup
from paddle.distributed.fleet.base.orthogonal_strategy import OrthogonalStrategy

dist.init_parallel_env()
strategy = OrthogonalStrategy([("dp", 2, DPGroup), ("mp", 2, MPGroup), ("pp", 2, PPGroup)], fused_strategy_dict={"check": ["mp", "pp"]})""",
        """CODE.import paddle
paddle.set_device('custom_cpu')
s = paddle.device.Stream()
e1 = s.record_event()

e2 = paddle.device.Event()
s.record_event(e2)""",
        """CODE.import paddle
paddle.set_device('custom_cpu')
s = paddle.device.Stream()
s.query()""",
        """CODE.import paddle
paddle.set_device('custom_cpu')
s1 = paddle.device.Stream()
s2 = paddle.device.Stream()
s1.wait_stream(s2)""",
        """CODE.import paddle
s1 = paddle.device.Stream()
s2 = paddle.device.Stream()
e = paddle.device.Event()
e.record(s1)
s2.wait_event(e)""",
        """CODE.import paddle
x = paddle.to_tensor([1.], stop_gradient=False)
is_train = False
with paddle.set_grad_enabled(is_train):
    y = x * 2
assert(y.stop_gradient == True)

paddle.set_grad_enabled(True)
y = x * 2
assert(y.stop_gradient == False)

paddle.set_grad_enabled(False)
y = x * 2
assert(y.stop_gradient == True)""",
        """CODE.import paddle.distributed as dist
from paddle.distributed.fleet.base.strategy_group import StrategyGroupBase

dist.init_parallel_env()
strategy_group = dist.fleet.base.strategy_group.StrategyGroupBase([[0, 1], [2, 3]])
print(strategy_group.world_size)  # 2""" .

<DEPENDENCY.paddlepaddle-gpu==2.5.2> <CONTAINS> """CODE.import paddle
dataset = paddle.distributed.fleet.DatasetBase()
dataset._set_batch_size(128)""",
        """CODE.import paddle
dataset = paddle.distributed.fleet.DatasetBase()
dataset._set_download_cmd("./read_from_afs")""",
        """CODE.import paddle
dataset = paddle.distributed.fleet.DatasetBase()
dataset._set_hdfs_config("my_fs_name", "my_fs_ugi")""",
        """CODE.import paddle
dataset = paddle.distributed.fleet.DatasetBase()
dataset._set_thread(12)""",
        """CODE.import paddle
dataset = paddle.distributed.fleet.DatasetBase()
dataset._set_use_var([data, label])""",
        """CODE.import paddle
dataset = paddle.distributed.fleet.DatasetBase()
print(dataset._desc())""",
        """CODE.import paddle
dataset = paddle.distributed.fleet.dataset.DatasetBase()
dataset._set_pipe_command("python my_script.py")""",
        """CODE.import paddle
paddle.enable_static()
dataset = paddle.distributed.InMemoryDataset()
dataset._set_fleet_send_batch_size(800)""",
        """CODE.import paddle
paddle.enable_static()
dataset = paddle.distributed.InMemoryDataset()
dataset._set_fleet_send_sleep_seconds(2)""",
        """CODE.import paddle
paddle.enable_static()
dataset = paddle.distributed.InMemoryDataset()
dataset._set_merge_by_lineid()""",
        """CODE.import paddle
paddle.enable_static()
dataset = paddle.distributed.InMemoryDataset()
dataset._set_parse_content(True)""",
        """CODE.import paddle
paddle.enable_static()
dataset = paddle.distributed.InMemoryDataset()
dataset._set_parse_ins_id(True)""",
        """CODE.import paddle
paddle.enable_static()
dataset = paddle.distributed.InMemoryDataset()
dataset._set_queue_num(12)""" .

<DEPENDENCY.paddlepaddle-gpu==2.6.0> <CONTAINS> """CODE.dataset = base.DatasetFactory().create_dataset("InMemoryDataset")
dataset.set_fleet_send_batch_size(800)""",
        """CODE.dataset = base.DatasetFactory().create_dataset()
dataset.set_batch_size(128)""",
        "CODE.dataset.set_fleet_send_batch_size(800)",
        """CODE.import paddle.base as base
dataset = base.DatasetFactory().create_dataset("InMemoryDataset")
dataset.set_fleet_send_sleep_seconds(2)""",
        """CODE.import paddle.base as base
dataset = base.DatasetFactory().create_dataset("InMemoryDataset")
dataset.set_merge_by_lineid()""",
        """CODE.import paddle.base as base
dataset = base.DatasetFactory().create_dataset("InMemoryDataset")
dataset.set_parse_content(True)""",
        """CODE.import paddle.base as base
dataset = base.DatasetFactory().create_dataset("InMemoryDataset")
dataset.set_parse_ins_id(True)""",
        """CODE.import paddle.base as base
dataset = base.DatasetFactory().create_dataset("InMemoryDataset")
dataset.set_queue_num(12)""",
        """CODE.import paddle.base as base
dataset = base.DatasetFactory().create_dataset()
dataset.set_download_cmd("./read_from_afs")""",
        """CODE.import paddle.base as base
dataset = base.DatasetFactory().create_dataset()
dataset.set_hdfs_config("my_fs_name", "my_fs_ugi")""",
        """CODE.import paddle.base as base
dataset = base.DatasetFactory().create_dataset()
dataset.set_pipe_command("python my_script.py")""",
        """CODE.import paddle.base as base
dataset = base.DatasetFactory().create_dataset()
dataset.set_so_parser_name("./abc.so")""",
        """CODE.import paddle.base as base
dataset = base.DatasetFactory().create_dataset()
dataset.set_thread(12)""",
        """CODE.import paddle.base as base
dataset = base.DatasetFactory().create_dataset()
print(dataset.desc())""",
        """CODE.import paddle.base as base
paddle.enable_static()
dataset = base.DatasetFactory().create_dataset()
data = paddle.static.data(name="data", shape=[None, 10, 10], dtype="int64")
label = paddle.static.data(name="label", shape=[None, 1], dtype="int64", lod_level=1)
dataset.set_use_var([data, label])""" .

<DEPENDENCY.pandas==0.10.0> <CONTAINS> "CODE.iterpairs([1, 2, 3, 4])",
        "CODE.list(split_ranges([1,0,0,1,0]))" .

<DEPENDENCY.pandas==0.10.1> <CONTAINS> """CODE.grouped = df.groupby(level='lvl1')
boxplot_frame_groupby(grouped)

grouped = df.unstack(level='lvl1').groupby(level=0, axis=1)
boxplot_frame_groupby(grouped, subplots=False)""",
        """CODE.parallel_coordinates(df, 'Name', colors=('#556270', '#4ECDC4', '#C7F464'))
plt.show()""",
        "CODE.scatter_matrix(df, alpha=0.2)" .

<DEPENDENCY.pandas==0.12.0> <CONTAINS> """CODE.@with_connectivity_check("http://www.yahoo.com")
def test_something_with_yahoo():
    raise IOError("Failure Message")

@with_connectivity_check("failing://url.blaher", check_before_test=True)
def test_something():
    print("I ran!")
    raise ValueError("Failure")""",
        "CODE._stata_elapsed_date_to_datetime(52, \"%tw\")                                datetime.datetime(1961, 1, 1, 0, 0)",
        """CODE.aapl = Options('aapl', 'yahoo')
calls = aapl.get_calls(9, 2012)
puts = aapl.get_puts(9, 2012)
cut_calls = aapl.get_near_stock_price(calls, above_below=3)
forward_calls, forward_puts = aapl.get_forward_data(8, call=True, put=True)""",
        """CODE.assertRaisesRegexp(ValueError, 'invalid literal for.*XYZ', int, 'XYZ')
import re
assertRaisesRegexp(ValueError, re.compile('literal'), int, 'XYZ')
assertRaisesRegexp(TypeError, 'literal', int, 'XYZ')
dct = {}
assertRaisesRegexp(KeyError, 'apple', dct.__getitem__, 'apple')
assertRaisesRegexp(Exception, 'operand type.*int.*dict', lambda : 2 + {})""",
        """CODE.boxplot_frame_groupby(grouped)

grouped = df.unstack(level='lvl1').groupby(level=0, axis=1)
boxplot_frame_groupby(grouped, subplots=False)""",
        """CODE.from pandas import read_csv
from pandas.tools.plotting import parallel_coordinates
from matplotlib import pyplot as plt
df = read_csv('https://raw.github.com/pydata/pandas/master/pandas/tests/data/iris.csv')
parallel_coordinates(df, 'Name', colors=('#556270', '#4ECDC4', '#C7F464'))
plt.show()""",
        """CODE.gs = DataReader("GS", "yahoo")
vix = DataReader("VIXCLS", "fred")
ff = DataReader("F-F_Research_Data_Factors", "famafrench")
ff = DataReader("F-F_Research_Data_Factors_weekly", "famafrench")
ff = DataReader("6_Portfolios_2x3", "famafrench")
ff = DataReader("F-F_ST_Reversal_Factor", "famafrench")""",
        """CODE.import pandas.core.config as cf
with cf.config_prefix("display.font"):
    cf.register_option("color", "red")
    cf.register_option("size", " 5 pt")
    cf.set_option(size, " 6 pt")
    cf.get_option(size)""",
        """CODE.import warnings
with assert_produces_warning():
...     warnings.warn(UserWarning())
...
with assert_produces_warning(False):
...     warnings.warn(RuntimeWarning())
...
Traceback (most recent call last):
    ...
AssertionError: Caused unexpected warning(s): ['RuntimeWarning'].
with assert_produces_warning(UserWarning):
...     warnings.warn(RuntimeWarning())
Traceback (most recent call last):
    ...
AssertionError: Did not see expected warning of class 'UserWarning'.""",
        "CODE.scatter_matrix(df, alpha=0.2)" .

<DEPENDENCY.pandas==0.13.0> <CONTAINS> "CODE.Counter('abcdeabcdabcaba').most_common(3)",
        "CODE.a.combine_first(b)",
        """CODE.c = Counter('ABCABC')
sorted(c.elements())
['A', 'A', 'B', 'B', 'C', 'C']

prime_factors = Counter({2: 2, 3: 3, 17: 1})
product = 1
for factor in prime_factors.elements():
    product *= factor
product
1836""",
        """CODE.c = Counter('abcdeabcdabcaba')
c.most_common(3)
sorted(c)
''.join(sorted(c.elements()))
sum(c.values())
c['a']
for elem in 'shazam':
...     c[elem] += 1
c['a']
del c['b']
c['b']
d = Counter('simsalabim')
c.update(d)
c['a']
c.clear()
c
Counter()
c = Counter('aaabbc')
c['b'] -= 2
c.most_common()                """,
        """CODE.c = Counter('which')
c.subtract('witch')
c.subtract(Counter('watch'))
c['h']
c['w']""",
        """CODE.c = Counter('which')
c.update('witch')
d = Counter('watch')
c.update(d)
c['h']
4""",
        """CODE.df = DataFrame([[1, 1.0]], columns=['x', 'y'])
row = next(df.iterrows())[1]
print(row['x'].dtype)
print(df['x'].dtype)
""",
        """CODE.df = DataFrame([[1, 2], [1, 4], [5, 6]], columns=['A', 'B'])
df.groupby('A', as_index=False).tail(1)
df.groupby('A').head(1)""",
        """CODE.df = DataFrame(randn(10, 2), columns=list('ab'))
df.eval('a + b')
df.eval('c=a + b')""",
        """CODE.df = DataFrame({'A': [1, 2, 3], 'B': ['a', 'b', 'f']})
df.isin([1, 3, 12, 'a'])

df = DataFrame({'A': [1, 2, 3], 'B': [1, 4, 7]})
df.isin({'A': [1, 3], 'B': [4, 7, 12]})

df = DataFrame({'A': [1, 2, 3], 'B': ['a', 'b', 'f']})
other = DataFrame({'A': [1, 3, 3, 2], 'B': ['e', 'f', 'f', 'e']})
df.isin(other)
""",
        """CODE.df.groupby('A').cumcount()
df.groupby('A').cumcount(ascending=False)""",
        """CODE.df.pivot('foo', 'bar', 'baz')
df.pivot('foo', 'bar')['baz']""",
        """CODE.from numpy.random import randn
from pandas import DataFrame
df = DataFrame(randn(10, 2), columns=list('ab'))
df.query('a > b')
df[df.a > df.b]  # same result as the previous expression
""",
        """CODE.from pandas import Series
s = Series(list('abc'))
s.isin(['a'])
""",
        """CODE.grouped.aggregate(np.sum)
b    3.0
q    7.0

grouped.aggregate([np.sum, np.mean, np.std])
   mean  std  sum
b  1.5   0.5  3
q  3.5   0.5  7

grouped.agg({'result' : lambda x: x.mean() / x.std(),
...              'total' : np.sum})
   result  total
b  2.121   3
q  4.95    7""",
        "CODE.grouped.filter(lambda x: x['A'].sum() + x['B'].sum() > 0)",
        "CODE.grouped.transform(lambda x: (x - x.mean()) / x.std())",
        """CODE.index - index2
index.diff(index2)""",
        """CODE.index = pd.MultiIndex.from_tuples([('one', 'a'), ('one', 'b'),
                                   ('two', 'a'), ('two', 'b')])
s = pd.Series(np.arange(1.0, 5.0), index=index)
s.unstack(level=-1)
s.unstack(level=0)
df = s.unstack(level=0)
df.unstack()
""",
        """CODE.indexer, mask = index.get_indexer(new_index)
new_values = cur_values.take(indexer)
new_values[-mask] = np.nan""",
        """CODE.result1 = s.sort_index(ascending=False)
result2 = s.sort_index(ascending=[1, 0])""",
        """CODE.s.unstack(level=-1)
s.unstack(level=0)""",
        """CODE.x
one   1
two   2
three 3

y
1  foo
2  bar
3  baz

x.map(y)""" .

<DEPENDENCY.pandas==0.13.1> <CONTAINS> "CODE.MultiIndex.from_product([numbers, colors], names=['number', 'color'])" .

<DEPENDENCY.pandas==0.14.1> <CONTAINS> """CODE.df = pd.DataFrame({'a': np.random.randn(6).astype('f4'),
                    'b': [True, False] * 3,
                    'c': [1.0, 2.0] * 3})
df.select_dtypes(include=['float64'])
df.select_dtypes(exclude=['floating'])""",
        """CODE.index - index2
index.diff(index2)""" .

<DEPENDENCY.pandas==0.15.0> <CONTAINS> "CODE.index.difference(index2)" .

<DEPENDENCY.pandas==0.15.2> <CONTAINS> """CODE.a = ([],)
isinstance(a, collections.Hashable)
True
is_hashable(a)
False""" .

<DEPENDENCY.pandas==0.16.0> <CONTAINS> """CODE.df = DataFrame({'A': range(1, 11), 'B': np.random.randn(10)})
df.assign(ln_A = lambda x: np.log(x.A))
newcol = np.log(df['A'])
df.assign(ln_A=newcol)""",
        """CODE.from numpy import nan
s = Series([3.0, nan, 1.0, 3.0, nan, nan])
s.index = MultiIndex.from_tuples([(1, 2, 'a', 0),
                                  (1, 2, 'a', 1),
                                  (1, 1, 'b', 0),
                                  (1, 1, 'b', 1),
                                  (2, 1, 'b', 0),
                                  (2, 1, 'b', 1)],
                                  names=['A', 'B', 'C', 'D'])
ss = s.to_sparse()
A, rows, columns = ss.to_coo(row_levels=['A', 'B'],
                             column_levels=['C', 'D'],
                             sort_labels=True)""",
        """CODE.from scipy import sparse
A = sparse.coo_matrix(([3.0, 1.0, 2.0], ([1, 0, 0], [0, 2, 3])), shape=(3, 4))
ss = SparseSeries.from_coo(A)""" .

<DEPENDENCY.pandas==0.16.1> <CONTAINS> """CODE.indexer, mask = index.get_indexer(new_index)
new_values = cur_values.take(indexer)
new_values[-mask] = np.nan""" .

<DEPENDENCY.pandas==0.16.3> <CONTAINS> """CODE.Resolution.get_reso('second')
Resolution.get_reso('second') == Resolution.RESO_SEC""",
        """CODE.Resolution.get_reso_from_freq('H')
Resolution.get_reso_from_freq('H') == Resolution.RESO_HR""",
        "CODE.Resolution.get_str(Resolution.RESO_SEC)",
        "CODE.f.Resolution.get_freq_group('day')" .

<DEPENDENCY.pandas==0.17.1> <CONTAINS> "CODE.Counter('abracadabra').most_common(3)",
        """CODE.c = Counter('ABCABC')
sorted(c.elements())""",
        "CODE.df.style.set_properties(color=\"white\", align=\"right\")" .

<DEPENDENCY.pandas==0.18.0> <CONTAINS> """CODE.df = pd.DataFrame({'A' : [1, 1, 2],
                   'B' : ['foo', 'bar', 'foo'],
                   'C' : np.arange(4.,7)})
df.to_xarray()

df = pd.DataFrame({'A' : [1, 1, 2],
                   'B' : ['foo', 'bar', 'foo'],
                   'C' : np.arange(4.,7)}
                 ).set_index(['B','A'])
df.to_xarray()

p = pd.Panel(np.arange(24).reshape(4,3,2),
             items=list('ABCD'),
             major_axis=pd.date_range('20130101', periods=3),
             minor_axis=['first', 'second'])
p.to_xarray()""",
        """CODE.df._set_axis_name("foo")
     A
foo
0    1
1    2
2    3
df.index = pd.MultiIndex.from_product([['A'], ['a', 'b', 'c']])
df._set_axis_name(["bar", "baz"])
         A
bar baz
A   a    1
    b    2
    c    3""",
        """CODE.df.style.format("{:.2%}")
df.style.format({'C': str.upper})""",
        "CODE.resampled.transform(lambda x: (x - x.mean()) / x.std())",
        """CODE.x = pd.Series([1, 2, 3])
x
0    1
1    2
2    3
dtype: int64
x.searchsorted(4)
array([3])
x.searchsorted([0, 4])
array([0, 3])
x.searchsorted([1, 3], side='left')
array([0, 2])
x.searchsorted([1, 3], side='right')
array([1, 3])
x.searchsorted([1, 2], side='right', sorter=[0, 2, 1])
array([1, 3])""" .

<DEPENDENCY.pandas==0.18.1> <CONTAINS> """CODE.# Instantiate object with ticker
aapl = Options('aapl', 'yahoo')

# Fetch next expiry call data
calls = aapl.get_call_data()

# Fetch next expiry put data
puts = aapl.get_put_data()

# cut down the call data to be 3 below and 3 above the stock price.
cut_calls = aapl.get_near_stock_price(call=True, above_below=3)

# Fetch call and put data with expiry from now to 8 months out
forward_data = aapl.get_forward_data(8, call=True, put=True)

# Fetch all call and put data
all_data = aapl.get_all_data()
""",
        """CODE.class C(object):
    attribute = 'original'

with patch(C, 'attribute', 'patched'):
    in_context = C.attribute

in_context
'patched'
C.attribute  # the value is reset when the context manager exists
'original'

with patch(C, 'attribute', 'patched'):
    in_context = C.attribute
    raise ValueError()

in_context
'patched'
C.attribute
'original'
""",
        "CODE.ordered_merge(A, B, fill_method='ffill', left_by='group')" .

<DEPENDENCY.pandas==0.19.0> <CONTAINS> """CODE.c = pd.Categorical(list('aabca'))
c._reverse_indexer()""",
        """CODE.format_percentiles([0.01999, 0.02001, 0.5, 0.666666, 0.9999])
format_percentiles([0, 0.5, 0.02001, 0.5, 0.666666, 0.9999])""",
        """CODE.from datetime import datetime
from dateutil.tz import tzlocal

with set_timezone('US/Eastern'):
    tzlocal().tzname(datetime.now())""" .

<DEPENDENCY.pandas==0.20.0> <CONTAINS> """CODE.@capture_stderr
def test_stderr_pass():
    sys.stderr.write("foo")
    out = sys.stderr.getvalue()
    assert out == "foo"

@capture_stderr
def test_stderr_fail():
    sys.stderr.write("foo")
    out = sys.stderr.getvalue()
    assert out == "bar\"""",
        """CODE.@capture_stdout
def test_print_pass():
    print("foo")
    out = sys.stdout.getvalue()
    assert out == "foo"

@capture_stdout
def test_print_fail():
    print("foo")
    out = sys.stdout.getvalue()
    assert out == "bar\"""",
        "CODE.IntervalIndex.from_arrays([0, 1, 2], [1, 2, 3])",
        "CODE.IntervalIndex.from_breaks([0, 1, 2, 3])",
        """CODE.IntervalIndex.from_intervals([Interval(0, 1), Interval(1, 2)])
Index([Interval(0, 1), Interval(1, 2)])""",
        """CODE.Point = namedtuple("Point", ["x", "y"])
p = Point(1, 2)
is_named_tuple(p)
True
is_named_tuple((1, 2))
False""",
        """CODE._iterable_not_string([1, 2, 3])
_iterable_not_string("foo")
_iterable_not_string(1)""",
        """CODE.df = pd.DataFrame(
    {'A': [1, 2, 3],
     'B': ['a', 'b', 'c'],
     'C': pd.date_range('2016-01-01', freq='d', periods=3),
    }, index=pd.Index(range(3), name='idx'))
build_table_schema(df)
""",
        """CODE.df = pd.DataFrame({'id': ['spam', 'egg', 'egg', 'spam',
                          'ham', 'ham'],
                   'value1': [1, 5, 5, 2, 5, 5],
                   'value2': list('abbaxy')})

df.groupby('id').nunique()

df.groupby('id').filter(lambda g: (g.nunique() > 1).any())""",
        """CODE.dt = np.datetime64(pd.datetime(2017, 1, 1))
is_datetimelike_v_numeric(1, 1)
False
is_datetimelike_v_numeric(dt, dt)
False
is_datetimelike_v_numeric(1, dt)
True
is_datetimelike_v_numeric(dt, 1)  # symmetric check
True
is_datetimelike_v_numeric(np.array([dt]), 1)
True
is_datetimelike_v_numeric(np.array([1]), dt)
True
is_datetimelike_v_numeric(np.array([dt]), np.array([1]))
True
is_datetimelike_v_numeric(np.array([1]), np.array([2]))
False
is_datetimelike_v_numeric(np.array([dt]), np.array([dt]))
False""",
        """CODE.from scipy.sparse import bsr_matrix
is_scipy_sparse(bsr_matrix([1, 2, 3]))
is_scipy_sparse(pd.SparseArray([1, 2, 3]))
is_scipy_sparse(pd.SparseSeries([1, 2, 3]))
""",
        """CODE.is_any_int_dtype(int)
is_any_int_dtype(np.uint64)
is_any_int_dtype(np.timedelta64)
is_any_int_dtype(np.array([], dtype=np.timedelta64))""",
        """CODE.is_bool_dtype(bool)
True
is_bool_dtype(np.bool)
True
is_bool_dtype(np.array([True, False]))
True""",
        """CODE.is_categorical_dtype(object)
is_categorical_dtype(CategoricalDtype())
is_categorical_dtype([1, 2, 3])
is_categorical_dtype(pd.Categorical([1, 2, 3]))
is_categorical_dtype(pd.CategoricalIndex([1, 2, 3]) )""",
        """CODE.is_datetime64_ns_dtype(DatetimeTZDtype("ns", "US/Eastern"))
is_datetime64_ns_dtype(pd.DatetimeIndex([1, 2, 3], dtype=np.datetime64))  # has 'ns' unit""",
        """CODE.is_dict_like({1: 2})
is_dict_like([1, 2, 3])""",
        """CODE.is_float_dtype(float)
True
is_float_dtype(pd.Index([1, 2.]))
True""",
        """CODE.is_integer_dtype(int)
is_integer_dtype(np.uint64)
is_integer_dtype(pd.Series([1, 2]))
is_integer_dtype(np.array([], dtype=np.timedelta64))""",
        """CODE.is_nested_list_like([[1, 2, 3]])
is_nested_list_like([{1, 2, 3}, {1, 2, 3}])
is_nested_list_like(["foo"])
is_nested_list_like([])
is_nested_list_like([[1, 2, 3], 1])""",
        """CODE.is_number(1)
is_number("foo")""",
        """CODE.is_numeric_dtype(str)
is_numeric_dtype(int)
is_numeric_dtype(float)
is_numeric_dtype(np.uint64)
is_numeric_dtype(np.datetime64)
is_numeric_dtype(np.timedelta64)
is_numeric_dtype(np.array(['a', 'b']))
is_numeric_dtype(pd.Series([1, 2]))
is_numeric_dtype(pd.Index([1, 2.]))
is_numeric_dtype(np.array([], dtype=np.timedelta64)""",
        """CODE.is_object_dtype(object)
True
is_object_dtype(int)
False
is_object_dtype(np.array([], dtype=object))
True
is_object_dtype(np.array([], dtype=int))
False
is_object_dtype([1, 2, 3])
False""",
        """CODE.is_re(re.compile(".*"))
True
is_re("foo")
False""",
        """CODE.is_re_compilable = lambda obj: isinstance(obj, str)

print(is_re_compilable(".*"))
print(is_re_compilable(1))""",
        """CODE.is_sequence(l)
is_sequence(iter(l))""",
        """CODE.is_signed_integer_dtype(int)
is_signed_integer_dtype(float)
is_signed_integer_dtype(np.uint64)  # unsigned
is_signed_integer_dtype(np.datetime64)
is_signed_integer_dtype(np.timedelta64)
is_signed_integer_dtype(np.array(['a', 'b']))
is_signed_integer_dtype(pd.Series([1, 2]))
is_signed_integer_dtype(np.array([], dtype=np.timedelta64))
is_signed_integer_dtype(pd.Index([1, 2.]))  # float
is_signed_integer_dtype(np.array([1, 2], dtype=np.uint32))  # unsigned""",
        """CODE.is_string_dtype(str)
True
is_string_dtype(object)
True
is_string_dtype(int)
False
is_string_dtype(np.array(['a', 'b']))
True
is_string_dtype(pd.Series([1, 2]))
False""",
        """CODE.is_string_like("foo")
True
is_string_like(1)
False""",
        """CODE.is_unsigned_integer_dtype(np.uint64)
True
is_unsigned_integer_dtype(np.array([1, 2], dtype=np.uint32))
True""",
        """CODE.needs_i8_conversion(np.datetime64)
needs_i8_conversion(pd.Series([], dtype="timedelta64[ns]"))
needs_i8_conversion(pd.DatetimeIndex([1, 2, 3], tz="US/Eastern")""" .

<DEPENDENCY.pandas==0.20.2> <CONTAINS> """CODE.Index([1, 2, 3])._is_strictly_monotonic_increasing
Index([1, 2, 2])._is_strictly_monotonic_increasing
Index([1, 3, 2])._is_strictly_monotonic_increasing""",
        """CODE.df = pd.DataFrame({"A": list("aaabba")})
df.groupby('A').ngroup()
0    0
1    0
2    0
3    1
4    1
5    0
dtype: int64
df.groupby('A').ngroup(ascending=False)
0    1
1    1
2    1
3    0
4    0
5    1
dtype: int64
df.groupby(["A", [1,1,2,3,2,1]]).ngroup()
0    0
1    0
2    1
3    3
4    2
5    0
dtype: int64""" .

<DEPENDENCY.pandas==0.20.3> <CONTAINS> """CODE.get_freq_code('3D')
get_freq_code('D')
get_freq_code(('D', 3))""",
        """CODE.table = pivot_table(df, values='D', index=['A', 'B'],
                     columns=['C'], aggfunc=np.sum)""" .

<DEPENDENCY.pandas==0.21.0> <CONTAINS> """CODE._ensure_index(['a', 'b'])
Index(['a', 'b'], dtype='object')

_ensure_index([('a', 'a'),  ('b', 'c')])
Index([('a', 'a'), ('b', 'c')], dtype='object')

_ensure_index([['a', 'a'], ['b', 'c']])
MultiIndex(levels=[['a'], ['b', 'c']],
           labels=[[0, 0], [0, 1]])""",
        """CODE._ensure_index_from_sequences([[1, 2, 3]], names=['name'])
_ensure_index_from_sequences([['a', 'a'], ['a', 'b']], names=['L1', 'L2'])""",
        """CODE.df = pd.DataFrame({"A": ["a", 1, 2, 3]})
df = df.iloc[1:]
df
   A
1  1
2  2
3  3

df.dtypes
A    object
dtype: object

df.infer_objects().dtypes
A    int64
dtype: object
""",
        """CODE.df = pd.DataFrame({"A": [1, 2, 3], "B": [4, 5, 6]})
df.rename(index=str, columns={"A": "a", "B": "c"})
df.rename(index=str, columns={"A": "a", "C": "c"})
df.rename(str.lower, axis='columns')
df.rename({1: 2, 2: 4}, axis='index')
""",
        "CODE.maybe_infer_dtype_type(Foo(np.dtype(\"i8\")))",
        """CODE.old_cat = pd.Index(['b', 'a', 'c'])
new_cat = pd.Index(['a', 'b'])
codes = np.array([0, 1, 1, 2])
_recode_for_categories(codes, old_cat, new_cat)
array([ 1,  0,  0, -1])""" .

<DEPENDENCY.pandas==0.22.0> <CONTAINS> """CODE.Resolution.get_reso('second')
Resolution.get_reso('second') == Resolution.RESO_SEC""",
        """CODE.Resolution.get_reso_from_freq('H')
Resolution.get_reso_from_freq('H') == Resolution.RESO_HR""",
        "CODE.Resolution.get_str(Resolution.RESO_SEC)",
        """CODE.Resolution.get_stride_from_decimal(1.5, 'T')
Resolution.get_stride_from_decimal(1.04, 'H')
Resolution.get_stride_from_decimal(1, 'D')""",
        "CODE.f.Resolution.get_freq_group('day')",
        """CODE.get_to_timestamp_base(get_freq_code('D')[0])
get_to_timestamp_base(get_freq_code('W')[0])
get_to_timestamp_base(get_freq_code('M')[0])
get_to_timestamp_base(get_freq_code('H')[0])
get_to_timestamp_base(get_freq_code('S')[0] )""" .

<DEPENDENCY.pandas==0.23.0> <CONTAINS> """CODE.   def take(self, indices, allow_fill=False, fill_value=None):
       from pandas.core.algorithms import take

       # If the ExtensionArray is backed by an ndarray, then
       # just pass that here instead of coercing to object.
       data = self.astype(object)

       if allow_fill and fill_value is None:
           fill_value = self.dtype.na_value

       # fill value should always be translated from the scalar
       # type for the array, to the physical storage type for
       # the data, before passing to take.

       result = take(data, indices, fill_value=fill_value,
                     allow_fill=allow_fill)
       return self._from_sequence(result)
""",
        """CODE.@classmethod
def construct_from_string(cls, string):
    if string == cls.name:
        return cls()
    else:
        raise TypeError("Cannot construct a '{}' from "
                        "'{}'".format(cls, string))""",
        "CODE.DocBuilder()._run_os('python', '--version')",
        "CODE.DocBuilder(num_jobs=4)._sphinx_build('html')",
        """CODE.animals = pd.Series(['lama', 'cow', 'lama', 'beetle', 'lama'])
animals.duplicated()
animals.duplicated(keep='first')
animals.duplicated(keep='last')
animals.duplicated(keep=False)""",
        """CODE.idx = pd.Index(["Watermelon", "Orange", "Apple",
...                 "Watermelon"]).astype("category")
idx.is_categorical()
True

idx = pd.Index([1, 3, 5, 7])
idx.is_categorical()
False

s = pd.Series(["Peter", "Victor", "Elisabeth", "Mar"])
s
0        Peter
1       Victor
2    Elisabeth
3          Mar
dtype: object
s.index.is_categorical()
False""",
        """CODE.idx = pd.Index(['lama', 'cow', 'lama', 'beetle', 'lama', 'hippo'])
idx.drop_duplicates(keep='first')
idx.drop_duplicates(keep='last')
idx.drop_duplicates(keep=False)""",
        """CODE.idx = pd.IntervalIndex.from_arrays([0, np.nan, 2], [1, np.nan, 3])
idx.to_tuples()
idx.to_tuples(na_tuple=False)""",
        """CODE.import pandas as pd
from pandas.io.stata import StataWriter117
data = pd.DataFrame([[1.0, 1, 'a']], columns=['a', 'b', 'c'])
writer = StataWriter117('./data_file.dta', data)
writer.write_file()

data = pd.DataFrame([['A relatively long string'], [''], ['']],
                    columns=['strls'])
writer = StataWriter117('./data_file_with_long_strings.dta', data,
                        convert_strl=['strls'])
writer.write_file()
""",
        """CODE.mask_zero_div_zero(x, y, result)
array([ inf,  nan, -inf])""",
        """CODE.pd.Index([1, 2, 2, 3, 3, 3, 4]).get_duplicates()
pd.Index([1., 2., 2., 3., 3., 3., 4.]).get_duplicates()
pd.Index(['a', 'b', 'b', 'c', 'c', 'c', 'd']).get_duplicates()
dates = pd.to_datetime(['2018-01-01', '2018-01-02', '2018-01-03',
                        '2018-01-03', '2018-01-04', '2018-01-04'],
                       format='%Y-%m-%d')
pd.Index(dates).get_duplicates()
pd.Index([1, 2, 3, 2, 3, 4, 3]).get_duplicates()
pd.Index([1, 2, 3, 4]).get_duplicates()
dates = pd.to_datetime(['2018-01-01', '2018-01-02', '2018-01-03'],
                       format='%Y-%m-%d')
pd.Index(dates).get_duplicates()""",
        """CODE.pd.Series([2, 1, 3, 3], name='A').unique()
pd.Series([pd.Timestamp('2016-01-01') for _ in range(3)]).unique()
pd.Series([pd.Timestamp('2016-01-01', tz='US/Eastern') for _ in range(3)]).unique()
pd.Series(pd.Categorical(list('baabc'))).unique()
pd.Series(pd.Categorical(list('baabc'), categories=list('abc'), ordered=True)).unique()""",
        """CODE.s = pd.Categorical(['lama', 'cow', 'lama', 'beetle', 'lama', 'hippo'])
s.isin(['cow', 'lama'])
s.isin(['lama'])""",
        """CODE.s = pd.Series(['a', 'b', 'c', 'd'], index=[3, 2, 1, 4])
s.sort_index()
1    c
2    b
3    a
4    d
dtype: object

s.sort_index(ascending=False)
4    d
3    a
2    b
1    c
dtype: object

s.sort_index(inplace=True)
s
1    c
2    b
3    a
4    d
dtype: object

s = pd.Series(['a', 'b', 'c', 'd'], index=[3, 2, 1, np.nan])
s.sort_index(na_position='first')
NaN     d
 1.0    c
 2.0    b
 3.0    a
dtype: object

arrays = [np.array(['qux', 'qux', 'foo', 'foo',
...                     'baz', 'baz', 'bar', 'bar']),
...           np.array(['two', 'one', 'two', 'one',
...                     'two', 'one', 'two', 'one'])]
s = pd.Series([1, 2, 3, 4, 5, 6, 7, 8], index=arrays)
s.sort_index(level=1)
bar  one    8
baz  one    6
foo  one    4
qux  one    2
bar  two    7
baz  two    5
foo  two    3
qux  two    1
dtype: int64

s.sort_index(level=1, sort_remaining=False)
qux  one    2
foo  one    4
baz  one    6
bar  one    8
qux  two    1
foo  two    3
baz  two    5
bar  two    7
dtype: int64""",
        """CODE.s = pd.Series(['lama', 'cow', 'lama', 'beetle', 'lama', 'hippo'],
              name='animal')
s
0      lama
1       cow
2      lama
3    beetle
4      lama
5     hippo
Name: animal, dtype: object

s.drop_duplicates()
0      lama
1       cow
3    beetle
5     hippo
Name: animal, dtype: object

s.drop_duplicates(keep='last')
1       cow
3    beetle
4      lama
5     hippo
Name: animal, dtype: object

s.drop_duplicates(keep=False, inplace=True)
s
1       cow
3    beetle
5     hippo
Name: animal, dtype: object
""",
        """CODE.s = pd.Series([-2, -1, 0, 1, 2], dtype='int8')
us = s.view('uint8')
us[0] = 128
""",
        """CODE.s = pd.Series([1, 2, 3])
s
0    1
1    2
2    3
dtype: int64

s.set_axis(['a', 'b', 'c'], axis=0, inplace=False)
a    1
b    2
c    3
dtype: int64

df = pd.DataFrame({"A": [1, 2, 3], "B": [4, 5, 6]})

Change the row labels.

df.set_axis(['a', 'b', 'c'], axis='index', inplace=False)
   A  B
a  1  4
b  2  5
c  3  6

Change the column labels.

df.set_axis(['I', 'II'], axis='columns', inplace=False)
   I  II
0  1   4
1  2   5
2  3   6

Now, update the labels inplace.

df.set_axis(['i', 'ii'], axis='columns', inplace=True)
df
   i  ii
0  1   4
1  2   5
2  3   6""",
        """CODE.s = pd.Series([np.nan, 1, 3, 10, 5])
s
0     NaN
1     1.0
2     3.0
3     10.0
4     5.0
dtype: float64

s.sort_values(ascending=True)
1     1.0
2     3.0
4     5.0
3    10.0
0     NaN
dtype: float64

s.sort_values(ascending=False)
3    10.0
4     5.0
2     3.0
1     1.0
0     NaN
dtype: float64

s.sort_values(ascending=False, inplace=True)
s
3    10.0
4     5.0
2     3.0
1     1.0
0     NaN
dtype: float64

s.sort_values(na_position='first')
0     NaN
1     1.0
2     3.0
4     5.0
3    10.0
dtype: float64

s = pd.Series(['z', 'b', 'd', 'a', 'c'])
s
0    z
1    b
2    d
3    a
4    c
dtype: object

s.sort_values()
3    a
1    b
4    c
2    d
0    z
dtype: object""",
        """CODE.s = pd.Series(pd.date_range('20180310', periods=2))
s
0   2018-03-10
1   2018-03-11
dtype: datetime64[ns]

s.dt.to_pydatetime()
array([datetime.datetime(2018, 3, 10, 0, 0),
       datetime.datetime(2018, 3, 11, 0, 0)], dtype=object)

pandas' nanosecond precision is truncated to microseconds.

s = pd.Series(pd.date_range('20180310', periods=2, freq='ns'))
s
0   2018-03-10 00:00:00.000000000
1   2018-03-10 00:00:00.000000001
dtype: datetime64[ns]

s.dt.to_pydatetime()
array([datetime.datetime(2018, 3, 10, 0, 0),
       datetime.datetime(2018, 3, 10, 0, 0)], dtype=object)""",
        """CODE.values = {'A': 0, 'B': 1, 'C': 2, 'D': 3}
df.fillna(value=values)
""" .

<DEPENDENCY.pandas==0.23.1> <CONTAINS> """CODE.np.array([1.0, 2.0, None], dtype='str')
construct_1d_ndarray_preserving_na([1.0, 2.0, None], dtype='str')""" .

<DEPENDENCY.pandas==0.23.4> <CONTAINS> """CODE._ensure_index(['a', 'b'])
Index(['a', 'b'], dtype='object')

_ensure_index([('a', 'a'),  ('b', 'c')])
Index([('a', 'a'), ('b', 'c')], dtype='object')

_ensure_index([['a', 'a'], ['b', 'c']])
MultiIndex(levels=[['a'], ['b', 'c']],
           labels=[[0, 0], [0, 1]])""" .

<DEPENDENCY.pandas==0.24.0> <CONTAINS> """CODE.SparseDtype(int, 0).update_dtype(float)
SparseDtype(int, 1).update_dtype(SparseDtype(float, np.nan)""",
        """CODE.df = pd.DataFrame({"y": [1,2,3]},
...                   index=pd.to_datetime(["2000-03-31 00:00:00",
...                                         "2000-05-31 00:00:00",
...                                         "2000-08-31 00:00:00"]))
df.index.to_period("M")
PeriodIndex(['2000-03', '2000-05', '2000-08'],
            dtype='period[M]', freq='M')

Infer the daily frequency

idx = pd.date_range("2017-01-01", periods=2)
idx.to_period()
PeriodIndex(['2017-01-01', '2017-01-02'],
            dtype='period[D]', freq='D')""",
        """CODE.dti = pd.date_range(start='2014-08-01 09:00',freq='H',
...                     periods=3, tz='Europe/Berlin')

dti
DatetimeIndex(['2014-08-01 09:00:00+02:00',
               '2014-08-01 10:00:00+02:00',
               '2014-08-01 11:00:00+02:00'],
                dtype='datetime64[ns, Europe/Berlin]', freq='H')

dti.tz_convert(None)
DatetimeIndex(['2014-08-01 07:00:00',
               '2014-08-01 08:00:00',
               '2014-08-01 09:00:00'],
                dtype='datetime64[ns]', freq='H')""",
        """CODE.ensure_index(['a', 'b'])
Index(['a', 'b'], dtype='object')

ensure_index([('a', 'a'),  ('b', 'c')])
Index([('a', 'a'), ('b', 'c')], dtype='object')

ensure_index([['a', 'a'], ['b', 'c']])
MultiIndex(levels=[['a'], ['b', 'c']],
           labels=[[0, 0], [0, 1]])""",
        """CODE.from scipy import sparse
A = sparse.coo_matrix(([3.0, 1.0, 2.0], ([1, 0, 0], [0, 2, 3])), shape=(3, 4))
ss = pd.SparseSeries.from_coo(A)""",
        """CODE.idx = pd.date_range(start='2014-08-01 10:00', freq='H',
...                     periods=3, tz='Asia/Calcutta')
idx
DatetimeIndex(['2014-08-01 10:00:00+05:30',
               '2014-08-01 11:00:00+05:30',
               '2014-08-01 12:00:00+05:30'],
                dtype='datetime64[ns, Asia/Calcutta]', freq='H')
idx.normalize()
DatetimeIndex(['2014-08-01 00:00:00+05:30',
               '2014-08-01 00:00:00+05:30',
               '2014-08-01 00:00:00+05:30'],
               dtype='datetime64[ns, Asia/Calcutta]', freq=None)""",
        """CODE.idx = pd.date_range(start='2018-01', freq='M', periods=3)
idx
DatetimeIndex(['2018-01-31', '2018-02-28', '2018-03-31'],
              dtype='datetime64[ns]', freq='M')
idx.month_name()
Index(['January', 'February', 'March'], dtype='object')""",
        """CODE.idx = pd.date_range(start='2018-01-01', freq='D', periods=3)
idx
DatetimeIndex(['2018-01-01', '2018-01-02', '2018-01-03'],
              dtype='datetime64[ns]', freq='D')
idx.day_name()
Index(['Monday', 'Tuesday', 'Wednesday'], dtype='object')""",
        """CODE.s = pd.Series(['Antelope', 'Lion', 'Zebra', np.nan])
s.str.contains(pat='a')
s.str.contains(pat='a', case=False)
s.str.contains(pat='a', na=False)
""",
        """CODE.s = pd.Series(pd.to_timedelta(np.arange(5), unit='d'))
s
0   0 days
1   1 days
2   2 days
3   3 days
4   4 days
dtype: timedelta64[ns]

s.dt.total_seconds()
0         0.0
1     86400.0
2    172800.0
3    259200.0
4    345600.0
dtype: float64

idx = pd.to_timedelta(np.arange(5), unit='d')
idx
TimedeltaIndex(['0 days', '1 days', '2 days', '3 days', '4 days'],
               dtype='timedelta64[ns]', freq=None)

idx.total_seconds()
Float64Index([0.0, 86400.0, 172800.0, 259200.00000000003, 345600.0],
             dtype='float64')""",
        """CODE.s = pd.to_datetime(pd.Series([
... '2015-03-29 02:30:00',
... '2015-03-29 03:30:00']))
s.dt.tz_localize('Europe/Warsaw', nonexistent='shift_forward')
0   2015-03-29 03:00:00+02:00
1   2015-03-29 03:30:00+02:00
dtype: datetime64[ns, 'Europe/Warsaw']
s.dt.tz_localize('Europe/Warsaw', nonexistent='shift_backward')
0   2015-03-29 01:59:59.999999999+01:00
1   2015-03-29 03:30:00+02:00
dtype: datetime64[ns, 'Europe/Warsaw']
s.dt.tz_localize('Europe/Warsaw', nonexistent=pd.Timedelta('1H'))
0   2015-03-29 03:30:00+02:00
1   2015-03-29 03:30:00+02:00
dtype: datetime64[ns, 'Europe/Warsaw']""",
        "CODE.self._unbox_scalar(Timedelta('10s'))",
        """CODE.ss = s.to_sparse()
A, rows, columns = ss.to_coo(row_levels=['A', 'B'],
                                 column_levels=['C', 'D'],
                                 sort_labels=True)
A
<3x4 sparse matrix of type '<class 'numpy.float64'>'
        with 3 stored elements in COOrdinate format>
A.todense()
matrix([[ 0.,  0.,  1.,  3.],
[ 3.,  0.,  0.,  0.],
[ 0.,  0.,  0.,  0.]])
rows
[(1, 1), (1, 2), (2, 1)]
columns
[('a', 0), ('a', 1), ('b', 0), ('b', 1)]""" .

<DEPENDENCY.pandas==0.25.0> <CONTAINS> """CODE.data = np.random.randn(25, 4)
df = pd.DataFrame(data, columns=list('ABCD'))
ax = df.plot.box()
""",
        """CODE.df = pd.DataFrame(
...     np.random.randint(1, 7, 6000),
...     columns = ['one'])
df['two'] = df['one'] + np.random.randint(1, 7, 6000)
ax = df.plot.hist(bins=12, alpha=0.5)""",
        """CODE.df = pd.DataFrame([[5.1, 3.5, 0], [4.9, 3.0, 0], [7.0, 3.2, 1],
                   [6.4, 3.2, 1], [5.9, 3.0, 2]],
                  columns=['length', 'width', 'species'])
ax1 = df.plot.scatter(x='length',
                      y='width',
                      c='DarkBlue')

ax2 = df.plot.scatter(x='length',
                      y='width',
                      c='species',
                      colormap='viridis')
""",
        """CODE.df = pd.DataFrame({"A": pd.SparseArray([0, 1, 0])})
df.sparse.to_dense()""",
        """CODE.df = pd.DataFrame({'lab':['A', 'B', 'C'], 'val':[10, 30, 20]})
ax = df.plot.bar(x='lab', y='val', rot=0)


speed = [0.1, 17.5, 40, 48, 52, 69, 88]
lifespan = [2, 8, 70, 1.5, 25, 12, 28]
index = ['snail', 'pig', 'elephant', 'rabbit', 'giraffe', 'coyote', 'horse']
df = pd.DataFrame({'speed': speed, 'lifespan': lifespan}, index=index)
ax = df.plot.bar(rot=0)


axes = df.plot.bar(rot=0, subplots=True)
axes[1].legend(loc=2)  # doctest: +SKIP


ax = df.plot.bar(y='speed', rot=0)


ax = df.plot.bar(x='lifespan', rot=0)
""",
        """CODE.df = pd.DataFrame({'mass': [0.330, 4.87 , 5.97],
                   'radius': [2439.7, 6051.8, 6378.1]},
                  index=['Mercury', 'Venus', 'Earth'])
plot = df.plot.pie(y='mass', figsize=(5, 5))
plot = df.plot.pie(subplots=True, figsize=(6, 3))
""",
        """CODE.import scipy.sparse
import pandas as pd

mat = scipy.sparse.eye(3)
pd.DataFrame.sparse.from_spmatrix(mat)""",
        """CODE.s = pd.Series([1, 3, 2])
s.plot.line()

df = pd.DataFrame({
   'pig': [20, 18, 489, 675, 1776],
   'horse': [4, 25, 281, 600, 1900]
   }, index=[1990, 1997, 2003, 2009, 2014])
lines = df.plot.line()

axes = df.plot.line(subplots=True)
type(axes)

lines = df.plot.line(x='pig', y='horse')
""" .

<DEPENDENCY.pandas==0.25.3> <CONTAINS> """CODE.df = pd.DataFrame([[1, 2, 3], [4, 5, 6], [7, 8, 9]],
                   index=['a', 'b', 'c'], columns=['A', 'B', 'C'])
df.index.get_values()
idx = pd.Index(['1', '2', '3'])
idx.get_values()
midx = pd.MultiIndex.from_arrays([[1, 2, 3], ['a', 'b', 'c']],
                                 names=('number', 'letter'))
midx.get_values()
midx.get_values().ndim""" .

<DEPENDENCY.pandas==0.4.1> <CONTAINS> """CODE.years = range(1960,1963)
panels = ['A', 'B', 'C']
panel_idx = panel_index(years, panels)
panel_idx
MultiIndex([(1960, 'A'), (1961, 'A'), (1962, 'A'), (1960, 'B'), (1961, 'B'),
   (1962, 'B'), (1960, 'C'), (1961, 'C'), (1962, 'C')], dtype=object)

import numpy as np
years = np.repeat(range(1960,1963), 3)
panels = np.tile(['A', 'B', 'C'], 3)
panel_idx = panel_index(years, panels)
panel_idx
MultiIndex([(1960, 'A'), (1960, 'B'), (1960, 'C'), (1961, 'A'), (1961, 'B'),
   (1961, 'C'), (1962, 'A'), (1962, 'B'), (1962, 'C')], dtype=object)""" .

<DEPENDENCY.pandas==0.5.0> <CONTAINS> "CODE.Counter('abcdeabcdabcaba').most_common(3)",
        """CODE.c = Counter('ABCABC')
sorted(c.elements())
['A', 'A', 'B', 'B', 'C', 'C']

prime_factors = Counter({2: 2, 3: 3, 17: 1})
product = 1
for factor in prime_factors.elements():
    product *= factor
product
1836""",
        """CODE.c = Counter('abcdeabcdabcaba')
c.most_common(3)
sorted(c)
''.join(sorted(c.elements()))
sum(c.values())
c['a']
for elem in 'shazam':
    c[elem] += 1
c['a']
del c['b']
c['b']
d = Counter('simsalabim')
c.update(d)
c['a']
c.clear()""",
        """CODE.c = Counter('which')
c.subtract('witch')
c.subtract(Counter('watch'))
c['h']
0
c['w']
-1""",
        """CODE.df
    c1  c2
a   1   0
b   0   2
c   3   0
d   0   4

df.sum(axis=0)
c1    4
c2    6""",
        """CODE.table = pivot_table(df, values='D', rows=['A', 'B'],
                     cols=['C'], aggfunc=np.sum)""" .

<DEPENDENCY.pandas==0.6.0> <CONTAINS> """CODE.a.combine_first(b)
    a's values prioritized, use values from b to fill holes""",
        """CODE.df.pivot('foo', 'bar', 'baz')
df.pivot('foo', 'bar')['baz']""",
        "CODE.melt(df, id_vars=['A'])",
        """CODE.s.unstack(level=-1)
s.unstack(level=0)""" .

<DEPENDENCY.pandas==0.6.1> <CONTAINS> """CODE.index - index2
index.diff(index2)""",
        """CODE.indexer, mask = index.get_indexer(new_index)
new_values = cur_values.take(indexer)
new_values[-mask] = np.nan""" .

<DEPENDENCY.pandas==0.7.0> <CONTAINS> """CODE.s.unstack(level=-1)
s.unstack(level=0)""",
        """CODE.x = np.linspace(0, 2*np.pi, 400)
y = np.sin(x**2)

f, ax = plt.subplots()
ax.plot(x, y)
ax.set_title('Simple plot')

f, (ax1, ax2) = plt.subplots(1, 2, sharey=True)
ax1.plot(x, y)
ax1.set_title('Sharing Y axis')
ax2.scatter(x, y)

plt.subplots(2, 2, subplot_kw=dict(polar=True) )""",
        """CODE.x.rename(str.upper)
x.rename({'foo' : 'a', 'bar' : 'b', 'baz' : 'c'})""" .

<DEPENDENCY.pandas==0.7.2> <CONTAINS> """CODE.x = np.linspace(0, 2*np.pi, 400)
y = np.sin(x**2)

# Just a figure and one subplot
f, ax = plt.subplots()
ax.plot(x, y)
ax.set_title('Simple plot')

# Two subplots, unpack the output array immediately
f, (ax1, ax2) = plt.subplots(1, 2, sharey=True)
ax1.plot(x, y)
ax1.set_title('Sharing Y axis')
ax2.scatter(x, y)

# Four polar axes
plt.subplots(2, 2, subplot_kw=dict(polar=True)""" .

<DEPENDENCY.pandas==0.7.3> <CONTAINS> """CODE.df = DataFrame(np.random.randn(1000, 4), columns=['A','B','C','D'])
scatter_matrix(df, alpha=0.2)""",
        """CODE.s.unstack(level=-1)
s.unstack(level=0)""",
        """CODE.x = pd.Series({'one': 1, 'two': 2, 'three': 3})
y = pd.Series({1: 'foo', 2: 'bar', 3: 'baz'})
x.map(y)""",
        """CODE.x.rename(str.upper)
x.rename({'foo' : 'a', 'bar' : 'b', 'baz' : 'c'})""" .

<DEPENDENCY.pandas==0.8.0> <CONTAINS> """CODE.cut(np.array([.2, 1.4, 2.5, 6.2, 9.7, 2.1]), 3, retbins=True)
cut(np.ones(5), 4, labels=False)""",
        """CODE.from scipy import stats
import numpy as np

a = np.arange(100)
stats.scoreatpercentile(a, 50)
""",
        """CODE.grouped = df.groupby(level='lvl1')
boxplot_frame_groupby(grouped)

grouped = df.unstack(level='lvl1').groupby(level=0, axis=1)
boxplot_frame_groupby(grouped, subplots=False)""",
        """CODE.i = Period(year=1,month=1,day=1,freq='D').asfreq('S', 'S')
i.ordinal
idx = PeriodIndex(year=year_arr, quarter=q_arr)
idx2 = PeriodIndex(start='2000', end='2010', freq='A')""",
        """CODE.pd.lreshape(data, {'year': ['year1', 'year2'],
                       'hr': ['hr1', 'hr2']})""",
        "CODE.stats.rankdata([0, 2, 2, 3])" .

<DEPENDENCY.pandas==0.9.0> <CONTAINS> """CODE.a = type("Duck",(),{})
a.attr1,a.attr2 ="fizz","buzz"
b = SimpleMock(a,"attr1","bar")
b.attr1 == "bar" and b.attr2 == "buzz"
True
a.attr1 == "fizz" and a.attr2 == "buzz"
True""",
        """CODE.with open('file.txt', 'r', encoding='AES') as f:
...     data = f.read()
...
print(data)
'AES'
import sys
_encoding = sys.stdin.encoding
with stdin_encoding('AES'): sys.stdin.encoding
'AES'
sys.stdin.encoding==_encoding
True""" .

<DEPENDENCY.pandas==1.0.0> <CONTAINS> """CODE.a = pd.array([True, False, pd.NA], dtype="boolean")
a.to_numpy()
array([True, False, NA], dtype=object)

pd.array([True, False], dtype="boolean").to_numpy(dtype="bool")
array([ True, False])
pd.array([1, 2], dtype="Int64").to_numpy("int64")
array([1, 2])
""",
        """CODE.df = pd.DataFrame([[0, 2, 3], [0, 4, 1], [10, 20, 30],
...                   index=[4, 5, 6], columns=['A', 'B', 'C'])
df
    A   B   C
4   0   2   3
5   0   4   1
6  10  20  30

Get value at specified row/column pair

df.at[4, 'B']
2

Set value at specified row/column pair

df.at[4, 'B'] = 10
df.at[4, 'B']
10

Get value within a Series

df.loc[5].at['B']
4""",
        """CODE.df = pd.DataFrame([[0, 2, 3], [0, 4, 1], [10, 20, 30]],
...                   columns=['A', 'B', 'C'])
df
    A   B   C
0   0   2   3
1   0   4   1
2  10  20  30

Get value at specified row/column pair

df.iat[1, 2]
1

Set value at specified row/column pair

df.iat[1, 2] = 10
df.iat[1, 2]
10

Get value within a series

df.loc[0].iat[1]""",
        """CODE.df = pd.DataFrame([[1, 2, 3], [4, 5, 6], [7, 8, 9]],
                   index=['a', 'b', 'c'], columns=['A', 'B', 'C'])
df.index._internal_get_values()
idx = pd.Index(['1', '2', '3'])
idx._internal_get_values()
midx = pd.MultiIndex.from_arrays([[1, 2, 3], ['a', 'b', 'c']],
                                  names=('number', 'letter'))
midx._internal_get_values()
midx._internal_get_values().ndim""",
        """CODE.mydict = [{'a': 1, 'b': 2, 'c': 3, 'd': 4},
          {'a': 100, 'b': 200, 'c': 300, 'd': 400},
          {'a': 1000, 'b': 2000, 'c': 3000, 'd': 4000 }]
df = pd.DataFrame(mydict)
df


type(df.iloc[0])
<class 'pandas.core.series.Series'>
df.iloc[0]
a    1
b    2
c    3
d    4
Name: 0, dtype: int64


df.iloc[[0]]
   a  b  c  d
0  1  2  3  4
type(df.iloc[[0]])
<class 'pandas.core.frame.DataFrame'>


df.iloc[[0, 1]]
     a    b    c    d
0    1    2    3    4
1  100  200  300  400


df.iloc[:3]
      a     b     c     d
0     1     2     3     4
1   100   200   300   400
2  1000  2000  3000  4000


df.iloc[[True, False, True]]
      a     b     c     d
0     1     2     3     4
2  1000  2000  3000  4000


df.iloc[lambda x: x.index % 2 == 0]
      a     b     c     d
0     1     2     3     4
2  1000  2000  3000  4000


df.iloc[0, 1]
2


df.iloc[[0, 2], [1, 3]]
      b     d
0     2     4
2  2000  4000


df.iloc[1:3, 0:3]
      a     b     c
1   100   200   300
2  1000  2000  3000


df.iloc[:, [True, False, True, False]]
      a     c
0     1     3
1   100   300
2  1000  3000


df.iloc[:, lambda df: [0, 2]]
      a     c
0     1     3
1   100   300
2  1000  3000
""",
        "CODE.pd.BooleanDtype()" .

<DEPENDENCY.pandas==1.0.5> <CONTAINS> """CODE._is_multi_agg_with_relabel(a_max=('a', 'max'),
                            a_min=('a', 'min'))""",
        "CODE._make_unique([('a', '<lambda>'), ('a', '<lambda>'), ('b', '<lambda>')])",
        "CODE._maybe_mangle_lambdas('sum')\\n\\n_maybe_mangle_lambdas([lambda: 1, lambda: 2])  # doctest: +SKIP",
        """CODE._normalize_keyword_aggregation({'output': ('input', 'sum')})
({'input': ['sum']}, ('output',), [('input', 'sum')])""",
        """CODE.old_cat = pd.Index(['b', 'a', 'c'])
new_cat = pd.Index(['a', 'b'])
codes = np.array([0, 1, 1, 2])
_recode_for_categories(codes, old_cat, new_cat)
array([ 1,  0,  0, -1], dtype=int8)""" .

<DEPENDENCY.pandas==1.1.0> <CONTAINS> """CODE.is_multi_agg_with_relabel(a="max")
is_multi_agg_with_relabel(a_max=("a", "max"), a_min=("a", "min"))
is_multi_agg_with_relabel()""",
        """CODE.kwarg_list = [('a', '<lambda>'), ('a', '<lambda>'), ('b', '<lambda>')]
_make_unique_kwarg_list(kwarg_list)
[('a', '<lambda>_0'), ('a', '<lambda>_1'), ('b', '<lambda>')]""",
        """CODE.maybe_mangle_lambdas('sum')

maybe_mangle_lambdas([lambda: 1, lambda: 2])  # doctest: +SKIP""",
        """CODE.normalize_keyword_aggregation({"output": ("input", "sum")})
(defaultdict(<class 'list'>, {'input': ['sum']}), ('output',), array([0]))""",
        """CODE.old_cat = pd.Index(['b', 'a', 'c'])
new_cat = pd.Index(['a', 'b'])
codes = np.array([0, 1, 1, 2])
recode_for_categories(codes, old_cat, new_cat)
array([ 1,  0,  0, -1], dtype=int8)""" .

<DEPENDENCY.pandas==1.1.5> <CONTAINS> """CODE._iterable_not_string([1, 2, 3])
_iterable_not_string("foo")
_iterable_not_string(1)""" .

<DEPENDENCY.pandas==1.2.0> <CONTAINS> """CODE.from pandas import DataFrame
from pandas.io.formats import format as fmt
df = DataFrame({"a": [1, 2], "b": ["b1", "b2"]})
formatter = fmt.DataFrameFormatter(df)
builder = LongTableBuilder(formatter, caption='a long table', label='tab:long', column_format='lrl')
table = builder.get_result()
print(table)""",
        """CODE.from pandas import DataFrame
from pandas.io.formats import format as fmt
df = DataFrame({"a": [1, 2], "b": ["b1", "b2"]})
formatter = fmt.DataFrameFormatter(df)
builder = RegularTableBuilder(formatter, caption='caption', label='lab', column_format='lrc')
table = builder.get_result()
print(table)""",
        """CODE.from pandas import DataFrame
from pandas.io.formats import format as fmt
df = DataFrame({"a": [1, 2], "b": ["b1", "b2"]})
formatter = fmt.DataFrameFormatter(df)
builder = TabularBuilder(formatter, column_format='lrc')
table = builder.get_result()
print(table)""",
        """CODE.iterable_not_string([1, 2, 3])
True
iterable_not_string("foo")
False
iterable_not_string(1)
False""",
        """CODE.iterable_not_string([1, 2, 3])
iterable_not_string("foo")
iterable_not_string(1)""" .

<DEPENDENCY.pandas==1.3.5> <CONTAINS> """CODE.s = pd.Series([1, 2, 3],
              index=pd.date_range('20180101', periods=3, freq='h'))
s
2018-01-01 00:00:00    1
2018-01-01 01:00:00    2
2018-01-01 02:00:00    3
Freq: H, dtype: int64

s.resample('30min').backfill()
2018-01-01 00:00:00    1
2018-01-01 00:30:00    2
2018-01-01 01:00:00    2
2018-01-01 01:30:00    3
2018-01-01 02:00:00    3
Freq: 30T, dtype: int64

s.resample('15min').backfill(limit=2)
2018-01-01 00:00:00    1.0
2018-01-01 00:15:00    NaN
2018-01-01 00:30:00    2.0
2018-01-01 00:45:00    2.0
2018-01-01 01:00:00    2.0
2018-01-01 01:15:00    NaN
2018-01-01 01:30:00    3.0
2018-01-01 01:45:00    3.0
2018-01-01 02:00:00    3.0
Freq: 15T, dtype: float64

df = pd.DataFrame({'a': [2, np.nan, 6], 'b': [1, 3, 5]},
                  index=pd.date_range('20180101', periods=3,
                                      freq='h'))
df
                   a  b
2018-01-01 00:00:00  2.0  1
2018-01-01 01:00:00  NaN  3
2018-01-01 02:00:00  6.0  5

df.resample('30min').backfill()
                   a  b
2018-01-01 00:00:00  2.0  1
2018-01-01 00:30:00  NaN  3
2018-01-01 01:00:00  NaN  3
2018-01-01 01:30:00  6.0  5
2018-01-01 02:00:00  6.0  5

df.resample('15min').backfill(limit=2)
                   a    b
2018-01-01 00:00:00  2.0  1.0
2018-01-01 00:15:00  NaN   NaN
2018-01-01 00:30:00   NaN   3.0
2018-01-01 00:45:00   NaN   3.0
2018-01-01 01:00:00   NaN   3.0
2018-01-01 01:15:00   NaN   NaN
2018-01-01 01:30:00   6.0   5.0
2018-01-01 01:45:00   6.0   5.0
2018-01-01 02:00:00   6.0   5.0""" .

<DEPENDENCY.pandas==1.4.0> <CONTAINS> """CODE.df = pd.DataFrame({
    'gender': ['male', 'male', 'female', 'male', 'female', 'male'],
    'education': ['low', 'medium', 'high', 'low', 'high', 'low'],
    'country': ['US', 'FR', 'US', 'FR', 'FR', 'FR']
})

df.groupby('gender').value_counts()
df.groupby('gender').value_counts(ascending=True)
df.groupby('gender').value_counts(normalize=True)
df.groupby('gender', as_index=False).value_counts()
df.groupby('gender', as_index=False).value_counts(normalize=True)""",
        """CODE.import pandas as pd

df = pd.DataFrame([[1, 2], [3, 4]])
repr_params = pd.io.formats.format.get_dataframe_repr_params()
repr(df) == df.to_string(**repr_params)""",
        """CODE.import pandas as pd

ser = pd.Series([1, 2, 3, 4])
repr_params = pd.io.formats.format.get_series_repr_params()
repr(ser) == ser.to_string(**repr_params)""",
        """CODE.reconstruct_data_with_by(df, by='h', cols=['a', 'b'])
   h1      h2
   a     b     a     b
0  1.0   3.0   NaN   NaN
1  3.0   4.0   NaN   NaN
2  NaN   NaN   5.0   6.0""",
        """CODE.s = pd.Series([1, 2, 3],
...               index=pd.date_range('20180101', periods=3, freq='h'))
s
2018-01-01 00:00:00    1
2018-01-01 01:00:00    2
2018-01-01 02:00:00    3
Freq: H, dtype: int64

s.resample('30min').bfill()
2018-01-01 00:00:00    1
2018-01-01 00:30:00    2
2018-01-01 01:00:00    2
2018-01-01 01:30:00    3
2018-01-01 02:00:00    3
Freq: 30T, dtype: int64

s.resample('15min').bfill(limit=2)
2018-01-01 00:00:00    1.0
2018-01-01 00:15:00    NaN
2018-01-01 00:30:00    2.0
2018-01-01 00:45:00    2.0
2018-01-01 01:00:00    2.0
2018-01-01 01:15:00    NaN
2018-01-01 01:30:00    3.0
2018-01-01 01:45:00    3.0
2018-01-01 02:00:00    3.0""",
        """CODE.s = pd.Series([1, 2, 3],
...               index=pd.date_range('20180101', periods=3, freq='h'))
s
2018-01-01 00:00:00    1
2018-01-01 01:00:00    2
2018-01:01 02:00:00    3
Freq: H, dtype: int64

s.resample('30min').bfill()
2018-01-01 00:00:00    1
2018-01-01 00:30:00    2
2018-01-01 01:00:00    2
2018-01-01 01:30:00    3
2018-01-01 02:00:00    3
Freq: 30T, dtype: int64

s.resample('15min').bfill(limit=2)
2018-01-01 00:00:00    1.0
2018-01-01 00:15:00    NaN
2018-01-01 00:30:00    2.0
2018-01-01 00:45:00    2.0
2018-01-01 01:00:00    2.0
2018-01-01 01:15:00    NaN
2018-01-01 01:30:00    3.0
2018-01-01 01:45:00    3.0
2018-01-01 02:00:00    3.0
Freq: 15T, dtype: float64

df = pd.DataFrame({'a': [2, np.nan, 6], 'b': [1, 3, 5]},
...                   index=pd.date_range('20180101', periods=3,
...                                       freq='h'))
df
                       a    b
2018-01-01 00:00:00  2.0  1
2018-01-01 01:00:00  NaN  3
2018-01-01 02:00:00  6.0  5

df.resample('30min').bfill()
                       a    b
2018-01-01 00:00:00  2.0   1
2018-01-01 00:30:00   NaN   3
2018-01-01 01:00:00   NaN   3
2018-01-01 01:30:00   6.0   5
2018-01-01 02:00:00   6.0   5

df.resample('15min').bfill(limit=2)
                       a    b
2018-01-01 00:00:00   2.0   1.0
2018-01-01 00:15:00   NaN   NaN
2018-01-01 00:30:00   NaN   3.0
2018-01-01 00:45:00   NaN   3.0
2018-01-01 01:00:00   NaN   3.0
2018-01-01 01:15:00   NaN   NaN
2018-01-01 01:30:00   6.0   5.0
2018-01-01 01:45:00   6.0   5.0
2018-01-02 02:00:00   6.0   5.0""" .

<DEPENDENCY.pandas==1.4.4> <CONTAINS> """CODE.ensure_nanosecond_dtype(np.dtype("M8[s]"))
ensure_nanosecond_dtype(np.dtype("m8[ps]"))""",
        "CODE.maybe_infer_dtype_type(Foo(np.dtype(\"i8\")))" .

<DEPENDENCY.pandas==1.5.0> <CONTAINS> """CODE._ensure_nanosecond_dtype(np.dtype("M8[s]"))
_ensure_nanosecond_dtype(np.dtype("m8[ps]"))""",
        "CODE._maybe_infer_dtype_type(Foo(np.dtype(\"i8\")))",
        """CODE.df = pd.DataFrame(data={'col1': [1, 2], 'col2': [4, 3]})
df.to_orc('df.orc')
pd.read_orc('df.orc')""",
        """CODE.df = pd.DataFrame(dict(A=[1, 1, 3], B=[5, None, 6], C=[1, 2, 3]))
df.groupby("A").last()
     B  C
A
1  5.0  2
3  6.0  3""",
        """CODE.import pyarrow as pa
pd.ArrowDtype(pa.int64())
int64[pyarrow]
pd.ArrowDtype(pa.timestamp("s", tz="America/New_York"))
timestamp[s, tz=America/New_York][pyarrow]
pd.ArrowDtype(pa.list_(pa.int64()))
list<item: int64>[pyarrow]""",
        """CODE.pd.read_sql('select * test', conn) # doctest: +SKIP
... # DatabaseError: Execution failed on sql 'test': near "test": syntax error""" .

<DEPENDENCY.pandas==2.0.0> <CONTAINS> """CODE.from pandas.api.types import is_any_real_numeric_dtype
is_any_real_numeric_dtype(int)
is_any_real_numeric_dtype(float)
is_any_real_numeric_dtype(object)
is_any_real_numeric_dtype(str)
is_any_real_numeric_dtype(complex(1, 2))
is_any_real_numeric_dtype(bool)""",
        """CODE.pd.json_normalize(data)
pd.json_normalize(data, max_level=0)
pd.json_normalize(data, max_level=1)
result = pd.json_normalize(data, "counties", ["state", "shortname", ["info", "governor"]])
pd.json_normalize(data, "A", record_prefix="Prefix.")""" .

<DEPENDENCY.pandas==2.1.0> <CONTAINS> """CODE.arr = pd.array([np.nan, np.nan, 2, 3, np.nan, np.nan])
arr._pad_or_backfill(method="backfill", limit=1)""",
        """CODE.arr = pd.arrays.NumpyExtensionArray(np.array([0, 1, np.nan, 3]))
arr.interpolate(method="linear",
                limit=3,
                limit_direction="forward",
                index=pd.Index([1, 2, 3, 4]),
                fill_value=1,
                copy=False,
                axis=0,
                limit_area="inside"
                )""",
        """CODE.df = pd.DataFrame({
...     'col1': ['A', 'A', 'B', np.nan, 'D', 'C'],
...     'col2': [2, 1, 9, 8, 7, 4],
...     'col3': [0, 1, 9, 4, 2, 3],
...     'col4': ['a', 'B', 'c', 'D', 'e', 'F']
... })
df
  col1  col2  col3 col4
0    A     2     0    a
1    A     1     1    B
2    B     9     9    c
3  NaN     8     4    D
4    D     7     2    e
5    C     4     3    F

Sort by col1

df.sort_values(by=['col1'])
  col1  col2  col3 col4
0    A     2     0    a
1    A     1     1    B
2    B     9     9    c
5    C     4     3    F
4    D     7     2    e
3  NaN     8     4    D

Sort by multiple columns

df.sort_values(by=['col1', 'col2'])
  col1  col2  col3 col4
1    A     1     1    B
0    A     2     0    a
2    B     9     9    c
5    C     4     3    F
4    D     7     2    e
3  NaN     8     4    D

Sort Descending

df.sort_values(by='col1', ascending=False)
  col1  col2  col3 col4
4    D     7     2    e
5    C     4     3    F
2    B     9     9    c
0    A     2     0    a
1    A     1     1    B
3  NaN     8     4    D

Putting NAs first

df.sort_values(by='col1', ascending=False, na_position='first')
  col1  col2  col3 col4
3  NaN     8     4    D
4    D     7     2    e
5    C     4     3    F
2    B     9     9    c
0    A     2     0    a
1    A     1     1    B

Sorting with a key function

df.sort_values(by='col4', key=lambda col: col.str.lower())
   col1  col2  col3 col4
0    A     2     0    a
1    A     1     1    B
2    B     9     9    c
3  NaN     8     4    D
4    D     7     2    e
5    C     4     3    F

Natural sort with the key argument,
using the `natsort <https://github.com/SethMMorton/natsort>` package.

df = pd.DataFrame({
...    "time": ['0hr', '128hr', '72hr', '48hr', '96hr'],
...    "value": [10, 20, 30, 40, 50]
... })
df
    time  value
0    0hr     10
1  128hr     20
2   72hr     30
3   48hr     40
4   96hr     50
from natsort import index_natsorted
df.sort_values(
...     by="time",
...     key=lambda x: np.argsort(index_natsorted(df["time"]))
... )
    time  value
0    0hr     10
3   48hr     40
2   72hr     30
4   96hr     50
1  """,
        """CODE.df.map(lambda x: len(str(x)))
df_copy = df.copy()
df_copy.iloc[0, 0] = pd.NA
df_copy.map(lambda x: len(str(x)), na_action='ignore')
df.map(lambda x: x**2)""",
        """CODE.idx = pd.Index([3, 2, 1])
idx.min()

idx = pd.Index(['c', 'b', 'a'])
idx.min()

idx = pd.MultiIndex.from_product([('a', 'b'), (2, 1)])
idx.min()""",
        """CODE.import pandas as pd
idx = pd.Index([10, 20, 30, 40, 50])
idx.diff()""",
        """CODE.import pandas as pd
idx = pd.Index([10.1234, 20.5678, 30.9123, 40.4567, 50.7890])
idx.round(decimals=2)""",
        """CODE.pd.array([1, 2])._hash_pandas_object(encoding='utf-8',
                                     hash_key="1000000000000000",
                                     categorize=False
                                     )""",
        """CODE.s = pd.Series([2, 1, 3, 4], index=['Falcon', 'Falcon', 'Parrot', 'Parrot'])
s.groupby(level=0).is_monotonic_increasing""",
        "CODE.s.groupby(level=0).is_monotonic_decreasing",
        "CODE.ser = df.groupby('animal')['breed'].unique()",
        """CODE.ser = pd.Series([1, 2, 3, 4], index=pd.DatetimeIndex(
                ['2023-01-01', '2023-01-15', '2023-02-01', '2023-02-15']))
ser.resample('MS').max()""",
        """CODE.ser = pd.Series([1, 2, 3, 4], index=pd.DatetimeIndex(
                ['2023-01-01', '2023-01-15', '2023-02-01', '2023-02-15']))
ser.resample('MS').prod()""",
        """CODE.ser = pd.Series([1, 2, 3, 4], index=pd.DatetimeIndex(
                ['2023-01-01', '2023-01-15', '2023-02-01', '2023-02-15']))
ser.resample('MS').sum()""",
        "CODE.ser.resample('MS').min()" .

<DEPENDENCY.pandas==2.2.0> <CONTAINS> """CODE.import pyarrow as pa
import pandas as pd

s = pd.Series(
    [
        [1, 2, 3],
        [3],
    ],
    dtype=pd.ArrowDtype(pa.list_(
        pa.int64()
    ))
)

s.list.len()""",
        """CODE.import pyarrow as pa
import pandas as pd

s = pd.Series(
    [
        {"version": 1, "project": "pandas"},
        {"version": 2, "project": "pandas"},
        {"version": 1, "project": "numpy"},
    ],
    dtype=pd.ArrowDtype(pa.struct(
        [("version", pa.int64()), ("project", pa.string())]
    ))
)

s.struct.dtypes
version     int64[pyarrow]
project    string[pyarrow]
dtype: object""",
        """CODE.import pyarrow as pa
s = pd.Series(
...     [
...         {"version": 1, "project": "pandas"},
...         {"version": 2, "project": "pandas"},
...         {"version": 1, "project": "numpy"},
...     ],
...     dtype=pd.ArrowDtype(pa.struct(
...         [("version", pa.int64()), ("project", pa.string())]
...     ))
... )

Extract by field name.

s.struct.field("project")
0    pandas
1    pandas
2     numpy
Name: project, dtype: string[pyarrow]

Extract by field index.

s.struct.field(0)
0    1
1    2
2    1
Name: version, dtype: int64[pyarrow]

Or an expression

import pyarrow.compute as pc
s.struct.field(pc.field("project"))
0    pandas
1    pandas
2     numpy
Name: project, dtype: string[pyarrow]

For nested struct types, you can pass a list of values to index
multiple levels:

version_type = pa.struct([
...     ("major", pa.int64()),
...     ("minor", pa.int64()),
... ])
s = pd.Series(
...     [
...         {"version": {"major": 1, "minor": 5}, "project": "pandas"},
...         {"version": {"major": 2, "minor": 1}, "project": "pandas"},
...         {"version": {"major": 1, "minor": 26}, "project": "numpy"},
...     ],
...     dtype=pd.ArrowDtype(pa.struct(
...         [("version", version_type), ("project", pa.string())]
...     ))
... )
s.struct.field(["version", "minor"])
0     5
1     1
2    26
Name: minor, dtype: int64[pyarrow]
s.struct.field([0, 0])
0    1
1    2
2    1
Name: major, dtype: int64[pyarrow]""",
        """CODE.s = pd.Series(
...     [
...         [1, 2, 3],
...         [3],
...     ],
...     dtype=pd.ArrowDtype(pa.list_(
...         pa.int64()
...     ))
... )
s.list.flatten()""",
        """CODE.s.struct.explode()
   version project
0        1  pandas
1        2  pandas
2        1   numpy""" .

<DEPENDENCY.pexpect==3.0> <CONTAINS> """CODE.count_unique(['a','b','c','a','c','c','a','c','c'])
count_unique(['a','b','c','a','c','c','a','c','c'])[0][1]""" .

<DEPENDENCY.prompt-toolkit==0.58> <CONTAINS> """CODE.style_from_dict({
    Token: '#ff0000 bold underline',
    Token.Title: 'blink',
    Token.SomethingElse: 'reverse',
})""" .

<DEPENDENCY.prompt-toolkit==0.59> <CONTAINS> """CODE.c = Callback(function)
c.fire()

c = Callback()
c += handler_function  # Add event handler.
c.fire()  # Fire event.
""" .

<DEPENDENCY.prompt-toolkit==1.0.10> <CONTAINS> """CODE.@Condition
def setting_is_true(cli):
    return True  # or False

registy = ConditionalRegistry(registry, setting_is_true)""",
        "CODE.registry.add_key_binding('j', 'j', filter=ViInsertMode())(prefix_meta)" .

<DEPENDENCY.prompt-toolkit==1.0.3> <CONTAINS> "CODE.registry.add_binding(Keys.ControlI)(display_completions_like_readline)" .

<DEPENDENCY.prompt-toolkit==2.0.10> <CONTAINS> """CODE.from prompt_toolkit.eventloop import use_asyncio_event_loop
from asyncio import get_event_loop

use_asyncio_event_loop()
get_event_loop().run_until_complete(
    application.run_async().to_asyncio_future())
""" .

<DEPENDENCY.prompt-toolkit==2.0.3> <CONTAINS> """CODE.input = PipeInput()
input.send('inputdata')""" .

<DEPENDENCY.prompt-toolkit==2.0.4> <CONTAINS> """CODE.input = PosixPipeInput()
input.send_text('inputdata')""" .

<DEPENDENCY.prompt-toolkit==2.0.7> <CONTAINS> """CODE.from prompt_toolkit.widgets import Frame, TextArea
print_container(
    Frame(TextArea(text='Hello world!')))""" .

<DEPENDENCY.prompt-toolkit==3.0.0> <CONTAINS> """CODE.async def interact() -> None:
    await yes_no_dialog("my title", "my text").run_async()

    prompt_session = PromptSession()
    text = await prompt_session.prompt_async("Type something: ")
    print_formatted_text('You said: ', text)

server = PromptToolkitSSHServer(interact=interact)
loop = get_event_loop()
loop.run_until_complete(
    asyncssh.create_server(
        lambda: MySSHServer(interact),
        "",
        port,
        server_host_keys=["/etc/ssh/..."],
    )
)
loop.run_forever()""" .

<DEPENDENCY.protobuf==2.6.0> <CONTAINS> """CODE.class Parent(message.Message):
  __metaclass__ = GeneratedProtocolMessageType
  DESCRIPTOR = descriptor
  class Child(message.Message):
    __metaclass__ = GeneratedProtocolMessageType
    DESCRIPTOR = descriptor.nested_types[0]

file_descriptor = descriptor_pb2.FileDescriptorProto()
file_descriptor.ParseFromString(proto2_string)
msg_descriptor = descriptor.MakeDescriptor(file_descriptor.message_type[0])
msg_class = reflection.MakeClass(msg_descriptor)
msg = msg_class()""" .

<DEPENDENCY.protobuf==3.20.3> <CONTAINS> """CODE.def myParamValidator(param):
  # Advanced logic here
  return True

mock_dao.DoSomething(Func(myParamValidator), true)""",
        "CODE.mock_dao.Connect(IsA(DbConnectInfo))",
        "CODE.mock_dao.GetUsersInfo(In('expectedUserName')).AndReturn(mock_result)",
        "CODE.mock_dao.ProcessUsers(SameElementsAs('stevepm', 'salomaki'))",
        "CODE.mock_dao.RunQuery(StrContains('IN (1, 2, 4, 5)')).AndReturn(mock_result)",
        "CODE.mock_dao.SetTimeout((IsAlmost(3.9)))",
        "CODE.mock_dao.UpdateUsers(ContainsKeyValue('stevepm', stevepm_user_info))",
        """CODE.stubs = StubOutForTesting()
stubs.Set(os.path, 'exists', lambda x: 1)
stubs.UnsetAll()""" .

<DEPENDENCY.protobuf==3.5.1> <CONTAINS> """CODE.def myParamValidator(param):
  # Advanced logic here
  return True

mock_dao.DoSomething(Func(myParamValidator), true)""",
        "CODE.mock_dao.GetUsersInfo(In('expectedUserName')).AndReturn(mock_result)",
        "CODE.mock_dao.ProcessUsers(SameElementsAs('stevepm', 'salomaki'))",
        "CODE.mock_dao.RunQuery(StrContains('IN (1, 2, 4, 5)')).AndReturn(mock_result)",
        "CODE.mock_dao.UpdateUsers(ContainsKeyValue('stevepm', stevepm_user_info))",
        "CODE.mymock.CastMagic(3, IgnoreArg(), 'disappear')" .

<DEPENDENCY.psutil==0.1.3> <CONTAINS> """CODE.c =  wmi.WMI ()
sp = c.Win32_SerialPort ()[0]

for i in sp.references ():
  print i

for i in sp.references (wmi_class="Win32_SerialPortSetting"):
  print i""",
        """CODE.c = wmi.WMI ()
pp = c.Win32_ParallelPort ()[0]

for i in pp.associators (wmi_association_class="Win32_PortResource"):
  print i

for i in pp.associators (wmi_result_class="Win32_PnPEntity"):
  print i""",
        """CODE.c = wmi.WMI ()
raw_wql = "SELECT * FROM __InstanceCreationEvent WITHIN 2 WHERE TargetInstance ISA 'Win32_Process'"
watcher = c.watch_for (raw_wql=raw_wql)
while 1:
  process_created = watcher ()
  print process_created.Name

watcher = c.watch_for (
  notification_type="Creation",
  wmi_class="Win32_Process",
  delay_secs=2,
  Name='calc.exe'
)
calc_created = watcher ()

import pythoncom
import wmi
c = wmi.WMI (privileges=["Security"])
watcher1 = c.watch_for (
  notification_type="Creation",
  wmi_class="Win32_NTLogEvent",
  Type="error"
)
watcher2 = c.watch_for (
  notification_type="Creation",
  wmi_class="Win32_NTLogEvent",
  Type="warning"
)

while 1:
  try:
    error_log = watcher1 (500)
  except wmi.x_wmi_timed_out:
    pythoncom.PumpWaitingMessages ()
  else:
    print error_log

  try:
    warning_log = watcher2 (500)
  except wmi.x_wmi_timed_out:
    pythoncom.PumpWaitingMessages ()
  else:
    print warning_log""",
        """CODE.import win32com.client
import wmi

wmiobj = win32com.client.GetObject ("winmgmts:Win32_LogicalDisk.DeviceID='C:'")
c_drive = wmi._wmi_object (wmiobj)
print c_drive""",
        """CODE.import win32con
import wmi
c = wmi.WMI ()
startup = c.Win32_ProcessStartup.new (ShowWindow=win32con.SW_SHOWMINIMIZED)
pid, retval = c.Win32_Process.Create (
  CommandLine="notepad.exe",
  ProcessStartupInformation=startup
)""",
        """CODE.pp0 = wmi.WMI ().Win32_ParallelPort ()[0]
print ' <- '.join (pp0.derivation ())""",
        """CODE.pp0 = wmi.WMI ().Win32_ParallelPort ()[0]
print pp0.path ().RelPath""",
        """CODE.pp0 = wmi.WMI().Win32_ParallelPort()[0]
print(' <- '.join(pp0.derivation()))""",
        "CODE.print \"%08X\" % signed_to_unsigned (-2147023174)",
        """CODE.remote_connetion = wmi.connect_server (
    server="remote_machine", user="myname", password="mypassword"
)
c = wmi.WMI (wmi=remote_connection)""",
        """CODE.wmi.WMI ().instances ("Win32_LogicalDisk")
wmi.WMI ().Win32_LogicalDisk ()""",
        """CODE.wmi.WMI().instances("Win32_LogicalDisk")
wmi.WMI().Win32_LogicalDisk()""" .

<DEPENDENCY.psutil==0.4.0> <CONTAINS> """CODE.bytes2human(10000)
bytes2human(100001221)""" .

<DEPENDENCY.psutil==0.5.0> <CONTAINS> """CODE.import psutil
from subprocess import PIPE
p = psutil.Popen(["/usr/bin/python", "-c", "print 'hi'"], stdout=PIPE)
p.name
p.uids
p.username
p.communicate()
p.terminate()
p.wait(timeout=2)""" .

<DEPENDENCY.psutil==0.7.0> <CONTAINS> """CODE.cpu_times_percent()
cpupercent(user=4.8, nice=0.0, system=4.8, idle=90.5, iowait=0.0,
             irq=0.0, softirq=0.0, steal=0.0, guest=0.0, guest_nice=0.0)""" .

<DEPENDENCY.psutil==1.0.0> <CONTAINS> "CODE.python setup.py clean' custom command." .

<DEPENDENCY.psutil==1.2.0> <CONTAINS> """CODE.def on_terminate(proc):
    print("process {} terminated".format(proc))

for p in procs:
    p.terminate()

gone, still_alive = wait_procs(procs, 3, callback=on_terminate)
for p in still_alive:
    p.kill()""" .

<DEPENDENCY.psutil==5.6.0> <CONTAINS> """CODE.if get_winver() <= WIN_VISTA:
    ...""" .

<DEPENDENCY.psutil==5.6.7> <CONTAINS> """CODE.if get_winver() <= WIN_VISTA:
    ...""" .

<DEPENDENCY.psutil==5.7.1> <CONTAINS> """CODE.ns = process_namespace(psutil.Process())
for fun, name in ns.iter(ns.getters):
    fun()""" .

<DEPENDENCY.psutil==5.9.4> <CONTAINS> """CODE.powershell(
    "Get-CIMInstance Win32_PageFileUsage | Select AllocatedBaseSize")""",
        """CODE.wmic("Win32_OperatingSystem", "FreePhysicalMemory")
2134124534""" .

<DEPENDENCY.pyarrow==0.14.0> <CONTAINS> """CODE.archery build --cc=clang-7 --cxx=clang++-7 --cxx-flags=-mavx2 clang7-build
archery build --targets=all --targets=test build""" .

<DEPENDENCY.pyarrow==0.2.0> <CONTAINS> """CODE.name = 'foo'
_column_name_to_strings(name)
'foo'
name = ('foo', 'bar')
_column_name_to_strings(name)
('foo', 'bar')
import pandas as pd
name = (1, pd.Timestamp('2017-02-01 00:00:00'))
_column_name_to_strings(name)
('1', '2017-02-01 00:00:00')""" .

<DEPENDENCY.pyarrow==0.4.0> <CONTAINS> """CODE.name = 'foo'
_column_name_to_strings(name)
'foo'
name = ('foo', 'bar')
_column_name_to_strings(name)
('foo', 'bar')
import pandas as pd
name = (1, pd.Timestamp('2017-02-01 00:00:00'))
_column_name_to_strings(name)
('1', '2017-02-01 00:00:00')""" .

<DEPENDENCY.pyarrow==0.4.1> <CONTAINS> """CODE.name = 'foo'
_column_name_to_strings(name)
'foo'
name = ('foo', 'bar')
_column_name_to_strings(name)
('foo', 'bar')
import pandas as pd
name = (1, pd.Timestamp('2017-02-01 00:00:00'))
_column_name_to_strings(name)
('1', '2017-02-01 00:00:00')""" .

<DEPENDENCY.pyarrow==0.8.0> <CONTAINS> """CODE.name = 'foo'
_column_name_to_strings(name)
'foo'
name = ('foo', 'bar')
_column_name_to_strings(name)
('foo', 'bar')
import pandas as pd
name = (1, pd.Timestamp('2017-02-01 00:00:00'))
_column_name_to_strings(name)
('1', '2017-02-01 00:00:00')""" .

<DEPENDENCY.pyarrow==1.0.0> <CONTAINS> "CODE.PyFileSystem(FSSpecHandler(fsspec_fs))",
        """CODE.arr = pa.array([1, 2, None, 3], type=pa.int8())
fill_value = pa.scalar(5, type=pa.int8())
arr.fill_null(fill_value)""",
        """CODE.cast(arr, pa.timestamp('ms'))
cast(arr, pa.timestamp('ms')).type
arr.cast('timestamp[ms]')
arr.cast('timestamp[ms]').type""",
        """CODE.import pyarrow as pa
arr = pa.array(["a", "b", "c", None, "e", "f"])
indices = pa.array([0, None, 4, 3])
arr.take(indices)""",
        """CODE.import pyarrow as pa
arr = pa.array(["a", "b", "c", None, "e"])
mask = pa.array([True, False, None, False, True])
arr.filter(mask)
arr.filter(mask, null_selection_behavior='emit_null')
""" .

<DEPENDENCY.pyarrow==10.0.0> <CONTAINS> "CODE.filters_to_expression([('foo', '==', 'bar')])" .

<DEPENDENCY.pyarrow==13.0.0> <CONTAINS> "CODE.func_c()" .

<DEPENDENCY.pyarrow==14.0.2> <CONTAINS> """CODE.import pyarrow as pa
import pandas as pd
df = pd.DataFrame({'year': [2020, 2022, 2021, 2022, 2019, 2021],
                   'n_legs': [2, 2, 4, 4, 5, 100],
                   'animal': ["Flamingo", "Parrot", "Dog", "Horse",
                              "Brittle stars", "Centipede"]})
table = pa.Table.from_pandas(df)
import pyarrow.parquet as pq
pq.write_table(table, 'table_V2.parquet')
dataset = pq.ParquetDataset('table_V2.parquet',
                            use_legacy_dataset=False)
dataset.read_pandas(columns=["n_legs"])
dataset.read_pandas(columns=["n_legs"]).schema.pandas_metadata""",
        """CODE.import pyarrow as pa
table = pa.table({'year': [2020, 2022, 2021, 2022, 2019, 2021],
                  'n_legs': [2, 2, 4, 4, 5, 100],
                  'animal': ["Flamingo", "Parrot", "Dog", "Horse",
                             "Brittle stars", "Centipede"]})
import pyarrow.parquet as pq
pq.write_to_dataset(table, root_path='dataset_v2_files',
                    partition_cols=['year'],
                    use_legacy_dataset=False)
dataset = pq.ParquetDataset('dataset_v2_files/',
                            use_legacy_dataset=False)
dataset.files""",
        """CODE.import pyarrow as pa
table = pa.table({'year': [2020, 2022, 2021, 2022, 2019, 2021],
                  'n_legs': [2, 2, 4, 4, 5, 100],
                  'animal': ["Flamingo", "Parrot", "Dog", "Horse",
                             "Brittle stars", "Centipede"]})
import pyarrow.parquet as pq
pq.write_to_dataset(table, root_path='dataset_v2_fragments',
                    partition_cols=['year'],
                    use_legacy_dataset=False)
dataset = pq.ParquetDataset('dataset_v2_fragments/',
                            use_legacy_dataset=False)
dataset.fragments
""",
        """CODE.import pyarrow as pa
table = pa.table({'year': [2020, 2022, 2021, 2022, 2019, 2021],
                  'n_legs': [2, 2, 4, 4, 5, 100],
                  'animal': ["Flamingo", "Parrot", "Dog", "Horse",
                             "Brittle stars", "Centipede"]})
import pyarrow.parquet as pq
pq.write_to_dataset(table, root_path='dataset_v2_schema',
                    partition_cols=['year'],
                    use_legacy_dataset=False)
dataset = pq.ParquetDataset('dataset_v2_schema/',
                            use_legacy_dataset=False)
dataset.schema""",
        """CODE.import pyarrow as pa
table = pa.table({'year': [2020, 2022, 2021, 2022, 2019, 2021],
...                   'n_legs': [2, 2, 4, 4, 5, 100],
...                   'animal': ["Flamingo", "Parrot", "Dog", "Horse",
...                              "Brittle stars", "Centipede"]})
import pyarrow.parquet as pq
pq.write_to_dataset(table, root_path='dataset_v2_read',
...                     partition_cols=['year'],
...                     use_legacy_dataset=False)
dataset = pq.ParquetDataset('dataset_v2_read/',
...                             use_legacy_dataset=False)
dataset.read(columns=["n_legs"])""" .

<DEPENDENCY.pyarrow==15.0.0> <CONTAINS> """CODE.import pyarrow as pa
table = pa.table({'year': [2020, 2022, 2021, 2022, 2019, 2021],
                  'n_legs': [2, 2, 4, 4, 5, 100],
                  'animal': ["Flamingo", "Parrot", "Dog", "Horse",
                             "Brittle stars", "Centipede"]})
import pyarrow.parquet as pq
pq.write_to_dataset(table, root_path='dataset_v2_schema',
                    partition_cols=['year'])
dataset = pq.ParquetDataset('dataset_v2_schema/')
dataset.schema""" .

<DEPENDENCY.pyarrow==2.0.0> <CONTAINS> """CODE.import pyarrow as pa
import pyarrow.compute as pc
arr = pa.array([1, 1, 2, 2, 3, 2, 2, 2])
pc.mode(arr)""" .

<DEPENDENCY.pyarrow==6.0.0> <CONTAINS> """CODE.import pyarrow as pa
import pyarrow.compute as pc
arr = pa.array(["a", "b", "c", None, "e", "f"])
pc.bottom_k_unstable(arr, k=3)""",
        """CODE.import pyarrow as pa
import pyarrow.compute as pc
arr = pa.array(["a", "b", "c", None, "e", "f"])
pc.top_k_unstable(arr, k=3)
""" .

<DEPENDENCY.pyarrow==6.0.2> <CONTAINS> """CODE.arr = pa.array(["a", "b", "c", None, "e"])
mask = pa.array([True, False, None, False, True])
arr.filter(mask)
arr.filter(mask, null_selection_behavior='emit_null')""",
        """CODE.import pyarrow as pa
import pyarrow.compute as pc
arr = pa.array([1, 1, 2, 2, 3, 2, 2, 2])
modes = pc.mode(arr, 2)
modes[0]
modes[1]""" .

<DEPENDENCY.pyarrow==8.0.0> <CONTAINS> """CODE.import pyarrow as pa
table = pa.table({'n_legs': [2, 2, 4, 4, 5, 100],
                  'animal': ["Flamingo", "Parrot", "Dog", "Horse",
                             "Brittle stars", "Centipede"]})
import pyarrow.parquet as pq
pq.write_table(table, 'example.parquet')
parquet_file = pq.ParquetFile('example.parquet')

parquet_file.num_row_groups""",
        """CODE.import pyarrow as pa
table = pa.table({'year': [2020, 2022, 2021, 2022, 2019, 2021],
                  'n_legs': [2, 2, 4, 4, 5, 100],
                  'animal': ["Flamingo", "Parrot", "Dog", "Horse",
                             "Brittle stars", "Centipede"]})
import pyarrow.parquet as pq
pq.write_to_dataset(table, root_path='dataset_v2_files',
                    partition_cols=['year'],
                    use_legacy_dataset=False)
dataset = pq._ParquetDatasetV2('dataset_v2_files/')
dataset.files""",
        """CODE.import pyarrow as pa
table = pa.table({'year': [2020, 2022, 2021, 2022, 2019, 2021],
                  'n_legs': [2, 2, 4, 4, 5, 100],
                  'animal': ["Flamingo", "Parrot", "Dog", "Horse",
                             "Brittle stars", "Centipede"]})
import pyarrow.parquet as pq
pq.write_to_dataset(table, root_path='dataset_v2_fragments',
                    partition_cols=['year'],
                    use_legacy_dataset=False)
dataset = pq._ParquetDatasetV2('dataset_v2_fragments/')
dataset.fragments
""",
        """CODE.import pyarrow as pa
table = pa.table({'year': [2020, 2022, 2021, 2022, 2019, 2021],
                  'n_legs': [2, 2, 4, 4, 5, 100],
                  'animal': ["Flamingo", "Parrot", "Dog", "Horse",
                             "Brittle stars", "Centipede"]})
import pyarrow.parquet as pq
pq.write_to_dataset(table, root_path='dataset_v2_schema',
                    partition_cols=['year'],
                    use_legacy_dataset=False)
dataset = pq._ParquetDatasetV2('dataset_v2_schema/')

dataset.schema""" .

<DEPENDENCY.pyrsistent==0.10.0> <CONTAINS> """CODE.from pyrsistent import PClass, field
class AClass(PClass):
...     x = field()
...
a = AClass(x=1)
a2 = a.set(x=2)
a3 = a.set('x', 3)
a
AClass(x=1)
a2
AClass(x=2)
a3
AClass(x=3)""",
        """CODE.pbag([]).count('non-existent')
0
pbag([1, 1, 2]).count(1)""",
        "CODE.pdeque([1, 2, 1]).count(1)",
        """CODE.pdeque([1, 2, 3]).reverse()
reversed(pdeque([1, 2, 3]))""",
        "CODE.pdeque([1, 2]).append(3)",
        "CODE.pdeque([1, 2]).appendleft(3)",
        "CODE.pdeque([1, 2]).extend([3, 4])",
        "CODE.pdeque([1, 2]).extendleft([3, 4])",
        """CODE.pdeque([1, 2]).pop()
pdeque([1])
pdeque([1, 2]).pop(2)
pdeque([])
pdeque([1, 2]).pop(-1)
pdeque([2])""",
        "CODE.pdeque([1, 2]).popleft()",
        """CODE.pdeque([2, 1, 2]).remove(2)
pdeque([1, 2])""",
        """CODE.s = pbag([1])
s2 = s.add(1)
s3 = s.add(2)""",
        """CODE.s2 = pbag([1, 1, 2])
s3 = s2.remove(1)
s4 = s3.remove(2)
s4
pbag([1, 1])""",
        """CODE.x = pdeque([1, 2, 3])
x.rotate(1)
x.rotate(-2)""" .

<DEPENDENCY.pyrsistent==0.11.0> <CONTAINS> """CODE.v1 = v(1, 2, 3, 4, 5)
v1.delete(1)
v1.delete(1, 3)""" .

<DEPENDENCY.pyrsistent==0.11.9> <CONTAINS> """CODE.v1 = v(1, 2, 3, 2, 1)
v2 = v1.remove(1)
v2
v2.remove(1)""" .

<DEPENDENCY.pyrsistent==0.12.0> <CONTAINS> """CODE.from pyrsistent import freeze
transaction = freeze({'name': 'Alice',
                      'purchase': {'items': ['Apple', 'Orange'],
                                   'costs': [0.50, 1.25]},
                      'credit card': '5555-1234-1234-1234'})
get_in(['purchase', 'items', 0], transaction)
get_in(['name'], transaction)
get_in(['purchase', 'total'], transaction)
get_in(['purchase', 'items', 'apple'], transaction)
get_in(['purchase', 'items', 10], transaction)
get_in(['purchase', 'total'], transaction, 0)
get_in(['y'], {}, no_default=True)""",
        """CODE.s = pbag([1])
s.update([1, 2])""",
        """CODE.s1 = s(1, 2)
s1.update([3, 4, 4])""" .

<DEPENDENCY.pyrsistent==0.4.0> <CONTAINS> """CODE.class PositivePoint(immutable('x, y')):
    __slots__ = tuple()
    def __new__(cls, x, y):
        if x > 0 and y > 0:
            return super(PositivePoint, cls).__new__(cls, x, y)
        raise Exception('Coordinates must be positive!')""",
        """CODE.s1 = s(1, 2, 3, 2)
s1""",
        """CODE.v1 = v(1, 2, 3)
v1.set(1, 4)
v1.set(3, 4)
v1.set(-1, 4)""",
        """CODE.v1 = v(1, 2, m(a=5, b=6))
v1.set_in((2, 'b'), 17)
v1.set_in((2, 'c', 'd'), 17)""" .

<DEPENDENCY.pyrsistent==0.5.0> <CONTAINS> """CODE.freeze(set([1, 2]))
freeze([1, {'a': 3}])
freeze((1, []))""",
        """CODE.from operator import add
m1 = m(a=1, b=2)
m1.merge_with(add, m(a=2))
pmap({'a': 3, 'b': 2})
m1 = m(a=1)
m1.merge_with(lambda l, r: l, m(a=2), {'a':3})
pmap({'a': 1})""",
        """CODE.pbag([1, 2, 3, 2])
pbag([1, 2, 2, 3])""",
        """CODE.pbag([]).count('non-existent')
0
pbag([1, 1, 2]).count(1)""",
        """CODE.s2 = pbag([1, 1, 2])
s3 = s2.remove(1)
s4 = s3.remove(2)
s4
pbag([1, 1])""",
        """CODE.thaw(s(1, 2))
thaw(v(1, m(a=3)))
thaw((1, v()))""" .

<DEPENDENCY.pyrsistent==0.6.0> <CONTAINS> """CODE.Point = immutable('x, y', name='Point')

p = Point(1, 2)

p2 = p.set(x=3)

Point(x=1, y=2)

Point(x=3, y=2)

Point = immutable('x, y, id_', name='Point')

p = Point(1, 2, id_=17)

p.set(x=3)

Point(x=3, y=2, id_=17)

p.set(id_=18)""",
        """CODE.class PositivePoint(immutable('x, y')):
    __slots__ = tuple()
    def __new__(cls, x, y):
        if x > 0 and y > 0:
            return super(PositivePoint, cls).__new__(cls, x, y)
        raise Exception('Coordinates must be positive!')""",
        """CODE.plist([1, 2, 3]).reverse()
reversed(plist([1, 2, 3]))""",
        "CODE.plist([1, 2]).cons(3)",
        """CODE.plist([1, 2]).mcons([3, 4])
plist([4, 3, 1, 2])""" .

<DEPENDENCY.pyrsistent==0.6.1> <CONTAINS> """CODE.Point = pclass('x, y', name='Point')

p = Point(1, 2)

p2 = p.set(x=3)

Point = pclass('x, y, id_', name='Point')

p = Point(1, 2, id_=17)

p.set(x=3)

p.set(id_=18)""",
        """CODE.class PositivePoint(pclass('x, y')):
    __slots__ = tuple()
    def __new__(cls, x, y):
        if x > 0 and y > 0:
            return super(PositivePoint, cls).__new__(cls, x, y)
        raise Exception('Coordinates must be positive!')""" .

<DEPENDENCY.pyrsistent==0.7.0> <CONTAINS> """CODE.v1 = v(1, 2, 3, 4, 5)
e = v1.evolver()
e[1] = 22
e.append(6)
e.extend([7, 8, 9])
e[8] += 1
len(e)
9
v2 = e.pvector()
v2
pvector([1, 22, 3, 4, 5, 6, 7, 8, 10])""" .

<DEPENDENCY.pyrsistent==0.8.0> <CONTAINS> """CODE.news_paper = freeze({'articles': [{'author': 'Sara', 'content': 'A short article'},
                                  {'author': 'Steve', 'content': 'A slightly longer article'}],
                     'weather': {'temperature': '11C', 'wind': '5m/s'}})
short_news = news_paper.transform(['articles', ny, 'content'], lambda c: c[:25] + '...' if len(c) > 25 else c)
very_short_news = news_paper.transform(['articles', ny, 'content'], lambda c: c[:15] + '...' if len(c) > 15 else c)""",
        """CODE.news_paper = freeze({'articles': [{'author': 'Sara', 'content': 'A short article'},
                                  {'author': 'Steve', 'content': 'A slightly longer article'}],
                     'weather': {'temperature': '11C', 'wind': '5m/s'}})
short_news = news_paper.transform(['articles', ny, 'content'], lambda c: c[:25] + '...' if len(c) > 25 else c)
very_short_news = news_paper.transform(['articles', ny, 'content'], lambda c: c[:15] + '...' if len(c) > 15 else c)
very_short_news.articles[0].content
very_short_news.articles[1].content""" .

<DEPENDENCY.pyrsistent==0.9.2> <CONTAINS> """CODE.Point = pclass('x, y', name='Point')

p = Point(1, 2)

p2 = p.set(x=3)

Point = pclass('x, y, id_', name='Point')

p = Point(1, 2, id_=17)

p.set(x=3)

p.set(id_=18)""",
        """CODE.class IntToFloatMap(CheckedPMap):
    __key_type__ = int
    __value_type__ = float
    __invariant__ = lambda k, v: (int(v) == k, 'Invalid mapping')

IntToFloatMap({1: 1.5, 2: 2.25})""",
        """CODE.class PositivePoint(pclass('x, y')):
    __slots__ = tuple()
    def __new__(cls, x, y):
        if x > 0 and y > 0:
            return super(PositivePoint, cls).__new__(cls, x, y)
        raise Exception('Coordinates must be positive!')""",
        """CODE.class Positives(CheckedPSet):
    __type__ = (long, int)
    __invariant__ = lambda n: (n >= 0, 'Negative')

Positives([1, 2, 3])""",
        """CODE.class Positives(CheckedPVector):
    __type__ = (long, int)
    __invariant__ = lambda n: (n >= 0, 'Negative')

Positives([1, 2, 3])""" .

<DEPENDENCY.pyrsistent==0.9.3> <CONTAINS> """CODE.Point = immutable('x, y', name='Point')

p = Point(1, 2)

p2 = p.set(x=3)

Point = immutable('x, y, id_', name='Point')

p = Point(1, 2, id_=17)

p.set(x=3)

p.set(id_=18)""",
        """CODE.class PositivePoint(immutable('x, y')):
    __slots__ = tuple()
    def __new__(cls, x, y):
        if x > 0 and y > 0:
            return super(PositivePoint, cls).__new__(cls, x, y)
        raise Exception('Coordinates must be positive!')

Point = immutable('x, y', name='Point')
p = Point(1, 2)
p2 = p.set(x=3)

Point = immutable('x, y, id_', name='Point')
p = Point(1, 2, id_=17)
p.set(x=3)""" .

<DEPENDENCY.pyrsistent==0.9.4> <CONTAINS> """CODE.m1 = m(a=5, b=6, c=v(1, 2))
m1.set_in(('c', 1), 17)""",
        """CODE.pbag([]).count('non-existent')
0
pbag([1, 1, 2]).count(1)""",
        "CODE.pdeque([1, 2, 1]).count(1)",
        """CODE.pdeque([1, 2, 3]).reverse()
reversed(pdeque([1, 2, 3]))""",
        "CODE.pdeque([1, 2]).append(3)",
        "CODE.pdeque([1, 2]).appendleft(3)",
        "CODE.pdeque([1, 2]).extend([3, 4])",
        "CODE.pdeque([1, 2]).extendleft([3, 4])",
        """CODE.pdeque([1, 2]).pop()
pdeque([1])
pdeque([1, 2]).pop(2)
pdeque([])
pdeque([1, 2]).pop(-1)
pdeque([2])""",
        "CODE.pdeque([1, 2]).popleft()",
        """CODE.pdeque([2, 1, 2]).remove(2)
pdeque([1, 2])""",
        """CODE.s2 = pbag([1, 1, 2])
s3 = s2.remove(1)
s4 = s3.remove(2)""",
        """CODE.s2 = s.add(1)
s3 = s.add(2)""",
        """CODE.v1 = v(1, 2, m(a=5, b=6))
v1.set_in((2, 'b'), 17)
v1.set_in((2, 'c', 'd'), 17)""",
        """CODE.x = pdeque([1, 2, 3])
x.rotate(1)
x.rotate(-2)""" .

<DEPENDENCY.pytest-xdist==1.19.0> <CONTAINS> """CODE.epsilon1(10, 20)
40
epsilon1(30)
1040""",
        """CODE.epsilon2(10, 20)
epsilon2(30)""" .

<DEPENDENCY.pytest-xdist==2.3.0> <CONTAINS> """CODE.def pytest_handlecrashitem(crashitem, report, sched):
    if should_rerun(crashitem):
        sched.mark_test_pending(crashitem)
        report.outcome = "rerun\"""" .

<DEPENDENCY.pytest==2.8.0> <CONTAINS> """CODE.with warns(RuntimeWarning):
    warnings.warn("my warning", RuntimeWarning)
""" .

<DEPENDENCY.pytest==2.8.1> <CONTAINS> """CODE.cache.get(key, default)
cache.set(key, value)""" .

<DEPENDENCY.pytest==3.0.0> <CONTAINS> """CODE.
0.1 + 0.2 == approx(0.3)
(0.1 + 0.2, 0.2 + 0.4) == approx((0.3, 0.6))
1.0001 == approx(1, rel=1e-3)
1.0001 == approx(1, abs=1e-3)
1 + 1e-8 == approx(1)
1 + 1e-8 == approx(1, abs=1e-12)
1 + 1e-8 == approx(1, rel=1e-6, abs=1e-12)
""" .

<DEPENDENCY.pytest==3.1.3> <CONTAINS> """CODE.abs((0.1 + 0.2) - 0.3) < 1e-6
True
from pytest import approx
0.1 + 0.2 == approx(0.3)
True
(0.1 + 0.2, 0.2 + 0.4) == approx((0.3, 0.6))
True
1.0001 == approx(1)
False
1.0001 == approx(1, rel=1e-3)
True
1.0001 == approx(1, abs=1e-3)
True
1 + 1e-8 == approx(1)
True
1 + 1e-8 == approx(1, abs=1e-12)
False
1 + 1e-8 == approx(1, rel=1e-6, abs=1e-12)
True""" .

<DEPENDENCY.pytest==3.2.0> <CONTAINS> """CODE.abs((0.1 + 0.2) - 0.3) < 1e-6
True
0.1 + 0.2 == approx(0.3)
True
(0.1 + 0.2, 0.2 + 0.4) == approx((0.3, 0.6))
True
{'a': 0.1 + 0.2, 'b': 0.2 + 0.4} == approx({'a': 0.3, 'b': 0.6})
True
import numpy as np
np.array([0.1, 0.2]) + np.array([0.2, 0.4]) == approx(np.array([0.3, 0.6]))
True""" .

<DEPENDENCY.pytest==3.5.0> <CONTAINS> """CODE.@pytest.mark.parametrize("test_input,expected", [
    ("3+5", 8),
    pytest.param("6*9", 42, marks=pytest.mark.xfail),
])
def test_eval(test_input, expected):
    assert eval(test_input) == expected""",
        """CODE.def test_function(record_property):
    record_property("example_key", 1)""" .

<DEPENDENCY.pytest==3.6.0> <CONTAINS> """CODE.import functools
def test_partial(monkeypatch):
    with monkeypatch.context() as m:
        m.setattr(functools, "partial", 3)""" .

<DEPENDENCY.pytest==4.5.0> <CONTAINS> """CODE... code-block:: python

    def test_foo(record_testsuite_property):
        record_testsuite_property("ARCH", "PPC")
        record_testsuite_property("STORAGE_TYPE", "CEPH")""" .

<DEPENDENCY.pytest==6.2.0> <CONTAINS> """CODE.class catch_threading_exception:
    def __enter__(self):
        self.args = None
        return self

    def __exit__(self, exc_type, exc_value, traceback):
        if exc_type is not None:
            self.args = (exc_type, exc_value, traceback)

with catch_threading_exception() as cm:
    # code spawning a thread which raises an exception
    ...
    # check the thread exception: use cm.args
    ...
# cm.args attribute no longer exists at this point
# (to break a reference cycle)
""",
        """CODE.pytester.makefile(".txt", "line1", "line2")
pytester.makefile(".ini", pytest="[pytest]\\naddopts=-rs\\n")""",
        """CODE.pytester.makepyfile("foobar")
pytester.makepyfile(custom="foobar")""",
        """CODE.pytester.maketxtfile("foobar")
pytester.maketxtfile(custom="foobar")""" .

<DEPENDENCY.pytest==7.2.0> <CONTAINS> "CODE.path.check(file=1, link=1)  # a link pointing to a file" .

<DEPENDENCY.pytest==8.0.0> <CONTAINS> """CODE.print(config.get_verbosity())
print(config.get_verbosity(Config.VERBOSITY_ASSERTIONS))  # 2""" .

<DEPENDENCY.pytorch-lightning==0.10.0> <CONTAINS> """CODE.# Setup model and trainer
model = MyModelClass(hparams)
trainer = pl.Trainer()

# Run lr finder
lr_finder = trainer.lr_find(model, ...)

# Inspect results
fig = lr_finder.plot(); fig.show()
suggested_lr = lr_finder.suggestion()

# Overwrite lr and create new model
hparams.lr = suggested_lr
model = MyModelClass(hparams)

# Ready to train with new learning rate
trainer.fit(model)
""",
        """CODE.class SimpleModel(LightningModule):
    def __init__(self):
        super().__init__()
        self.l1 = torch.nn.Linear(in_features=64, out_features=4)

    def forward(self, x):
        return torch.relu(self.l1(x.view(x.size(0), -1)))

model = SimpleModel()
torch.jit.save(model.to_torchscript(), "model.pt")
os.path.isfile("model.pt")
""",
        """CODE.def backward(self, trainer, loss, optimizer, optimizer_idx):
    loss.backward()""",
        """CODE.def configure_apex(self, amp, model, optimizers, amp_level):
    model, optimizers = amp.initialize(
        model, optimizers, opt_level=amp_level,
    )

    return model, optimizers""",
        """CODE.def on_load_checkpoint(self, checkpoint):
    # 99% of the time you don't need to implement this method
    self.something_cool_i_want_to_save = checkpoint['something_cool_i_want_to_save']""",
        """CODE.def on_save_checkpoint(self, checkpoint):
    # 99% of use cases you don't need to implement this method
    checkpoint['something_cool_i_want_to_save'] = my_cool_pickable_object
""",
        """CODE.def prepare_data(self):
    # good
    download_data()
    tokenize()
    etc()

    # bad
    self.split = data_split
    self.some_state = some_other_state()

# DEFAULT
# called once per node on LOCAL_RANK=0 of that node
Trainer(prepare_data_per_node=True)

# call on GLOBAL_RANK=0 (great for shared file systems)
Trainer(prepare_data_per_node=False)

model.prepare_data()
    if ddp/tpu: init()
model.setup(stage)
model.train_dataloader()
model.val_dataloader()
model.test_dataloader()
""",
        """CODE.def test_dataloader(self):
    transform = transforms.Compose([transforms.ToTensor(),
                                    transforms.Normalize((0.5,), (1.0,))])
    dataset = MNIST(root='/path/to/mnist/', train=False, transform=transform,
                    download=True)
    loader = torch.utils.data.DataLoader(
        dataset=dataset,
        batch_size=self.batch_size,
        shuffle=False
    )

    return loader

# can also return multiple dataloaders
def test_dataloader(self):
    return [loader_a, loader_b, ..., loader_n]
""",
        """CODE.def train_dataloader(self):
    transform = transforms.Compose([transforms.ToTensor(),
                                    transforms.Normalize((0.5,), (1.0,))])
    dataset = MNIST(root='/path/to/mnist/', train=True, transform=transform,
                    download=True)
    loader = torch.utils.data.DataLoader(
        dataset=dataset,
        batch_size=self.batch_size,
        shuffle=True
    )
    return loader""",
        """CODE.def transfer_batch_to_device(self, batch, device):
    if isinstance(batch, CustomBatch):
        # move all tensors in your custom data structure to the device
        batch.samples = batch.samples.to(device)
        batch.targets = batch.targets.to(device)
    else:
        batch = super().transfer_batch_to_device(data, device)
    return batch""",
        """CODE.def val_dataloader(self):
    transform = transforms.Compose([transforms.ToTensor(),
                                    transforms.Normalize((0.5,), (1.0,))])
    dataset = MNIST(root='/path/to/mnist/', train=False,
                    transform=transform, download=True)
    loader = torch.utils.data.DataLoader(
        dataset=dataset,
        batch_size=self.batch_size,
        shuffle=False
    )

    return loader

# can also return multiple dataloaders
def val_dataloader(self):
    return [loader_a, loader_b, ..., loader_n]
""",
        """CODE.embeddings = torch.tensor([[1., 2., 3., 4.], [1., 2., 3., 4.], [4., 5., 6., 7.]])
embedding_similarity(embeddings)""",
        """CODE.from pytorch_lightning import Trainer
parser = ArgumentParser(add_help=False)
parser = Trainer.add_argparse_args(parser)
parser.add_argument('--my_custom_arg', default='something')  # doctest: +SKIP
args = Trainer.parse_argparser(parser.parse_args(""))
trainer = Trainer.from_argparse_args(args, logger=False)""",
        """CODE.parser = argparse.ArgumentParser()
parser = Trainer.add_argparse_args(parser)
args = parser.parse_args([])
pprint.pprint(vars(args))  # doctest: +ELLIPSIS +NORMALIZE_WHITESPACE
{...
 'check_val_every_n_epoch': 1,
 'checkpoint_callback': True,
 'default_root_dir': None,
 'deterministic': False,
 'distributed_backend': None,
 'early_stop_callback': False,
 ...
 'logger': True,
 'max_epochs': 1000,
 'max_steps': None,
 'min_epochs': 1,
 'min_steps': None,
 ...
 'profiler': None,
 'progress_bar_refresh_rate': 1,
 ...}""",
        """CODE.values = {'loss': loss, 'acc': acc, ..., 'metric_n': metric_n}
self.log_dict(values)""" .

<DEPENDENCY.pytorch-lightning==0.3.3> <CONTAINS> """CODE.for optimizer in optimizers:
    optimizer.step()
    model.on_before_zero_grad(optimizer) # < ---- called here
    optimizer.zero_grad""" .

<DEPENDENCY.pytorch-lightning==0.5.2> <CONTAINS> """CODE.@RunIf(...)
@pytest.mark.parametrize("arg1", [1, 2.0])
def test_wrapper(arg1):
    assert arg1 > 0.0""",
        """CODE.Backbone()
(l1): Linear(...)
(l2): Linear(...)  """,
        """CODE.Backbone()
Backbone(
  (l1): Linear(...)
  (l2): Linear(...)
)""",
        """CODE.DQN(10, 5)
DQN(
  (net): Sequential(...)
)""",
        """CODE.DQNLightning(env="CartPole-v1")
DQNLightning(
  (net): DQN(
    (net): Sequential(...)
  )
  (target_net): DQN(
    (net): Sequential(...)
  )
)""",
        """CODE.Discriminator(img_shape=(1, 28, 28))
Discriminator(
  (model): Sequential(...)
)""",
        """CODE.DoubleConv(4, 4)
DoubleConv(
  (net): Sequential(...)
)""",
        """CODE.Down(
  (net): Sequential(
    (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (1): DoubleConv(
      (net): Sequential(...)
    )
  )
)""",
        """CODE.GAN(img_shape=(1, 8, 8))
GAN(
  (generator): Generator(
    (model): Sequential(...)
  )
  (discriminator): Discriminator(
    (model): Sequential(...)
  )
)""",
        """CODE.Generator(img_shape=(1, 8, 8))
Generator(
  (model): Sequential(...)
)""",
        """CODE.ImageNetLightningModel(data_path='missing')
ImageNetLightningModel(
  (model): ResNet(...)
)""",
        """CODE.LitAutoEncoder()  # doctest: +ELLIPSIS +NORMALIZE_WHITESPACE
LitAutoEncoder(
  (encoder): ...
  (decoder): ...
)""",
        """CODE.LitClassifier(Backbone())  # doctest: +ELLIPSIS +NORMALIZE_WHITESPACE
LitClassifier(
  (backbone): ...
)""",
        """CODE.LitClassifier(Backbone())  # doctest: +NORMALIZE_WHITESPACE
LitClassifier(
  (backbone): ...
)""",
        "CODE.RLDataset(ReplayBuffer(5))",
        "CODE.ReplayBuffer(5)",
        """CODE.UNet(num_classes=2, num_layers=3)
(layers): ModuleList(
(0): DoubleConv(...)
(1): Down(...)
(2): Down(...)
(3): Up(...)
(4): Up(...)
(5): Conv2d(64, 2, kernel_size=(1, 1), stride=(1, 1))""",
        """CODE.Up(
  (upsample): ConvTranspose2d(8, 4, kernel_size=(2, 2), stride=(2, 2))
  (conv): DoubleConv(
    (net): Sequential(...)
  )
)""",
        """CODE.dataset_path = os.path.join(".", "Kitti")
_create_synth_kitti_dataset(dataset_path, image_dims=(1024, 512))
SegModel(dataset_path)  # doctest: +ELLIPSIS +NORMALIZE_WHITESPACE
SegModel(
  (net): UNet(
    (layers): ModuleList(
      (0): DoubleConv(...)
      (1): Down(...)
      (2): Down(...)
      (3): Up(...)
      (4): Up(...)
      (5): Conv2d(64, 19, kernel_size=(1, 1), stride=(1, 1))
    )
  )
)""",
        """CODE.def start_server(self, host, port):
    self._process = subprocess.Popen(["flask", "run" "--host", host, "--port", str(port)])""",
        """CODE.def stop_server(self):
    self._process.kill()""",
        """CODE.env = gym.make("CartPole-v1")
buffer = ReplayBuffer(10)
Agent(env, buffer)  # doctest: +ELLIPSIS""",
        """CODE.from examples import DATASETS_PATH
dataset_path = os.path.join(DATASETS_PATH, "Kitti")
_create_synth_kitti_dataset(dataset_path, image_dims=(1024, 512))
KITTI(dataset_path, 'train')
""",
        """CODE.from lightning import LightningFlow
class RootFlow(LightningFlow):
    def __init__(self):
        super().__init__()
        self.counter = 0
    def run(self):
        self.counter += 1

flow = RootFlow()
flow.run()
assert flow.counter == 1
assert flow.state["vars"]["counter"] == 1
""",
        """CODE.from lightning_app import LightningFlow

class Flow(LightningFlow):
    def run(self):
        if self.schedule("hourly"):
            # run some code once every hour.
            print("run this every hour")

from lightning_app import LightningFlow
from lightning_app.structures import List

class SchedulerDAG(LightningFlow):
    def __init__(self):
        super().__init__()
        self.dags = List()

    def run(self):
        if self.schedule("hourly"):
            self.dags.append(DAG(...))

        for dag in self.dags:
            payload = dag.run()
""",
        """CODE.from lightning_app import LightningFlow, LightningWork
from lightning_app.core import Dict

class CounterWork(LightningWork):
    def __init__(self):
        super().__init__()
        self.counter = 0
    def run(self):
        self.counter += 1

class RootFlow(LightningFlow):
    def __init__(self):
        super().__init__()
        self.dict = Dict(**{"work_0": CounterWork(), "work_1": CounterWork()})
    def run(self):
        for work_name, work in self.dict.items():
            work.run()

flow = RootFlow()
flow.run()
assert flow.dict["work_0"].counter == 1
""",
        """CODE.from lightning_app import LightningFlow, LightningWork
from lightning_app.core import List

class CounterWork(LightningWork):
    def __init__(self):
        super().__init__()
        self.counter = 0
    def run(self):
        self.counter += 1

class RootFlow(LightningFlow):
    def __init__(self):
        super().__init__()
        self.list = List(*[CounterWork(), CounterWork()])
    def run(self):
        for work in self.list:
            work.run()

flow = RootFlow()
flow.run()
assert flow.list[0].counter == 1
""",
        """CODE.from lightning_app.frontend import StaticWebFrontend

class Flow(LightningFlow):
    ...

    def configure_layout(self):
        return StaticWebFrontend("path/to/folder/to/serve")

from lightning_app.frontend import StaticWebFrontend

class Flow(LightningFlow):
    ...

    def configure_layout(self):
        return StreamlitFrontend(render_fn=my_streamlit_ui)


def my_streamlit_ui(state):
    # add your streamlit code here!
    import streamlit as st

    st.button("Hello!")

class Flow(LightningFlow):
    ...

    def configure_layout(self):
        return [
            dict(name="First Tab", content=self.child0),
            dict(name="Second Tab", content=self.child1),
            # You can include direct URLs too
            dict(name="Lightning", content="https://lightning.ai"),
        ]
""",
        """CODE.model = PPOLightning("CartPole-v0")
trainer = Trainer()
trainer.fit(model)""" .

<DEPENDENCY.pytorch-lightning==0.5.3> <CONTAINS> """CODE.@RunIf(...)
@pytest.mark.parametrize("arg1", [1, 2.0])
def test_wrapper(arg1):
    assert arg1 > 0.0""",
        """CODE.Backbone()
(l1): Linear(...)
(l2): Linear(...)  """,
        """CODE.DQN(10, 5)
DQN(
  (net): Sequential(...)
)""",
        """CODE.DQNLightning(env="CartPole-v1")
DQNLightning(
  (net): DQN(
    (net): Sequential(...)
  )
  (target_net): DQN(
    (net): Sequential(...)
  )
)""",
        """CODE.Discriminator(img_shape=(1, 28, 28))
Discriminator(
  (model): Sequential(...)
)""",
        """CODE.DoubleConv(4, 4)
DoubleConv(
  (net): Sequential(...)
)""",
        """CODE.Down(
  (net): Sequential(
    (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (1): DoubleConv(
      (net): Sequential(...)
    )
  )
)""",
        """CODE.GAN(img_shape=(1, 8, 8))
(generator): Generator(
(model): Sequential(...)
)
(discriminator): Discriminator(
(model): Sequential(...)
)""",
        """CODE.GAN(img_shape=(1, 8, 8))
GAN(
  (generator): Generator(
    (model): Sequential(...)
  )
  (discriminator): Discriminator(
    (model): Sequential(...)
  )
)""",
        """CODE.Generator(img_shape=(1, 8, 8))
Generator(
  (model): Sequential(...)
)""",
        """CODE.ImageNetLightningModel(data_path='missing')
ImageNetLightningModel(
  (model): ResNet(...)
)""",
        """CODE.LitAutoEncoder()  # doctest: +ELLIPSIS +NORMALIZE_WHITESPACE
LitAutoEncoder(
  (encoder): ...
  (decoder): ...
)""",
        """CODE.LitAutoEncoder()  # doctest: +NORMALIZE_WHITESPACE
LitAutoEncoder(
  (encoder): ...
  (decoder): ...
)""",
        """CODE.LitClassifier(Backbone())  # doctest: +ELLIPSIS +NORMALIZE_WHITESPACE
LitClassifier(
  (backbone): ...
)""",
        "CODE.RLDataset(ReplayBuffer(5))",
        "CODE.ReplayBuffer(5)",
        """CODE.ReplayBuffer(5)  # doctest: +ELLIPSIS
<...reinforce_learn_Qnet.ReplayBuffer object at ...>""",
        """CODE.UNet(num_classes=2, num_layers=3)
(layers): ModuleList(
(0): DoubleConv(...)
(1): Down(...)
(2): Down(...)
(3): Up(...)
(4): Up(...)
(5): Conv2d(64, 2, kernel_size=(1, 1), stride=(1, 1))""",
        """CODE.Up(
  (upsample): ConvTranspose2d(8, 4, kernel_size=(2, 2), stride=(2, 2))
  (conv): DoubleConv(
    (net): Sequential(...)
  )
)""",
        """CODE.dataset_path = os.path.join(".", "Kitti")
_create_synth_kitti_dataset(dataset_path, image_dims=(1024, 512))
SegModel(dataset_path)  # doctest: +ELLIPSIS +NORMALIZE_WHITESPACE
SegModel(
  (net): UNet(
    (layers): ModuleList(
      (0): DoubleConv(...)
      (1): Down(...)
      (2): Down(...)
      (3): Up(...)
      (4): Up(...)
      (5): Conv2d(64, 19, kernel_size=(1, 1), stride=(1, 1))
    )
  )
)""",
        """CODE.def start_server(self, host, port):
    self._process = subprocess.Popen(["flask", "run" "--host", host, "--port", str(port)])""",
        """CODE.def stop_server(self):
    self._process.kill()""",
        """CODE.env = gym.make("CartPole-v1")
buffer = ReplayBuffer(10)
Agent(env, buffer)  # doctest: +ELLIPSIS
<...reinforce_learn_Qnet.Agent object at ...>""",
        """CODE.from examples import DATASETS_PATH
dataset_path = os.path.join(DATASETS_PATH, "Kitti")
_create_synth_kitti_dataset(dataset_path, image_dims=(1024, 512))
KITTI(dataset_path, 'train')
""",
        """CODE.from lightning import LightningFlow
class RootFlow(LightningFlow):
...     def __init__(self):
...         super().__init__()
...         self.counter = 0
...     def run(self):
...         self.counter += 1
...
flow = RootFlow()
flow.run()
assert flow.counter == 1
assert flow.state["vars"]["counter"] == 1
""",
        """CODE.from lightning_app import LightningFlow

class Flow(LightningFlow):
    def run(self):
        if self.schedule("hourly"):
            # run some code once every hour.
            print("run this every hour")


from lightning_app import LightningFlow
from lightning_app.structures import List

class SchedulerDAG(LightningFlow):
    def __init__(self):
        super().__init__()
        self.dags = List()

    def run(self):
        if self.schedule("hourly"):
            self.dags.append(DAG(...))

        for dag in self.dags:
            payload = dag.run()
""",
        """CODE.from lightning_app import LightningFlow, LightningWork
from lightning_app.core import Dict

class CounterWork(LightningWork):
    def __init__(self):
        super().__init__()
        self.counter = 0
    def run(self):
        self.counter += 1

class RootFlow(LightningFlow):
    def __init__(self):
        super().__init__()
        self.dict = Dict(**{"work_0": CounterWork(), "work_1": CounterWork()})
    def run(self):
        for work_name, work in self.dict.items():
            work.run()

flow = RootFlow()
flow.run()
assert flow.dict["work_0"].counter == 1
""",
        """CODE.from lightning_app import LightningFlow, LightningWork
from lightning_app.core import List

class CounterWork(LightningWork):
    def __init__(self):
        super().__init__()
        self.counter = 0
    def run(self):
        self.counter += 1

class RootFlow(LightningFlow):
    def __init__(self):
        super().__init__()
        self.list = List(*[CounterWork(), CounterWork()])
    def run(self):
        for work in self.list:
            work.run()

flow = RootFlow()
flow.run()
assert flow.list[0].counter == 1
""",
        """CODE.from lightning_app.frontend import StaticWebFrontend

class Flow(LightningFlow):
    ...

    def configure_layout(self):
        return StaticWebFrontend("path/to/folder/to/serve")

from lightning_app.frontend import StaticWebFrontend

class Flow(LightningFlow):
    ...

    def configure_layout(self):
        return StreamlitFrontend(render_fn=my_streamlit_ui)

def my_streamlit_ui(state):
    # add your streamlit code here!
    import streamlit as st

    st.button("Hello!")

class Flow(LightningFlow):
    ...

    def configure_layout(self):
        return [
            dict(name="First Tab", content=self.child0),
            dict(name="Second Tab", content=self.child1),
            # You can include direct URLs too
            dict(name="Lightning", content="https://lightning.ai"),
        ]
""",
        """CODE.model = PPOLightning("CartPole-v0")
trainer = Trainer()
trainer.fit(model)""" .

<DEPENDENCY.pytorch-lightning==0.5.4> <CONTAINS> """CODE.Backbone()
Backbone(
  (l1): Linear(...)
  (l2): Linear(...)
)""",
        """CODE.DQN(10, 5)
DQN(
  (net): Sequential(...)
)""",
        """CODE.DQNLightning(env="CartPole-v1")
DQNLightning(
  (net): DQN(
    (net): Sequential(...)
  )
  (target_net): DQN(
    (net): Sequential(...)
  )
)""",
        """CODE.Discriminator(img_shape=(1, 28, 28))
Discriminator(
  (model): Sequential(...)
)""",
        """CODE.GAN(img_shape=(1, 8, 8))
GAN(
  (generator): Generator(
    (model): Sequential(...)
  )
  (discriminator): Discriminator(
    (model): Sequential(...)
  )
)""",
        """CODE.Generator(img_shape=(1, 8, 8))
Generator(
  (model): Sequential(...)
)""",
        """CODE.LitAutoEncoder()  # doctest: +ELLIPSIS +NORMALIZE_WHITESPACE
LitAutoEncoder(
  (encoder): ...
  (decoder): ...
)""",
        """CODE.LitClassifier(Backbone())  # doctest: +ELLIPSIS +NORMALIZE_WHITESPACE
LitClassifier(
  (backbone): ...
)""",
        "CODE.RLDataset(ReplayBuffer(5))",
        "CODE.ReplayBuffer(5)",
        """CODE.env = gym.make("CartPole-v1")
buffer = ReplayBuffer(10)
Agent(env, buffer)  # doctest: +ELLIPSIS""",
        """CODE.model = PPOLightning("CartPole-v0")
trainer = Trainer()
trainer.fit(model)""" .

<DEPENDENCY.pytorch-lightning==0.5.6> <CONTAINS> """CODE.# Synchronous reading, run_forever() is blocking
def print_log_msg(ws_app, msg):
    print(msg)


flow_logs_socket = client.create_lightning_logs_socket("project_id", "app_id", "flow", print_log_msg)
flow_socket.run_forever()


# Asynchronous reading (with Threads)
def print_log_msg(ws_app, msg):
    print(msg)


flow_logs_socket = client.create_lightning_logs_socket("project_id", "app_id", "flow", print_log_msg)
work_logs_socket = client.create_lightning_logs_socket("project_id", "app_id", "work_1", print_log_msg)
flow_logs_thread = Thread(target=flow_logs_socket.run_forever)
work_logs_thread = Thread(target=work_logs_socket.run_forever)
flow_logs_thread.start()
work_logs_thread.start()
# .......
flow_logs_socket.close()
work_logs_thread.close()
""" .

<DEPENDENCY.pytorch-lightning==0.6.0> <CONTAINS> """CODE.def print_log_msg(ws_app, msg):
    print(msg)

logs_socket = client.create_cluster_logs_socket("cluster_id", 1661100000, 1661101000, print_log_msg)
logs_socket.run_forever()


def print_log_msg(ws_app, msg):
    print(msg)

logs_socket = client.create_cluster_logs_socket("cluster_id", 1661100000, 1661101000, print_log_msg)

logs_thread = Thread(target=cluster_logs_socket.run_forever)

logs_thread.start()
# .......

logs_socket.close()
""",
        """CODE.import panel as pn

pn.panel("Hello **Panel â¡** World").servable()


import lightning as L
from lightning.app.frontend.panel import PanelFrontend


class LitPanel(L.LightningFlow):
    def configure_layout(self):
        return PanelFrontend("panel_app_basic.py")


class LitApp(L.LightningFlow):
    def __init__(self):
        super().__init__()
        self.lit_panel = LitPanel()

    def configure_layout(self):
        return {"name": "home", "content": self.lit_panel}


app = L.LightningApp(LitApp())
""",
        """CODE.import param
app = AppStateWatcher()
app.state.counter = 1

@param.depends(app.param.state, watch=True)
def update(state):
    print(f"The counter was updated to {state.counter}")

app.state.counter += 1""" .

<DEPENDENCY.pytorch-lightning==0.7.0> <CONTAINS> "CODE.self.logger.experiment.some_test_tube_function()" .

<DEPENDENCY.pytorch-lightning==0.7.2> <CONTAINS> """CODE.Part of the code was copied from
https://github.com/pytorch/vision/blob/build/v0.5.0/torchvision/datasets/mnist.py

Args:
    root: Root directory of dataset where ``MNIST/processed/training.pt``
        and  ``MNIST/processed/test.pt`` exist.
    train: If ``True``, creates dataset from ``training.pt``,
        otherwise from ``test.pt``.
    normalize: mean and std deviation of the MNIST dataset.
    download: If true, downloads the dataset from the internet and
        puts it in root directory. If dataset is already downloaded, it is not
        downloaded again.

Examples:
    dataset = MNIST(download=True)
    len(dataset)
    60000
    torch.bincount(dataset.targets)
    tensor([5923, 6742, 5958, 6131, 5842, 5421, 5918, 6265, 5851, 5949])""",
        """CODE.accum = TensorRunningAccum(5)
accum.last(), accum.mean()
accum.append(torch.tensor(1.5))
accum.last(), accum.mean()
accum.append(torch.tensor(2.5)
accum.last(), accum.mean()
accum.reset()
_= [accum.append(torch.tensor(i)) for i in range(13)]
accum.last(), accum.mean(), accum.min(), accum.max()
""",
        """CODE.def training_epoch_end(self, outputs):
    train_acc_mean = 0
    for output in outputs:
        train_acc_mean += output['train_acc']

    train_acc_mean /= len(outputs)

    # log training accuracy at the end of an epoch
    results = {
        'log': {'train_acc': train_acc_mean.item()}
    }
    return results


def training_epoch_end(self, outputs):
    train_acc_mean = 0
    i = 0
    for dataloader_outputs in outputs:
        for output in dataloader_outputs:
            train_acc_mean += output['train_acc']
            i += 1

    train_acc_mean /= i

    # log training accuracy at the end of an epoch
    results = {
        'log': {'train_acc': train_acc_mean.item(), 'step': self.current_epoch}
    }
    return results
""",
        """CODE.import numpy as np

def merge_dicts(dicts, agg_key_funcs=None, default_func=None):
    if agg_key_funcs is None:
        agg_key_funcs = {}
    merged_dict = {}
    for d in dicts:
        for key, value in d.items():
            if key in agg_key_funcs:
                if key in merged_dict:
                    merged_dict[key].append(value)
                else:
                    merged_dict[key] = [value]
            else:
                merged_dict[key] = value
    for key, values in merged_dict.items():
        if key in agg_key_funcs:
            merged_dict[key] = agg_key_funcs[key](values)
        elif default_func is not None:
            merged_dict[key] = default_func(values)
    return merged_dict

import pprint
d1 = {'a': 1.7, 'b': 2.0, 'c': 1}
d2 = {'a': 1.1, 'b': 2.2, 'v': 1}
d3 = {'a': 1.1, 'v': 2.3}
dflt_func = min
agg_funcs = {'a': np.mean, 'v': max}
pprint.pprint(merge_dicts([d1, d2, d3], agg_funcs, dflt_func)""",
        """CODE.params = {"float": 0.3,
          "int": 1,
          "string": "abc",
          "bool": True,
          "list": [1, 2, 3],
          "namespace": Namespace(foo=3),
          "layer": torch.nn.BatchNorm1d}
import pprint
pprint.pprint(LightningLoggerBase._sanitize_params(params))  # doctest: +NORMALIZE_WHITESPACE""",
        "CODE.self.logger.experiment.some_trains_function()" .

<DEPENDENCY.pytorch-lightning==0.7.4> <CONTAINS> """CODE.class LitProgressBar(ProgressBarBase):
    def __init__(self):
        super().__init__()  # don't forget this :)
        self.enabled = True

    def disable(self):
        self.enableenabled = False

    def on_batch_end(self, trainer, pl_module):
        super().on_batch_end(trainer, pl_module)  # don't forget this :)
        percent = (self.train_batch_idx / self.total_train_batches) * 100
        sys.stdout.flush()
        sys.stdout.write(f'{percent:.01f} percent complete \\r')

bar = LitProgressBar()
trainer = Trainer(callbacks=[bar])""",
        """CODE.lr_find enables the user to do a range test of good initial learning rates,
to reduce the amount of guesswork in picking a good starting learning rate.

Args:
    model: Model to do range testing for

    train_dataloader: A PyTorch
        DataLoader with training samples. If the model has
        a predefined train_dataloader method this will be skipped.

    min_lr: minimum learning rate to investigate

    max_lr: maximum learning rate to investigate

    num_training: number of learning rates to test

    mode: search strategy, either 'linear' or 'exponential'. If set to
        'linear' the learning rate will be searched by linearly increasing
        after each batch. If set to 'exponential', will increase learning
        rate exponentially.

    num_accumulation_steps: number of batches to calculate loss over.

Example::

    # Setup model and trainer
    model = MyModelClass(hparams)
    trainer = pl.Trainer()

    # Run lr finder
    lr_finder = trainer.lr_find(model, ...)

    # Inspect results
    fig = lr_finder.plot(); fig.show()
    suggested_lr = lr_finder.suggest()

    # Overwrite lr and create new model
    hparams.lr = suggested_lr
    model = MyModelClass(hparams)

    # Ready to train with new learning rate
    trainer.fit(model)""",
        """CODE.strtobool('YES')
strtobool('FALSE')""" .

<DEPENDENCY.pytorch-lightning==0.7.6> <CONTAINS> """CODE.class ExampleModule(DeviceDtypeModuleMixin):
    def __init__(self, weight: torch.Tensor):
        super().__init__()
        self.register_buffer('weight', weight)
_ = torch.manual_seed(0)
module = ExampleModule(torch.rand(3, 4))
module.weight #doctest: +ELLIPSIS
tensor([[...]])
module.to(torch.double)
ExampleModule()
module.weight #doctest: +ELLIPSIS
tensor([[...]], dtype=torch.float64)
cpu = torch.device('cpu')
module.to(cpu, dtype=torch.half, non_blocking=True)
ExampleModule()
module.weight #doctest: +ELLIPSIS
tensor([[...]], dtype=torch.float16)
module.to(cpu)
ExampleModule()
module.weight #doctest: +ELLIPSIS
tensor([[...]], dtype=torch.float16)
""",
        """CODE.hparams = Namespace(batch_size=32, learning_rate=0.001, data_root='./any/path/here')
path_csv = './testing-hparams.csv'
save_hparams_to_tags_csv(path_csv, hparams)
hparams_new = load_hparams_from_tags_csv(path_csv)
vars(hparams) == hparams_new
os.remove(path_csv)""",
        """CODE.hparams = Namespace(batch_size=32, learning_rate=0.001, data_root='./any/path/here')
path_yaml = './testing-hparams.yaml'
save_hparams_to_yaml(path_yaml, hparams)
hparams_new = load_hparams_from_yaml(path_yaml)
vars(hparams) == hparams_new
os.remove(path_yaml)""",
        """CODE.hparams = {'c': 4}
update_hparams(hparams, {'a': {'b': 2}, 'c': 1})
hparams['a']['b'], hparams['c']
update_hparams(hparams, {'a': {'b': 4}, 'c': 7})
hparams['a']['b'], hparams['c']""",
        """CODE.parser = ArgumentParser(add_help=False)
parser = Trainer.add_argparse_args(parser)
args = Trainer.parse_argparser(parser.parse_args(""))
trainer = Trainer.from_argparse_args(args)""",
        """CODE.strtobool('YES')
strtobool('FALSE')""" .

<DEPENDENCY.pytorch-lightning==0.8.0> <CONTAINS> """CODE.accuracy(x, y)
tensor(0.7500)""",
        """CODE.ad = AttributeDict({'key1': 1, 'key2': 'abc'})
ad.update({'my-key': 3.14})
ad.update(mew_key=42)
ad.key1 = 2
ad""",
        """CODE.def confusion_matrix(pred, target, normalize=False):
    num_classes = max(max(pred), max(target)) + 1
    C = torch.zeros(num_classes, num_classes)
    for p, t in zip(pred, target):
        C[p, t] += 1
    if normalize:
        C = C / C.sum(1, keepdim=True)
    return C
""",
        """CODE.def multiclass_roc(pred, target):
    # code for computing ROC for multiclass predictors
    # return roc for each class
    # Number of classes, false-positive rate (fpr), true-positive rate (tpr), thresholds
    pass
""",
        """CODE.def precision_recall(pred, target, num_classes, reduction='elementwise_mean'):
    # compute precision and recall
    # code here

    return precision, recall

x = torch.tensor([0, 1, 2, 3])
y = torch.tensor([0, 1, 2, 2])
precision_recall(x, y)
""",
        """CODE.def recall(pred, target, num_classes, reduction='elementwise_mean'):
    # Computes recall score.

    # Your code here

    return recall_tensor
""",
        """CODE.def transfer_batch_to_device(self, batch, device):
    if isinstance(batch, CustomBatch):
        # move all tensors in your custom data structure to the device
        batch.samples = batch.samples.to(device)
        batch.targets = batch.targets.to(device)
    else:
        batch = super().transfer_batch_to_device(data, device)
    return batch
""",
        """CODE.from collections import OrderedDict
class ManuallyArgsModel(LightningModule):
...     def __init__(self, arg1, arg2, arg3):
...         super().__init__()
...         # manually assine arguments
...         self.save_hyperparameters('arg1', 'arg3')
...     def forward(self, *args, **kwargs):
...         ...
model = ManuallyArgsModel(1, 'abc', 3.14)
model.hparams
"arg1": 1
"arg3": 3.14

class AutomaticArgsModel(LightningModule):
...     def __init__(self, arg1, arg2, arg3):
...         super().__init__()
...         # equivalent automatic
...         self.save_hyperparameters()
...     def forward(self, *args, **kwargs):
...         ...
model = AutomaticArgsModel(1, 'abc', 3.14)
model.hparams
"arg1": 1
"arg2": abc
"arg3": 3.14

class SingleArgModel(LightningModule):
...     def __init__(self, params):
...         super().__init__()
...         # manually assign single argument
...         self.save_hyperparameters(params)
...     def forward(self, *args, **kwargs):
...         ...
model = SingleArgModel(Namespace(p1=1, p2='abc', p3=3.14))
model.hparams
"p1": 1
"p2": abc
"p3": 3.14""",
        """CODE.model = torch.nn.Conv2d(3, 8, 3)
summary = LayerSummary(model)
summary.num_parameters
224
summary.layer_type
'Conv2d'
output = model(torch.rand(1, 3, 5, 5))
summary.in_size
[1, 3, 5, 5]
summary.out_size
[1, 8, 3, 3]""",
        "CODE.precision(x, y)",
        """CODE.precision, recall, thresholds = precision_recall_curve(pred, target)
precision
tensor([0.3333, 0.0000, 0.0000, 1.0000])
recall
tensor([1., 0., 0., 0.])
thresholds
tensor([1, 2, 3])""",
        """CODE.pred = torch.tensor([0, 1, 2, 2])
target = torch.tensor([0, 1, 2, 2])
metric = ConfusionMatrix()
metric(pred, target)
tensor([[1., 0., 0.],
        [0., 1., 0.],
        [0., 0., 2.]])""",
        """CODE.pred = torch.tensor([0, 1, 2, 3])
target = torch.tensor([0, 1, 2, 2])
metric = AUROC()
metric(pred, target)
tensor(0.3333)""",
        """CODE.pred = torch.tensor([0, 1, 2, 3])
target = torch.tensor([0, 1, 2, 2])
metric = Accuracy()
metric(pred, target)""",
        """CODE.pred = torch.tensor([0, 1, 2, 3])
target = torch.tensor([0, 1, 2, 2])
metric = Accuracy()
metric(pred, target)
tensor(0.7500)""",
        """CODE.pred = torch.tensor([0, 1, 2, 3])
target = torch.tensor([0, 1, 2, 2])
metric = AveragePrecision()
metric(pred, target)
tensor(0.3333)""",
        """CODE.pred = torch.tensor([0, 1, 2, 3])
target = torch.tensor([0, 1, 2, 2])
metric = F1()
metric(pred, target)
tensor(0.6667)""",
        """CODE.pred = torch.tensor([0, 1, 2, 3])
target = torch.tensor([0, 1, 2, 2])
metric = FBeta(0.25)
metric(pred, target)
tensor(0.7361)""",
        """CODE.pred = torch.tensor([0, 1, 2, 3])
target = torch.tensor([0, 1, 2, 2])
metric = Precision(num_classes=4)
metric(pred, target)""",
        """CODE.pred = torch.tensor([0, 1, 2, 3])
target = torch.tensor([0, 1, 2, 2])
metric = PrecisionRecall()
prec, recall, thr = metric(pred, target)
prec
tensor([0.3333, 0.0000, 0.0000, 1.0000])
recall
tensor([1., 0., 0., 0.])
thr
tensor([1., 2., 3.])""",
        """CODE.pred = torch.tensor([0, 1, 2, 3])
target = torch.tensor([0, 1, 2, 2])
metric = ROC()
fps, tps, thresholds = metric(pred, target)
fps
tensor([0.0000, 0.3333, 0.6667, 0.6667, 1.0000])
tps
tensor([0., 0., 0., 1., 1.])
thresholds
tensor([4., 3., 2., 1., 0.])""",
        """CODE.pred = torch.tensor([0, 1, 2, 3])
target = torch.tensor([0, 1, 2, 2])
metric = Recall()
metric(pred, target)
tensor(0.6250)""",
        """CODE.pred = torch.tensor([[0, 0, 0, 0, 0, 0, 0, 0],
...                      [0, 0, 1, 1, 1, 0, 0, 0],
...                      [0, 0, 0, 0, 0, 0, 0, 0]])
target = torch.tensor([[0, 0, 0, 0, 0, 0, 0, 0],
...                        [0, 0, 0, 1, 1, 1, 0, 0],
...                        [0, 0, 0, 0, 0, 0, 0, 0]])
metric = IoU()
metric(pred, target)
tensor(0.7045)""",
        """CODE.pred = torch.tensor([[0.85, 0.05, 0.05, 0.05],
                        [0.05, 0.85, 0.05, 0.05],
                        [0.05, 0.05, 0.85, 0.05],
                        [0.05, 0.05, 0.05, 0.85]])
target = torch.tensor([0, 1, 3, 2])
metric = MulticlassPrecisionRecall()
metric(pred, target)   # doctest: +NORMALIZE_WHITESPACE""",
        """CODE.pred = torch.tensor([[0.85, 0.05, 0.05, 0.05],
...                      [0.05, 0.85, 0.05, 0.05],
...                      [0.05, 0.05, 0.85, 0.05],
...                      [0.05, 0.05, 0.05, 0.85]])
target = torch.tensor([0, 1, 3, 2])
metric = DiceCoefficient()
metric(pred, target)
tensor(0.3333)""",
        """CODE.pred = torch.tensor([[0.85, 0.05, 0.05, 0.05],
...                      [0.05, 0.85, 0.05, 0.05],
...                      [0.05, 0.05, 0.85, 0.05],
...                      [0.05, 0.05, 0.05, 0.85]])
target = torch.tensor([0, 1, 3, 2])
nb_classes, precision, recall, thresholds = multiclass_precision_recall_curve(pred, target)
nb_classes
(tensor([1., 1.]), tensor([1., 0.]), tensor([0.8500]))
precision
(tensor([1., 1.]), tensor([1., 0.]), tensor([0.8500]))
recall
(tensor([0.2500, 0.0000, 1.0000]), tensor([1., 0., 0.]), tensor([0.0500, 0.8500]))
thresholds   # doctest: +NORMALIZE_WHITESPACE
(tensor([0.2500, 0.0000, 1.0000]), tensor([1., 0., 0.]), tensor([0.0500, 0.8500]))""",
        """CODE.pred = torch.tensor([[0.85, 0.05, 0.05, 0.05],
...                     [0.05, 0.85, 0.05, 0.05],
...                     [0.05, 0.05, 0.85, 0.05],
...                     [0.05, 0.05, 0.05, 0.85]])
target = torch.tensor([0, 1, 3, 2])
metric = MulticlassROC()
classes_roc = metric(pred, target)
metric(pred, target)   # doctest: +NORMALIZE_WHITESPACE
((tensor([0., 0., 1.]), tensor([0., 1., 1.]), tensor([1.8500, 0.8500, 0.0500])),
 (tensor([0., 0., 1.]), tensor([0., 1., 1.]), tensor([1.8500, 0.8500, 0.0500])),
 (tensor([0.0000, 0.3333, 1.0000]), tensor([0., 0., 1.]), tensor([1.8500, 0.8500, 0.0500])),
 (tensor([0.0000, 0.3333, 1.0000]), tensor([0., 0., 1.]), tensor([1.8500, 0.8500, 0.0500]))))""",
        """CODE.str_to_bool('YES')
str_to_bool('FALSE')""",
        """CODE.target = torch.randint(0, 1, (10, 25, 25))
pred = torch.tensor(target)
pred[2:5, 7:13, 9:15] = 1 - pred[2:5, 7:13, 9:15]
iou(pred, target)
""",
        """CODE.to_categorical(x)
    tensor([1, 0])""",
        """CODE.to_onehot(x)
    x = torch.tensor([1, 2, 3])
    to_onehot(x)
    tensor([[0, 1, 0, 0],
            [0, 0, 1, 0],
            [0, 0, 0, 1]])""",
        """CODE.x = torch.tensor([0, 1, 2, 3])
y = torch.tensor([0, 1, 2, 2])
auc(x, y)
""",
        """CODE.x = torch.tensor([0, 1, 2, 3])
y = torch.tensor([0, 1, 2, 2])
average_precision(x, y)""",
        """CODE.x = torch.tensor([0, 1, 2, 3])
y = torch.tensor([0, 1, 2, 2])
fpr, tpr, thresholds = roc(x, y)
fpr
tpr
thresholds""",
        """CODE.y_pred = torch.tensor([0, 1, 2, 3])
y_true = torch.tensor([0, 1, 2, 2])
metric = AUC()
metric(y_pred, y_true)""" .

<DEPENDENCY.pytorch-lightning==0.8.1> <CONTAINS> """CODE.from pytorch_lightning import Trainer
from pytorch_lightning.loggers import TrainsLogger
trains_logger = TrainsLogger(
    project_name='pytorch lightning',
    task_name='default',
    output_uri='.',
)
trainer = Trainer(logger=trains_logger)

from pytorch_lightning import LightningModule
class LitModel(LightningModule):
    def training_step(self, batch, batch_idx):
        # example
        self.logger.experiment.whatever_trains_supports(...)

    def any_lightning_module_function_or_hook(self):
        self.logger.experiment.whatever_trains_supports(...)""",
        "CODE.self.logger.experiment.some_trains_function()" .

<DEPENDENCY.pytorch-lightning==0.8.2> <CONTAINS> """CODE.class TransferableDataType:
    pass

import torch

class CustomObject:
    def __init__(self):
        self.x = torch.rand(2, 2)
    def to(self, device):
        self.x = self.x.to(device)
        return self

isinstance(dict, TransferableDataType)
isinstance(torch.rand(2, 3), TransferableDataType)
isinstance(CustomObject(), TransferableDataType)""" .

<DEPENDENCY.pytorch-lightning==0.8.5> <CONTAINS> """CODE.def mse(pred, target, reduction='elementwise_mean'):
    return torch.mean((pred - target) ** 2)

x = torch.tensor([0., 1, 2, 3])
y = torch.tensor([0., 1, 2, 2])
mse(x, y)
""",
        """CODE.def rmsle(pred, target, reduction='elementwise_mean'):
    return torch.sqrt(torch.mean((torch.log1p(pred) - torch.log1p(target))**2))

x = torch.tensor([0., 1, 2, 3])
y = torch.tensor([0., 1, 2, 2])
rmsle(x, y)
""",
        """CODE.mae(x, y)
    tensor(0.2500)""",
        """CODE.pred = torch.tensor([[0.0, 1.0], [2.0, 3.0]])
target = torch.tensor([[3.0, 2.0], [1.0, 0.0]])
metric = PSNR()
metric(pred, target)
tensor(2.5527)""",
        """CODE.x = torch.tensor([0., 1, 2, 3])
y = torch.tensor([0., 1, 2, 2])
rmse(x, y)
tensor(0.5000)""" .

<DEPENDENCY.pytorch-lightning==0.9.0> <CONTAINS> """CODE.    def transfer_batch_to_device(self, batch, device):
        if isinstance(batch, CustomBatch):
            # move all tensors in your custom data structure to the device
            batch.samples = batch.samples.to(device)
            batch.targets = batch.targets.to(device)
        else:
            batch = super().transfer_batch_to_device(data, device)
        return batch
""",
        """CODE.class MyDataModule(LightningDataModule):
    def __init__(self):
        super().__init__()
    def prepare_data(self):
        # download, split, etc...
        # only called on 1 GPU/TPU in distributed
    def setup(self):
        # make assignments here (val/train/test split)
        # called on every process in DDP
    def train_dataloader(self):
        train_split = Dataset(...)
        return DataLoader(train_split)
    def val_dataloader(self):
        val_split = Dataset(...)
        return DataLoader(val_split)
    def test_dataloader(self):
        test_split = Dataset(...)
        return DataLoader(test_split)
""",
        """CODE.class SimpleModel(LightningModule):
    def __init__(self):
        super().__init__()
        self.l1 = torch.nn.Linear(in_features=64, out_features=4)

    def forward(self, x):
        return torch.relu(self.l1(x.view(x.size(0), -1)))

with tempfile.NamedTemporaryFile(suffix='.onnx', delete=False) as tmpfile:
    model = SimpleModel()
    input_sample = torch.randn((1, 64))
    model.to_onnx(tmpfile.name, input_sample, export_params=True)
    os.path.isfile(tmpfile.name)""",
        """CODE.def bleu_score(translate_corpus, reference_corpus, n_gram=4, smooth=False):
    # Calculate BLEU score of machine translated text with one or more references
    # Args:
    #     translate_corpus: An iterable of machine translated corpus
    #     reference_corpus: An iterable of iterables of reference corpus
    #     n_gram: Gram value ranged from 1 to 4 (Default 4)
    #     smooth: Whether or not to apply smoothing â Lin et al. 2004
    # Return:
    #     Tensor with BLEU Score

    # Example:
    #     translate_corpus = ['the cat is on the mat'.split()]
    #     reference_corpus = [['there is a cat on the mat'.split(), 'a cat is on the mat'.split()]]
    #     bleu_score(translate_corpus, reference_corpus)
    #     tensor(0.7598)

    # Your code goes here
    pass
""",
        """CODE.def configure_apex(self, amp, model, optimizers, amp_level):
    model, optimizers = amp.initialize(
        model, optimizers, opt_level=amp_level,
    )

    return model, optimizers""",
        """CODE.def configure_ddp(self, model, device_ids):
    # Lightning DDP simply routes to test_step, val_step, etc...
    model = LightningDistributedDataParallel(
        model,
        device_ids=device_ids,
        find_unused_parameters=True
    )
    return model""",
        """CODE.def on_load_checkpoint(self, checkpoint):
    # 99% of the time you don't need to implement this method
    self.something_cool_i_want_to_save = checkpoint['something_cool_i_want_to_save']""",
        """CODE.def on_save_checkpoint(self, checkpoint):
    # 99% of use cases you don't need to implement this method
    checkpoint['something_cool_i_want_to_save'] = my_cool_pickable_object
""",
        """CODE.def prepare_data(self):
    # good
    download_data()
    tokenize()
    etc()

    # bad
    self.split = data_split
    self.some_state = some_other_state()

# DEFAULT
# called once per node on LOCAL_RANK=0 of that node
Trainer(prepare_data_per_node=True)

# call on GLOBAL_RANK=0 (great for shared file systems)
Trainer(prepare_data_per_node=False)

model.prepare_data()
    if ddp/tpu: init()
model.setup(stage)
model.train_dataloader()
model.val_dataloader()
model.test_dataloader()
""",
        """CODE.def prepare_data(self):
    download_imagenet()
    clean_imagenet()
    cache_imagenet()""",
        """CODE.def setup(self, stage):
    data = load_data(...)
    self.train_ds, self.val_ds, self.test_ds = split_data(data)""",
        """CODE.def test_dataloader(self):
    dataset = MNIST(root=PATH, train=False, transform=transforms.ToTensor(), download=False)
    loader = torch.utils.data.DataLoader(dataset=dataset, shuffle=False)
    return loader""",
        """CODE.def test_dataloader(self):
    transform = transforms.Compose([transforms.ToTensor(),
                                    transforms.Normalize((0.5,), (1.0,))])
    dataset = MNIST(root='/path/to/mnist/', train=False, transform=transform,
                    download=True)
    loader = torch.utils.data.DataLoader(
        dataset=dataset,
        batch_size=self.batch_size,
        shuffle=False
    )

    return loader

# can also return multiple dataloaders
def test_dataloader(self):
    return [loader_a, loader_b, ..., loader_n]
""",
        """CODE.def train_dataloader(self):
    dataset = MNIST(root=PATH, train=True, transform=transforms.ToTensor(), download=False)
    loader = torch.utils.data.DataLoader(dataset=dataset)
    return loader""",
        """CODE.def train_dataloader(self):
    transform = transforms.Compose([transforms.ToTensor(),
                                    transforms.Normalize((0.5,), (1.0,))])
    dataset = MNIST(root='/path/to/mnist/', train=True, transform=transform,
                    download=True)
    loader = torch.utils.data.DataLoader(
        dataset=dataset,
        batch_size=self.batch_size,
        shuffle=True
    )
    return loader""",
        """CODE.def transfer_batch_to_device(self, batch, device):
    if isinstance(batch, CustomBatch):
        # move all tensors in your custom data structure to the device
        batch.samples = batch.samples.to(device)
        batch.targets = batch.targets.to(device)
    else:
        batch = super().transfer_batch_to_device(data, device)
    return batch""",
        """CODE.def transfer_batch_to_device(self, batch, device):
    if isinstance(batch, CustomBatch):
        # move all tensors in your custom data structure to the device
        batch.samples = batch.samples.to(device)
        batch.targets = batch.targets.to(device)
    else:
        batch = super().transfer_batch_to_device(data, device)
    return batch
""",
        """CODE.def val_dataloader(self):
    dataset = MNIST(root=PATH, train=False, transform=transforms.ToTensor(), download=False)
    loader = torch.utils.data.DataLoader(dataset=dataset, shuffle=False)
    return loader""",
        """CODE.def val_dataloader(self):
    transform = transforms.Compose([transforms.ToTensor(),
                                    transforms.Normalize((0.5,), (1.0,))])
    dataset = MNIST(root='/path/to/mnist/', train=False,
                    transform=transform, download=True)
    loader = torch.utils.data.DataLoader(
        dataset=dataset,
        batch_size=self.batch_size,
        shuffle=False
    )

    return loader

# can also return multiple dataloaders
def val_dataloader(self):
    return [loader_a, loader_b, ..., loader_n]
""",
        """CODE.dm.prepare_data()
dm.setup()

def prepare_data(self):
    download_imagenet()
    clean_imagenet()
    cache_imagenet()
""",
        """CODE.from pytorch_lightning import Trainer
from pytorch_lightning.loggers import CSVLogger
logger = CSVLogger("logs", name="my_exp_name")
trainer = Trainer(logger=logger)""",
        """CODE.metric = DCG()
metric(y_score, y_true)""",
        """CODE.params = dict(
    in_features=28 * 28,
    hidden_dim=1000,
    out_features=10,
    drop_prob=0.2,
    learning_rate=0.001 * 8,
    batch_size=2,
    data_root='./datasets',
    num_workers=4,
)
model = LightningTemplateModel(**params)""",
        """CODE.parser = ArgumentParser(add_help=False)
parser = LightningDataModule.add_argparse_args(parser)
module = LightningDataModule.from_argparse_args(args)""",
        """CODE.parser = ArgumentParser(add_help=False)
parser = Trainer.add_argparse_args(parser)
parser.add_argument('--my_custom_arg', default='something')  # doctest: +SKIP
args = Trainer.parse_argparser(parser.parse_args(""))
trainer = Trainer.from_argparse_args(args, logger=False)""",
        """CODE.parser = argparse.ArgumentParser()
parser = Trainer.add_argparse_args(parser)
args = parser.parse_args([])
pprint.pprint(vars(args))  # doctest: +ELLIPSIS +NORMALIZE_WHITESPACE""",
        """CODE.pred = torch.rand([16, 1, 16, 16])
target = pred * 0.75
metric = SSIM()
metric(pred, target)""",
        """CODE.pred = torch.rand([16, 1, 16, 16])
target = pred * 0.75
metric = SSIM()
metric(pred, target)
tensor(0.9219)""",
        """CODE.pred = torch.tensor([0, 1, 2, 2])
target = torch.tensor([0, 1, 2, 2])
metric = ConfusionMatrix()
metric(pred, target)
tensor([[1., 0., 0.],
        [0., 1., 0.],
        [0., 0., 2.])""",
        """CODE.pred = torch.tensor([0, 1, 2, 2])
target = torch.tensor([0, 1, 2, 2])
metric = ConfusionMatrix()
metric(pred, target)
tensor([[1., 0., 0.],
        [0., 1., 0.],
        [0., 0., 2.]])""",
        """CODE.pred = torch.tensor([0, 1, 2, 3])
target = torch.tensor([0, 1, 2, 2])
metric = AUROC()
metric(pred, target)
tensor(0.3333)""",
        """CODE.pred = torch.tensor([0, 1, 2, 3])
target = torch.tensor([0, 1, 2, 2])
metric = AveragePrecision()
metric(pred, target)
tensor(0.3333)""",
        """CODE.pred = torch.tensor([0, 1, 2, 3])
target = torch.tensor([0, 1, 2, 2])
metric = F1()
metric(pred, target)
tensor(0.6667)""",
        """CODE.pred = torch.tensor([0, 1, 2, 3])
target = torch.tensor([0, 1, 2, 2])
metric = FBeta(0.25)
metric(pred, target)
tensor(0.7361)""",
        """CODE.pred = torch.tensor([0, 1, 2, 3])
target = torch.tensor([0, 1, 2, 2])
metric = Precision(num_classes=4)
metric(pred, target)
tensor(0.7500)""",
        """CODE.pred = torch.tensor([0, 1, 2, 3])
target = torch.tensor([0, 1, 2, 2])
metric = PrecisionRecall()
prec, recall, thr = metric(pred, target)
prec
tensor([0.3333, 0.0000, 0.0000, 1.0000])
recall
tensor([1., 0., 0., 0.])
thr
tensor([1., 2., 3.])""",
        """CODE.pred = torch.tensor([0, 1, 2, 3])
target = torch.tensor([0, 1, 2, 2])
metric = ROC()
fps, tps, thresholds = metric(pred, target)""",
        """CODE.pred = torch.tensor([0, 1, 2, 3])
target = torch.tensor([0, 1, 2, 2])
metric = ROC()
fps, tps, thresholds = metric(pred, target)
fps
tensor([0.0000, 0.3333, 0.6667, 0.6667, 1.0000])
tps
tensor([0., 0., 0., 1., 1.])
thresholds
tensor([4., 3., 2., 1., 0.])""",
        """CODE.pred = torch.tensor([0, 1, 2, 3])
target = torch.tensor([0, 1, 2, 2])
metric = Recall()
metric(pred, target)
tensor(0.6250)""",
        """CODE.pred = torch.tensor([0., 1, 2, 3])
target = torch.tensor([0., 1, 2, 2])
metric = MAE()
metric(pred, target)
tensor(0.2500)""",
        """CODE.pred = torch.tensor([0., 1, 2, 3])
target = torch.tensor([0., 1, 2, 2])
metric = MSE()
metric(pred, target)""",
        """CODE.pred = torch.tensor([0., 1, 2, 3])
target = torch.tensor([0., 1, 2, 2])
metric = RMSE()
metric(pred, target)
tensor(0.5000)""",
        """CODE.pred = torch.tensor([0., 1, 2, 3])
target = torch.tensor([0., 1, 2, 2])
metric = RMSLE()
metric(pred, target)
tensor(0.0207)""",
        """CODE.pred = torch.tensor([[0, 0, 0, 0, 0, 0, 0, 0],
                      [0, 0, 1, 1, 1, 0, 0, 0],
                      [0, 0, 0, 0, 0, 0, 0, 0]])
target = torch.tensor([[0, 0, 0, 0, 0, 0, 0, 0],
                        [0, 0, 0, 1, 1, 1, 0, 0],
                        [0, 0, 0, 0, 0, 0, 0, 0]])
metric = IoU()
metric(pred, target)""",
        """CODE.pred = torch.tensor([[0.0, 1.0], [2.0, 3.0]])
target = torch.tensor([[3.0, 2.0], [1.0, 0.0]])
metric = PSNR()
metric(pred, target)
tensor(2.5527)""",
        """CODE.pred = torch.tensor([[0.85, 0.05, 0.05, 0.05],
                        [0.05, 0.85, 0.05, 0.05],
                        [0.05, 0.05, 0.85, 0.05],
                        [0.05, 0.05, 0.05, 0.85]])
target = torch.tensor([0, 1, 3, 2])
metric = MulticlassPrecisionRecall()
metric(pred, target)   # doctest: +NORMALIZE_WHITESPACE""",
        """CODE.pred = torch.tensor([[0.85, 0.05, 0.05, 0.05],
...                      [0.05, 0.85, 0.05, 0.05],
...                      [0.05, 0.05, 0.85, 0.05],
...                      [0.05, 0.05, 0.05, 0.85]])
target = torch.tensor([0, 1, 3, 2])
metric = DiceCoefficient()
metric(pred, target)
tensor(0.3333)""",
        """CODE.pred = torch.tensor([[0.85, 0.05, 0.05, 0.05],
...                     [0.05, 0.85, 0.05, 0.05],
...                     [0.05, 0.05, 0.85, 0.05],
...                     [0.05, 0.05, 0.05, 0.85]])
target = torch.tensor([0, 1, 3, 2])
metric = MulticlassROC()
classes_roc = metric(pred, target)
metric(pred, target)   # doctest: +NORMALIZE_WHITESPACE
((tensor([0., 0., 1.]), tensor([0., 1., 1.]), tensor([1.8500, 0.8500, 0.0500])),
 (tensor([0., 0., 1.]), tensor([0., 1., 1.]), tensor([1.8500, 0.8500, 0.0500])),
 (tensor([0.0000, 0.3333, 1.0000]), tensor([0., 0., 1.]), tensor([1.8500, 0.8500, 0.0500])),
 (tensor([0.0000, 0.3333, 1.0000]), tensor([0., 0., 1.]), tensor([1.8500, 0.8500, 0.0500])))""",
        """CODE.pred_decision = torch.tensor([-2.17, -0.97, -0.19, -0.43])
y_true = torch.tensor([1, 1, 0, 0])
metric = Hinge()
metric(pred_decision, y_true)
tensor([1.6300])""",
        """CODE.predictions_to_write = {'preds': ['cat', 'dog'], 'ids': tensor([0, 1])}
result.write_dict(predictions_to_write)""",
        """CODE.result = pl.EvalResult()
result.write('ids', [0, 1, 2])
result.write('preds', ['cat', 'dog', 'dog'])""",
        """CODE.result.log('train_loss', loss)
result.log(
    name,
    value,
    on_step=True,
    on_epoch=False,
    logger=True,
    prog_bar=False,
    reduce_fx=torch.mean,
    enable_graph=False
)""",
        """CODE.result.log('val_loss', loss)
result.log(
    name,
    value,
    on_step=False,
    on_epoch=True,
    logger=True,
    prog_bar=False,
    reduce_fx=torch.mean
)""",
        "CODE.self.logger.experiment.some_experiment_writer_function()",
        """CODE.translate_corpus = ['the cat is on the mat'.split()]
reference_corpus = [['there is a cat on the mat'.split(), 'a cat is on the mat'.split()]]
metric = BLEUScore()
metric(translate_corpus, reference_corpus)""",
        """CODE.translate_corpus = ['the cat is on the mat'.split()]
reference_corpus = [['there is a cat on the mat'.split(), 'a cat is on the mat'.split()]]
metric = BLEUScore()
metric(translate_corpus, reference_corpus)
tensor(0.7598)""",
        """CODE.values = {'loss': loss, 'acc': acc, ..., 'metric_n': metric_n}
result.log_dict(values)""",
        """CODE.y_pred = torch.tensor([0, 0, 0, 1])
y_true = torch.tensor([0, 0, 1, 1])
metric = BalancedAccuracy()
metric(y_pred, y_true)
tensor([0.7500])""",
        """CODE.y_pred = torch.tensor([0, 1, 2, 3])
y_true = torch.tensor([1, 1, 2, 3])
metric = Hamming()
metric(y_pred, y_true)
tensor([0.2500])""",
        """CODE.y_pred = torch.tensor([0.5, 0.5, 2., 2.])
y_true = torch.tensor([2, 0.5, 1, 4])
metric = MeanGammaDeviance()
metric(y_pred, y_true)
tensor([1.0569])""",
        """CODE.y_pred = torch.tensor([1, 1, 1])
y_true = torch.tensor([0, 1, 1])
metric = Jaccard()
metric(y_pred, y_true)
tensor([0.3333])""",
        """CODE.y_pred = torch.tensor([1, 2, 0, 2])
y_true = torch.tensor([2, 2, 2, 1])
metric = CohenKappaScore()
metric(y_pred, y_true)
tensor([-0.3333])""",
        """CODE.y_pred = torch.tensor([2, 0.5, 1, 4])
y_true = torch.tensor([0.5, 0.5, 2., 2.])
metric = MeanPoissonDeviance()
metric(y_pred, y_true)
tensor([0.9034])""",
        """CODE.y_pred = torch.tensor([2, 0.5, 1, 4])
y_true = torch.tensor([0.5, 0.5, 2., 2.])
metric = MeanTweedieDeviance()
metric(y_pred, y_true)
tensor([1.8125])""",
        """CODE.y_pred = torch.tensor([2.5, 0.0, 2, 8])
y_true = torch.tensor([3, -0.5, 2, 7])
metric = ExplainedVariance()
metric(y_pred, y_true)
tensor([0.9572])""",
        """CODE.y_pred = torch.tensor([2.5, 0.0, 2, 8])
y_true = torch.tensor([3, -0.5, 2, 7])
metric = MedianAbsoluteError()
metric(y_pred, y_true)""",
        """CODE.y_pred = torch.tensor([2.5, 0.0, 2, 8])
y_true = torch.tensor([3, -0.5, 2, 7])
metric = MedianAbsoluteError()
metric(y_pred, y_true)
tensor([0.5000])""",
        """CODE.y_pred = torch.tensor([2.5, 0.0, 2, 8])
y_true = torch.tensor([3, -0.5, 2, 7])
metric = R2Score()
metric(y_pred, y_true)
tensor([0.9486])""",
        """CODE.y_pred = torch.tensor([2.5, 5, 4, 8])
y_true = torch.tensor([3, 5, 2.5, 7])
metric = MeanSquaredLogError()
metric(y_pred, y_true)
tensor([0.0397])""",
        """CODE.y_score = torch.tensor([[.1, .2, .3, 4, 70]])
y_true = torch.tensor([[10, 0, 0, 1, 5]])
metric = DCG()
metric(y_score, y_true)
tensor([9.4995])""" .

<DEPENDENCY.pytorch-lightning==1.0.0> <CONTAINS> """CODE.def backward(self, loss, optimizer, optimizer_idx):
    loss.backward()""",
        """CODE.def training_step(...):
    (opt_a, opt_b) = self.optimizers()
    loss = ...
    # automatically applies scaling, etc...
    self.manual_backward(loss, opt_a)""",
        """CODE.from pytorch_lightning import Trainer
parse_env_variables(Trainer)
Namespace()
import os
os.environ["PL_TRAINER_GPUS"] = '42'
os.environ["PL_TRAINER_BLABLABLA"] = '1.23'
parse_env_variables(Trainer)
Namespace(gpus=42)
del os.environ["PL_TRAINER_GPUS"]""" .

<DEPENDENCY.pytorch-lightning==1.0.3> <CONTAINS> """CODE.def mse(pred, target, reduction='elementwise_mean', return_state=False):
    squared_error = (pred - target) ** 2
    if reduction == 'elementwise_mean':
        mse = squared_error.mean()
    elif reduction == 'sum':
        mse = squared_error.sum()
    else:
        mse = squared_error
    if return_state:
        return mse, squared_error
    return mse

x = torch.tensor([0., 1, 2, 3])
y = torch.tensor([0., 1, 2, 2])
print(mse(x, y))
""",
        """CODE.def rmse(pred, target, reduction='elementwise_mean', return_state=False):
    error = (pred - target) ** 2
    if reduction == 'elementwise_mean':
        score = error.mean().sqrt()
    elif reduction == 'sum':
        score = error.sum().sqrt()
    else:
        score = error.sqrt()
    if return_state:
        return score, error
    return score

x = torch.tensor([0., 1, 2, 3])
y = torch.tensor([0., 1, 2, 2])
print(rmse(x, y))
""",
        """CODE.mae = nn.L1Loss(reduction='mean')
x = torch.tensor([0., 1, 2, 3])
y = torch.tensor([0., 1, 2, 2])
mae_value = mae(x, y)
print(mae_value)""",
        """CODE.mae(x, y)
tensor(0.2500)""",
        "CODE.rmsle(x, y)",
        """CODE.x = torch.tensor([0., 1, 2, 3])

y = torch.tensor([0., 1, 2, 2])

mse(x, y)""",
        """CODE.x = torch.tensor([0., 1, 2, 3])

y = torch.tensor([0., 1, 2, 2])

rmsle(x, y)""" .

<DEPENDENCY.pytorch-lightning==1.0.4> <CONTAINS> """CODE.class MyDDP(DDPPlugin):

    def configure_ddp(self, model, device_ids):
        model = MyDDPWrapper(model, device_ids)
        return model
""",
        """CODE.from pytorch_lightning.metrics import SSIM
preds = torch.rand([16, 1, 16, 16])
target = preds * 0.75
ssim = SSIM()
ssim(preds, target)""",
        """CODE.from pytorch_lightning.metrics.functional import explained_variance
target = torch.tensor([3, -0.5, 2, 7])
preds = torch.tensor([2.5, 0.0, 2, 8])
explained_variance(preds, target)

target = torch.tensor([[0.5, 1], [-1, 1], [7, -6]])
preds = torch.tensor([[0, 2], [-1, 2], [8, -5]])
explained_variance(preds, target, multioutput='raw_values')""",
        """CODE.mean_absolute_error(x, y)
tensor(0.2500)""",
        "CODE.mean_squared_error(x, y)",
        """CODE.mean_squared_log_error(x, y)
tensor(0.0207)""",
        """CODE.x = torch.tensor([0., 1, 2, 3])

y = torch.tensor([0., 1, 2, 2])

mean_squared_error(x, y)""",
        """CODE.x = torch.tensor([0., 1, 2, 3])

y = torch.tensor([0., 1, 2, 2])

mean_squared_log_error(x, y)""",
        """CODE.x = torch.tensor([0., 1, 2, 3])
y = torch.tensor([0., 1, 2, 2])
mean_absolute_error(x, y)
tensor(0.2500)""" .

<DEPENDENCY.pytorch-lightning==1.0.7> <CONTAINS> """CODE.class Model():
    def __init__(self, hparams, *my_args, anykw=42, **my_kwargs):
        pass

parse_class_init_keys(Model)
('self', 'my_args', 'my_kwargs')""" .

<DEPENDENCY.pytorch-lightning==1.0.8> <CONTAINS> """CODE.pred = torch.tensor([[0.85, 0.05, 0.05, 0.05],
    ...                      [0.05, 0.85, 0.05, 0.05],
    ...                      [0.05, 0.05, 0.85, 0.05],
    ...                      [0.05, 0.05, 0.05, 0.85]])

target = torch.tensor([0, 1, 3, 2])

multiclass_roc(pred, target)""" .

<DEPENDENCY.pytorch-lightning==1.1.0> <CONTAINS> """CODE.DeviceType.CPU == DeviceType.from_str('cpu')
DeviceType.GPU == 'GPU'
DeviceType.TPU == 'tpu'""",
        """CODE.DistributedType.DDP == 'ddp'
DistributedType.DDP2 == 'DDP2'""",
        """CODE.Example (binary case):

    pred = torch.tensor([0, 1, 2, 3])
    target = torch.tensor([0, 1, 1, 1])
    average_precision = AveragePrecision(pos_label=1)
    average_precision(pred, target)
    tensor(1.)

Example (multiclass case):

    pred = torch.tensor([[0.75, 0.05, 0.05, 0.05, 0.05],
    ...                      [0.05, 0.75, 0.05, 0.05, 0.05],
    ...                      [0.05, 0.05, 0.75, 0.05, 0.05],
    ...                      [0.05, 0.05, 0.05, 0.75, 0.05]])
    target = torch.tensor([0, 1, 3, 2])
    average_precision = AveragePrecision(num_classes=5)
    average_precision(pred, target)
    [tensor(1.), tensor(1.), tensor(0.2500), tensor(0.2500), tensor(nan)]
""",
        "CODE.LoggerStages.TRAIN == 'train'",
        """CODE._load_long_description(PROJECT_ROOT)  # doctest: +ELLIPSIS +NORMALIZE_WHITESPACE
'<div align="center">...""",
        """CODE._load_requirements(PROJECT_ROOT)  # doctest: +ELLIPSIS +NORMALIZE_WHITESPACE
['numpy...', 'torch...', ...]""",
        """CODE._module_available('os')
_module_available('bla.bla')""",
        """CODE.class MyPlugin(DDPPlugin):
    def required_plugins(self):
        return [MyCustomAMPPlugin()]

# Will automatically add the necessary AMP plugin
trainer = Trainer(plugins=[MyPlugin()])

# Crash as MyPlugin enforces custom AMP plugin
trainer = Trainer(plugins=[MyPlugin(), NativeAMPPlugin()])""",
        """CODE.def select_topk(prob_tensor, topk, dim):
    values, indices = torch.topk(prob_tensor, topk, dim=dim)
    binary_tensor = torch.zeros_like(prob_tensor, dtype=torch.int32)
    binary_tensor.scatter_(dim, indices, 1)
    return binary_tensor

x = torch.tensor([[1.1, 2.0, 3.0], [2.0, 1.0, 0.5]])
result = select_topk(x, topk=2)
print(result)
""",
        """CODE.def training_step(...):
    (opt_a, opt_b) = self.optimizers()
    loss_a = ...
    # automatically applies scaling, etc...
    self.manual_backward(loss_a, opt_a)
    opt_a.step()

def training_step(self, batch, batch_idx):
    # using Boring Model
    opt = self.optimizers() # only 1 optimizer

    def compute_loss():
        x = batch[0]
        x = F.dropout(x, 0.1)
        predictions = self(x)
        predictions = F.dropout(predictions, 0.1)
        loss = self.loss(None, predictions)
        return loss

    def closure():
        # emulate MC dropout training
        num_backward = 1
        losses = []
        for backward_idx in range(num_backward + 1):
            loss = compute_loss()
            losses.append(loss)
            retain_graph = num_backward!= backward_idx
            self.manual_backward(loss, opt, retain_graph=retain_graph)
        loss_mean = torch.stack(losses).mean()
        loss_std = torch.stack(losses).std()
        self.log("train_loss_mean", loss_mean, on_step=True, prog_bar=True, on_epoch=True)
        self.log("train_loss_std", loss_std, on_step=True, prog_bar=True, on_epoch=True)

    opt.step(loss, closure=closure)

def training_step(self, batch, batch_idx, optimizer_idx):

    # emulate gans training
    opt_gen, opt_dis = self.optimizers()

    # Note: Be careful, don't log on the same key in self.log in both closure
    # as they will be aggregated together on epoch_end

    def gen_closure():
        ... forward and compute loss for generator
        loss_gen = ...
        self.log("loss_gen", loss_gen, on_step=True, on_epoch=True)
        self.manual_backward(loss_gen, opt_gen)

    def dis_closure():
        ... forward and compute loss for discriminator
        loss_dis = ...
        self.log("loss_dis", loss_dis, on_step=True, on_epoch=True)
        self.manual_backward(loss_dis, opt_dis)

    # this will accumulate gradients for 2 batches and then call opt_gen.step()
    opt_gen.step(closure=gen_closure, make_optimizer_step=batch_idx % 2 == 0)

    # update discriminator every 4 batches
    # therefore, no gradient accumulation for discriminator
    if batch_idx % 4 == 0 :
        # Note: Set make_optimizer_step to True or it will use by default
        # Trainer(accumulate_grad_batches=x)
        opt_dis.step(closure=optimizer_closure, make_optimizer_step=True)""",
        """CODE.path_img = _download_badge('https://img.shields.io/pypi/pyversions/pytorch-lightning',
                            'PyPI - Python Version', '.')
os.path.isfile(path_img)
path_img  # doctest: +ELLIPSIS
os.remove(path_img)""",
        """CODE.pred = torch.tensor([0, 1, 2, 3])
target = torch.tensor([0, 1, 1, 1])
roc = ROC(pos_label=1)
fpr, tpr, thresholds = roc(pred, target)
fpr
tensor([0., 0., 0., 0., 1.])
tpr
tensor([0.0000, 0.3333, 0.6667, 1.0000, 1.0000])
thresholds
tensor([4, 3, 2, 1, 0])

pred = torch.tensor([[0.75, 0.05, 0.05, 0.05],
...                      [0.05, 0.75, 0.05, 0.05],
...                      [0.05, 0.05, 0.75, 0.05],
...                      [0.05, 0.05, 0.05, 0.75]])
target = torch.tensor([0, 1, 3, 2])
roc = ROC(num_classes=4)
fpr, tpr, thresholds = roc(pred, target)
fpr
[tensor([0., 0., 1.]), tensor([0., 0., 1.]), tensor([0.0000, 0.3333, 1.0000]), tensor([0.0000, 0.3333, 1.0000])]
tpr
[tensor([0., 1., 1.]), tensor([0., 1., 1.]), tensor([0., 0., 1.]), tensor([0., 0., 1.])]
thresholds # doctest: +NORMALIZE_WHITESPACE
[tensor([1.7500, 0.7500, 0.0500]),
 tensor([1.7500, 0.7500, 0.0500]),
 tensor([1.7500, 0.7500, 0.0500]),
 tensor([1.7500, 0.7500, 0.0500)]
""",
        """CODE.pred = torch.tensor([[0.85, 0.05, 0.05, 0.05],
    ...                      [0.05, 0.85, 0.05, 0.05],
    ...                      [0.05, 0.05, 0.85, 0.05],
    ...                      [0.05, 0.05, 0.05, 0.85]])

target = torch.tensor([0, 1, 3, 2])

__multiclass_roc(pred, target)""",
        """CODE.pred = torch.tensor([[0.85, 0.05, 0.05, 0.05],
...                      [0.05, 0.85, 0.05, 0.05],
...                      [0.05, 0.05, 0.85, 0.05],
...                      [0.05, 0.05, 0.05, 0.85]])
target = torch.tensor([0, 1, 3, 2])
multiclass_auroc(pred, target, num_classes=4)
tensor(0.6667)""",
        """CODE.x = torch.tensor([0, 1, 2, 3])
y = torch.tensor([0, 1, 1, 1])
fpr, tpr, thresholds = __roc(x, y)
fpr
tensor([0., 0., 0., 0., 1.])
tpr
tensor([0.0000, 0.3333, 0.6667, 1.0000, 1.0000])
thresholds
tensor([4, 3, 2, 1, 0])""",
        """CODE.x = torch.tensor([0, 1, 2, 3])
y = torch.tensor([0, 1, 1, 1])
fpr, tpr, thresholds = __roc(x, y)
fpr
tensor([0., 0., 0., 0., 1.])
tpr
tensor([0.0000, 0.3333, 0.6667, 1.0000, 1.0000])
thresholds
tensor([4, 3, 2, 1, 0])
""" .

<DEPENDENCY.pytorch-lightning==1.1.1> <CONTAINS> """CODE._fpr, _tpr, _thresholds = _roc(x, y)
_fpr
_tpr
_thresholds""",
        """CODE.x = torch.tensor([0, 1, 2, 3])
y = torch.tensor([0, 1, 1, 1])
fpr, tpr, thresholds = _roc(x, y)
fpr
tensor([0., 0., 0., 0., 1.])
tpr
tensor([0.0000, 0.3333, 0.6667, 1.0000, 1.0000])
thresholds
tensor([4, 3, 2, 1, 0])""" .

<DEPENDENCY.pytorch-lightning==1.1.2> <CONTAINS> """CODE.Backbone()
Backbone(
  (l1): Linear(...)
  (l2): Linear(...)
)""",
        """CODE.Discriminator(img_shape=(1, 28, 28))
Discriminator(
  (model): Sequential(...)
)""",
        """CODE.GAN(img_shape=(1, 8, 8))
(generator): Generator(
(model): Sequential(...)
)
(discriminator): Discriminator(
(model): Sequential(...)
)""",
        """CODE.Generator(img_shape=(1, 8, 8))
Generator(
  (model): Sequential(...)
)""",
        """CODE.LitAutoEncoder()  # doctest: +ELLIPSIS +NORMALIZE_WHITESPACE
LitAutoEncoder(
  (encoder): ...
  (decoder): ...
)""",
        """CODE.LitClassifier(Backbone())  # doctest: +ELLIPSIS +NORMALIZE_WHITESPACE
LitClassifier(
  (backbone): ...
)""",
        """CODE.LitResnet()  # doctest: +ELLIPSIS +NORMALIZE_WHITESPACE
LitResnet(
  (sequential_module): Sequential(...)
)""",
        "CODE.RandomDataset(size=10, length=20)" .

<DEPENDENCY.pytorch-lightning==1.1.8> <CONTAINS> "CODE.LoggerStages.TRAIN == 'train'",
        """CODE._load_long_description(PROJECT_ROOT)  # doctest: +ELLIPSIS +NORMALIZE_WHITESPACE
'<div align="center">...""",
        """CODE.def configure_apex(amp, model, optimizers, amp_level):
    model, optimizers = amp.initialize(
        model, optimizers, opt_level=amp_level,
    )

    return model, optimizers""",
        """CODE.path_img = _download_badge('https://img.shields.io/pypi/pyversions/pytorch-lightning',
                            'PyPI - Python Version', '.')
os.path.isfile(path_img)
path_img  # doctest: +ELLIPSIS
os.remove(path_img)""",
        """CODE.predictions_to_write = {'preds': ['cat', 'dog'], 'ids': tensor([0, 1])}
result.write_dict(predictions_to_write)
""",
        """CODE.ref_model = accelerator.get_reference_model(model)
ref_model.training_step(...)""",
        """CODE.ref_model = ddp_plugin.get_model_from_plugin(model)
ref_model.training_step(...)""",
        """CODE.result = pl.EvalResult()
result.write('ids', [0, 1, 2])
result.write('preds', ['cat', 'dog', 'dog'])""",
        """CODE.result.log('train_loss', loss)
result.log(
    name,
    value,
    on_step=True,
    on_epoch=False,
    logger=True,
    prog_bar=False,
    reduce_fx=torch.mean,
    enable_graph=False
)""",
        """CODE.result.log('val_loss', loss)

# defaults used
result.log(
    name,
    value,
    on_step=False,
    on_epoch=True,
    logger=True,
    prog_bar=False,
    reduce_fx=torch.mean
)
""",
        """CODE.values = {'loss': loss, 'acc': acc, ..., 'metric_n': metric_n}
result.log_dict(values)""" .

<DEPENDENCY.pytorch-lightning==1.2.0> <CONTAINS> "CODE.RunningStage.TRAINING == 'train'",
        """CODE._load_readme_description(_PROJECT_ROOT)  # doctest: +ELLIPSIS +NORMALIZE_WHITESPACE
'<div align="center">...""",
        """CODE.class MyModel(LightningModule):

    ...

    def configure_optimizer(self):
        # Make sure to filter the parameters based on `requires_grad`
        return Adam(filter(lambda p: p.requires_grad, self.parameters))

class FeatureExtractorFreezeUnfreeze(BaseFinetuning):

    def __init__(self, unfreeze_at_epoch=10)
        self._unfreeze_at_epoch = unfreeze_at_epoch

    def freeze_before_training(self, pl_module):
        # freeze any module you want
        # Here, we are freezing ``feature_extractor``
        self.freeze(pl_module.feature_extractor)

    def finetune_function(self, pl_module, current_epoch, optimizer, optimizer_idx):
        # When `current_epoch` is 10, feature_extractor will start training.
        if current_epoch == self._unfreeze_at_epoch:
            self.unfreeze_and_add_param_group(
                module=pl_module.feature_extractor,
                optimizer=optimizer,
                train_bn=True,
            )
""",
        """CODE.def configure_apex(self, amp, model, optimizers, amp_level):
    model, optimizers = amp.initialize(
        model, optimizers, opt_level=amp_level,
    )

    return model, optimizers""",
        """CODE.def configure_callbacks(self):
    early_stop = EarlyStopping(monitor="val_acc", mode="max")
    checkpoint = ModelCheckpoint(monitor="val_loss")
    return [early_stop, checkpoint]""",
        """CODE.def on_after_batch_transfer(self, batch, dataloader_idx):
    batch['x'] = gpu_transforms(batch['x'])
    return batch""",
        """CODE.def on_before_batch_transfer(self, batch, dataloader_idx):
    batch['x'] = transforms(batch['x'])
    return batch""",
        """CODE.def on_post_move_to_device(self):
    self.decoder.weight = self.encoder.weight""",
        """CODE.from pytorch_lightning import Trainer
from pytorch_lightning.callbacks import LambdaCallback
trainer = Trainer(callbacks=[LambdaCallback(setup=lambda *args: print('setup'))])""",
        """CODE.from pytorch_lightning.metrics import MetricCollection, Accuracy, Precision, Recall
target = torch.tensor([0, 2, 0, 2, 0, 1, 0, 2])
preds = torch.tensor([2, 1, 2, 0, 1, 2, 2, 2])
metrics = MetricCollection([Accuracy(),
                            Precision(num_classes=3, average='macro'),
                            Recall(num_classes=3, average='macro')])
metrics(preds, target)

metrics = MetricCollection({'micro_recall': Recall(num_classes=3, average='micro'),
                            'macro_recall': Recall(num_classes=3, average='macro')})
metrics(preds, target)""",
        """CODE.from pytorch_lightning.metrics.functional import r2score
target = torch.tensor([3, -0.5, 2, 7])
preds = torch.tensor([2.5, 0.0, 2, 8])
r2score(preds, target)

target = torch.tensor([[0.5, 1], [-1, 1], [7, -6]])
preds = torch.tensor([[0, 2], [-1, 2], [8, -5]])
r2score(preds, target, multioutput='raw_values')""",
        """CODE.image = torch.arange(0, 1*1*5*5, dtype=torch.float32)
image = torch.reshape(image, (1, 1, 5, 5))
dy, dx = image_gradients(image)
dy[0, 0, :, :]""",
        """CODE.loaders = {'a': torch.utils.data.DataLoader(range(6), batch_size=4),
            'b': torch.utils.data.DataLoader(range(15), batch_size=5)}
combined_loader = CombinedLoader(loaders, 'max_size_cycle')
for item in combined_loader:
    print(item)
combined_loader = CombinedLoader(loaders, 'min_size')
for item in combined_loader:
    print(item)""",
        """CODE.model = PPOLightning("CartPole-v0")
trainer = Trainer()
trainer.fit(model)""",
        """CODE.pred_dict = {'pred1': torch.tensor(...), 'pred2': torch.tensor(...)}
self.write_prediction_dict(pred_dict)""",
        "CODE.self.write_prediction('pred', torch.tensor(...), filename='my_predictions.pt')",
        """CODE.x = torch.tensor([0., 1, 2, 3])
y = torch.tensor([0., 1, 2, 2])
mean_relative_error(x, y)""" .

<DEPENDENCY.pytorch-lightning==1.2.10> <CONTAINS> """CODE.RandomDataset(size=10, length=20)  # doctest: +ELLIPSIS
<...bug_report_model.RandomDataset object at ...>""",
        """CODE._x = torch.tensor([0, 1, 2, 3])
_y = torch.tensor([0, 1, 1, 1])
_fpr, _tpr, _thresholds = _roc(_x, _y)""",
        """CODE.class LitModel(...):
    def __init__(self):
        self.l1 = None

    def prepare_data(self):
        download_data()
        tokenize()

        # don't do this
        self.something = else

    def setup(stage):
        data = Load_data(...)
        self.l1 = nn.Linear(28, data.num_classes)""" .

<DEPENDENCY.pytorch-lightning==1.2.5> <CONTAINS> "CODE._compare_version(\"torch\", operator.ge, \"0.1\")" .

<DEPENDENCY.pytorch-lightning==1.2.6> <CONTAINS> """CODE.@RunIf(min_torch="0.0")
@pytest.mark.parametrize("arg1", [1, 2.0])
def test_wrapper(arg1):
    assert arg1 > 0.0""" .

<DEPENDENCY.pytorch-lightning==1.3.0> <CONTAINS> """CODE.class LitModel(...):
    def __init__(self):
        self.l1 = None

    def prepare_data(self):
        download_data()
        tokenize()

        # don't do this
        self.something = else

    def setup(stage):
        data = Load_data(...)
        self.l1 = nn.Linear(28, data.num_classes)""",
        """CODE.def optimizer_zero_grad(self, epoch, batch_idx, optimizer, optimizer_idx):
    optimizer.zero_grad()

def optimizer_zero_grad(self, epoch, batch_idx, optimizer, optimizer_idx):
    optimizer.zero_grad(set_to_none=True)""",
        """CODE.from pytorch_lightning import Trainer
from pytorch_lightning.callbacks import Timer

# stop training after 12 hours
timer = Timer(duration="00:12:00:00")

# or provide a datetime.timedelta
from datetime import timedelta
timer = Timer(duration=timedelta(weeks=1))

# or provide a dictionary
timer = Timer(duration=dict(weeks=4, days=2))

# force training to stop after given time limit
trainer = Trainer(callbacks=[timer])

# query training/validation/test time (in seconds)
timer.time_elapsed("train")
timer.start_time("validate")
timer.end_time("test")""",
        """CODE.import torch
from pytorch_lightning.callbacks import BasePredictionWriter

class CustomWriter(BasePredictionWriter):

    def __init__(self, output_dir: str, write_interval: str):
        super().__init__(write_interval)
        self.output_dir

    def write_on_batch_end(
        self, trainer, pl_module: 'LightningModule', prediction: Any, batch_indices: List[int], batch: Any,
        batch_idx: int, dataloader_idx: int
    ):
        torch.save(prediction, os.path.join(self.output_dir, dataloader_idx, f"{batch_idx}.pt"))

    def write_on_epoch_end(
        self, trainer, pl_module: 'LightningModule', predictions: List[Any], batch_indices: List[Any]
    ):
        torch.save(predictions, os.path.join(self.output_dir, "predictions.pt"))""",
        """CODE.register_ddp_comm_hook(
    model=ddp_model,
    ddp_comm_hook=default.fp16_compress_hook,
)

register_ddp_comm_hook(
    model=ddp_model,
    ddp_comm_state=powerSGD.PowerSGDState(
        process_group=None,
        matrix_approximation_rank=1,
        start_powerSGD_iter=5000,
    ),
    ddp_comm_hook=powerSGD.powerSGD_hook,
)

register_ddp_comm_hook(
    model=ddp_model,
    ddp_comm_state=powerSGD.PowerSGDState(
        process_group=None,
        matrix_approximation_rank=1,
        start_powerSGD_iter=5000,
    ),
    ddp_comm_hook=powerSGD.powerSGD_hook,
    ddp_comm_wrapper=default.fp16_compress_wrapper,
)""",
        """CODE.str_to_bool_or_int("FALSE")
str_to_bool_or_int("1")
str_to_bool_or_int("2")
str_to_bool_or_int("abc")""" .

<DEPENDENCY.pytorch-lightning==1.4.0> <CONTAINS> """CODE.class ManuallyArgsModel(HyperparametersMixin):
    def __init__(self, arg1, arg2, arg3):
        super().__init__()
        # manually assign arguments
        self.save_hyperparameters('arg1', 'arg3')
    def forward(self, *args, **kwargs):
        ...

class AutomaticArgsModel(HyperparametersMixin):
    def __init__(self, arg1, arg2, arg3):
        super().__init__()
        # equivalent automatic
        self.save_hyperparameters()
    def forward(self, *args, **kwargs):
        ...

class SingleArgModel(HyperparametersMixin):
    def __init__(self, params):
        super().__init__()
        # manually assign single argument
        self.save_hyperparameters(params)
    def forward(self, *args, **kwargs):
        ...

class ManuallyArgsModel(HyperparametersMixin):
    def __init__(self, arg1, arg2, arg3):
        super().__init__()
        # pass argument(s) to ignore as a string or in a list
        self.save_hyperparameters(ignore='arg2')
    def forward(self, *args, **kwargs):
        ...""" .

<DEPENDENCY.pytorch-lightning==1.5.0> <CONTAINS> """CODE.PrecisionType.HALF == 16
PrecisionType.HALF in (16, "16")""",
        """CODE._convert_optim_dict({0: {"loss": 0.0}, 2: {"loss": 0.2}}, num_optimizers=3)
[{'loss': 0.0}, None, {'loss': 0.2}]""",
        "CODE._get_max_shape([[], [[1], [2]], []])",
        """CODE._precision_allowed_type("32")
_precision_allowed_type("bf16")""",
        """CODE._recursive_pad([[], [1], [2, 3], [4]], fill_value=0)  # doctest: +NORMALIZE_WHITESPACE
array([[0, 0], [1, 0], [2, 3], [4, 0]], dtype=object)""",
        "CODE._recursive_unpad([[[0, 1, 0]], [2], [0, 0]], value=0)",
        """CODE.class SimpleDataFetcher(AbstractDataFetcher):
    def fetching_function(self):
        while True:
            try:
                return next(self.dataloader_iter), False
            except StopIteration:
                return None, True""",
        """CODE.from pytorch_lightning import Trainer
from pytorch_lightning.callbacks import RichModelSummary

trainer = Trainer(callbacks=RichModelSummary())


from pytorch_lightning import Trainer
from pytorch_lightning.callbacks import RichProgressBar

trainer = Trainer(callbacks=RichProgressBar())
""",
        """CODE.from pytorch_lightning import Trainer
from pytorch_lightning.callbacks import RichProgressBar

trainer = Trainer(callbacks=RichProgressBar())
""",
        """CODE.if self.global_rank == 0:
    # let process 0 download the dataset
    dataset.download_files()

# let all processes wait before reading the dataset
self.barrier()

# now all processes can read the files and start training""" .

<DEPENDENCY.pytorch-lightning==1.5.10> <CONTAINS> """CODE._package_available('os')
_package_available('bla')""" .

<DEPENDENCY.pytorch-lightning==1.6.0> <CONTAINS> """CODE.# you can match the type with string
_StrategyType.DDP == 'ddp'
# which is case invariant
_StrategyType.DDP2 in ('ddp2', )""",
        """CODE.result = _ResultCollection(training=True, torch.device("cpu"))
result.log('training_step', 'acc', torch.tensor(...), on_step=True, on_epoch=True)
result.log('validation_step', 'recall', torch.tensor(...), on_step=True, on_epoch=True)""",
        """CODE.with self.profile('load training data'):
    # load training data code""" .

<DEPENDENCY.pytorch-lightning==1.6.5> <CONTAINS> """CODE._load_readme_description(_PROJECT_ROOT, "", "")  # doctest: +ELLIPSIS +NORMALIZE_WHITESPACE
'<div align="center">...""" .

<DEPENDENCY.pytorch-lightning==1.7.0> <CONTAINS> """CODE._RequirementAvailable("torch>=0.1")
bool(_RequirementAvailable("torch>=0.1"))
bool(_RequirementAvailable("torch>100.0"))""",
        """CODE.class Flow(LightningFlow):
    def __init__(self):
        super().__init__()
        self.names = []

    def configure_commands(self):
        return {"my_command_name": self.my_remote_method}

    def my_remote_method(self, name):
        self.names.append(name)
""",
        "CODE.create_meta_package(os.path.join(_PROJECT_ROOT, \"src\"))",
        """CODE.def serve_step(self, x: torch.Tensor) -> Dict[str, torch.Tensor]:
    return {"predictions": self(x)}
""",
        """CODE.def start_server(self, host, port):
    self._process = subprocess.Popen(["flask", "run" "--host", host, "--port", str(port)])""",
        """CODE.def stop_server(self):
    self._process.kill()""",
        """CODE.from dataclasses import dataclass
from lightning_app import BuildConfig

@dataclass
class MyOwnBuildConfig(BuildConfig):

    def build_commands(self):
        return ["apt-get install libsparsehash-dev"]""",
        """CODE.from lightning import LightningFlow
class RootFlow(LightningFlow):
    def __init__(self):
        super().__init__()
        self.counter = 0
    def run(self):
        self.counter += 1

flow = RootFlow()
flow.run()
assert flow.counter == 1
assert flow.state["vars"]["counter"] == 1
""",
        """CODE.from lightning_app import LightningFlow

class Flow(LightningFlow):
    def run(self):
        if self.schedule("hourly"):
            print("run some code every hour")


from lightning_app import LightningFlow
from lightning_app.structures import List

class SchedulerDAG(LightningFlow):
    def __init__(self):
        super().__init__()
        self.dags = List()

    def run(self):
        if self.schedule("hourly"):
            self.dags.append(DAG(...))

        for dag in self.dags:
            payload = dag.run()
""",
        """CODE.from lightning_app import LightningFlow, LightningWork
from lightning_app.core import Dict

class CounterWork(LightningWork):
    def __init__(self):
        super().__init__()
        self.counter = 0
    def run(self):
        self.counter += 1

class RootFlow(LightningFlow):
    def __init__(self):
        super().__init__()
        self.dict = Dict(**{"work_0": CounterWork(), "work_1": CounterWork()})
    def run(self):
        for work_name, work in self.dict.items():
            work.run()

flow = RootFlow()
flow.run()
assert flow.dict["work_0"].counter == 1
""",
        """CODE.from lightning_app import LightningFlow, LightningWork
from lightning_app.core import List

class CounterWork(LightningWork):
    def __init__(self):
        super().__init__()
        self.counter = 0
    def run(self):
        self.counter += 1

class RootFlow(LightningFlow):
    def __init__(self):
        super().__init__()
        self.list = List(*[CounterWork(), CounterWork()])
    def run(self):
        for work in self.list:
            work.run()

flow = RootFlow()
flow.run()
assert flow.list[0].counter == 1""",
        """CODE.from lightning_app.frontend import StaticWebFrontend

class Flow(LightningFlow):
    ...

    def configure_layout(self):
        return StaticWebFrontend("path/to/folder/to/serve")

from lightning_app.frontend import StaticWebFrontend

class Flow(LightningFlow):
    ...

    def configure_layout(self):
        return StreamlitFrontend(render_fn=my_streamlit_ui)

def my_streamlit_ui(state):
    # add your streamlit code here!
    import streamlit as st

class Flow(LightningFlow):
    def configure_layout(self):
        return [
            dict(name="First Tab", content=self.child0),
            dict(name="Second Tab", content=self.child1),
            dict(name="Lightning", content="https://lightning.ai"),
        ]
""",
        """CODE.from typing import Dict, Any, Callable

import torch

from pytorch_lightning import Trainer
from pytorch_lightning.demos.boring_classes import BoringModel
from pytorch_lightning.serve.servable_module_validator import ServableModule, ServableModuleValidator


class ServableBoringModel(BoringModel, ServableModule):
    def configure_payload(self) -> Dict[str, Any]:
        return {"body": {"x": list(range(32))}}

    def configure_serialization(self) -> Tuple[Dict[str, Callable], Dict[str, Callable]]:
        def deserialize(x):
            return torch.tensor(x, dtype=torch.float)

        def serialize(x):
            return x.tolist()

        return {"x": deserialize}, {"output": serialize}

    def serve_step(self, x: torch.Tensor) -> Dict[str, torch.Tensor]:
        return {"output": torch.tensor([0, 1])}

    def configure_response(self):
        return {"output": [0, 1]}


serve_cb = ServableModuleValidator()
trainer = Trainer(
    max_epochs=1,
    limit_train_batches=2,
    limit_val_batches=0,
    callbacks=[serve_cb],
)
trainer.fit(ServableBoringModel())
assert serve_cb.resp.json() == {"output": [0, 1]}
""",
        """CODE.import os

def replace_block_with_imports(lines, import_path, block_type):
    new_lines = []
    in_block = False
    for line in lines:
        if block_type == "class" and "class" in line:
            in_block = True
        elif block_type == "def" and "def" in line:
            in_block = True

        if in_block:
            if line.strip().endswith(":"):
                new_lines.append(f"from {import_path} import {line.strip()[:-1]}")
                in_block = False
            else:
                new_lines.append(line)
        else:
            new_lines.append(line)

    return new_lines

py_file = os.path.join(_PROJECT_ROOT, "src", "pytorch_lightning", "loggers", "logger.py")
import_path = ".".join(["pytorch_lightning", "loggers", "logger"])
with open(py_file, encoding="utf-8") as fp:
    lines = [ln.rstrip() for ln in fp.readlines()]
lines = replace_block_with_imports(lines, import_path, "class")
lines = replace_block_with_imports(lines, import_path, "def")""",
        """CODE.import os

py_file = os.path.join(_PROJECT_ROOT, "src", "pytorch_lightning", "loggers", "__init__.py")
import_path = ".".join(["pytorch_lightning", "loggers"])

with open(py_file, encoding="utf-8") as fp:
    lines = [ln.rstrip() for ln in fp.readlines()]

lines = prune_func_calls(lines)""",
        """CODE.import os

py_file = os.path.join(_PROJECT_ROOT, "src", "pytorch_lightning", "utilities", "cli.py")
import_path = ".".join(["pytorch_lightning", "utilities", "cli"])

with open(py_file, encoding="utf-8") as fp:
    lines = [ln.rstrip() for ln in fp.readlines()]

lines = prune_imports_callables(lines)""",
        """CODE.import os

py_file = os.path.join(_PROJECT_ROOT, "src", "pytorch_lightning", "utilities", "cli.py")
import_path = ".".join(["pytorch_lightning", "utilities", "cli"])

with open(py_file, encoding="utf-8") as fp:
    lines = [ln.rstrip() for ln in fp.readlines()]

lines = prune_imports_callables(lines)
lines = prune_empty_statements(lines)""",
        """CODE.load_readme_description(_PROJECT_ROOT, "", "")  # doctest: +ELLIPSIS +NORMALIZE_WHITESPACE
'<div align="center">...""",
        """CODE.load_readme_description(_PROJECT_ROOT, "", "")  # doctest: +ELLIPSIS +NORMALIZE_WHITESPACE
'<div align="center">...'""",
        """CODE.path_req = os.path.join(_PROJECT_ROOT, "requirements")
load_requirements(path_req)  # doctest: +ELLIPSIS +NORMALIZE_WHITESPACE
['numpy...', 'torch...', ...]""",
        """CODE.py_file = os.path.join(_PROJECT_ROOT, "src", "pytorch_lightning", "loggers", "csv_logs.py")
import_path = ".".join(["pytorch_lightning", "loggers", "csv_logs"])
with open(py_file, encoding="utf-8") as fp:
...     lines = [ln.rstrip() for ln in fp.readlines()]
lines = prune_comments_docstrings(lines)""",
        """CODE.py_file = os.path.join(_PROJECT_ROOT, "src", "pytorch_lightning", "utilities", "imports.py")
import_path = ".".join(["pytorch_lightning", "utilities", "imports"])
with open(py_file, encoding="utf-8") as fp:
...     lines = [ln.rstrip() for ln in fp.readlines()]
lines = replace_vars_with_imports(lines, import_path)""",
        """CODE.snapshot = _GlobalStateSnapshot.capture()
snapshot.restore()""" .

<DEPENDENCY.pytorch-lightning==1.8.0> <CONTAINS> """CODE.# 1. Customize the BatchSizeFinder callback to run at different epochs. This feature is
# useful while fine-tuning models since you can't always use the same batch size after
# unfreezing the backbone.
from pytorch_lightning.callbacks import BatchSizeFinder


class FineTuneBatchSizeFinder(BatchSizeFinder):
    def __init__(self, milestones, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.milestones = milestones

    def on_fit_start(self, *args, **kwargs):
        return

    def on_train_epoch_start(self, trainer, pl_module):
        if trainer.current_epoch in self.milestones or trainer.current_epoch == 0:
            self.scale_batch_size(trainer, pl_module)


trainer = Trainer(callbacks=[FineTuneBatchSizeFinder(milestones=(5, 10))])
trainer.fit(...)

# 2. Run batch size finder for validate/test/predict.
from pytorch_lightning.callbacks import BatchSizeFinder


class EvalBatchSizeFinder(BatchSizeFinder):
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)

    def on_fit_start(self, *args, **kwargs):
        return

    def on_test_start(self, trainer, pl_module):
        self.scale_batch_size(trainer, pl_module)


trainer = Trainer(callbacks=[EvalBatchSizeFinder()])
trainer.test(...)
""",
        """CODE.# Synchronous reading, run_forever() is blocking

def print_log_msg(ws_app, msg):
    print(msg)

flow_logs_socket = client.create_lightning_logs_socket("project_id", "app_id", "flow", print_log_msg)
flow_socket.run_forever()

# Asynchronous reading (with Threads)

def print_log_msg(ws_app, msg):
    print(msg)

flow_logs_socket = client.create_lightning_logs_socket("project_id", "app_id", "flow", print_log_msg)
work_logs_socket = client.create_lightning_logs_socket("project_id", "app_id", "work_1", print_log_msg)

flow_logs_thread = Thread(target=flow_logs_socket.run_forever)
work_logs_thread = Thread(target=work_logs_socket.run_forever)

flow_logs_thread.start()
work_logs_thread.start()
# .......

flow_logs_socket.close()
work_logs_thread.close()
""",
        """CODE.class CustomObject:
    def __init__(self):
        self.x = torch.rand(2, 2)
    def to(self, device):
        self.x = self.x.to(device)
        return self

isinstance(CustomObject(), _TransferableDataType) True""",
        """CODE.def print_log_msg(ws_app, msg):
    print(msg)

logs_socket = client.create_cluster_logs_socket("cluster_id", 1661100000, 1661101000, print_log_msg)
logs_socket.run_forever()


def print_log_msg(ws_app, msg):
    print(msg)

logs_socket = client.create_cluster_logs_socket("cluster_id", 1661100000, 1661101000, print_log_msg)

logs_thread = Thread(target=cluster_logs_socket.run_forever)

logs_thread.start()
# .......

logs_socket.close()
""",
        """CODE.from typing import Callable
from lightning import LightningApp, LightningFlow
from lightning.app.frontend import JustPyFrontend


class Flow(LightningFlow):
    def __init__(self):
        super().__init__()
        self.counter = 0

    def run(self):
        print(self.counter)

    def configure_layout(self):
        return JustPyFrontend(render_fn=render_fn)


def render_fn(get_state: Callable) -> Callable:
    import justpy as jp

    def my_click(self, *_):
        state = get_state()
        old_counter = state.counter
        state.counter += 1
        self.text = f"Click Me ! Old Counter: {old_counter} New Counter: {state.counter}"

    def webpage():
        wp = jp.WebPage()
        d = jp.Div(text="Hello ! Click Me!")
        d.on("click", my_click)
        wp.add(d)
        return wp

    return webpage


app = LightningApp(Flow())
""",
        """CODE.from typing import List
from sqlmodel import SQLModel, Field
from uuid import uuid4

from lightning_app import LightningFlow, LightningApp
from lightning_app.components.database import Database, DatabaseClient

class CounterModel(SQLModel, table=True):
    __table_args__ = {"extend_existing": True}

    id: int = Field(default=None, primary_key=True)
    count: int


class Flow(LightningFlow):

    def __init__(self):
        super().__init__()
        self._private_token = uuid4().hex
        self.db = Database(models=[CounterModel])
        self._client = None
        self.counter = 0

    def run(self):
        self.db.run(token=self._private_token)

        if not self.db.alive():
            return

        if self.counter == 0:
            self._client = DatabaseClient(
                model=CounterModel,
                db_url=self.db.url,
                token=self._private_token,
            )

        rows = self._client.select_all()

        print(f"{self.counter}: {rows}")

        if not rows:
            self._client.insert(CounterModel(count=0))
        else:
            row: CounterModel = rows[0]
            row.count += 1
            self._client.update(row)

        if self.counter >= 100:
            row: CounterModel = rows[0]
            self._client.delete(row)
            self._exit()

        self.counter += 1

app = LightningApp(Flow())

from typing import List
from sqlmodel import SQLModel, Field
from sqlalchemy import Column

from lightning_app.components.database.utilities import pydantic_column_type

class KeyValuePair(SQLModel):
    name: str
    value: str

class CounterModel(SQLModel, table=True):
    __table_args__ = {"extend_existing": True}

    name: int = Field(default=None, primary_key=True)

    # RIGHT THERE ! You need to use Field and Column with the `pydantic_column_type` utility.""",
        """CODE.import panel as pn

pn.panel("Hello **Panel â¡** World").servable()

import lightning as L
from lightning_app.frontend.panel import PanelFrontend


class LitPanel(L.LightningFlow):
    def configure_layout(self):
        return PanelFrontend("panel_app_basic.py")


class LitApp(L.LightningFlow):
    def __init__(self):
        super().__init__()
        self.lit_panel = LitPanel()

    def configure_layout(self):
        return {"name": "home", "content": self.lit_panel}


app = L.LightningApp(LitApp())""",
        """CODE.import param
app = AppStateWatcher()
app.state.counter = 1

@param.depends(app.param.state, watch=True)
def update(state):
    print(f"The counter was updated to {state.counter}")

app.state.counter += 1""" .

<DEPENDENCY.pytorch-lightning==1.8.4> <CONTAINS> """CODE._format_elapsed_seconds(5)
_format_elapsed_seconds(60)""",
        """CODE.from lightning_app.frontend import StaticWebFrontend

class Work(LightningWork):
    def configure_layout(self):
        return StaticWebFrontend("path/to/folder/to/serve")

class Work(LightningWork):
    def configure_layout(self):
        return [
            dict(name="First Tab", content=self.child0),
            dict(name="Second Tab", content=self.child1),
            dict(name="Lightning", content="https://lightning.ai"),
        ]
""",
        """CODE.import lightning as L

# Example 1: Auto-scaling serve component out-of-the-box
app = L.LightningApp(
    L.app.components.AutoScaler(
        MyPythonServer,
        min_replicas=1,
        max_replicas=8,
        autoscale_interval=10,
    )
)

# Example 2: Customizing the scaling logic
class MyAutoScaler(L.app.components.AutoScaler):
    def scale(self, replicas: int, metrics: dict) -> int:
        pending_requests_per_running_or_pending_work = metrics["pending_requests"] / (
            replicas + metrics["pending_works"]
        )

        # upscale
        max_requests_per_work = self.max_batch_size
        if pending_requests_per_running_or_pending_work >= max_requests_per_work:
            return replicas + 1

        # downscale
        min_requests_per_work = max_requests_per_work * 0.25
        if pending_requests_per_running_or_pending_work < min_requests_per_work:
            return replicas - 1

        return replicas


app = L.LightningApp(
    MyAutoScaler(
        MyPythonServer,
        min_replicas=1,
        max_replicas=8,
        autoscale_interval=10,
        max_batch_size=8,  # for auto batching
        timeout_batching=1,  # for auto batching
    )
)""" .

<DEPENDENCY.pytorch-lightning==1.8.6> <CONTAINS> """CODE._augment_requirement("arrow<=1.2.2,>=1.2.0  # anything", unfreeze="")
_augment_requirement("arrow<=1.2.2,>=1.2.0  # strict", unfreeze="")
_augment_requirement("arrow<=1.2.2,>=1.2.0  # my name", unfreeze="all")
_augment_requirement("arrow>=1.2.0, <=1.2.2  # strict", unfreeze="all")
_augment_requirement("arrow", unfreeze="all")
_augment_requirement("arrow>=1.2.0, <=1.2.2  # cool", unfreeze="major")
_augment_requirement("arrow>=1.2.0, <=1.2.2  # strict", unfreeze="major")
_augment_requirement("arrow>=1.2.0", unfreeze="major")
_augment_requirement("arrow", unfreeze="major")""" .

<DEPENDENCY.pytorch-lightning==1.9.0> <CONTAINS> """CODE.[r.adjust('none') for r in _parse_requirements(txt)]
[r.adjust('none') for r in _parse_requirements(txt)]""",
        """CODE._RequirementWithComment("arrow<=1.2.2,>=1.2.0", comment="# anything").adjust("none")
_RequirementWithComment("arrow<=1.2.2,>=1.2.0", comment="# strict").adjust("none")
_RequirementWithComment("arrow<=1.2.2,>=1.2.0", comment="# my name").adjust("all")
_RequirementWithComment("arrow>=1.2.0, <=1.2.2", comment="# strict").adjust("all")
_RequirementWithComment("arrow").adjust("all")
_RequirementWithComment("arrow>=1.2.0, <=1.2.2", comment="# cool").adjust("major")
_RequirementWithComment("arrow>=1.2.0, <=1.2.2", comment="# strict").adjust("major")
_RequirementWithComment("arrow>=1.2.0").adjust("major")
_RequirementWithComment("arrow").adjust("major")""",
        """CODE.if self.global_rank == 0:
    # let process 0 download the dataset
    dataset.download_files()

# let all processes wait before reading the dataset
self.barrier()

# now all processes can read the files and start training""",
        """CODE.with self.profile('load training data'):
    # load training data code""" .

<DEPENDENCY.pytorch-lightning==1.9.2> <CONTAINS> """CODE... code-block:: python

    class Flow(LightningFlow):
        def __init__(self):
            super().__init()

    def configure_plugins(self):
        return [{"my_plugin_name": MyPlugin()}]""" .

<DEPENDENCY.pytorch-lightning==1.9.5> <CONTAINS> """CODE.from torch.distributed.algorithms.ddp_comm_hooks import ( # doctest: +SKIP
    ...     default_hooks as default,
    ...     powerSGD_hook as powerSGD,
    ...     post_localSGD_hook as post_localSGD,
    ... )

    # fp16_compress_hook for compress gradients
    ddp_model = ...
    register_ddp_comm_hook( # doctest: +SKIP
    ...     model=ddp_model,
    ...     ddp_comm_hook=default.fp16_compress_hook,
    ... )

    # powerSGD_hook
    ddp_model = ...
    register_ddp_comm_hook( # doctest: +SKIP
    ...     model=ddp_model,
    ...     ddp_comm_state=powerSGD.PowerSGDState(
    ...         process_group=None,
    ...         matrix_approximation_rank=1,
    ...         start_powerSGD_iter=5000,
    ...     ),
    ...     ddp_comm_hook=powerSGD.powerSGD_hook,
    ... )

    # post_localSGD_hook
    subgroup, _ = torch.distributed.new_subgroups() # doctest: +SKIP
    ddp_model = ...
    register_ddp_comm_hook( # doctest: +SKIP
    ...     model=ddp_model,
    ...     state=post_localSGD.PostLocalSGDState(
    ...         process_group=None,
    ...         subgroup=subgroup,
    ...         start_localSGD_iter=1_000,
    ...     ),
    ...     ddp_comm_hook=post_localSGD.post_localSGD_hook,
    ... )

    # fp16_compress_wrapper combined with other communication hook
    ddp_model = ...
    register_ddp_comm_hook( # doctest: +SKIP
    ...     model=ddp_model,
    ...     ddp_comm_state=powerSGD.PowerSGDState(
    ...         process_group=None,
    ...         matrix_approximation_rank=1,
    ...         start_powerSGD_iter=5000,
    ...     ),
    ...     ddp_comm_hook=powerSGD.powerSGD_hook,
    ...     ddp_comm_wrapper=default.fp16_compress_wrapper,
    ... )""",
        """CODE.import os
os.environ["PL_TRAINER_GPUS"] = '42'
os.environ["PL_TRAINER_BLABLABLA"] = '1.23'
parse_env_variables(Trainer)
Namespace(gpus=42)""" .

<DEPENDENCY.pytorch-lightning==2.0.0> <CONTAINS> """CODE.class LitProgressBar(ProgressBar):
    def __init__(self):
        super().__init__()  # don't forget this :)
        self.enable = True

    def disable(self):
        self.enable = False

    def on_train_batch_end(self, trainer, pl_module, outputs, batch_idx):
        super().on_train_batch_end(trainer, pl_module, outputs, batch_idx)  # don't forget this :)
        percent = (self.train_batch_idx / self.total_train_batches) * 100
        sys.stdout.flush()
        sys.stdout.write(f'{percent:.01f} percent complete \\r')

bar = LitProgressBar()
trainer = Trainer(callbacks=[bar])""",
        """CODE.def get_metrics(self, trainer, model):
    # don't show the version number
    items = super().get_metrics(trainer, model)
    items.pop("v_num", None)
    return items
""",
        """CODE.from torch.distributed.algorithms.ddp_comm_hooks import ( # doctest: +SKIP
    ...     default_hooks as default,
    ...     powerSGD_hook as powerSGD,
    ...     post_localSGD_hook as post_localSGD,
    ... )

    # fp16_compress_hook for compress gradients
    ddp_model = ...
    _register_ddp_comm_hook( # doctest: +SKIP
    ...     model=ddp_model,
    ...     ddp_comm_hook=default.fp16_compress_hook,
    ... )

    # powerSGD_hook
    ddp_model = ...
    _register_ddp_comm_hook( # doctest: +SKIP
    ...     model=ddp_model,
    ...     ddp_comm_state=powerSGD.PowerSGDState(
    ...         process_group=None,
    ...         matrix_approximation_rank=1,
    ...         start_powerSGD_iter=5000,
    ...     ),
    ...     ddp_comm_hook=powerSGD.powerSGD_hook,
    ... )

    # post_localSGD_hook
    subgroup, _ = torch.distributed.new_subgroups() # doctest: +SKIP
    ddp_model = ...
    _register_ddp_comm_hook( # doctest: +SKIP
    ...     model=ddp_model,
    ...     state=post_localSGD.PostLocalSGDState(
    ...         process_group=None,
    ...         subgroup=subgroup,
    ...         start_localSGD_iter=1_000,
    ...     ),
    ...     ddp_comm_hook=post_localSGD.post_localSGD_hook,
    ... )

    # fp16_compress_wrapper combined with other communication hook
    ddp_model = ...
    _register_ddp_comm_hook( # doctest: +SKIP
    ...     model=ddp_model,
    ...     ddp_comm_state=powerSGD.PowerSGDState(
    ...         process_group=None,
    ...         matrix_approximation_rank=1,
    ...         start_powerSGD_iter=5000,
    ...     ),
    ...     ddp_comm_hook=powerSGD.powerSGD_hook,
    ...     ddp_comm_wrapper=default.fp16_compress_wrapper,
    ... )""",
        """CODE.import os
os.environ["PL_TRAINER_DEVICES"] = '42'
os.environ["PL_TRAINER_BLABLABLA"] = '1.23'
_parse_env_variables(Trainer)
Namespace(devices=42)""" .

<DEPENDENCY.pytorch-lightning==2.1.0> <CONTAINS> """CODE.with _EmptyInit():
    model = BigModel()
model.load_state_dict(torch.load("checkpoint.pt"))""" .

<DEPENDENCY.pytorch-lightning==2.2.0> <CONTAINS> """CODE.    fabric = Fabric(logger=logger)
    throughput = ThroughputMonitor()
    t0 = time()
    for i in range(1, 100):
        do_work()
        if torch.cuda.is_available(): torch.cuda.synchronize()  # required or else time() won't be correct
        throughput.update(time=time() - t0, batches=i, samples=i)
        if i % 10 == 0:
            throughput.compute_and_log(step=i)""",
        """CODE.model_fwd = lambda: model(x)
fwd_flops = measure_flops(model, model_fwd)

model_loss = lambda y: y.sum()
fwd_and_bwd_flops = measure_flops(model, model_fwd, model_loss)
""",
        """CODE.throughput = Throughput()
t0 = time()
for i in range(1000):
    do_work()
    if torch.cuda.is_available(): torch.cuda.synchronize()  # required or else time() won't be correct
    throughput.update(time=time() - t0, samples=i)
    if i % 10 == 0:
        print(throughput.compute())""" .

<DEPENDENCY.ray==0.2.0> <CONTAINS> """CODE.dist_class, dist_dim = ModelCatalog.get_action_dist(env.action_space)
model = ModelCatalog.get_model(inputs, dist_dim)
dist = dist_class(model.outputs)
action_op = dist.sample()""" .

<DEPENDENCY.ray==0.3.1> <CONTAINS> "CODE.print(ev.sample())",
        """CODE.samples = ev1.sample()
grads = ev2.compute_gradients(samples)
ev1.apply_gradients(grads)""",
        """CODE.weights = ev1.get_weights()
ev2.set_weights(weights)""" .

<DEPENDENCY.ray==0.4.0> <CONTAINS> """CODE.b = _Bracket(1, 10, 2, 3)
b.on_result(trial1, 1, 2)  # CONTINUE
b.on_result(trial2, 1, 4)  # CONTINUE
b.cutoff(b._rungs[-1][1]) == 3.0  # rungs are reversed
b.on_result(trial3, 1, 1)  # STOP
b.cutoff(b._rungs[0][1]) == 2.0""",
        """CODE.batch = ev.sample()
grads, info = ev2.compute_gradients(samples)""",
        "CODE.ev.compute_apply(samples)",
        """CODE.samples = ev1.sample()
grads, info = ev2.compute_gradients(samples)
ev1.apply_gradients(grads)""",
        "CODE.weights = ev1.get_weights()",
        """CODE.weights = ev1.get_weights()
ev2.set_weights(weights)""" .

<DEPENDENCY.ray==0.5.3> <CONTAINS> """CODE.network = ConvNetBuilder(...)
with tf.variable_scope('cg', custom_getter=network.get_custom_getter()):
  network.conv(...)
  # Call more methods of network here
""" .

<DEPENDENCY.ray==0.6.1> <CONTAINS> """CODE.agent = MyAgent()
for _ in range(10):
    agent.train()
agent.export_policy_model("/tmp/export_dir")""" .

<DEPENDENCY.ray==0.6.2> <CONTAINS> """CODE.agent = MyAgent()
for _ in range(10):
    agent.train()
agent.export_policy_checkpoint("/tmp/export_dir")""" .

<DEPENDENCY.ray==0.6.3> <CONTAINS> "CODE.provider.nodes({TAG_RAY_NODE_TYPE: \"worker\"})" .

<DEPENDENCY.ray==0.6.4> <CONTAINS> """CODE.provider.non_terminated_nodes({TAG_RAY_NODE_TYPE: "worker"})
["node-1", "node-2"]""" .

<DEPENDENCY.ray==0.6.5> <CONTAINS> """CODE.@ray.remote
class MyActor(RayServeMixin):
    # This is optional, by default it is "__call__"
    serve_method = 'my_method'

    def my_method(self, arg):
        ...""" .

<DEPENDENCY.ray==0.7.0> <CONTAINS> "CODE.ev.learn_on_batch(samples)",
        """CODE.parameters = [
    {"name": "x1", "type": "range", "bounds": [0.0, 1.0]},
    {"name": "x2", "type": "range", "bounds": [0.0, 1.0]},
]
algo = AxSearch(parameters=parameters,
    objective_name="hartmann6", max_concurrent=4)""",
        """CODE.tune.run(my_trainable, name="my_exp", local_dir="~/tune_results")
analysis = ExperimentAnalysis(
    experiment_path="~/tune_results/my_exp")""" .

<DEPENDENCY.ray==0.7.1> <CONTAINS> "CODE.ev.learn_on_batch(samples)" .

<DEPENDENCY.ray==0.7.2> <CONTAINS> """CODE.worker = RolloutWorker(
  env_creator=lambda _: gym.make("CartPole-v0"),
  policy=PGTFPolicy)
print(worker.sample())


worker = RolloutWorker(
  env_creator=lambda _: MultiAgentTrafficGrid(num_cars=25),
  policies={
      "car_policy1":
        (PGTFPolicy, Box(...), Discrete(...), {"gamma": 0.99}),
      "car_policy2":
        (PGTFPolicy, Box(...), Discrete(...), {"gamma": 0.95}),
      "traffic_light_policy":
        (PGTFPolicy, Box(...), Discrete(...), {}),
  },
  policy_mapping_fn=lambda agent_id:
    random.choice(["car_policy1", "car_policy2"])
    if agent_id.startswith("car_") else "traffic_light_policy")
print(worker.sample())
""" .

<DEPENDENCY.ray==0.7.3> <CONTAINS> """CODE.@ray.remote
class MyActor(RayServeMixin):
    # This is optional, by default it is "__call__"
    serve_method = 'my_method'

    def my_method(self, arg):
        ...""",
        """CODE.def __init__(self, *args, **kwargs):
    super(MyModelClass, self).__init__(*args, **kwargs)
    cell_size = 256

    # Define input layers
    input_layer = tf.keras.layers.Input(
        shape=(None, obs_space.shape[0]))
    state_in_h = tf.keras.layers.Input(shape=(256, ))
    state_in_c = tf.keras.layers.Input(shape=(256, ))
    seq_in = tf.keras.layers.Input(shape=())

    # Send to LSTM cell
    lstm_out, state_h, state_c = tf.keras.layers.LSTM(
        cell_size, return_sequences=True, return_state=True,
        name="lstm")(
            inputs=input_layer,
            mask=tf.sequence_mask(seq_in),
            initial_state=[state_in_h, state_in_c])
    output_layer = tf.keras.layers.Dense(...)(lstm_out)

    # Create the RNN model
    self.rnn_model = tf.keras.Model(
        inputs=[input_layer, seq_in, state_in_h, state_in_c],
        outputs=[output_layer, state_h, state_c])
    self.register_variables(self.rnn_model.variables)
    self.rnn_model.summary()""",
        """CODE.def __init__(self, *args, **kwargs):
    super(MyModelClass, self).__init__(*args, **kwargs)
    input_layer = tf.keras.layers.Input(...)
    hidden_layer = tf.keras.layers.Dense(...)(input_layer)
    output_layer = tf.keras.layers.Dense(...)(hidden_layer)
    value_layer = tf.keras.layers.Dense(...)(hidden_layer)
    self.base_model = tf.keras.Model(
        input_layer, [output_layer, value_layer])
    self.register_variables(self.base_model.variables)""",
        """CODE.def forward(self, input_dict, state, seq_lens):
    model_out, self._value_out = self.base_model(input_dict["obs"])
    return model_out, state""",
        """CODE.def forward_rnn(self, inputs, state, seq_lens):
    model_out, h, c = self.rnn_model([inputs, seq_lens] + state)
    return model_out, [h, c]""",
        """CODE.def get_initial_state(self):
    return [
        np.zeros(self.cell_size, np.float32),
        np.zeros(self.cell_size, np.float32),
    ]""",
        """CODE.def value_function(self):
    return self._value_out""" .

<DEPENDENCY.ray==0.7.4> <CONTAINS> """CODE.import ConfigSpace as CS
config_space = CS.ConfigurationSpace()
config_space.add_hyperparameter(
        CS.UniformFloatHyperparameter('width', lower=0, upper=20))
config_space.add_hyperparameter(
        CS.UniformFloatHyperparameter('height', lower=-100, upper=100))
config_space.add_hyperparameter(
        CS.CategoricalHyperparameter(
            name='activation', choices=['relu', 'tanh']))
algo = TuneBOHB(
        config_space, max_concurrent=4, metric='mean_loss', mode='min')
bohb = HyperBandForBOHB(
        time_attr='training_iteration',
        metric='mean_loss',
        mode='min',
        max_t=100)
run(MyTrainableClass, scheduler=bohb, search_alg=algo)""" .

<DEPENDENCY.ray==0.7.5> <CONTAINS> """CODE.@ray.remote
class RayServeActor(RayServeMixin, MyClass):
    pass""",
        """CODE.@ray.remote
class TaskRunnerActor(TaskRunnerBackend):
    pass""",
        """CODE.BytesEncoder will walk the JSON tree and decode bytes with utf-8 codec.

Example:
json.dumps({b'a': b'c'}, cls=BytesEncoder)""",
        "CODE.await JSONResponse({\"k\": \"v\"})(scope, receive, send)",
        """CODE.import uvicorn
uvicorn.run(HTTPProxy(kv_store_actor_handle, router_handle))""",
        """CODE.store_ns1 = NamespacedKVStore(namespace="ns1")
store_ns2 = NamespacedKVStore(namespace="ns2")
store_ns1.put("same-key", 1)
store_ns1.get("same-key")
store_ns2.put("same-key", 2)
store_ns2.get("same-key", 2)""" .

<DEPENDENCY.ray==0.8.1> <CONTAINS> """CODE.@serve.accept_batch
def serving_func(flask_request):
    assert isinstance(flask_request, list)
    ...

@serve.accept_batch
def __call__(self, *, python_arg=None):
    assert isinstance(python_arg, list)""",
        """CODE.a1, a2 = Actor.remote(), Actor.remote()
pool = ActorPool([a1, a2])
print(pool.map(lambda a, v: a.double.remote(v), [1, 2, 3, 4]))
[2, 4, 6, 8]""",
        """CODE.from_iterators([range(100), range(100)])
from_iterators([lambda: range(100), lambda: range(100)])""",
        """CODE.it = from_items([0, 1, 2]).filter(lambda x: x > 0)
next(it.gather_sync())""",
        "CODE.next(from_range(10, 1).batch(4).flatten())",
        """CODE.next(from_range(4).for_each(lambda x: x * 2).gather_sync())
[0, 2, 4, 8]""",
        """CODE.pool = ActorPool(...)
pool.submit(lambda a, v: a.double.remote(v), 1)
print(pool.has_next())
print(pool.get_next())
print(pool.has_next())""" .

<DEPENDENCY.ray==0.8.2> <CONTAINS> """CODE... code-block:: python

    import time
    from ray import tune
    from ray.tune import Stopper

    class TimeStopper(Stopper):
        def __init__(self):
            self._start = time.time()
            self._deadline = 300

        def __call__(self, trial_id, result):
            return False

        def stop_all(self):
            return time.time() - self._start > self.deadline

    tune.run(Trainable, num_samples=200, stop=TimeStopper())""",
        """CODE.def model_creator(config):
    return nn.Linear(1, 1)


def optimizer_creator(model, config):
    return torch.optim.SGD(
        model.parameters(), lr=config.get("lr", 1e-4))


def data_creator(config):
    return LinearDataset(2, 5), LinearDataset(2, 5, size=400)

trainer = PyTorchTrainer(
    model_creator,
    data_creator,
    optimizer_creator,
    loss_creator=nn.MSELoss,
    use_gpu=True
)
trainer.train()
""",
        """CODE.grads_op = rollouts.for_each(ComputeGradients(workers))
print(next(grads_op))""",
        """CODE.it = from_range(10, 1).local_shuffle(shuffle_buffer_size=2)
it = it.gather_sync()
next(it)
next(it)
next(it)
next(it)
""",
        """CODE.rollouts = ParallelRollouts(...)
rollouts = rollouts.combine(ConcatBatches(min_batch_size=10000))
print(next(rollouts).count)
10000""",
        """CODE.rollouts = ParallelRollouts(...)
train_op = rollouts.for_each(TrainOneStep(workers))
print(next(train_op))  # This trains the policy on one batch.
{"learner_stats": {"policy_loss": ...}}""",
        """CODE.serve.utils.expand([1,2,[3,4,5],6])
serve.utils.expand(["a", ["b", "c"], "d", ["e", "f"]])""" .

<DEPENDENCY.ray==0.8.3> <CONTAINS> """CODE.@serve.route("/path")
def my_handler(flask_request):
    ...""",
        """CODE.AsyncGradients(workers)
grads_op = AsyncGradients(workers)
print(next(grads_op))""",
        """CODE.actors = [ReplayActor.remote() for _ in range(4)]
replay_op = ParallelReplay(actors)
next(replay_op)
SampleBatch(...)""",
        """CODE.actors = [ReplayActor.remote() for _ in range(4)]
rollouts = ParallelRollouts(...)
store_op = rollouts.for_each(StoreToReplayActors(actors))
next(store_op)
SampleBatch(...)""",
        """CODE.buf = ReplayBuffer(1000)
rollouts = ParallelRollouts(...)
store_op = rollouts.for_each(StoreToReplayBuffer(buf))
next(store_op)""",
        """CODE.from ray import tune
from dragonfly.opt.gp_bandit import EuclideanGPBandit
from dragonfly.exd.experiment_caller import EuclideanFunctionCaller
from dragonfly import load_config

domain_vars = [{
    "name": "LiNO3_vol",
    "type": "float",
    "min": 0,
    "max": 7
}, {
    "name": "Li2SO4_vol",
    "type": "float",
    "min": 0,
    "max": 7
}, {
    "name": "NaClO4_vol",
    "type": "float",
    "min": 0,
    "max": 7
}]

domain_config = load_config({"domain": domain_vars})
func_caller = EuclideanFunctionCaller(None,
    domain_config.domain.list_of_domains[0])
optimizer = EuclideanGPBandit(func_caller, ask_tell_mode=True)

algo = DragonflySearch(optimizer, max_concurrent=4,
    metric="objective", mode="max")

tune.run(my_func, algo=algo)
""",
        "CODE.name = self.trial_name",
        """CODE.pg = PGTrainer(
...     env="CartPole-v0", config={
...         "input": lambda ioctx:
...             PolicyServerInput(ioctx, addr, port),
...         "num_workers": 0,  # Run just 1 server, in the trainer.
...     }
while True:
        pg.train()

client = PolicyClient("localhost:9900", inference_mode="local")
eps_id = client.start_episode()
action = client.get_action(eps_id, obs)
...
client.log_returns(eps_id, reward)
...
client.log_returns(eps_id, reward)""",
        """CODE.provider.non_terminated_nodes({TAG_RAY_NODE_TYPE: "worker"})
["node-1", "node-2"]""",
        """CODE.queue = queue.Queue(100)
read_op = Dequeue(queue)
combined_op = Concurrently([write_op, read_op], mode="async")
next(combined_op)""",
        """CODE.queue = queue.Queue(100)
write_op = ParallelRollouts(...).for_each(Enqueue(queue))
read_op = Dequeue(queue)
combined_op = Concurrently([write_op, read_op], mode="async")
next(combined_op)""",
        """CODE.ray.init()

def model_creator(config):
    return nn.Linear(1, 1)


def optimizer_creator(model, config):
    return torch.optim.SGD(
        model.parameters(), lr=config.get("lr", 1e-4))


def data_creator(config):
    batch_size = config["batch_size"]
    train_data, val_data = LinearDataset(2, 5), LinearDataset(2, 5)
    train_loader = DataLoader(train_data, batch_size=batch_size)
    val_loader = DataLoader(val_data, batch_size=batch_size)
    return train_loader, val_loader


trainer = TorchTrainer(
    model_creator=model_creator,
    data_creator=data_creator,
    optimizer_creator=optimizer_creator,
    loss_creator=nn.MSELoss,
    config={"batch_size": 32},
    use_gpu=True
)
for i in range(4):
    trainer.train()
""",
        """CODE.sim_op = ParallelRollouts(...).for_each(...)
replay_op = LocalReplay(...).for_each(...)
combined_op = Concurrently([sim_op, replay_op])""",
        "CODE.trial_id = self.trial_id" .

<DEPENDENCY.ray==0.8.4> <CONTAINS> """CODE.TorchTrainable = TorchTrainer.as_trainable(
    model_creator=ResNet18,
    data_creator=cifar_creator,
    optimizer_creator=optimizer_creator,
    loss_creator=nn.CrossEntropyLoss,
    num_gpus=2
)
analysis = tune.run(
    TorchTrainable,
    config={"lr": tune.grid_search([0.01, 0.1])}
)""",
        "CODE.await Response({\"k\": \"v\"}).send(scope, receive, send)",
        """CODE.serve.split("service-name", {
    "backend:v1": 0.5,
    "backend:v2": 0.5
})""",
        """CODE.trainer = MyTrainer()
trainer.import_policy_model_from_h5("/tmp/weights.h5")
for _ in range(10):
    trainer.train()""" .

<DEPENDENCY.ray==0.8.5> <CONTAINS> """CODE.import time
from ray import tune
from ray.tune import track

def run_me(config):
    for iter in range(100):
        time.sleep(1)
        track.log(hello="world", ray="tune")

analysis = tune.run(run_me)""",
        """CODE.serve.set_traffic("service-name", {
    "backend:v1": 0.5,
    "backend:v2": 0.5
})""" .

<DEPENDENCY.ray==0.8.6> <CONTAINS> """CODE.import time
from ray import tune

def run_me(config):
    for iter in range(100):
        time.sleep(1)
        tune.report(hello="world", ray="tune")

analysis = tune.run(run_me)""" .

<DEPENDENCY.ray==1.0.1> <CONTAINS> """CODE... code-block:: python

    from ray import tune

    def train(config, data=None):
        for sample in data:
            # ...
            tune.report(loss=loss)

    data = HugeDataset(download=True)

    tune.run(
        tune.with_parameters(train, data=data),
        #...
    )""",
        """CODE.@ray.remote
def f():
    pg = get_current_placement_group()
    pg = placement_group([{"CPU": 2}])
    f.options(placement_group=pg).remote()

ray.init()
assert get_current_placement_group() is None""",
        """CODE.counter = Counter("name")
counter2 = counter.set_default_tags({"a": "b"})
counter = Counter("name").set_default_tags({"a": "b"})""",
        """CODE.counter = Counter("name", description="desc")
print(counter.info)
{
    "name": "name",
    "description": "desc"
    "tag_keys": ("ray.key")
    "default_tags": {"ray.key": "abc"}
}""",
        """CODE.from ray import tune
from ray.tune import Callback


class MyCallback(Callback):
    def on_trial_result(self, iteration, trials, trial, result,
                        **info):
        print(f"Got result: {result['metric']}")


def train(config):
    for i in range(10):
        tune.report(metric=i)


tune.run(
    train,
    callbacks=[MyCallback()])
""",
        """CODE.from ray.tune.integration.docker import DockerSyncer
tune.run(train,
         sync_config=tune.SyncConfig(
             sync_to_driver=DockerSyncer))""",
        """CODE.pb2 = PB2(
    time_attr="timesteps_total",
    metric="episode_reward_mean",
    mode="max",
    perturbation_interval=10000,
    hyperparam_mutations={
        # These must be continuous, currently a limitation.
        "factor_1": lambda: random.uniform(0.0, 20.0),
    })
tune.run({...}, num_samples=8, scheduler=pb2)""" .

<DEPENDENCY.ray==1.1.0> <CONTAINS> """CODE.# create dummy data
spark.range(...).write.parquet(...)
# create MLDataset
data = ray.util.data.read_parquet(...)
# convert to TorchMLDataset
ds = data.to_torch(feature_columns=..., label_column=...)
# get the given shard data
shard = ds.get_shard(0)
# create the DataLoader from the shard data and this can be used for
# the TorchTrainer data creator as well
data = DataLoader(shard, batch_size=32)
""",
        """CODE.from ray.tune.logger import DEFAULT_LOGGERS
from ray.tune.integration.wandb import WandbLoggerCallback
tune.run(
    train_fn,
    config={
        # define search space here
        "parameter_1": tune.choice([1, 2, 3]),
        "parameter_2": tune.choice([4, 5, 6]),
    },
    callbacks=[WandbLoggerCallback(
        project="Optimization_Project",
        api_key_file="/path/to/file",
        log_config=True)])""",
        """CODE.lm.get_static_node_resources_by_ip()
{127.0.0.1: {"CPU": 1}, 127.0.0.2: {"CPU": 4, "GPU": 8}}""" .

<DEPENDENCY.ray==1.10.0> <CONTAINS> """CODE.output = BlockOutputBuffer(udf, 500 * 1024 * 1024)
for item in generator():
    output.add(item)
    if output.has_next():
        yield output.next()
output.finalize()
if output.has_next():
    yield output.next()""",
        """CODE.subscriber = GcsErrorSubscriber()
subscriber.subscribe()
while running:
    error_id, error_data = subscriber.poll()
subscriber.close()""",
        """CODE.subscriber = GcsFunctionKeySubscriber()
subscriber.subscribe()
while running:
    key = subscriber.poll()
subscriber.close()""",
        """CODE.subscriber = GcsLogSubscriber()
subscriber.subscribe()
while running:
    log = subscriber.poll()
subscriber.close()""" .

<DEPENDENCY.ray==1.11.1> <CONTAINS> """CODE.asynchronous_parallel_sample(
    trainer,
    actors=trainer.workers.remote_workers(),
    ray_wait_timeout_s=0.1,
    remote_fn=lambda w: time.sleep(1)  # sleep 1sec
)
print(len(batches))
2
# Expect a timeout to have happened.
batches[0] is None and batches[1] is None
True""" .

<DEPENDENCY.ray==1.12.1> <CONTAINS> """CODE.ds = ray.data.range_arrow(1000)
ds.map(lambda r: {"v2": r["value"] * 2}).show()
""" .

<DEPENDENCY.ray==1.13.0> <CONTAINS> """CODE.# Based on
# huggingface/notebooks/examples/language_modeling_from_scratch.ipynb

# Hugging Face imports
from datasets import load_dataset
import transformers
from transformers import AutoConfig, AutoModelForCausalLM, AutoTokenizer

import ray
from ray.ml.train.integrations.huggingface import HuggingFaceTrainer

model_checkpoint = "gpt2"
tokenizer_checkpoint = "sgugger/gpt2-like-tokenizer"
block_size = 128

datasets = load_dataset("wikitext", "wikitext-2-raw-v1")
tokenizer = AutoTokenizer.from_pretrained(tokenizer_checkpoint)

def tokenize_function(examples):
    return tokenizer(examples["text"])

tokenized_datasets = datasets.map(
    tokenize_function, batched=True, num_proc=1, remove_columns=["text"]
)

def group_texts(examples):
    # Concatenate all texts.
    concatenated_examples = {
        k: sum(examples[k], []) for k in examples.keys()
    }
    total_length = len(concatenated_examples[list(examples.keys())[0]])
    # We drop the small remainder, we could add padding if the model
    # supported it.
    # instead of this drop, you can customize this part to your needs.
    total_length = (total_length // block_size) * block_size
    # Split by chunks of max_len.
    result = {
        k: [
            t[i : i + block_size]
            for i in range(0, total_length, block_size)
        ]
        for k, t in concatenated_examples.items()
    }
    result["labels"] = result["input_ids"].copy()
    return result

lm_datasets = tokenized_datasets.map(
    group_texts,
    batched=True,
    batch_size=1000,
    num_proc=1,
)
ray_train_ds = ray.data.from_huggingface(lm_datasets["train"])
ray_evaluation_ds = ray.data.from_huggingface(
    lm_datasets["evaluation"]
)

def trainer_init_per_worker(train_dataset, eval_dataset, **config):
    model_config = AutoConfig.from_pretrained(model_checkpoint)
    model = AutoModelForCausalLM.from_config(model_config)
    args = transformers.TrainingArguments(
        output_dir=f"{model_checkpoint}-wikitext2",
        evaluation_strategy="epoch",
        learning_rate=2e-5,
        weight_decay=0.01,
    )
    return transformers.Trainer(
        model=model,
        args=args,
        train_dataset=train_dataset,
        eval_dataset=eval_dataset,
    )

scaling_config = {"num_workers": 3}
# If using GPUs, use the below scaling config instead.
# scaling_config = {"num_workers": 3, "use_gpu": True}
trainer = HuggingFaceTrainer(
    trainer_init_per_worker=trainer_init_per_worker,
    scaling_config=scaling_config,
    datasets={"train": ray_train_ds, "evaluation": ray_evaluation_ds},
)
result = trainer.fit()
""",
        """CODE.def train_loop_per_worker():
        ...

def train_loop_per_worker(config: Dict):
        ...

def train_loop_per_worker():
        # Report intermediate results for callbacks or logging.
        train.report(...)

        # Checkpoints the provided args as restorable state.
        train.save_checkpoint(...)

        # Returns dict of last saved checkpoint.
        train.load_checkpoint()

        # Returns the Ray Dataset shard for the given key.
        train.get_dataset_shard("my_dataset")

        # Returns the total number of workers executing training.
        train.get_world_size()

        # Returns the rank of this worker.
        train.get_world_rank()

        # Returns the rank of the worker on the current node.
        train.get_local_rank()

import ray
import ray.train as train
import ray.train.torch. # Need this to use `train.torch.get_device()`
import horovod.torch as hvd
import torch
import torch.nn as nn
from ray.ml.train.integrations.horovod import HorovodTrainer

input_size = 1
layer_size = 15
output_size = 1
num_epochs = 3

class NeuralNetwork(nn.Module):
    def __init__(self):
        super(NeuralNetwork, self).__init__()
        self.layer1 = nn.Linear(input_size, layer_size)
        self.relu = nn.ReLU()
        self.layer2 = nn.Linear(layer_size, output_size)
    def forward(self, input):
        return self.layer2(self.relu(self.layer1(input)))

def train_loop_per_worker():
    hvd.init()
    dataset_shard = train.get_dataset_shard("train")
    model = NeuralNetwork()
    device = train.torch.get_device()
    model.to(device)
    loss_fn = nn.MSELoss()
    lr_scaler = 1
    optimizer = torch.optim.SGD(model.parameters(), lr=0.1 * lr_scaler)
    # Horovod: wrap optimizer with DistributedOptimizer.
    optimizer = hvd.DistributedOptimizer(
        optimizer,
        named_parameters=model.named_parameters(),
        op=hvd.Average,
    )
    for epoch in range(num_epochs):
        model.train()
        for inputs, labels in iter(
            dataset_shard.to_torch(
                label_column="y",
                label_column_dtype=torch.float,
                feature_column_dtypes=torch.float,
                batch_size=32,
            )
        ):
            inputs.to(device)
            labels.to(device)
            outputs = model(inputs)
            loss = loss_fn(outputs, labels)
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
            print(f"epoch: {epoch}, loss: {loss.item()}")
        train.save_checkpoint(model=model.state_dict())
train_dataset = ray.data.from_items([{"x": x, "y": x + 1} for x in range(32)])
scaling_config = {"num_workers": 3}
# If using GPUs, use the below scaling config instead.
# scaling_config = {"num_workers": 3, "use_gpu": True}
trainer = HorovodTrainer(
    train_loop_per_worker=train_loop_per_worker,
    scaling_config={"num_workers": 3},
    datasets={"train": train_dataset},
)
result = trainer.fit()
""",
        """CODE.from ray.ml.config import RunConfig
from ray.ml.train.integrations.rl import RLTrainer

trainer = RLTrainer(
    run_config=RunConfig(stop={"training_iteration": 5}),
    scaling_config={
        "num_workers": 2,
        "use_gpu": False,
    },
    algorithm="PPO",
    config={
        "env": "CartPole-v0",
        "framework": "tf",
        "evaluation_num_workers": 1,
        "evaluation_interval": 1,
        "evaluation_config": {"input": "sampler"},
    },
)
result = trainer.fit()

import ray
from ray.ml.config import RunConfig
from ray.ml.train.integrations.rl import RLTrainer
from ray.rllib.agents.marwil.bc import BCTrainer

dataset = ray.data.read_json(
    "/tmp/data-dir", parallelism=2, ray_remote_args={"num_cpus": 1}
)

trainer = RLTrainer(
    run_config=RunConfig(stop={"training_iteration": 5}),
    scaling_config={
        "num_workers": 2,
        "use_gpu": False,
    },
    datasets={"train": dataset},
    algorithm=BCTrainer,
    config={
        "env": "CartPole-v0",
        "framework": "tf",
        "evaluation_num_workers": 1,
        "evaluation_interval": 1,
        "evaluation_config": {"input": "sampler"},
    },
)
result = trainer.fit()""",
        """CODE.import numpy as np
from sklearn.ensemble import RandomForestClassifier
from ray.ml.predictors.sklearn import SklearnPredictor

train_X = np.array([[1, 2], [3, 4]])
train_y = np.array([0, 1])

model = RandomForestClassifier().fit(train_X, train_y)
predictor = SklearnPredictor(model=model)

data = np.array([[1, 2], [3, 4]])
predictions = predictor.predict(data)

# Only use first and second column as the feature
data = np.array([[1, 2, 8], [3, 4, 9]])
predictions = predictor.predict(data, feature_columns=[0, 1])

import pandas as pd
from sklearn.ensemble import RandomForestClassifier
from ray.ml.predictors.sklearn import SklearnPredictor

train_X = pd.DataFrame([[1, 2], [3, 4]], columns=["A", "B"])
train_y = pd.Series([0, 1])

model = RandomForestClassifier().fit(train_X, train_y)
predictor = SklearnPredictor(model=model)

# Pandas dataframe.
data = pd.DataFrame([[1, 2], [3, 4]], columns=["A", "B"])
predictions = predictor.predict(data)

# Only use first and second column as the feature
data = pd.DataFrame([[1, 2, 8], [3, 4, 9]], columns=["A", "B", "C"])
predictions = predictor.predict(data, feature_columns=["A", "B"])""",
        """CODE.import pandas as pd
from transformers import AutoConfig, AutoModelForCausalLM, AutoTokenizer
from transformers.pipelines import pipeline
from ray.ml.predictors.integrations.huggingface import HuggingFacePredictor

model_checkpoint = "gpt2"
tokenizer_checkpoint = "sgugger/gpt2-like-tokenizer"
tokenizer = AutoTokenizer.from_pretrained(tokenizer_checkpoint)

model_config = AutoConfig.from_pretrained(model_checkpoint)
model = AutoModelForCausalLM.from_config(model_config)
predictor = HuggingFacePredictor(
    pipeline=pipeline(
        task="text-generation", model=model, tokenizer=tokenizer
    )
)

prompts = pd.DataFrame(
    ["Complete me", "And me", "Please complete"], columns=["sentences"]
)
predictions = predictor.predict(prompts)""",
        """CODE.import ray

from ray.ml.train.integrations.sklearn import SklearnTrainer
from sklearn.ensemble import RandomForestRegressor

train_dataset = ray.data.from_items(
    [{"x": x, "y": x + 1} for x in range(32)])
trainer = SklearnTrainer(
    sklearn_estimator=RandomForestRegressor,
    label_column="y",
    scaling_config={
        "trainer_resources": {"CPU": 4}
    },
    datasets={"train": train_dataset}
)
result = trainer.fit()
""",
        """CODE.import ray
ds = ray.data.range_table(1000)
ds.map(lambda r: {"v2": r["value"] * 2}).show()
""",
        """CODE.import ray
from ray.data.datasource import SimpleTorchDatasource

dataset_factory = lambda: torchvision.datasets.MNIST("data", download=True)
dataset = ray.data.read_datasource(
    SimpleTorchDatasource(), parallelism=1, dataset_factory=dataset_factory
)
dataset.take(1)""",
        """CODE.import ray.data
from ray.data.datasource import SimpleTensorFlowDatasource
import tensorflow_datasets as tfds

def dataset_factory():
    return tfds.load("cifar10", split=["train"], as_supervised=True)[0]

dataset = ray.data.read_datasource(
    SimpleTensorFlowDatasource(),
    parallelism=1,
    dataset_factory=dataset_factory
)

features, label = dataset.take(1)[0]
features.shape
label""" .

<DEPENDENCY.ray==1.13.1> <CONTAINS> """CODE.import numpy as np
    from sklearn.ensemble import RandomForestClassifier
    from ray.ml.predictors.sklearn import SklearnPredictor

    train_X = np.array([[1, 2], [3, 4]])
    train_y = np.array([0, 1])

    model = RandomForestClassifier().fit(train_X, train_y)
    predictor = SklearnPredictor(model=model)

    data = np.array([[1, 2], [3, 4]])
    predictions = predictor.predict(data)

    # Only use first and second column as the feature
    data = np.array([[1, 2, 8], [3, 4, 9]])
    predictions = predictor.predict(data, feature_columns=[0, 1])

import pandas as pd
    from sklearn.ensemble import RandomForestClassifier
    from ray.ml.predictors.sklearn import SklearnPredictor

    train_X = pd.DataFrame([[1, 2], [3, 4]], columns=["A", "B"])
    train_y = pd.Series([0, 1])

    model = RandomForestClassifier().fit(train_X, train_y)
    predictor = SklearnPredictor(model=model)

    # Pandas dataframe.
    data = pd.DataFrame([[1, 2], [3, 4]], columns=["A", "B"])
    predictions = predictor.predict(data)

    # Only use first and second column as the feature
    data = pd.DataFrame([[1, 2, 8], [3, 4, 9]], columns=["A", "B", "C"])
    predictions = predictor.predict(data, feature_columns=["A", "B"])""",
        """CODE.import numpy as np
    import lightgbm as lgbm
    from ray.ml.predictors.lightgbm import LightGBMPredictor

    train_X = np.array([[1, 2], [3, 4]])
    train_y = np.array([0, 1])

    model = lgbm.LGBMClassifier().fit(train_X, train_y)
    predictor = LightGBMPredictor(model=model.booster_)

    data = np.array([[1, 2], [3, 4]])
    predictions = predictor.predict(data)

    # Only use first and second column as the feature
    data = np.array([[1, 2, 8], [3, 4, 9]])
    predictions = predictor.predict(data, feature_columns=[0, 1])

import pandas as pd
    import lightgbm as lgbm
    from ray.ml.predictors.lightgbm import LightGBMPredictor

    train_X = pd.DataFrame([[1, 2], [3, 4]], columns=["A", "B"])
    train_y = pd.Series([0, 1])

    model = lgbm.LGBMClassifier().fit(train_X, train_y)
    predictor = LightGBMPredictor(model=model.booster_)

    # Pandas dataframe.
    data = pd.DataFrame([[1, 2], [3, 4]], columns=["A", "B"])
    predictions = predictor.predict(data)

    # Only use first and second column as the feature
    data = pd.DataFrame([[1, 2, 8], [3, 4, 9]], columns=["A", "B", "C"])
    predictions = predictor.predict(data, feature_columns=["A", "B"])""",
        """CODE.import numpy as np
    import xgboost as xgb
    from ray.ml.predictors.xgboost import XGBoostPredictor

    train_X = np.array([[1, 2], [3, 4]])
    train_y = np.array([0, 1])

    model = xgb.XGBClassifier().fit(train_X, train_y)
    predictor = XGBoostPredictor(model=model.get_booster())

    data = np.array([[1, 2], [3, 4]])
    predictions = predictor.predict(data)

    # Only use first and second column as the feature
    data = np.array([[1, 2, 8], [3, 4, 9]])
    predictions = predictor.predict(data, feature_columns=[0, 1])

import pandas as pd
    import xgboost as xgb
    from ray.ml.predictors.xgboost import XGBoostPredictor

    train_X = pd.DataFrame([[1, 2], [3, 4]], columns=["A", "B"])
    train_y = pd.Series([0, 1])

    model = xgb.XGBClassifier().fit(train_X, train_y)
    predictor = XGBoostPredictor(model=model.get_booster())

    # Pandas dataframe.
    data = pd.DataFrame([[1, 2], [3, 4]], columns=["A", "B"])
    predictions = predictor.predict(data)

    # Only use first and second column as the feature
    data = pd.DataFrame([[1, 2, 8], [3, 4, 9]], columns=["A", "B", "C"])
    predictions = predictor.predict(data, feature_columns=["A", "B"])""",
        """CODE.import pandas as pd
    from transformers import AutoConfig, AutoModelForCausalLM, AutoTokenizer
    from transformers.pipelines import pipeline
    from ray.ml.predictors.integrations.huggingface import HuggingFacePredictor

    model_checkpoint = "gpt2"
    tokenizer_checkpoint = "sgugger/gpt2-like-tokenizer"
    tokenizer = AutoTokenizer.from_pretrained(tokenizer_checkpoint)

    model_config = AutoConfig.from_pretrained(model_checkpoint)
    model = AutoModelForCausalLM.from_config(model_config)
    predictor = HuggingFacePredictor(
        pipeline=pipeline(
            task="text-generation", model=model, tokenizer=tokenizer
        )
    )

    prompts = pd.DataFrame(
        ["Complete me", "And me", "Please complete"], columns=["sentences"]
    )
    predictions = predictor.predict(prompts)""" .

<DEPENDENCY.ray==1.2.0> <CONTAINS> "CODE.ImportedBackend(\"module.Class\")",
        """CODE.MyClass = import_class("module.submodule.MyClass")
from module.submodule import MyClass""",
        """CODE.collector.add_init_obs(my_episode, 0, "pol0", -1, obs)
collector.add_action_reward_next_obs(12345, 0, "pol0", False, {
    "action": action, "obs": obs, "reward": r, "done": done
})""",
        """CODE.from ray.tune.integration.mlflow import MLflowLoggerCallback
tune.run(
    train_fn,
    config={
        # define search space here
        "parameter_1": tune.choice([1, 2, 3]),
        "parameter_2": tune.choice([4, 5, 6]),
    },
    callbacks=[MLflowLoggerCallback(
        experiment_name="experiment1",
        save_artifact=True)])""",
        """CODE.from ray.tune.integration.mlflow import mlflow_mixin

@mlflow_mixin
def train_fn(config):
    ...
    mlflow.log_metric(...)


from ray.tune.integration.mlflow import mlflow_mixin

@mlflow_mixin
def train_fn(config):
    mlflow.autolog()
    xgboost_results = xgb.train(config, ...)


from ray import tune
from ray.tune.integration.mlflow import mlflow_mixin

import mlflow

# Create the MlFlow expriment.
mlflow.create_experiment("my_experiment")

@mlflow_mixin
def train_fn(config):
    for i in range(10):
        loss = self.config["a"] + self.config["b"]
        mlflow.log_metric(key="loss", value=loss})
    tune.report(loss=loss, done=True)

tune.run(
    train_fn,
    config={
        # define search space here
        "a": tune.choice([1, 2, 3]),
        "b": tune.choice([4, 5, 6]),
        # mlflow configuration
        "mlflow": {
            "experiment_name": "my_experiment",
            "tracking_uri": mlflow.get_tracking_uri()
        }
    })
""",
        """CODE.from ray.tune.stopper import CombinedStopper, MaximumIterationStopper, TrialPlateauStopper

stopper = CombinedStopper(
    MaximumIterationStopper(max_iter=20),
    TrialPlateauStopper(metric="my_metric")
)

tune.run(train, stop=stopper)""",
        """CODE.import pytorch_lightning as ptl
from ray.util.lightning_accelerators import HorovodRayAccelerator

ptl_model = MNISTClassifier(...)
# 2 nodes, 4 workers per node, each using 1 CPU and 1 GPU.
accelerator = HorovodRayAccelerator(num_hosts=2, num_slots=4,
    use_gpu=True)

# If using GPUs, set the ``gpus`` arg to a value > 0.
# The actual number of GPUs is determined by ``num_slots``.
trainer = pl.Trainer(..., gpus=1, accelerator=accelerator)
trainer.fit(ptl_model)""",
        """CODE.old = {"a": 1, "b": 2}
new = {"a": 3, "d": 4}
compute_dict_delta(old, new)
""" .

<DEPENDENCY.ray==1.3.0> <CONTAINS> """CODE.@serve.batch(max_batch_size=50, batch_wait_timeout_s=0.5)
async def handle_batch(self, batch: List[str]):
    return [s.lower() for s in batch]""",
        """CODE.@serve.deployment("deployment1", version="v1")
class MyDeployment:
    pass

MyDeployment.deploy(*init_args)""",
        """CODE.app = FastAPI()
@serve.deployment
    @serve.ingress(app)
    class App:
        pass
App.deploy()""",
        """CODE.config = {
    "width": tune.uniform(0, 20),
    "height": tune.uniform(-100, 100)
}

hebo = HEBOSearch(metric="mean_loss", mode="min")
tune.run(my_func, config=config, search_alg=hebo)


from ray import tune
from ray.tune.suggest.hebo import HEBOSearch
from hebo.design_space.design_space import DesignSpace

space_config = [
    {'name' : 'width', 'type' : 'num', 'lb' : 0, 'ub' : 20},
    {'name' : 'height', 'type' : 'num', 'lb' : -100, 'ub' : 100},
]
space = DesignSpace().parse(space_config)

hebo = HEBOSearch(space, metric="mean_loss", mode="min")
tune.run(my_func, search_alg=hebo)
""",
        """CODE.def unflatten_list_dict(dt):
    result = {}
    for key, value in dt.items():
        keys = key.split('/')
        current = result
        for k in keys[:-1]:
            if k not in current:
                current[k] = {}
            current = current[k]
        current[keys[-1]] = value
    return result

dt = {"aaa/0/bb": 12, "aaa/1/cc": 56, "aaa/1/dd": 92}
print(unflatten_list_dict(dt))
""",
        """CODE.from ray import tune

analysis = tune.run(
    tune.durable("PPO"),
    config={"env": "CartPole-v0"},
    checkpoint_freq=1,
    sync_config=tune.SyncConfig(
        sync_to_driver=False,
        upload_dir="s3://your-s3-bucket/durable-ppo/",
    ))

tune.run(
    tune.durable(your_training_fn),
    # ...
)

tune.run(
    tune.durable(YourTrainableClass),
    # ...
)
""",
        """CODE.my_pkg = load_package("~/path/to/my_pkg.yaml")
my_pkg = ray.util.load_package(
  "https://raw.githubusercontent.com/user/repo/refspec"
  "/path/to/package/my_pkg.yaml")
print(my_pkg._runtime_env)
my_pkg.my_func.remote(1, 2)
actor = my_pkg.MyActor.remote(3, 4)
@ray.remote(runtime_env=my_pkg._runtime_env)
def f(): ...
""",
        """CODE.ray.get_runtime_context().job_id
ray.get_runtime_context().get()""",
        """CODE.serve.set_traffic("service-name", {
    "backend:v1": 0.5,
    "backend:v2": 0.5
})""",
        """CODE.with Archive("file.tar.gz") as archive:
    with archive.subdir("logfiles", root="/tmp/logs") as sd:
        # Will be added as `logfiles/nested/file.txt`
        sd.add("/tmp/logs/nested/file.txt")
""" .

<DEPENDENCY.ray==1.5.0> <CONTAINS> """CODE.output = DummyOutputDatasource()
ray.data.range(10).write_datasource(output)
assert output.num_ok == 1""" .

<DEPENDENCY.ray==1.5.2> <CONTAINS> """CODE.trainer = MyTrainer()
for _ in range(10):
    trainer.train()
trainer.export_policy_checkpoint("/tmp/export_dir")""",
        """CODE.trainer = MyTrainer()
for _ in range(10):
    trainer.train()
trainer.export_policy_model("/tmp/export_dir")""",
        """CODE.trainer = MyTrainer()
trainer.import_policy_model_from_h5("/tmp/weights.h5")
for _ in range(10):
    trainer.train()""" .

<DEPENDENCY.ray==1.6.0> <CONTAINS> """CODE.# Create an inference pipeline.
ds = ray.data.read_binary_files(dir)
pipe = ds.pipeline(parallelism=10).map(infer)
DatasetPipeline(num_stages=2, length=40)

# The higher the stage parallelism, the shorter the pipeline.
pipe = ds.pipeline(parallelism=20).map(infer)
DatasetPipeline(num_stages=2, length=20)

# Outputs can be incrementally read from the pipeline.
for item in pipe.iter_rows():
    print(item)
""",
        """CODE.@Deprecated
def func(x):
    return x""",
        """CODE.@workflow.step
def book_flight(origin: str, dest: str) -> Flight:
    return Flight(...)

@workflow.step
def book_hotel(location: str) -> Reservation:
    return Reservation(...)

@workflow.step
def finalize_trip(bookings: List[Any]) -> Trip:
    return Trip(...)

flight1 = book_flight.step("OAK", "SAN")
flight2 = book_flight.step("SAN", "OAK")
hotel = book_hotel.step("SAN")
trip = finalize_trip.step([flight1, flight2, hotel])
result = ray.get(trip.run_async())""",
        """CODE.@workflow.step
def book_flight(origin: str, dest: str) -> Flight:
    return Flight(...)

@workflow.step
def book_hotel(location: str) -> Reservation:
    return Reservation(...)

@workflow.step
def finalize_trip(bookings: List[Any]) -> Trip:
    return Trip(...)

flight1 = book_flight.step("OAK", "SAN")
flight2 = book_flight.step("SAN", "OAK")
hotel = book_hotel.step("SAN")
trip = finalize_trip.step([flight1, flight2, hotel])
result = trip.run()""",
        """CODE.Multi-GPU version of TrainOneStep.
This should be used with the .for_each() operator. A tuple of the input
and learner stats will be returned.

Examples:
    rollouts = ParallelRollouts(...)
    train_op = rollouts.for_each(MultiGPUTrainOneStep(workers, ...))
    print(next(train_op))  # This trains the policy on one batch.
    SampleBatch(...), {"learner_stats": ...}

Updates the STEPS_TRAINED_COUNTER counter and LEARNER_INFO field in the
local iterator context.""",
        """CODE.ds = ray.data.range_tensor(1000, shape=(3, 10))
ds.map_batches(lambda arr: arr ** 2).show()
""",
        """CODE.ds.random_shuffle()
ds.random_shuffle(seed=12345)""",
        """CODE.import pytorch_lightning as ptl
from ray.util.lightning_accelerators import HorovodRayAccelerator

ptl_model = MNISTClassifier(...)
# 2 nodes, 4 workers per node, each using 1 CPU and 1 GPU.
accelerator = HorovodRayAccelerator(num_hosts=2, num_slots=4,
    use_gpu=True)

# If using GPUs, set the ``gpus`` arg to a value > 0.
# The actual number of GPUs is determined by ``num_slots``.
trainer = pl.Trainer(..., gpus=1, accelerator=accelerator)
trainer.fit(ptl_model)""",
        """CODE.import time
from ray.util import sgd

def train_func():
    for iter in range(100):
        time.sleep(1)
        if sgd.world_rank() == 0:
            print("Worker 0")

trainer = Trainer(backend="torch")
trainer.start()
trainer.run(train_func)
trainer.shutdown()""",
        """CODE.jobs = workflow.list_all()
assert jobs == [("failed_job", workflow.FAILED)]
assert workflow.resume_all(include_failed=True).get("failed_job") is not None""",
        """CODE.ray.data.range(5).repeat().take()
ray.data.range(5).repeat().map(lambda x: -x).take()
ray.data.range(5).repeat().random_shuffle().take()""",
        """CODE.ray.data.read_numpy("s3://bucket/path")
ray.data.read_numpy(["/path/to/file1", "/path/to/file2"])
ray.data.read_numpy(["s3://bucket/path1", "s3://bucket/path2"])""",
        """CODE.ray.data.read_text("s3://bucket/path")
ray.data.read_text(["/path/to/file1", "/path/to/file2"])""",
        """CODE.serve.set_traffic("service-name", {
    "backend:v1": 0.5,
    "backend:v2": 0.5
})""",
        """CODE.source = BinaryDatasource()
ray.data.read_datasource(source, paths="/path/to/dir").take()""",
        "CODE.trial_resources = self.trial_resources",
        """CODE.trip = start_trip.step()
res1 = trip.async_run(workflow_id="trip1")
res2 = workflow.get_output("trip1")
assert ray.get(res1) == ray.get(res2)
""",
        """CODE.worker_group = WorkerGroup(num_workers=2)
output = worker_group.execute(lambda: 1)
assert len(output) == 2
assert all(o == 1 for o in output)
""",
        """CODE.workflow_step = trip.step()
output = workflow_step.run(workflow_id="trip")
assert workflow.SUCCESSFUL == workflow.get_status("trip")""" .

<DEPENDENCY.ray==1.7.0> <CONTAINS> """CODE.@classmethod
def construct_from_string(cls, string):
    pattern = re.compile(r"^my_type\\[(?P<arg_name>.+)\\]$")
    match = pattern.match(string)
    if match:
        return cls(**match.groupdict())
    else:
        raise TypeError(
            f"Cannot construct a '{cls.__name__}' from '{string}'"
        )""",
        """CODE.class Trainer:
    def __init__(self, config):
        self.config = config

    def train_epoch(self):
        ...
        return 1

config = {"lr": 0.1}
trainer = Trainer(num_workers=2, backend="torch")
workers = trainer.to_worker_group(train_cls=Trainer, config=config)
futures = [w.train_epoch.remote() for w in workers]
assert ray.get(futures) == [1, 1]
assert ray.get(workers[0].train_epoch.remote()) == 1
workers.shutdown()
""",
        """CODE.d1, d2, d3 = ray.data.range(10).split_at_indices([2, 5])
d1.take()
d2.take()
d3.take()""",
        """CODE.def right_zero_pad(self, max_len, exclude_states=False):
    self.zero_padded = True
    self.max_seq_len = max_len

    if self[SampleBatch.SEQ_LENS] is None:
        raise ValueError

    for key in self.keys():
        if key != SampleBatch.SEQ_LENS and (not exclude_states or "state_in" not in key):
            self[key] += [0] * (max_len - len(self[key]))

    return self
""",
        """CODE.def take(self, indices, allow_fill=False, fill_value=None):
    from pandas.core.algorithms import take

    data = self.astype(object)

    if allow_fill and fill_value is None:
        fill_value = self.dtype.na_value

    result = take(data, indices, fill_value=fill_value,
                  allow_fill=allow_fill)
    return self._from_sequence(result, dtype=self.dtype)
""",
        """CODE.def train_func(config):
    ...
    for _ in config["epochs"]:
        metrics = train()
        metrics = validate(...)
        ray.sgd.report(**metrics)
    return model

iterator = trainer.run_iterator(train_func, config=config)

for result in iterator:
    do_stuff(result)
    latest_ckpt = trainer.get_latest_checkpoint()

assert iterator.is_finished()
model = iterator.get_fin()[0]
""",
        """CODE.from ray.util import sgd

def train_func():
    checkpoint = sgd.load_checkpoint()
    for iter in range(checkpoint["epoch"], 5):
        print(iter)

trainer = Trainer(backend="torch")
trainer.start()
trainer.run(train_func, checkpoint={"epoch": 3})
# 3
# 4
trainer.shutdown()
""",
        """CODE.import time
from ray.util import sgd

def train_func():
    if torch.cuda.is_available():
        torch.cuda.set_device(sgd.local_rank())
    ...

trainer = Trainer(backend="torch", use_gpu=True)
trainer.start()
trainer.run(train_func)
trainer.shutdown()""" .

<DEPENDENCY.ray==1.9.0> <CONTAINS> """CODE.from ray.rllib.env.env_context import EnvContext

bad_env = FaultInjectEnv(
    EnvContext({"bad_indices": [1, 2]},
               worker_index=1, num_workers=3))
""",
        """CODE.grouped_ds.aggregate(AggregateFn(
...     init=lambda k: [],
...     accumulate=lambda a, r: a + [r],
...     merge=lambda a1, a2: a1 + a2,
...     finalize=lambda a: a
... ))""",
        """CODE.ray.data.range(100).groupby(lambda x: x % 3).count()
ray.data.from_items([
    {"A": x % 3, "B": x} for x in range(100)]).groupby(
    "A").count()""",
        """CODE.ray.data.range(100).groupby(lambda x: x % 3).max()
ray.data.from_items([
    (i % 3, i, i**2)
    for i in range(100)])
    .groupby(lambda x: x[0] % 3)
    .max(lambda x: x[2])
ray.data.range_arrow(100).groupby("value").max()
ray.data.from_items([
    {"A": i % 3, "B": i, "C": i**2}
    for i in range(100)])
    .groupby("A")
    .max(["B", "C"])
""",
        """CODE.ray.data.range(100).groupby(lambda x: x % 3).mean()
ray.data.from_items([
    (i % 3, i, i**2)
    for i in range(100)])
    .groupby(lambda x: x[0] % 3)
    .mean(lambda x: x[2])
ray.data.range_arrow(100).groupby("value").mean()
ray.data.from_items([
    {"A": i % 3, "B": i, "C": i**2}
    for i in range(100)])
    .groupby("A")
    .mean(["B", "C"])
""",
        """CODE.ray.data.range(100).groupby(lambda x: x % 3).min()
ray.data.from_items([
    (i % 3, i, i**2)
    for i in range(100)]).groupby(lambda x: x[0] % 3).min(lambda x: x[2)
ray.data.range_arrow(100).groupby("value").min()
ray.data.from_items([
    {"A": i % 3, "B": i, "C": i**2}
    for i in range(100)]).groupby("A").min(["B", "C"])
""",
        """CODE.ray.data.range(100).groupby(lambda x: x % 3).std()
ray.data.from_items([
    (i % 3, i, i**2)
    for i in range(100)]).groupby(lambda x: x[0] % 3).std(lambda x: x[2)
ray.data.range_arrow(100).groupby("value").std(ddof=0)
ray.data.from_items([
    {"A": i % 3, "B": i, "C": i**2}
    for i in range(100)]).groupby("A").std(["B", "C"])
""",
        """CODE.ray.data.range(100).groupby(lambda x: x % 3).sum()
ray.data.from_items([
    (i % 3, i, i**2)
    for i in range(100)]).groupby(lambda x: x[0] % 3).sum(lambda x: x[2)
ray.data.range_arrow(100).groupby("value").sum()
ray.data.from_items([
    {"A": i % 3, "B": i, "C": i**2}
    for i in range(100)]).groupby("A").sum(["B", "C"])
""",
        """CODE.subscriber = GcsAioSubscriber()
await subscriber.subscribe_error()
while running:
    error_id, error_data = await subscriber.poll_error()
......
await subscriber.close()""",
        """CODE.subscriber = GcsSubscriber()
subscriber.subscribe_error()
while running:
    error_id, error_data = subscriber.poll_error()
    ......
subscriber.close()""" .

<DEPENDENCY.ray==2.3.0> <CONTAINS> """CODE.# Create resource manager
resource_manager = ResourceManager()

# Create resource request
resource_request = ResourceRequest([{"CPU": 4}])

# Pass to resource manager
resource_manager.request_resources(resource_request)

# Wait until ready
while not resource_manager.has_resources_ready(resource_request):
    time.sleep(1)

# Once ready, acquire resources
acquired_resource = resource_manager.acquire_resources(resource_request)

# Bind to remote task or actor
annotated_remote_fn = acquired_resource.annotate_remote_entities(
    [remote_fn])

# Run remote function. This will use the acquired resources
ray.get(annotated_remote_fn.remote())

# After using the resources, free
resource_manager.free_resources(annotated_resources)
""",
        """CODE.class MapOperator(PhysicalOperator):
    def __init__(self):
        self.active_tasks = []

    def add_input(self, refs, _):
        self.active_tasks.append(map_task.remote(refs))

    def has_next(self):
        ready, _ = ray.wait(self.active_tasks, timeout=0)
        return len(ready) > 0

    def get_next(self):
        ready, remaining = ray.wait(self.active_tasks, num_returns=1)
        self.active_tasks = remaining
        return ready[0]""",
        """CODE.def to_tensor(batch: np.ndarray) -> torch.Tensor:
    tensor = torch.as_tensor(batch, dtype=torch.float)
    # (B, H, W, C) -> (B, C, H, W)
    tensor = tensor.permute(0, 3, 1, 2).contiguous()
    # [0., 255.] -> [0., 1.]
    tensor = tensor.div(255)
    return tensor
transform = transforms.Compose([
    transforms.Lambda(to_tensor),
    transforms.Resize((224, 224))
])
preprocessor = TorchVisionPreprocessor(
    ["image"], transform=transform, batched=True
)
preprocessor.transform(dataset)  # doctest: +ellipsis
Dataset(num_blocks=..., num_rows=..., schema={image: ArrowTensorType(shape=(3, 224, 224), dtype=float)})
""",
        """CODE.from ray.air.execution import ResourceRequest
from ray.air.execution._internal import EventType, RayEventManager

event_manager = RayEventManager()

# Request an actor
tracked_actor = event_manager.add_actor(
    ActorClass,
    kwargs={},
    resource_request=ResourceRequest([{"CPU": 1}])
)
tracked_actor.on_start(actor_start_callback)
tracked_actor.on_stop(actor_stop_callback)
tracked_actor.on_fail(actor_fail_callback)

# Yield control to event manager to start actor
event_manager.wait(timeout=1)

# Start task on the actor (ActorClass.foo.remote())
tracked_actor_task = event_manager.schedule_actor_task(
    tracked_actor,
    method_name="foo"
)
tracked_actor_task.on_result(task_result_callback)
tracked_actor_task.on_error(task_error_callback)

# Again yield control to event manager to process task futures
event_manager.wait(event_type=EventType.TASKS)
""",
        """CODE.from ray.air.integrations.keras import ReportCheckpointCallback
def train_loop_per_worker():
    strategy = tf.distribute.MultiWorkerMirroredStrategy()
    with strategy.scope():
        model = build_model()

    model.fit(dataset_shard, callbacks=[ReportCheckpointCallback()])""",
        """CODE.import ray
for batch in ray.data.range(
    1000000
).iterator().iter_batches():
    print(batch)
""",
        """CODE.logger = DatasetLogger(__name__)
logger.get_logger().info("This logs to file and stdout")
logger.get_logger(log_to_stdout=False).info("This logs to file only)
logger.get_logger().warning("Can call the usual Logger methods")
""" .

<DEPENDENCY.ray==2.3.1> <CONTAINS> """CODE.spec = ["foo", ("bar", "baz")]
output = _convert_to_canonical_format(spec)
# output = SpecDict({"foo": None, ("bar", "baz"): None})

spec = {"foo": int, "bar": {"baz": None}}
output = _convert_to_canonical_format(spec)
# output = SpecDict(
#   {"foo": TypeSpec(int), "bar": SpecDict({"baz": None})}
# )

spec = {"foo": int, "bar": {"baz": str}}
output = _convert_to_canonical_format(spec)
# output = SpecDict(
#   {"foo": TypeSpec(int), "bar": SpecDict({"baz": TypeSpec(str)})}
# )

spec = {"foo": int, "bar": {"baz": TorchTensorSpec("b,h")}}
output = _convert_to_canonical_format(spec)
# output = SpecDict(
#   {"foo": TypeSpec(int), "bar": SpecDict({"baz": TorchTensorSpec("b,h")})}
# )

spec = int
output = _convert_to_canonical_format(spec)
# output = TypeSpec(int)

spec = None
output = _convert_to_canonical_format(spec)
# output = None

spec = TorchTensorSpec("b,h")
output = _convert_to_canonical_format(spec)
# output = TorchTensorSpec("b,h")
""" .

<DEPENDENCY.ray==2.4.0> <CONTAINS> """CODE.spec = ["foo", ("bar", "baz")]
output = convert_to_canonical_format(spec)
# output = SpecDict({"foo": None, ("bar", "baz"): None})

spec = {"foo": int, "bar": {"baz": None}}
output = convert_to_canonical_format(spec)
# output = SpecDict(
#   {"foo": TypeSpec(int), "bar": SpecDict({"baz": None})}
# )

spec = {"foo": int, "bar": {"baz": str}}
output = convert_to_canonical_format(spec)
# output = SpecDict(
#   {"foo": TypeSpec(int), "bar": SpecDict({"baz": TypeSpec(str)})}
# )

spec = {"foo": int, "bar": {"baz": TorchTensorSpec("b,h")}}
output = convert_to_canonical_format(spec)
# output = SpecDict(
#   {"foo": TypeSpec(int), "bar": SpecDict({"baz": TorchTensorSpec("b,h")})}
# )


# Example of canoncial format #2:

spec = int
output = convert_to_canonical_format(spec)
# output = TypeSpec(int)

spec = None
output = convert_to_canonical_format(spec)
# output = None

spec = TorchTensorSpec("b,h")
output = convert_to_canonical_format(spec)
# output = TorchTensorSpec("b,h")
""" .

<DEPENDENCY.ray==2.5.0> <CONTAINS> """CODE.# Based on
# huggingface/notebooks/examples/language_modeling_from_scratch.ipynb

# Hugging Face imports
from datasets import load_dataset
import transformers
from transformers import AutoConfig, AutoModelForCausalLM, AutoTokenizer

import ray
from ray.train.huggingface import TransformersTrainer
from ray.air.config import ScalingConfig

# If using GPUs, set this to True.
use_gpu = False

model_checkpoint = "gpt2"
tokenizer_checkpoint = "sgugger/gpt2-like-tokenizer"
block_size = 128

datasets = load_dataset("wikitext", "wikitext-2-raw-v1")
tokenizer = AutoTokenizer.from_pretrained(tokenizer_checkpoint)

def tokenize_function(examples):
    return tokenizer(examples["text"])

tokenized_datasets = datasets.map(
    tokenize_function, batched=True, num_proc=1, remove_columns=["text"]
)

def group_texts(examples):
    # Concatenate all texts.
    concatenated_examples = {
        k: sum(examples[k], []) for k in examples.keys()
    }
    total_length = len(concatenated_examples[list(examples.keys())[0]])
    # We drop the small remainder, we could add padding if the model
    # supported it.
    # instead of this drop, you can customize this part to your needs.
    total_length = (total_length // block_size) * block_size
    # Split by chunks of max_len.
    result = {
        k: [
            t[i : i + block_size]
            for i in range(0, total_length, block_size)
        ]
        for k, t in concatenated_examples.items()
    }
    result["labels"] = result["input_ids"].copy()
    return result

lm_datasets = tokenized_datasets.map(
    group_texts,
    batched=True,
    batch_size=1000,
    num_proc=1,
)
ray_train_ds = ray.data.from_huggingface(lm_datasets["train"])
ray_evaluation_ds = ray.data.from_huggingface(
    lm_datasets["validation"]
)

def trainer_init_per_worker(train_dataset, eval_dataset, **config):
    model_config = AutoConfig.from_pretrained(model_checkpoint)
    model = AutoModelForCausalLM.from_config(model_config)
    args = transformers.TrainingArguments(
        output_dir=f"{model_checkpoint}-wikitext2",
        evaluation_strategy="epoch",
        save_strategy="epoch",
        logging_strategy="epoch",
        learning_rate=2e-5,
        weight_decay=0.01,
        no_cuda=(not use_gpu),
    )
    return transformers.Trainer(
        model=model,
        args=args,
        train_dataset=train_dataset,
        eval_dataset=eval_dataset,
    )

scaling_config = ScalingConfig(num_workers=3, use_gpu=use_gpu)
trainer = TransformersTrainer(
    trainer_init_per_worker=trainer_init_per_worker,
    scaling_config=scaling_config,
    datasets={"train": ray_train_ds, "evaluation": ray_evaluation_ds},
)
result = trainer.fit()
""",
        """CODE._join_path_or_uri(local_path, path_to_join)
_join_path_or_uri(uri, path_to_join)""",
        """CODE.config = RecurrentEncoderConfig(
    recurrent_layer_type="lstm",
    input_dims=[16],  # must be 1D tensor
    hidden_dim=128,
    num_layers=2,
    output_dims=[256],  # maybe None or a 1D tensor
    output_activation="linear",
    use_bias=True,
)
model = config.build(framework="torch")

config = RecurrentEncoderConfig(
    recurrent_layer_type="gru",
    input_dims=[32],  # must be 1D tensor
    hidden_dim=64,
    num_layers=1,
    output_dims=None,  # maybe None or a 1D tensor
    use_bias=False,
)
model = config.build(framework="torch")
""",
        """CODE.import pandas as pd
from transformers import AutoConfig, AutoModelForCausalLM, AutoTokenizer
from transformers.pipelines import pipeline
from ray.train.huggingface import TransformersPredictor

model_checkpoint = "gpt2"
tokenizer_checkpoint = "sgugger/gpt2-like-tokenizer"
tokenizer = AutoTokenizer.from_pretrained(tokenizer_checkpoint)

model_config = AutoConfig.from_pretrained(model_checkpoint)
model = AutoModelForCausalLM.from_config(model_config)
predictor = TransformersPredictor(
    pipeline=pipeline(
        task="text-generation", model=model, tokenizer=tokenizer
    )
)

prompts = pd.DataFrame(
    ["Complete me", "And me", "Please complete"], columns=["sentences"]
)
predictions = predictor.predict(prompts)""",
        """CODE.import ray
dataset = ray.data.range(10)
next(iter(dataset.iterator().iter_rows()))""",
        """CODE.import ray
ds = ray.data.range(5)
ds
Dataset(num_blocks=5, num_rows=5, schema={id: int64})
ds.iterator()
DataIterator(Dataset(num_blocks=5, num_rows=5, schema={id: int64}))""",
        """CODE.import ray
ds = ray.data.read_csv(
    "s3://anonymous@air-example-data/iris.csv"
)
it = ds.iterator(); it
DataIterator(Dataset(
   num_blocks=1,
   num_rows=150,
   schema={
      sepal length (cm): double,
      sepal width (cm): double,
      petal length (cm): double,
      petal width (cm): double,
      target: int64
   }
))

it.to_tf(feature_columns="sepal length (cm)", label_columns="target")  # doctest: +SKIP
<_OptionsDataset element_spec=(TensorSpec(shape=(None,), dtype=tf.float64, name='sepal length (cm)'), TensorSpec(shape=(None,), dtype=tf.int64, name='target'))>

it.to_tf(["sepal length (cm)", "sepal width (cm)"], "target")  # doctest: +SKIP
<_OptionsDataset element_spec=({'sepal length (cm)': TensorSpec(shape=(None,), dtype=tf.float64, name='sepal length (cm)'), 'sepal width (cm)': TensorSpec(shape=(None,), dtype=tf.float64, name='sepal width (cm)')}, TensorSpec(shape=(None,), dtype=tf.int64, name='target'))>

from ray.data.preprocessors import Concatenator
preprocessor = Concatenator(output_column_name="features", exclude="target")
it = preprocessor.transform(ds).iterator()
it
DataIterator(Concatenator
+- Dataset(
      num_blocks=1,
      num_rows=150,
      schema={
         sepal length (cm): double,
         sepal width (cm): double,
         petal length (cm): double,
         petal width (cm): double,
         target: int64
      }
   ))
it.to_tf("features", "target")  # doctest: +SKIP
<_OptionsDataset element_spec=(TensorSpec(shape=(None, 4), dtype=tf.float64, name='features'), TensorSpec(shape=(None,), dtype=tf.int64, name='target'))>
""",
        """CODE.import ray
for batch in ray.data.range(1000000).iterator().iter_batches():
    print(batch)
""",
        """CODE.import ray
import pandas as pd
import numpy as np

ds = ray.data.from_items([
    {"group": 1, "value": 1},
    {"group": 1, "value": 2},
    {"group": 2, "value": 3},
    {"group": 2, "value": 4}]
)

ds.groupby("group").map_groups(
    lambda g: {"result": np.array([g["value"][0]])}
)

df = pd.DataFrame(
    {"A": ["a", "a", "b"], "B": [1, 1, 3], "C": [4, 6, 5]}
)

ds = ray.data.from_pandas(df)

grouped = ds.groupby("A")

grouped.map_groups(
    lambda g: g.apply(
        lambda c: c / g[c.name].sum() if c.name in ["B", "C"] else c
    )
)""",
        """CODE.import ray
ray.data.from_items([
    {"A": x % 3, "B": x} for x in range(100)]).groupby(
    "A").count()""",
        """CODE.import ray
ray.data.le(100).groupby("value").max()
ray.data.from_items([
    {"A": i % 3, "B": i, "C": i**2}
    for i in range(100)]) \\
    .groupby("A") \\
    .max(["B", "C"])
""",
        """CODE.import ray
ray.data.le(100).groupby("value").min()
ray.data.from_items([
    {"A": i % 3, "B": i, "C": i**2}
    for i in range(100)]) \\
    .groupby("A") \\
    .min(["B", "C"])
""",
        """CODE.ray.data.le(100).groupby("value").mean()
ray.data.from_items([
    {"A": i % 3, "B": i, "C": i**2}
    for i in range(100)]) \\
    .groupby("A") \\
    .mean(["B", "C"])
""" .

<DEPENDENCY.ray==2.7.0> <CONTAINS> """CODE.Example:
uri = URI("s3://bucket/a/b/c/?param=1")
print(str(uri.rstrip_subpath(Path("b/c")))

uri = URI("/tmp/a/b/c/")
print(str(uri.rstrip_subpath(Path("/b/c/.//")))""",
        """CODE._insert_into_sorted_list(list, {"a": 1, "b": 0}, lambda x: x["a"])
_insert_into_sorted_list(list, {"a": 3, "b": 1}, lambda x: x["a"])
_insert_into_sorted_list(list, {"a": 4, "b": 2}, lambda x: x["a"])
_insert_into_sorted_list(list, {"a": 1, "b": 3}, lambda x: x["a"])""",
        """CODE.ds.write_images("local:///tmp/images", column="image")
ds.write_images("local:///tmp/images", column="image", file_format="jpeg")
ds.write_images("local:///tmp/images", column="image", file_format="png")
ds.write_images("local:///tmp/images", column="image", file_format="jpg")
ds.write_images("local:///tmp/images", column="image", file_format="bmp")""",
        """CODE.from ray.train._internal.storage import StorageContext
import os
os.environ["RAY_AIR_LOCAL_CACHE_DIR"] = "/tmp/ray_results"
storage = StorageContext(
    storage_path="mock://netloc/bucket/path?param=1",
    experiment_dir_name="exp_name",
)
storage.storage_filesystem   # Auto-resolved  # doctest: +ELLIPSIS
<pyarrow._fs._MockFileSystem object...
storage.experiment_fs_path
'bucket/path/exp_name'
storage.experiment_local_path
'/tmp/ray_results/exp_name'
storage.trial_dir_name = "trial_dir"
storage.trial_fs_path
'bucket/path/exp_name/trial_dir'
storage.trial_local_path
'/tmp/ray_results/exp_name/trial_dir'
storage.current_checkpoint_index = 1
storage.checkpoint_fs_path
'bucket/path/exp_name/trial_dir/checkpoint_000001'

from ray.train._internal.storage import StorageContext
import os
os.environ["RAY_AIR_LOCAL_CACHE_DIR"] = "/tmp/ray_results"
storage = StorageContext(
    storage_path=None,
    experiment_dir_name="exp_name",
)
storage.storage_path  # Auto-resolved
'/tmp/ray_results'
storage.storage_local_path
'/tmp/ray_results'
storage.experiment_local_path
'/tmp/ray_results/exp_name'
storage.experiment_fs_path
'/tmp/ray_results/exp_name'
storage.syncer is None
True
storage.storage_filesystem   # Auto-resolved  # doctest: +ELLIPSIS
<pyarrow._fs.LocalFileSystem object...

pyarrow.fs.copy_files(
    local_dir,
    os.path.join(storage.trial_fs_path, "subdir"),
    destination_filesystem=storage.filesystem
)
""",
        """CODE.from typing import Dict, List, Optional

from ray.tune import Callback
from ray.tune.experiment import Trial

class MyCallback(Callback):
    def __init__(self):
        self._trial_ids = set()

    def on_trial_start(
        self, iteration: int, trials: List["Trial"], trial: "Trial", **info
    ):
        self._trial_ids.add(trial.trial_id)

    def get_state(self) -> Optional[Dict]:
        return {"trial_ids": self._trial_ids.copy()}

    def set_state(self, state: Dict) -> Optional[Dict]:
        self._trial_ids = state["trial_ids"]
""",
        """CODE.import ray
from ray import serve

@serve.deployment
def f(val: int) -> int:
    return val * 2

serve.run(f.bind(), name="my_app")
handle = serve.get_app_handle("my_app")
assert handle.remote(3).result() == 6
""",
        """CODE.import ray
from ray import serve
from ray.serve.handle import DeploymentHandle, DeploymentResponse

@serve.deployment
class Downstream:
    def say_hi(self, message: str):
        return f"Hello {message}!"
        self._message = message

@serve.deployment
class Ingress:
    def __init__(self, handle: DeploymentHandle):
        self._downstream_handle = handle

    async def __call__(self, name: str) -> str:
        response = self._handle.say_hi.remote(name)
        return await response

app = Ingress.bind(Downstream.bind())
handle: DeploymentHandle = serve.run(app)
response = handle.remote("world")
assert response.result() == "Hello world!"
""",
        """CODE.import sqlite3
import ray

connection = sqlite3.connect("example.db")
connection.cursor().execute("CREATE TABLE movie(title, year, score)")
dataset = ray.data.from_items([
    {"title": "Monty Python and the Holy Grail", "year": 1975, "score": 8.2},
    {"title": "And Now for Something Completely Different", "year": 1971, "score": 7.5}
])

dataset.write_sql(
    "INSERT INTO movie VALUES(?, ?, ?)", lambda: sqlite3.connect("example.db")
)

result = connection.cursor().execute("SELECT * FROM movie ORDER BY year")
print(result.fetchall())""",
        """CODE.response: DeploymentResponse = handle.options(
    method_name="other_method",
    multiplexed_model_id="model:v1",
).remote()""",
        """CODE.scheduling_strategy=NodeLabelSchedulingStrategy({
      "region": In("us"),
      "gpu_type": Exists()
})""" .

<DEPENDENCY.ray==2.9.0> <CONTAINS> """CODE.        from ray import serve
        from ray.serve.schema import LoggingConfig
        # Set log level for the deployment.
        @serve.deployment(LoggingConfig(log_level="DEBUG")
        class MyDeployment:
            def __call__(self) -> str:
                return "Hello world!"
        # Set log directory for the deployment.
        @serve.deployment(LoggingConfig(logs_dir="/my_dir")
        class MyDeployment:
            def __call__(self) -> str:
                return "Hello world!\"""",
        """CODE.class CSVDatasink(BlockBasedFileDatasink):
    def __init__(self, path: str):
        super().__init__(path, file_format="csv")

    def write_block_to_file(self, block: BlockAccessor, file: "pyarrow.NativeFile"):
        from pyarrow import csv
        csv.write_csv(block.to_arrow(), file)""",
        """CODE.import ray
from ray.data.datasource import FilenameProvider

class ImageFilenameProvider(FilenameProvider):

    def __init__(self, file_format: str):
        self.file_format = file_format

    def get_filename_for_row(self, row, task_index, block_index, row_index):
        return (
            f"{row['label']}_{task_index:06}_{block_index:06}"
            f"_{row_index:06}.{self.file_format}"
        )

ds = ray.data.read_parquet("s3://anonymous@ray-example-data/images.parquet")
ds.write_images(
    "/tmp/results",
    column="image",
    filename_provider=ImageFilenameProvider("png")
)""" .

<DEPENDENCY.requests==0.10.1> <CONTAINS> """CODE.get_host('http://google.com/mail/')
get_host('google.com:80')""" .

<DEPENDENCY.requests==0.10.2> <CONTAINS> """CODE.d = MultiDict()
d.setlist('foo', ['1', '2'])
d['foo']
'1'
d.getlist('foo')
['1', '2']""",
        """CODE.d = MultiDict({"foo": [1, 2, 3]})
d.pop("foo")
"foo" in d""",
        """CODE.d = MultiDict({"foo": [1, 2, 3]})
zip(d.keys(), d.listvalues()) == d.lists()
True""" .

<DEPENDENCY.requests==0.12.1> <CONTAINS> """CODE.d = MultiDict()
d.setlist('foo', ['1', '2'])
d['foo']
'1'
d.getlist('foo')
['1', '2']""",
        """CODE.d = MultiDict({"foo": [1, 2, 3]})
zip(d.keys(), d.listvalues()) == d.lists()
True""",
        "CODE.real_value, coded_value = value_encode(VALUE)" .

<DEPENDENCY.requests==0.13.7> <CONTAINS> """CODE.to_key_val_list([('key', 'val')])
to_key_val_list({'key': 'val'})""" .

<DEPENDENCY.requests==0.14.1> <CONTAINS> """CODE.from collections import OrderedDict

def from_key_val_list(obj):
    if isinstance(obj, dict):
        return OrderedDict(obj)
    else:
        raise ValueError('need more than 1 value to unpack')

# äº¤äºå¼å½ä»¤è½¬æ¢æèæ¬ä¸­çä»£ç 
from_key_val_list([('key', 'val')])
from_key_val_list({'key': 'val'})""" .

<DEPENDENCY.requests==0.2.3> <CONTAINS> """CODE.c_auth = requests.AuthObject('kennethreitz', 'xxxxxxx')
requests.add_autoauth('https://convore.com/api/', c_auth)
r = requests.get('https://convore.com/api/account/verify.json')
""" .

<DEPENDENCY.requests==0.7.1> <CONTAINS> """CODE.d = parse_dict_header('foo="is a fish", bar="as well"')
type(d) is dict
sorted(d.items())

parse_dict_header('key_without_value')
""" .

<DEPENDENCY.requests==2.14.0> <CONTAINS> """CODE... code::

        u = UniversalDetector()
        u.feed(some_bytes)
        u.close()
        detected = u.result""" .

<DEPENDENCY.requests==2.15.1> <CONTAINS> """CODE.        u = UniversalDetector()
        u.feed(some_bytes)
        u.close()
        detected = u.result""",
        """CODE.headers = HTTPHeaderDict(foo='bar')
headers.add('Foo', 'baz')
headers['foo']""",
        """CODE.manager = PoolManager(num_pools=2)
r = manager.request('GET', 'http://google.com/')
r = manager.request('GET', 'http://google.com/mail')
r = manager.request('GET', 'http://yahoo.com/')
len(manager.pools)
""",
        """CODE.proxy = urllib3.ProxyManager('http://localhost:3128/')
r1 = proxy.request('GET', 'http://google.com/')
r2 = proxy.request('GET', 'http://httpbin.org/')
len(proxy.pools)
r3 = proxy.request('GET', 'https://httpbin.org/')
r4 = proxy.request('GET', 'https://twitter.com/')
len(proxy.pools)
""",
        """CODE.retries = Retry(connect=5, read=2, redirect=5)
http = PoolManager(retries=retries)
response = http.request('GET', 'http://example.com/')

response = http.request('GET', 'http://example.com/', retries=Retry(10))

response = http.request('GET', 'http://example.com/', retries=False)
""" .

<DEPENDENCY.requests==2.16.3> <CONTAINS> """CODE.        u = UniversalDetector()
        u.feed(some_bytes)
        u.close()
        detected = u.result""",
        """CODE.Url('http', 'username:password', 'host.com', 80,
    '/path', 'query', 'fragment').url""",
        """CODE.def parse_url(url):
    from urllib.parse import urlparse
    parsed_url = urlparse(url)
    return Url(scheme=parsed_url.scheme, host=parsed_url.hostname, port=parsed_url.port, path=parsed_url.path, query=parsed_url.query, ...)

parse_url('http://google.com/mail/')
parse_url('google.com:80')
parse_url('/foo?bar')
""",
        """CODE.from urllib3.util import ssl_
context = ssl_.create_urllib3_context()
context.options &= ~ssl_.OP_NO_SSLv3
""",
        """CODE.headers = HTTPHeaderDict(foo='bar')
headers.add('Foo', 'baz')
headers['foo']""",
        """CODE.manager = PoolManager(num_pools=2)
r = manager.request('GET', 'http://google.com/')
r = manager.request('GET', 'http://google.com/mail')
r = manager.request('GET', 'http://yahoo.com/')
len(manager.pools)
""",
        """CODE.parse_url('http://google.com/mail/')
parse_url('google.com:80')
parse_url('/foo?bar')""",
        """CODE.proxy = urllib3.ProxyManager('http://localhost:3128/')
r1 = proxy.request('GET', 'http://google.com/')
r2 = proxy.request('GET', 'http://httpbin.org/')
len(proxy.pools)
r3 = proxy.request('GET', 'https://httpbin.org/')
r4 = proxy.request('GET', 'https://twitter.com/')
len(proxy.pools)
""",
        """CODE.retries = Retry(connect=5, read=2, redirect=5)
http = PoolManager(retries=retries)
response = http.request('GET', 'http://example.com/')

response = http.request('GET', 'http://example.com/', retries=Retry(10))

response = http.request('GET', 'http://example.com/', retries=False)
""",
        """CODE.retries = Retry(connect=5, read=2, redirect=5)
http = PoolManager(retries=retries)
response = http.request('GET', 'http://example.com/')
response = http.request('GET', 'http://example.com/', retries=Retry(10))
response = http.request('GET', 'http://example.com/', retries=False)
""" .

<DEPENDENCY.requests==2.3.0> <CONTAINS> """CODE.headers = HTTPHeaderDict(foo='bar')
headers.add('Foo', 'baz')
headers['foo']""" .

<DEPENDENCY.requests==2.4.0> <CONTAINS> """CODE.retries = Retry(connect=5, read=2, redirect=5)
http = PoolManager(retries=retries)
response = http.request('GET', 'http://example.com/')

response = http.request('GET', 'http://example.com/', retries=Retry(10))

response = http.request('GET', 'http://example.com/', retries=False)
""" .

<DEPENDENCY.scikit-learn==0.10> <CONTAINS> """CODE.X = [[0], [1], [2], [3]]
y = [0, 0, 1, 1]
from sklearn.neighbors import NeighborsRegressor
neigh = NeighborsRegressor(n_neighbors=2)
neigh.fit(X, y)
NeighborsRegressor(algorithm='auto', classification_type='knn_vote',
          leaf_size=30, n_neighbors=2, radius=1.0)
print neigh.predict([[1.5]])
[ 0.5]
""",
        """CODE.from sklearn.neighbors import NeighborsClassifier
neigh = NeighborsClassifier(n_neighbors=2)
neigh.fit(X, y)""" .

<DEPENDENCY.scikit-learn==0.11> <CONTAINS> """CODE.from sklearn import datasets
from sklearn.semi_supervised import LabelPropagation
label_prop_model = LabelPropagation()
iris = datasets.load_iris()
random_unlabeled_points = np.where(np.random.random_integers(0, 1,
   size=len(iris.target)))
labels = np.copy(iris.target)
labels[random_unlabeled_points] = -1
label_prop_model.fit(iris.data, labels)
""",
        """CODE.from sklearn import datasets
from sklearn.semi_supervised import LabelSpreading
label_prop_model = LabelSpreading()
iris = datasets.load_iris()
random_unlabeled_points = np.where(np.random.random_integers(0, 1,
...    size=len(iris.target)))
labels = np.copy(iris.target)
labels[random_unlabeled_points] = -1
label_prop_model.fit(iris.data, labels)""",
        """CODE.from sklearn.cross_validation import StratifiedShuffleSplit
X = np.array([[1, 2], [3, 4], [1, 2], [3, 4]])
y = np.array([0, 0, 1, 1])
sss = StratifiedShuffleSplit(y, 3, test_size=0.5, random_state=0)
len(sss)
3
print sss       # doctest: +ELLIPSIS
StratifiedShuffleSplit(labels=[0 0 1 1], n_iterations=3, ...)
for train_index, test_index in sss:
...    print "TRAIN:", train_index, "TEST:", test_index
...    X_train, X_test = X[train_index], X[test_index]
...    y_train, y_test = y[train_index], y[test_index]
TRAIN: [0 3] TEST: [1 2]
TRAIN: [0 2] TEST: [1 3]
TRAIN: [1 2] TEST: [0 3]""",
        """CODE.from sklearn.ensemble import GradientBoostingClassifier
gb = GradientBoostingClassifier().fit(samples, labels)
print gb.predict([[0.5, 0, 0]])
""",
        """CODE.from sklearn.ensemble import GradientBoostingRegressor
samples = [[0, 0, 2], [1, 0, 0]]
labels = [0, 1]
gb = GradientBoostingRegressor().fit(samples, labels)
print gb.predict([[0, 0, 0]])    # doctest: +ELLIPSIS""",
        """CODE.from sklearn.feature_extraction import DictVectorizer
v = DictVectorizer(sparse=False)
D = [{'foo': 1, 'bar': 2}, {'foo': 3, 'baz': 1}]
X = v.fit_transform(D)
X
array([[ 2.,  0.,  1.],
       [ 0.,  1.,  3.])
v.inverse_transform(X) == [{'bar': 2.0, 'foo': 1.0}, {'baz': 1.0, 'foo': 3.0}]
True
v.transform({'foo': 4, 'unseen_feature': 3})
array([[ 0.,  0.,  4.]]))""",
        """CODE.from sklearn.neighbors.nearest_centroid import NearestCentroid
import numpy as np
X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2])
y = np.array([1, 1, 1, 2, 2, 2])
clf = NearestCentroid()
clf.fit(X, y)
NearestCentroid(metric='euclidean', shrink_threshold=None)
print clf.predict([[-0.8, -1]])
[1]""",
        """CODE.import numpy as np
from sklearn.cross_validation import train_test_split
a, b = np.arange(10).reshape((5, 2)), range(5)
a
array([[0, 1],
       [2, 3],
       [4, 5],
       [6, 7],
       [8, 9]])
b
[0, 1, 2, 3, 4]

a_train, a_test, b_train, b_test = train_test_split(
...     a, b, test_size=0.33, random_state=42)
...
a_train
array([[4, 5],
       [0, 1],
       [6, 7]])
b_train
array([2, 0, 3])
a_test
array([[2, 3],
       [8, 9]])
b_test
array([1, 4])""" .

<DEPENDENCY.scikit-learn==0.12> <CONTAINS> """CODE.from numpy import *
a = random.randn(9, 6)
a = np.dot(a, a.T)
B = pinvh(a)
allclose(a, dot(a, dot(B, a)))
allclose(B, dot(B, dot(a, B)))
""",
        """CODE.from sklearn import preprocessing
le = preprocessing.LabelEncoder()
le.fit([1, 2, 2, 6])
le.classes_
le.transform([1, 1, 2, 6])
le.inverse_transform([0, 0, 1, 2])

le = preprocessing.LabelEncoder()
le.fit(["paris", "paris", "tokyo", "amsterdam"])
list(le.classes_)
le.transform(["tokyo", "tokyo", "paris"])
list(le.inverse_transform([2, 2, 1]))""",
        """CODE.from sklearn.metrics.cluster import normalized_mutual_info_score
normalized_mutual_info_score([0, 0, 1, 1], [0, 0, 1, 1])
normalized_mutual_info_score([0, 0, 1, 1], [1, 1, 0, 0])
normalized_mutual_info_score([0, 0, 0, 0], [0, 1, 2, 3])""" .

<DEPENDENCY.scikit-learn==0.13> <CONTAINS> """CODE.cartesian(([1, 2, 3], [4, 5], [6, 7]))
array([[1, 4, 6],
       [1, 4, 7],
       [1, 5, 6],
       [1, 5, 7],
       [2, 4, 6],
       [2, 4, 7],
       [2, 5, 6],
       [2, 5, 7],
       [3, 4, 6],
       [3, 4, 7],
       [3, 5, 6],
       [3, 5, 7]])""",
        """CODE.from sklearn.ensemble import GradientBoostingClassifier
gb = GradientBoostingClassifier().fit(samples, labels)
kwargs = dict(X=samples, percentiles=(0, 1), grid_resolution=2)
partial_dependence(gb, [0], **kwargs)
(array([[-10.72892297,  10.72892297]]), [array([ 0.,  1.])])""",
        """CODE.from sklearn.metrics import accuracy_score
y_pred = [0, 2, 1, 3]
y_true = [0, 1, 2, 3]
accuracy_score(y_true, y_pred)
0.5""",
        """CODE.from sklearn.metrics import mean_absolute_error
y_true = [3, -0.5, 2, 7]
y_pred = [2.5, 0.0, 2, 8]
mean_absolute_error(y_true, y_pred)
y_true = [[0.5, 1], [-1, 1], [7, -6]]
y_pred = [[0, 2], [-1, 2], [8, -5]]
mean_absolute_error(y_true, y_pred)
""",
        """CODE.from sklearn.metrics import zero_one_loss
y_pred = [1, 2, 3, 4]
y_true = [2, 2, 3, 4]
zero_one_loss(y_true, y_pred)
zero_one_loss(y_true, y_pred, normalize=False)
""",
        """CODE.from sklearn.preprocessing import OneHotEncoder
enc = OneHotEncoder()
enc.fit([[0, 0, 3], [1, 1, 0], [0, 2, 1], [1, 0, 2]])
enc.n_values_
enc.feature_indices_
enc.transform([[0, 1, 1]]).toarray()""",
        """CODE.from sklearn.preprocessing import add_dummy_feature
add_dummy_feature([[0, 1], [1, 0])
array([[ 1.,  0.,  1.],
       [ 1.,  1.,  0.]])""" .

<DEPENDENCY.scikit-learn==0.13.1> <CONTAINS> """CODE.from scipy.sparse import cs_graph_components
import numpy as np
D = np.eye(4)
D[0,1] = D[1,0] = 1
cs_graph_components(D)

from scipy.sparse import dok_matrix
cs_graph_components(dok_matrix(D))""",
        """CODE.from scipy.sparse import cs_graph_components
import numpy as np
D = np.eye(4)
D[0,1] = D[1,0] = 1
cs_graph_components(D)
from scipy.sparse import dok_matrix
cs_graph_components(dok_matrix(D))""" .

<DEPENDENCY.scikit-learn==0.14> <CONTAINS> """CODE.BayesianRidge(alpha_1=1e-06, alpha_2=1e-06, compute_score=False,
        copy_X=True, fit_intercept=True, lambda_1=1e-06, lambda_2=1e-06,
        n_iter=300, normalize=False, tol=0.001, verbose=False)""",
        """CODE.Linear Model trained with L1 prior as regularizer (aka the Lasso)

The optimization objective for Lasso is::

    (1 / (2 * n_samples)) * ||y - Xw||^2_2 + alpha * ||w||_1

Technically the Lasso model is optimizing the same objective function as
the Elastic Net with ``l1_ratio=1.0`` (no L2 penalty).

Parameters
----------
alpha : float, optional
    Constant that multiplies the L1 term. Defaults to 1.0.
    ``alpha = 0`` is equivalent to an ordinary least square, solved
    by the :class:`LinearRegression` object. For numerical
    reasons, using ``alpha = 0`` is with the Lasso object is not advised
    and you should prefer the LinearRegression object.

fit_intercept : boolean
    whether to calculate the intercept for this model. If set
    to false, no intercept will be used in calculations
    (e.g. data is expected to be already centered).

normalize : boolean, optional, default False
    If ``True``, the regressors X will be normalized before regression.

copy_X : boolean, optional, default True
    If ``True``, X will be copied; else, it may be overwritten.

precompute : True | False | 'auto' | array-like
    Whether to use a precomputed Gram matrix to speed up
    calculations. If set to ``'auto'`` let us decide. The Gram
    matrix can also be passed as argument. For sparse input
    this option is always ``True`` to preserve sparsity.

max_iter: int, optional
    The maximum number of iterations

tol : float, optional
    The tolerance for the optimization: if the updates are
    smaller than ``tol``, the optimization code checks the
    dual gap for optimality and continues until it is smaller
    than ``tol``.

warm_start : bool, optional
    When set to True, reuse the solution of the previous call to fit as
    initialization, otherwise, just erase the previous solution.

positive : bool, optional
    When set to ``True``, forces the coefficients to be positive.

Attributes
----------
``coef_`` : array, shape = (n_features,) | (n_targets, n_features)
    parameter vector (w in the cost function formula)

``sparse_coef_`` : scipy.sparse matrix, shape = (n_features, 1) |             (n_targets, n_features)
    ``sparse_coef_`` is a readonly property derived from ``coef_``

``intercept_`` : float | array, shape = (n_targets,)
    independent term in decision function.

Examples
--------
from sklearn import linear_model
clf = linear_model.Lasso(alpha=0.1)
clf.fit([[0,0], [1, 1], [2, 2]], [0, 1, 2])
Lasso(alpha=0.1, copy_X=True, fit_intercept=True, max_iter=1000,
   normalize=False, positive=False, precompute='auto', tol=0.0001,
   warm_start=False)
print(clf.coef_)
[ 0.85  0.  ]
print(clf.intercept_)
0.15

See also
--------
lars_path
lasso_path
LassoLars
LassoCV
LassoLarsCV
sklearn.decomposition.sparse_encode

Notes
-----
The algorithm used to fit the model is coordinate descent.

To avoid unnecessary memory duplication the X argument of the fit method
should be directly passed as a Fortran-contiguous numpy array.""",
        """CODE.from sklearn import linear_model
clf = linear_model.MultiTaskLasso(alpha=0.1)
clf.fit([[0,0], [1, 1], [2, 2]], [[0, 0], [1, 1], [2, 2]])
print(clf.coef_)
print(clf.intercept_)
""",
        """CODE.from sklearn.grid_search import ParameterGrid
param_grid = {'a': [1, 2], 'b': [True, False]}
list(ParameterGrid(param_grid)) == (
   [{'a': 1, 'b': True}, {'a': 1, 'b': False},
    {'a': 2, 'b': True}, {'a': 2, 'b': False}])

grid = [{'kernel': ['linear']}, {'kernel': ['rbf'], 'gamma': [1, 10]}]
list(ParameterGrid(grid)) == [{'kernel': 'linear'},
                              {'kernel': 'rbf', 'gamma': 1},
                              {'kernel': 'rbf', 'gamma': 10}]""",
        """CODE.from sklearn.grid_search import ParameterSampler
from scipy.stats.distributions import expon
import numpy as np
np.random.seed(0)
param_grid = {'a':[1, 2], 'b': expon()}
param_list = list(ParameterSampler(param_grid, n_iter=4))
rounded_list = [dict((k, round(v, 6)) for (k, v) in d.items())
                for d in param_list]
rounded_list == [{'b': 0.89856, 'a': 1},
                 {'b': 0.923223, 'a': 1},
                 {'b': 1.878964, 'a': 2},
                 {'b': 1.038159, 'a': 2}]""",
        """CODE.from sklearn.lda import LDA
clf = LDA()
clf.fit(X, y)
print(clf.predict([[-0.8, -1]]))""",
        """CODE.from sklearn.metrics import fbeta_score, make_scorer
ftwo_scorer = make_scorer(fbeta_score, beta=2)
from sklearn.grid_search import GridSearchCV
from sklearn.svm import LinearSVC
grid = GridSearchCV(LinearSVC(), param_grid={'C': [1, 10]},
                    scoring=ftwo_scorer)""",
        """CODE.from sklearn.metrics import hamming_loss
y_pred = [1, 2, 3, 4]
y_true = [2, 2, 3, 4]
hamming_loss(y_true, y_pred)

hamming_loss(np.array([[0.0, 1.0], [1.0, 1.0]]), np.zeros((2, 2)))

hamming_loss([(1, 2), (3, )], [(1, 2), tuple()])
""",
        """CODE.from sklearn.metrics.metrics import _tp_tn_fp_fn
y_pred = [0, 1, 0, 0]
y_true = [0, 1, 0, 1]
_tp_tn_fp_fn(y_true, y_pred)
(array([2, 1]), array([1, 2]), array([1, 0]), array([0, 1]))

y_true = np.array([0, 1, 2, 0, 1, 2])
y_pred = np.array([0, 2, 1, 0, 0, 1])
_tp_tn_fp_fn(y_true, y_pred)
(array([2, 0, 0]), array([3, 2, 3]), array([1, 2, 1]), array([0, 2, 2]))

_tp_tn_fp_fn(np.array([[0.0, 1.0], [1.0, 1.0]]), np.zeros((2, 2)))
(array([0, 0]), array([1, 0]), array([0, 0]), array([1, 2]))

_tp_tn_fp_fn([(1, 2), (3, )], [(1, 2), tuple()])
(array([1, 1, 0]), array([1, 1, 1]), array([0, 0, 0]), array([0, 0, 1])""",
        """CODE.from sklearn.preprocessing import label_binarize
label_binarize([1, 6], classes=[1, 2, 4, 6])
label_binarize([1, 6], classes=[1, 6, 4, 2])
label_binarize([(1, 2), (6,), ()], multilabel=True, classes=[1, 6, 4, 2])
""",
        """CODE.import numpy as np
from sklearn.metrics import jaccard_similarity_score
y_pred = [0, 2, 1, 3]
y_true = [0, 1, 2, 3]
jaccard_similarity_score(y_true, y_pred)
0.5
jaccard_similarity_score(y_true, y_pred, normalize=False)
2
jaccard_similarity_score(np.array([[0.0, 1.0], [1.0, 1.0]]), np.ones((2, 2)))
0.75
jaccard_similarity_score([(1, ), (3, )], [(1, 2), tuple()])
0.25""",
        """CODE.import numpy as np
from sklearn.metrics import roc_auc_score
y_true = np.array([0, 0, 1, 1])
y_scores = np.array([0.1, 0.4, 0.35, 0.8])
roc_auc_score(y_true, y_scores)
0.75""",
        """CODE.import numpy as np
from sklearn.neural_network import BernoulliRBM
X = np.array([[0, 0, 0], [0, 1, 1], [1, 0, 1], [1, 1, 1]])
model = BernoulliRBM(n_components=2)
model.fit(X)
""",
        """CODE.import numpy as np
from sklearn.utils.multiclass import is_label_indicator_matrix
is_label_indicator_matrix([0, 1, 0, 1])
is_label_indicator_matrix([[1], [0, 2], []])
is_label_indicator_matrix(np.array([[1, 0], [0, 0]]))
is_label_indicator_matrix(np.array([[1], [0], [0]]))
is_label_indicator_matrix(np.array([[1, 0, 0]]))""",
        """CODE.import numpy as np
from sklearn.utils.multiclass import is_multilabel
is_multilabel([0, 1, 0, 1])
is_multilabel([[1], [0, 2], []])
is_multilabel(np.array([[1, 0], [0, 0]]))
is_multilabel(np.array([[1], [0], [0]]))
is_multilabel(np.array([[1, 0, 0]]))
""",
        """CODE.import numpy as np
from sklearn.utils.multiclass import is_multilabel
is_sequence_of_sequences([0, 1, 0, 1])
is_sequence_of_sequences([[1], [0, 2], []])
is_sequence_of_sequences(np.array([[1], [0, 2], []], dtype=object))
is_sequence_of_sequences([(1,), (0, 2), ()])
is_sequence_of_sequences(np.array([[1, 0], [0, 0]]))
is_sequence_of_sequences(np.array([[1], [0], [0]]))
is_sequence_of_sequences(np.array([[1, 0, 0]]))""" .

<DEPENDENCY.scikit-learn==0.14.1> <CONTAINS> """CODE.from sklearn.metrics import zero_one
y_pred = [1, 2, 3, 4]
y_true = [2, 2, 3, 4]
zero_one(y_true, y_pred)
zero_one(y_true, y_pred, normalize=True)
""",
        """CODE.from sklearn.metrics.metrics import _tp_tn_fp_fn
y_pred = [0, 1, 0, 0]
y_true = [0, 1, 0, 1]
_tp_tn_fp_fn(y_true, y_pred)
(array([2, 1]), array([1, 2]), array([1, 0]), array([0, 1]))

y_true = np.array([0, 1, 2, 0, 1, 2])
y_pred = np.array([0, 2, 1, 0, 0, 1])
_tp_tn_fp_fn(y_true, y_pred)
(array([2, 0, 0]), array([3, 2, 3]), array([1, 2, 1]), array([0, 2, 2]))

_tp_tn_fp_fn(np.array([[0.0, 1.0], [1.0, 1.0]]), np.zeros((2, 2)))
(array([0, 0]), array([1, 0]), array([0, 0]), array([1, 2]))

_tp_tn_fp_fn([(1, 2), (3, )], [(1, 2), tuple()])
(array([1, 1, 0]), array([1, 1, 1]), array([0, 0, 0]), array([0, 0, 1])""" .

<DEPENDENCY.scikit-learn==0.15.0> <CONTAINS> """CODE.X = [[0, 2, 0, 3], [0, 1, 4, 3], [0, 1, 1, 3]]
selector = VarianceThreshold()
selector.fit_transform(X)
""",
        """CODE.from sklearn.metrics.pairwise import paired_distances
X = [[0, 1], [1, 1]]
Y = [[0, 1], [2, 1]]
paired_distances(X, Y)
""",
        """CODE.from sklearn.preprocessing import PolynomialFeatures
import numpy as np

X = np.arange(6).reshape(3, 2)
poly = PolynomialFeatures(2)
poly.fit_transform(X)

poly = PolynomialFeatures(interaction_only=True)
poly.fit_transform(X)
""",
        """CODE.from sklearn.utils import gen_batches
list(gen_batches(7, 3))
list(gen_batches(6, 3))
list(gen_batches(2, 3))""",
        """CODE.import numpy as np
from sklearn.manifold import TSNE
X = np.array([[0, 0, 0], [0, 1, 1], [1, 0, 1], [1, 1, 1]])
model = TSNE(n_components=2, random_state=0)
model.fit_transform(X) # doctest: +ELLIPSIS, +NORMALIZE_WHITESPACE
array([[  887.28...,   238.61...],
       [ -714.79...,  3243.34...],
       [  957.30..., -2505.78...],
       [-1130.28...,  -974.78...])""",
        """CODE.make_union(PCA(), TruncatedSVD())
FeatureUnion(n_jobs=1,
             transformer_list=[('pca', PCA(copy=True, n_components=None,
                                           whiten=False)),
                               ('truncatedsvd',
                                TruncatedSVD(algorithm='randomized',
                                             n_components=2, n_iter=5,
                                             random_state=None, tol=0.0))],
             transformer_weights=None)""",
        """CODE.mlb = MultiLabelBinarizer()
mlb.fit_transform([(1, 2), (3,)])
array([[1, 1, 0],
       [0, 0, 1]])
mlb.classes_
array([1, 2, 3])

mlb.fit_transform([set(['sci-fi', 'thriller']), set(['comedy'])])
array([[0, 1, 1],
       [1, 0, 0]])
list(mlb.classes_)
['comedy', 'sci-fi', 'thriller']""" .

<DEPENDENCY.scikit-learn==0.15.2> <CONTAINS> """CODE.import numpy as np
from sklearn.metrics import auc_score
y_true = np.array([0, 0, 1, 1])
y_scores = np.array([0.1, 0.4, 0.35, 0.8])
auc_score(y_true, y_scores)
0.75""" .

<DEPENDENCY.scikit-learn==0.16.0> <CONTAINS> """CODE._linkcode_resolve('py', {'module': 'tty',
...                          'fullname': 'setraw'},
...                   package='tty',
...                   url_fmt='http://hg.python.org/cpython/file/'
...                           '{revision}/Lib/{package}/{path}#L{lineno}',
...                   revision='xxxx')
'http://hg.python.org/cpython/file/xxxx/Lib/tty/tty.py#L18'""",
        """CODE._shape_repr((1, 2))
_shape_repr((one, 2 * one))
_shape_repr((1,))
_shape_repr(())""",
        """CODE.class MetaEst(object):
    def __init__(self, sub_est):
        self.sub_est = sub_est

    @if_delegate_has_method(delegate='sub_est')
    def predict(self, X):
        return self.sub_est.predict(X)

class HasPredict(object):
    def predict(self, X):
        return X.sum(axis=1)

class HasNoPredict(object):
    pass
""",
        """CODE.from sklearn.cross_validation import PredefinedSplit
X = np.array([[1, 2], [3, 4], [1, 2], [3, 4]])
y = np.array([0, 0, 1, 1])
ps = PredefinedSplit(test_fold=[0, 1, -1, 1])
len(ps)
2
print(ps)       # doctest: +NORMALIZE_WHITESPACE +ELLIPSIS
sklearn.cross_validation.PredefinedSplit(test_fold=[ 0  1 -1  1])
for train_index, test_index in ps:
...    print("TRAIN:", train_index, "TEST:", test_index)
...    X_train, X_test = X[train_index], X[test_index]
...    y_train, y_test = y[train_index], y[test_index]
TRAIN: [1 2 3] TEST: [0]
TRAIN: [0 2] TEST: [1 3]""",
        """CODE.from sklearn.kernel_ridge import KernelRidge
import numpy as np
n_samples, n_features = 10, 5
rng = np.random.RandomState(0)
y = rng.randn(n_samples)
X = rng.randn(n_samples, n_features)
clf = KernelRidge(alpha=1.0)
clf.fit(X, y)
""",
        """CODE.from sklearn.metrics import median_absolute_error
y_true = [3, -0.5, 2, 7]
y_pred = [2.5, 0.0, 2, 8]
median_absolute_error(y_true, y_pred)""",
        """CODE.from sklearn.neighbors import LSHForest

X_train = [[5, 5, 2], [21, 5, 5], [1, 1, 1], [8, 9, 1], [6, 10, 2]]
X_test = [[9, 1, 6], [3, 1, 10], [7, 10, 3]]
lshf = LSHForest()
lshf.fit(X_train)
distances, indices = lshf.kneighbors(X_test, n_neighbors=2)
distances
indices
""",
        """CODE.from sklearn.svm import SVC
has_fit_parameter(SVC(), "sample_weight")
True""",
        """CODE.from sklearn.utils import _get_n_jobs
_get_n_jobs(4)
jobs = _get_n_jobs(-2)
assert jobs == max(cpu_count() - 1, 1)
_get_n_jobs(0)
""",
        """CODE.import numpy as np
from sklearn.decomposition import IncrementalPCA
X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])
ipca = IncrementalPCA(n_components=2, batch_size=3)
ipca.fit(X)
ipca.transform(X) # doctest: +SKIP""",
        """CODE.import numpy as np
from sklearn.metrics import brier_score_loss
y_true = np.array([0, 1, 1, 0])
y_true_categorical = np.array(["spam", "ham", "ham", "spam"])
y_prob = np.array([0.1, 0.9, 0.8, 0.3])
brier_score_loss(y_true, y_prob)  # doctest: +ELLIPSIS
0.037...
brier_score_loss(y_true, 1-y_prob, pos_label=0)  # doctest: +ELLIPSIS
0.037...
brier_score_loss(y_true_categorical, y_prob, pos_label="ham")  # doctest: +ELLIPSIS
0.037...
brier_score_loss(y_true, np.array(y_prob) > 0.5)
0.0""",
        """CODE.import numpy as np
from sklearn.metrics import label_ranking_average_precision_score
y_true = np.array([[1, 0, 0], [0, 0, 1]])
y_score = np.array([[0.75, 0.5, 1], [1, 0.2, 0.1]])
label_ranking_average_precision_score(y_true, y_score)         # doctest: +ELLIPSIS""" .

<DEPENDENCY.scikit-learn==0.16.1> <CONTAINS> """CODE.from sklearn import cross_validation
bs = cross_validation.Bootstrap(9, random_state=0)
for train_index, test_index in bs:
    print("TRAIN:", train_index, "TEST:", test_index)""",
        """CODE.from sklearn.qda import QDA
import numpy as np
X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2])
y = np.array([1, 1, 1, 2, 2, 2])
clf = QDA()
clf.fit(X, y)
print(clf.predict([[-0.8, -1]]))
""",
        """CODE.import numpy as np
from sklearn.utils.multiclass import is_label_indicator_matrix
is_label_indicator_matrix([0, 1, 0, 1])
is_label_indicator_matrix([[1], [0, 2], []])
is_label_indicator_matrix(np.array([[1, 0], [0, 0]]))
is_label_indicator_matrix(np.array([[1], [0], [0]]))
is_label_indicator_matrix(np.array([[1, 0, 0]]))""" .

<DEPENDENCY.scikit-learn==0.17> <CONTAINS> """CODE.clf = LinearDiscriminantAnalysis()
clf.fit(X, y)
LinearDiscriminantAnalysis(n_components=None, priors=None, shrinkage=None,
              solver='svd', store_covariance=False, tol=0.0001)
print(clf.predict([[-0.8, -1]]))""",
        """CODE.from sklearn.cross_validation import LabelKFold
import numpy as np

X = np.array([[1, 2], [3, 4], [5, 6], [7, 8])
y = np.array([1, 2, 3, 4])
labels = np.array([0, 0, 2, 2])
label_kfold = LabelKFold(labels, n_folds=2)

for train_index, test_index in label_kfold:
    X_train, X_test = X[train_index], X[test_index]
    y_train, y_test = y[train_index], y[test_index]
    print(X_train, X_test, y_train, y_test)""",
        """CODE.from sklearn.datasets import load_breast_cancer
data = load_breast_cancer()
data.target[[10, 50, 85]]
list(data.target_names)""",
        """CODE.from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis
import numpy as np
X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2])
y = np.array([1, 1, 1, 2, 2, 2])
clf = QuadraticDiscriminantAnalysis()
clf.fit(X, y)
print(clf.predict([[-0.8, -1]])""",
        """CODE.import numpy as np
from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import GaussianNB
from sklearn.ensemble import RandomForestClassifier
clf1 = LogisticRegression(random_state=1)
clf2 = RandomForestClassifier(random_state=1)
clf3 = GaussianNB()
X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])
y = np.array([1, 1, 1, 2, 2, 2])
eclf1 = VotingClassifier(estimators=[
...         ('lr', clf1), ('rf', clf2), ('gnb', clf3)], voting='hard')
eclf1 = eclf1.fit(X, y)
print(eclf1.predict(X))
[1 1 1 2 2 2]
eclf2 = VotingClassifier(estimators=[
...         ('lr', clf1), ('rf', clf2), ('gnb', clf3)],
...         voting='soft')
eclf2 = eclf2.fit(X, y)
print(eclf2.predict(X))
[1 1 1 2 2 2]
eclf3 = VotingClassifier(estimators=[
...        ('lr', clf1), ('rf', clf2), ('gnb', clf3)],
...        voting='soft', weights=[2,1,1])
eclf3 = eclf3.fit(X, y)
print(eclf3.predict(X))
[1 1 1 2 2 2]""" .

<DEPENDENCY.scikit-learn==0.18> <CONTAINS> """CODE.from sklearn.metrics.cluster import fowlkes_mallows_score
fowlkes_mallows_score([0, 0, 1, 1], [0, 0, 1, 1])
fowlkes_mallows_score([0, 0, 1, 1], [1, 1, 0, 0])
fowlkes_mallows_score([0, 0, 0, 0], [0, 1, 2, 3])""",
        """CODE.from sklearn.model_selection import GridSearchCV
from sklearn.svm import LinearSVC
from sklearn.exceptions import FitFailedWarning
import warnings
warnings.simplefilter('always', FitFailedWarning)
gs = GridSearchCV(LinearSVC(), {'C': [-1, -2]}, error_score=0)
X, y = [[1, 2], [3, 4], [5, 6], [7, 8], [8, 9]], [0, 0, 0, 1, 1]
with warnings.catch_warnings(record=True) as w:
    try:
        gs.fit(X, y)   # This will raise a ValueError since C is < 0
    except ValueError:
        pass
    print(repr(w[-1].message))
""",
        """CODE.from sklearn.model_selection import GroupKFold
import numpy as np

X = np.array([[1, 2], [3, 4], [5, 6], [7, 8])
y = np.array([1, 2, 3, 4])
groups = np.array([0, 0, 2, 2])
group_kfold = GroupKFold(n_splits=2)

for train_index, test_index in group_kfold.split(X, y, groups):
    X_train, X_test = X[train_index], X[test_index]
    y_train, y_test = y[train_index], y[test_index]

TRAIN: [0 1] TEST: [2 3]
[[1 2]
 [3 4]] [[5 6]
 [7 8]] [1 2] [3 4]
TRAIN: [2 3] TEST: [0 1]
[[5 6]
 [7 8]] [[1 2]
 [3 4]] [3 4] [1 2]
""",
        """CODE.from sklearn.model_selection import LeaveOneGroupOut
X = np.array([[1, 2], [3, 4], [5, 6], [7, 8])
y = np.array([1, 2, 1, 2])
groups = np.array([1, 1, 2, 2])
lol = LeaveOneGroupOut()
lol.get_n_splits(X, y, groups)
2
for train_index, test_index in lol.split(X, y, groups):
...    X_train, X_test = X[train_index], X[test_index]
...    y_train, y_test = y[train_index], y[test_index]
...    print(X_train, X_test, y_train, y_test)""",
        """CODE.from sklearn.model_selection import LeavePGroupsOut
X = np.array([[1, 2], [3, 4], [5, 6])
y = np.array([1, 2, 1])
groups = np.array([1, 2, 3])
lpl = LeavePGroupsOut(n_groups=2)
lpl.get_n_splits(X, y, groups)
3
for train_index, test_index in lpl.split(X, y, groups):
...    print("TRAIN:", train_index, "TEST:", test_index)
...    X_train, X_test = X[train_index], X[test_index]
...    y_train, y_test = y[train_index], y[test_index]
...    print(X_train, X_test, y_train, y_test)
TRAIN: [2] TEST: [0 1]
[[5 6]] [[1 2]
 [3 4]] [1] [1 2]
TRAIN: [1] TEST: [0 2]
[[3 4]] [[1 2]
 [5 6]] [2] [1 1]
TRAIN: [0] TEST: [1 2]
[[1 2]] [[3 4]
 [5 6]] [1] [2 1]""",
        """CODE.from sklearn.model_selection import TimeSeriesSplit
X = np.array([[1, 2], [3, 4], [1, 2], [3, 4]])
y = np.array([1, 2, 3, 4])
tscv = TimeSeriesSplit(n_splits=3)
for train_index, test_index in tscv.split(X):
...    X_train, X_test = X[train_index], X[test_index]
...    y_train, y_test = y[train_index], y[test_index]""" .

<DEPENDENCY.scikit-learn==0.18.2> <CONTAINS> """CODE.import numpy as np
a = np.random.randn(9, 6)
a = np.dot(a, a.T)
B = pinvh(a)
np.allclose(a, np.dot(a, np.dot(B, a)))
np.allclose(B, np.dot(B, np.dot(a, B)))
""" .

<DEPENDENCY.scikit-learn==0.19.0> <CONTAINS> """CODE.clf1 = LogisticRegression()
clf2 = RandomForestClassifier()
eclf = VotingClassifier(estimators=[('lr', clf1), ('rf', clf2)]
eclf.set_params(rf=None)""",
        """CODE.from sklearn.datasets import load_wine
data = load_wine()
data.target[[10, 80, 140]]
list(data.target_names)""",
        """CODE.from sklearn.model_selection import RepeatedKFold
X = np.array([[1, 2], [3, 4], [1, 2], [3, 4]])
y = np.array([0, 0, 1, 1])
rkf = RepeatedKFold(n_splits=2, n_repeats=2, random_state=2652124)
for train_index, test_index in rkf.split(X):
...     print("TRAIN:", train_index, "TEST:", test_index)
...     X_train, X_test = X[train_index], X[test_index]
...     y_train, y_test = y[train_index], y[test_index]""",
        """CODE.from sklearn.model_selection import RepeatedStratifiedKFold
X = np.array([[1, 2], [3, 4], [1, 2], [3, 4]])
y = np.array([0, 0, 1, 1])
rskf = RepeatedStratifiedKFold(n_splits=2, n_repeats=2,
...     random_state=36851234)
for train_index, test_index in rskf.split(X, y):
...     print("TRAIN:", train_index, "TEST:", test_index)
...     X_train, X_test = X[train_index], X[test_index]
...     y_train, y_test = y[train_index], y[test_index]""",
        """CODE.import sklearn
from sklearn.utils.validation import assert_all_finite
with sklearn.config_context(assume_finite=True):
    assert_all_finite([float('nan')])
with sklearn.config_context(assume_finite=True):
    with sklearn.config_context(assume_finite=False):
        assert_all_finite([float('nan')])
""",
        """CODE.y_true = [1, 0, 2]
y_score = [[0.15, 0.55, 0.2], [0.7, 0.2, 0.1], [0.06, 0.04, 0.9]]
ndcg_score(y_true, y_score, k=2)
1.0
y_score = [[0.9, 0.5, 0.8], [0.7, 0.2, 0.1], [0.06, 0.04, 0.9]]
ndcg_score(y_true, y_score, k=2)
0.66666666666666663""" .

<DEPENDENCY.scikit-learn==0.19.2> <CONTAINS> """CODE.from sklearn import cross_validation
import numpy as np

X = np.array([[1, 2], [3, 4], [5, 6])
y = np.array([1, 2, 1])
labels = np.array([1, 2, 3])
lpl = cross_validation.LeavePLabelOut(labels, p=2)

for train_index, test_index in lpl:
    X_train, X_test = X[train_index], X[test_index]
    y_train, y_test = y[train_index], y[test_index]
    print(X_train, X_test, y_train, y_test)

sklearn.cross_validation.LeavePLabelOut(labels=[1 2 3], p=2)
""",
        """CODE.from sklearn import cross_validation
import numpy as np

X = np.array([[1, 2], [3, 4], [5, 6], [7, 8])
y = np.array([1, 2, 1, 2])
labels = np.array([1, 1, 2, 2])
lol = cross_validation.LeaveOneLabelOut(labels)

for train_index, test_index in lol:
    X_train, X_test = X[train_index], X[test_index]
    y_train, y_test = y[train_index], y[test_index]
    print(X_train, X_test, y_train, y_test)""",
        """CODE.from sklearn.cross_validation import LabelKFold
X = np.array([[1, 2], [3, 4], [5, 6], [7, 8]])
y = np.array([1, 2, 3, 4])
labels = np.array([0, 0, 2, 2])
label_kfold = LabelKFold(labels, n_folds=2)
len(label_kfold)
2
print(label_kfold)
sklearn.cross_validation.LabelKFold(n_labels=4, n_folds=2)
for train_index, test_index in label_kfold:
...     print("TRAIN:", train_index, "TEST:", test_index)
...     X_train, X_test = X[train_index], X[test_index]
...     y_train, y_test = y[train_index], y[test_index]
...     print(X_train, X_test, y_train, y_test)""",
        """CODE.from sklearn.utils import _get_n_jobs
_get_n_jobs(4)
jobs = _get_n_jobs(-2)
assert jobs == max(cpu_count() - 1, 1)
_get_n_jobs(0)
""" .

<DEPENDENCY.scikit-learn==0.20.0> <CONTAINS> """CODE.    attr_expr = Group(label + Suppress(':') + OneOrMore(data_word).setParseAction(' '.join))
    attr_expr = Group(label + Suppress(':') + OneOrMore(data_word, stopOn=label).setParseAction(' '.join))
    (attr_expr * (1,)).parseString(text).pprint()""",
        """CODE.@traceParseAction
def remove_duplicate_chars(tokens):
    return ''.join(sorted(set(''.join(tokens)))

wds = OneOrMore(wd).setParseAction(remove_duplicate_chars)
print(wds.parseString("slkdjs sld sldd sdlf sdljf"))""",
        """CODE.Example::
    punc = oneOf(list(".,;:/-!?"))
    print(list(punc.split("This, this?, this sentence, is badly punctuated!"))
prints::
    ['This', ' this', '', ' this sentence', ' is badly punctuated', '']""",
        """CODE.Example::
    pyparsing_common.number.runTests('''
        # any int or real number, returned as the appropriate type
        100
        -100
        +100
        3.14159
        6.02e23
        1e-12
        ''')

    pyparsing_common.fnumber.runTests('''
        # any int or real number, returned as float
        100
        -100
        +100
        3.14159
        6.02e23
        1e-12
        ''')

    pyparsing_common.hex_integer.runTests('''
        # hex numbers
        100
        FF
        ''')

    pyparsing_common.fraction.runTests('''
        # fractions
        1/2
        -3/4
        ''')

    pyparsing_common.mixed_integer.runTests('''
        # mixed fractions
        1
        1/2
        -3/4
        1-3/4
        ''')

    import uuid
    pyparsing_common.uuid.setParseAction(tokenMap(uuid.UUID))
    pyparsing_common.uuid.runTests('''
        # uuid
        12345678-1234-5678-1234-567812345678
        ''')""",
        """CODE.Keyword("start").parseString("start")  # -> ['start']
Keyword("start").parseString("starting")  # -> Exception
""",
        """CODE.Literal('blah').parseString('blah')  # -> ['blah']
Literal('blah').parseString('blahfooblah')  # -> ['blah']
Literal('blah').parseString('bla')  # -> Exception: Expected "blah\"""",
        "CODE.OneOrMore(CaselessKeyword(\"CMD\")).parseString(\"cmd CMD Cmd10\") # -> ['CMD', 'CMD']",
        "CODE.OneOrMore(CaselessLiteral(\"CMD\")).parseString(\"cmd CMD Cmd10\") # -> ['CMD', 'CMD', 'CMD']",
        """CODE.OneOrMore(Word(alphas)).parseString("abc def\\nghi jkl")  # -> ['abc', 'def', 'ghi', 'jkl']
ParserElement.setDefaultWhitespaceChars(" \\t")
OneOrMore(Word(alphas)).parseString("abc def\\nghi jkl")  # -> ['abc', 'def']""",
        """CODE.X = np.random.RandomState(0).rand(5, 3)
D_chunk = next(pairwise_distances_chunked(X))
D_chunk  # doctest: +ELLIPSIS
array([[0.  ..., 0.29..., 0.41..., 0.19..., 0.57...],
       [0.29..., 0.  ..., 0.57..., 0.41..., 0.76...],
       [0.41..., 0.57..., 0.  ..., 0.44..., 0.90...],
       [0.19..., 0.41..., 0.44..., 0.  ..., 0.51...],
       [0.57..., 0.76..., 0.90..., 0.51..., 0.  ...]])

r = .2
def reduce_func(D_chunk, start):
...     neigh = [np.flatnonzero(d < r) for d in D_chunk]
...     avg_dist = (D_chunk * (D_chunk < r)).mean(axis=1)
...     return neigh, avg_dist
gen = pairwise_distances_chunked(X, reduce_func=reduce_func)
neigh, avg_dist = next(gen)
neigh
[array([0, 3]), array([1]), array([2]), array([0, 3]), array([4])]
avg_dist  # doctest: +ELLIPSIS
array([0.039..., 0.        , 0.        , 0.039..., 0.        ])

r = [.2, .4, .4, .3, .1]
def reduce_func(D_chunk, start):
...     neigh = [np.flatnonzero(d < r[i])
...              for i, d in enumerate(D_chunk, start)]
...     return neigh
neigh = next(pairwise_distances_chunked(X, reduce_func=reduce_func))
neigh
[array([0, 3]), array([0, 1]), array([2]), array([0, 3]), array([4])]

gen = pairwise_distances_chunked(X, reduce_func=reduce_func,
...                                  working_memory=0)
next(gen)
[array([0, 3])]
next(gen)
[array([0, 1])]
""",
        "CODE.[['qs = QuotedString(\\'\"\\')\\nprint(qs.searchString(\\'lsjdf \"This is the quote\" sldjf\\'))\\ncomplex_qs = QuotedString(\\'{{\\', endQuoteChar=\\'}}\\')\\nprint(complex_qs.searchString(\\'lsjdf {{This is the \"quote\"}} sldjf\\'))\\nsql_qs = QuotedString(\\'\"\\', escQuote=\\'\"\"\\')\\nprint(sql_qs.searchString(\\'lsjdf \"This is the quote with \"\"embedded\"\" quotes\" sldjf\\'))']]",
        """CODE.a,a_end = makeHTMLTags("A")
link_expr = a + SkipTo(a_end)("link_text") + a_end

for link in link_expr.searchString(text):
    print(link.link_text, '->', link.href)""",
        """CODE.attr_expr = (label + Suppress(':') + OneOrMore(data_word, stopOn=label).setParseAction(' '.join))
print(OneOrMore(attr_expr).parseString(text).dump())

attr_label = label
attr_value = Suppress(':') + OneOrMore(data_word, stopOn=label).setParseAction(' '.join)

result = dictOf(attr_label, attr_value).parseString(text)
print(result.dump())
print(result['shape'])
print(result.shape)
print(result.asDict())""",
        """CODE.comp_oper = oneOf("< = > <= >= !=")
var = Word(alphas)
number = Word(nums)
term = var | number
comparison_expr = term + comp_oper + term
print(comparison_expr.searchString("B = 12  AA=23 B<=AA AA>12"))""",
        """CODE.countedArray(Word(alphas)).parseString('2 ab cd ef')
binaryConstant = Word('01').setParseAction(lambda t: int(t[0], 2))
countedArray(Word(alphas), intExpr=binaryConstant).parseString('10 ab cd ef')""",
        """CODE.csv_value = CharsNotIn(',')
print(delimitedList(csv_value).parseString("dkls,lsdkjf,s12 34,@!#,213"))""",
        """CODE.data_word = Word(alphas)
label = data_word + FollowedBy(':')
attr_expr = Group(label + Suppress(':') + OneOrMore(data_word).setParseAction(' '.join))

text = "shape: SQUARE posn: upper left color: light blue texture: burlap"
attr_expr = (label + Suppress(':') + OneOrMore(data_word, stopOn=label).setParseAction(' '.join))

print(OneOrMore(attr_expr).parseString(text).dump())

result = Dict(OneOrMore(Group(attr_expr))).parseString(text)
print(result.dump())

print(result['shape'])
print(result.asDict())""",
        """CODE.data_word = Word(alphas)
label = data_word + FollowedBy(':')
attr_expr = Group(label + Suppress(':') + OneOrMore(data_word, stopOn=label).setParseAction(' '.join))

OneOrMore(attr_expr).parseString("shape: SQUARE color: BLACK posn: upper left").pprint()""",
        """CODE.date_expr = pyparsing_common.iso8601_date.copy()
date_expr.setParseAction(pyparsing_common.convertToDate())
print(date_expr.parseString("1999-12-31"))""",
        """CODE.def _run_search(self, evaluate_candidates):
    'Try C=0.1 only if C=1 is better than C=10'
    all_results = evaluate_candidates([{'C': 1}, {'C': 10}])
    score = all_results['mean_test_score']
    if score[0] < score[1]:
        evaluate_candidates([{'C': 0.1}])""",
        """CODE.def make_palindrome(tokens):
    tokens.extend(reversed([t[::-1] for t in tokens]))
    return ''.join(tokens)

patt = OneOrMore(Word(alphas))
print(patt.addParseAction(make_palindrome).parseString("lskdj sdlkjf lksd")) # -> 'lskdjsdlkjflksddsklfjkldsjdksl'""",
        """CODE.def remove_first(tokens):
    tokens.pop(0)
print(OneOrMore(Word(nums)).parseString("0 123 321")) # -> ['0', '123', '321']
print(OneOrMore(Word(nums)).addParseAction(remove_first).parseString("0 123 321")) # -> ['123', '321']

label = Word(alphas)
patt = label("LABEL") + OneOrMore(Word(nums))
print(patt.parseString("AAB 123 321").dump())

# Use pop() in a parse action to remove named result (note that corresponding value is not
# removed from list form of results)
def remove_LABEL(tokens):
    tokens.pop("LABEL")
    return tokens
patt.addParseAction(remove_LABEL)
print(patt.parseString("AAB 123 321").dump())""",
        """CODE.div,div_end = makeHTMLTags("div")

# only match div tag having a type attribute with value "grid"
div_grid = div().setParseAction(withAttribute(type="grid"))
grid_expr = div_grid + SkipTo(div | div_end)("body")
for grid_header in grid_expr.searchString(html):
    print(grid_header.body)

# construct a match with any div tag having a type attribute, regardless of the value
div_any_type = div().setParseAction(withAttribute(type=withAttribute.ANY_VALUE))
div_expr = div_any_type + SkipTo(div | div_end)("body")
for div_header in div_expr.searchString(html):
    print(div_header.body)""",
        """CODE.div,div_end = makeHTMLTags("div")
div_grid = div().setParseAction(withClass("grid"))

grid_expr = div_grid + SkipTo(div | div_end)("body")
for grid_header in grid_expr.searchString(html):
    print(grid_header.body)

div_any_type = div().setParseAction(withClass(withAttribute.ANY_VALUE))
div_expr = div_any_type + SkipTo(div | div_end)("body")
for div_header in div_expr.searchString(html):
    print(div_header.body)""",
        """CODE.dt_expr = pyparsing_common.iso8601_datetime.copy()
dt_expr.setParseAction(pyparsing_common.convertToDatetime())""",
        """CODE.expr = Word(nums)
assert expr.matches("100")""",
        """CODE.first = Word(nums)
second = matchPreviousExpr(first)
matchExpr = first + ":" + second""",
        """CODE.from scipy.misc import bytescale
img = np.array([[ 91.06794177,   3.39058326,  84.4221549 ],
...                 [ 73.88003259,  80.91433048,   4.88878881],
...                 [ 51.53875334,  34.45808177,  27.5873488 ]])
bytescale(img)
array([[255,   0, 236],
       [205, 225,   4],
       [140,  90,  70]], dtype=uint8)
bytescale(img, high=200, low=100)
array([[200, 100, 192],
       [180, 188, 102],
       [155, 135, 128]], dtype=uint8)
bytescale(img, cmin=0, cmax=255)
array([[91,  3, 84],
       [74, 81,  5],
       [52, 34, 28]], dtype=uint8)""",
        """CODE.from scipy.misc import imsave
x = np.zeros((255, 255), dtype=np.uint8)
x[:] = np.arange(255)
imsave('gradient.png', x)

rgb = np.zeros((255, 255, 3), dtype=np.uint8)
rgb[..., 0] = np.arange(255)
rgb[..., 1] = 55
rgb[..., 2] = 1 - np.arange(255)
imsave('rgb_gradient.png', rgb)""",
        """CODE.from sklearn.metrics import balanced_accuracy_score
y_true = [0, 1, 0, 0, 1, 0]
y_pred = [0, 1, 0, 0, 0, 1]
balanced_accuracy_score(y_true, y_pred)""",
        """CODE.from sklearn.preprocessing import OrdinalEncoder
enc = OrdinalEncoder()
X = [['Male', 1], ['Female', 3], ['Female', 2]]
enc.fit(X)
enc.categories_
enc.transform([['Female', 3], ['Male', 1]])
enc.inverse_transform([[1, 0], [0, 1]])""",
        """CODE.hex_ints = OneOrMore(Word(hexnums)).setParseAction(tokenMap(int, 16))
upperword = Word(alphas).setParseAction(tokenMap(str.upper))
wd = Word(alphas).setParseAction(tokenMap(str.title)).setParseAction(' '.join)""",
        """CODE.ident = Word(alphas)
num = Word(nums)
term = ident | num
func = ident + Optional(delimitedList(term))
print(func.parseString("fn a,b,100"))  # -> ['fn', 'a', 'b', '100']

func = ident + Group(Optional(delimitedList(term)))
print(func.parseString("fn a,b,100"))  # -> ['fn', ['a', 'b', '100']]""",
        """CODE.ident = Word(alphas, alphanums)
num = Word(nums)
func = Forward()
term = ident | num | Group('(' + func + ')')
func <<= ident + Group(Optional(delimitedList(term)))
result = func.parseString("fna a,b,(fnb c,d,200),100")
result.pprint(width=40)""",
        """CODE.import numpy as np
from sklearn.impute import SimpleImputer
imp_mean = SimpleImputer(missing_values=np.nan, strategy='mean')
imp_mean.fit([[7, 2, 3], [4, np.nan, 6], [10, 5, 9])
X = [[np.nan, 2, 3], [4, np.nan, 6], [10, np.nan, 9]]
print(imp_mean.transform(X))""",
        """CODE.import numpy as np
from sklearn.preprocessing import power_transform
data = [[1, 2], [3, 2], [4, 5]]
print(power_transform(data))
""",
        """CODE.import pyparsing
pyparsing.ParserElement.enablePackrat()""",
        """CODE.indicator = MissingIndicator()
indicator.fit(X1)
X2_tr = indicator.transform(X2)
X2_tr
array([[False,  True],
       [ True, False],
       [False, False]])""",
        """CODE.integer = Word(nums)
SEP = Suppress('|')
string_data = SkipTo(SEP, ignore=quotedString)
string_data.setParseAction(tokenMap(str.strip))
ticket_expr = (integer("issue_num") + SEP
              + string_data("sev") + SEP
              + string_data("desc") + SEP
              + integer("days_open"))

for tkt in ticket_expr.searchString(report):
    print tkt.dump()""",
        """CODE.integer = Word(nums)
date_str = integer + '/' + integer + '/' + integer

date_str.parseString("1999/12/31")  # -> ['1999', '/', '12', '/', '31']

# use parse action to convert to ints at parse time
integer = Word(nums).setParseAction(lambda toks: int(toks[0]))
date_str = integer + '/' + integer + '/' + integer

# note that integer fields are now ints, not strings
date_str.parseString("1999/12/31")  # -> [1999, '/', 12, '/', 31]""",
        """CODE.integer = Word(nums)
date_str = integer("year") + '/' + integer("month") + '/' + integer("day")

result = date_str.parseString('12/31/1999')
print(result.dump())""",
        """CODE.integer = Word(nums)
date_str = integer("year") + '/' + integer("month") + '/' + integer("day")
result = date_str.parseString('12/31/1999')
result_dict = result.asDict()""",
        """CODE.integer = Word(nums)
date_str = integer("year") + '/' + integer("month") + '/' + integer("day")

result = date_str.parseString("1999/12/31")
print(result.get("year")) # -> '1999'
print(result.get("hour", "not specified")) # -> 'not specified'
print(result.get("hour")) # -> None""",
        """CODE.integer = Word(nums)
date_str = integer("year") + '/' + integer("month") + '/' + integer("day")
date_str.parseString("1999/12/31")

ParserElement.inlineLiteralsUsing(Suppress)
date_str = integer("year") + '/' + integer("month") + '/' + integer("day")
date_str.parseString("1999/12/31")""",
        """CODE.integer = Word(nums)
name_expr = OneOrMore(Word(alphas))
expr = And([integer("id"),name_expr("name"),integer("age")])
expr = integer("id") + name_expr("name") + integer("age")""",
        """CODE.integer = Word(nums)
ssn_expr = Regex(r"\\d\\d\\d-\\d\\d-\\d\\d\\d\\d")
house_number_expr = Suppress('#') + Word(nums, alphanums)
user_data = (Group(house_number_expr)("house_number")
            | Group(ssn_expr)("ssn")
            | Group(integer)("age"))
user_info = OneOrMore(user_data)

result = user_info.parseString("22 111-22-3333 #221B")
for item in result:
    print(item.getName(), ':', item[0])""",
        """CODE.integer = Word(nums).setParseAction(lambda toks: int(toks[0]))
integerK = integer.copy().addParseAction(lambda toks: toks[0]*1024) + Suppress("K")
integerM = integer.copy().addParseAction(lambda toks: toks[0]*1024*1024) + Suppress("M")
print(OneOrMore(integerK | integerM | integer).parseString("5K 100 640K 256M"))
integerM = integer().addParseAction(lambda toks: toks[0]*1024*1024) + Suppress("M")""",
        """CODE.integer = Word(nums).setParseAction(lambda toks: int(toks[0]))
year_int = integer.copy()
year_int.addCondition(lambda toks: toks[0] >= 2000, message="Only support years 2000 and later")
date_str = year_int + '/' + integer + '/' + integer""",
        """CODE.integer = pyparsing_common.signed_integer
varname = pyparsing_common.identifier

arith_expr = infixNotation(integer | varname,
    [
    ('-', 1, opAssoc.RIGHT),
    (oneOf('* /'), 2, opAssoc.LEFT),
    (oneOf('+ -'), 2, opAssoc.LEFT),
    ])
""",
        """CODE.integer.setResultsName("year")
integer.setResultsName("month")
integer.setResultsName("day")
integer("year")
integer("month")
integer("day")""",
        """CODE.is_scalar_nan(np.nan)
True
is_scalar_nan(float("nan"))
True
is_scalar_nan(None)
False
is_scalar_nan("")
False
is_scalar_nan([np.nan])
False""",
        """CODE.make_column_transformer(
    (['numerical_column'], StandardScaler()),
    (['categorical_column'], OneHotEncoder())
)""",
        """CODE.num = Word(nums).setParseAction(lambda toks: int(toks[0])
na = oneOf("N/A NA").setParseAction(replaceWith(math.nan)
term = na | num
OneOrMore(term).parseString("324 234 N/A 234") # -> [324, 234, nan, 234]""",
        """CODE.patt = CloseMatch("ATCATCGAATGGA")
patt.parseString("ATCATCGAAXGGA") # -> (['ATCATCGAAXGGA'], {'mismatches': [[9]], 'original': ['ATCATCGAATGGA']})
patt.parseString("ATCAXCGAAXGGA") # -> Exception: Expected 'ATCATCGAATGGA' (with up to 1 mismatches) (at char 0), (line:1, col:1)

# exact match
patt.parseString("ATCATCGAATGGA") # -> (['ATCATCGAATGGA'], {'mismatches': [[]], 'original': ['ATCATCGAATGGA']})

# close match allowing up to 2 mismatches
patt = CloseMatch("ATCATCGAATGGA", maxMismatches=2)
patt.parseString("ATCAXCGAAXGGA") # -> (['ATCAXCGAAXGGA'], {'mismatches': [[4, 9]], 'original': ['ATCATCGAATGGA']})""",
        """CODE.patt = OneOrMore(Word(alphas))
patt.parseString('ablaj /* comment */ lskjd') # -> ['ablaj']

patt.ignore(cStyleComment)
patt.parseString('ablaj /* comment */ lskjd') # -> ['ablaj', 'lskjd']""",
        """CODE.patt = OneOrMore(Word(alphas))
result = patt.parseString("sldkj lsdkj sldkj")
print(type(result), result)
result_list = result.asList()
print(type(result_list), result_list)""",
        """CODE.print(OneOrMore(Word(nums)).parseString("0 123 321")) # -> ['0', '123', '321']

# use a parse action to insert the parse location in the front of the parsed results
def insert_locn(locn, tokens):
    tokens.insert(0, locn)
print(OneOrMore(Word(nums)).addParseAction(insert_locn).parseString("0 123 321")) # -> [0, '0', '123', '321']""",
        """CODE.print(OneOrMore(Word(nums)).parseString("0 123 321")) # -> ['0', '123', '321']

# use a parse action to compute the sum of the parsed integers, and add it to the end
def append_sum(tokens):
    tokens.append(sum(map(int, tokens)))
print(OneOrMore(Word(nums)).addParseAction(append_sum).parseString("0 123 321")) # -> ['0', '123', '321', 444]""",
        """CODE.quotedString.setParseAction(removeQuotes)
quotedString.parseString("'Now is the Winter of our Discontent'")
quotedString.setParseAction(removeQuotes)
quotedString.parseString("'Now is the Winter of our Discontent'")""",
        """CODE.real = Word(nums) + '.' + Word(nums)
print(real.parseString('3.1416'))
real = Combine(Word(nums) + '.' + Word(nums))
print(real.parseString('3.1416'))""",
        """CODE.realnum = Regex(r"[+-]?\\d+\\.\\d*")
date = Regex(r'(?P<year>\\d{4})-(?P<month>\\d\\d?)-(?P<day>\\d\\d?)')
roman = Regex(r"M{0,4}(CM|CD|D?C{0,3})(XC|XL|L?X{0,3})(IX|IV|V?I{0,3})")""",
        """CODE.src = "this is test <b> bold <i>text</i> </b> normal text "
for tag in ("b","i"):
    opener,closer = makeHTMLTags(tag)
    patt = originalTextFor(opener + SkipTo(closer) + closer)
    print(patt.searchString(src)[0])
""",
        """CODE.td,td_end = makeHTMLTags("TD")
table_text = td + SkipTo(td_end).setParseAction(pyparsing_common.stripHTMLTags)("body") + td_end

print(table_text.parseString(text).body) # -> 'More info at the pyparsing wiki page'""",
        """CODE.test = '''        AAA this line
    AAA and this line
      AAA but not this one
    B AAA and definitely not this one
    '''

for t in (LineStart() + 'AAA' + restOfLine).searchString(test):
    print(t)""",
        """CODE.try:
    Word(nums).setName("integer").parseString("ABC")
except ParseException as pe:
    print(pe)
    print("column: {}".format(pe.col))""",
        """CODE.wd = Word(alphas)
for match in locatedExpr(wd).searchString("ljsdf123lksdjjf123lkkjj1222"):
    print(match)""",
        """CODE.wd = Word(alphas)
wd.setParseAction(lambda toks: toks[0].title())

print(wd.transformString("now is the winter of our discontent made glorious summer by this sun of york."))""",
        """CODE.wd = Word(alphas).setName("alphaword")
integer = Word(nums).setName("numword")
term = wd | integer

# turn on debugging for wd
wd.setDebug()

OneOrMore(term).parseString("abc 123 xyz 890")""",
        """CODE.wd_list1 = wd + ZeroOrMore(',' + wd)
print(wd_list1.parseString(source))

wd_list2 = wd + ZeroOrMore(Suppress(',') + wd)
print(wd_list2.parseString(source))""",
        """CODE.zip = Combine(Word(nums, exact=5) + Optional('-' + Word(nums, exact=4))
zip.runTests('''
    # traditional ZIP code
    12345

    # ZIP+4 form
    12101-0001

    # invalid ZIP
    98765-
    ''')""" .

<DEPENDENCY.scikit-learn==0.21.0> <CONTAINS> """CODE.clf1 = LogisticRegression()
clf2 = RandomForestClassifier()
eclf = VotingClassifier(estimators=[('lr', clf1), ('rf', clf2)]
eclf.set_params(rf=None)""",
        """CODE.import numpy as np
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor
from sklearn.ensemble import VotingRegressor

r1 = LinearRegression()
r2 = RandomForestRegressor(n_estimators=10, random_state=1)
X = np.array([[1, 1], [2, 4], [3, 9], [4, 16], [5, 25], [6, 36]])
y = np.array([2, 6, 12, 20, 30, 42])
er = VotingRegressor([('lr', r1), ('rf', r2)])
print(er.fit(X, y).predict(X))""" .

<DEPENDENCY.scikit-learn==0.21.3> <CONTAINS> """CODE.clf1 = LogisticRegression()
clf2 = RandomForestClassifier()
eclf = VotingClassifier(estimators=[('lr', clf1), ('rf', clf2)]
eclf.set_params(rf=None)""" .

<DEPENDENCY.scikit-learn==0.22> <CONTAINS> """CODE.from sklearn.datasets import load_diabetes
from sklearn.linear_model import RidgeCV
from sklearn.svm import LinearSVR
from sklearn.ensemble import RandomForestRegressor
from sklearn.ensemble import StackingRegressor
X, y = load_diabetes(return_X_y=True)
estimators = [
...     ('lr', RidgeCV()),
...     ('svr', LinearSVR(random_state=42))
... ]
reg = StackingRegressor(
...     estimators=estimators,
...     final_estimator=RandomForestRegressor(n_estimators=10,
...                                           random_state=42)
... )
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(
...     X, y, random_state=42
... )
reg.fit(X_train, y_train).score(X_test, y_test)
0.3...""",
        """CODE.from sklearn.metrics import mean_poisson_deviance
y_true = [2, 0, 1, 4]
y_pred = [0.5, 0.5, 2., 2.]
mean_poisson_deviance(y_true, y_pred)
""",
        """CODE.from sklearn.metrics import mean_tweedie_deviance
y_true = [2, 0, 1, 4]
y_pred = [0.5, 0.5, 2., 2.]
mean_tweedie_deviance(y_true, y_pred, power=1)
""",
        """CODE.from sklearn.metrics.pairwise import nan_euclidean_distances
nan = float("NaN")
X = [[0, 1], [1, nan]]
nan_euclidean_distances(X, X) # distance between rows of X
nan_euclidean_distances(X, [[0, 0]]) # get distance to origin
""",
        """CODE.from sklearn.naive_bayes import CategoricalNB
clf = CategoricalNB()
clf.fit(X, y)
print(clf.predict(X[2:3]))""",
        """CODE.from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import make_column_transformer
from sklearn.compose import make_column_selector
import pandas as pd
X = pd.DataFrame({'city': ['London', 'London', 'Paris', 'Sallisaw'],
...                   'rating': [5, 3, 4, 5]})
ct = make_column_transformer(
...       (StandardScaler(),
...        make_column_selector(dtype_include=np.number)),
...       (OneHotEncoder(),
...        make_column_selector(dtype_include=object)))
ct.fit_transform(X)  """,
        """CODE.from sklearn.utils.fixes import loguniform
rv = loguniform(1e-3, 1e1)
rvs = rv.rvs(random_state=42, size=1000)
rvs.min()  # doctest: +SKIP
rvs.max()  # doctest: +SKIP
""",
        """CODE.import matplotlib.pyplot as plt
import numpy as np
from sklearn import metrics
y = np.array([0, 0, 1, 1])
pred = np.array([0.1, 0.4, 0.35, 0.8])
fpr, tpr, thresholds = metrics.roc_curve(y, pred)
roc_auc = metrics.auc(fpr, tpr)
display = metrics.RocCurveDisplay(fpr=fpr, tpr=tpr, roc_auc=roc_auc, estimator_name='example estimator')
display.plot()
plt.show()""",
        """CODE.import numpy as np
from sklearn.decomposition import IncrementalPCA
X = np.array([[-1, -1], [-2, -1], [-3, -2],
              [1, 1], [2, 1], [3, 2]])
ipca = IncrementalPCA(n_components=2, batch_size=3)
ipca.fit(X)
ipca.transform(X) # doctest: +SKIP""",
        """CODE.import numpy as np
from sklearn.impute import KNNImputer
X = [[1, 2, np.nan], [3, 4, 3], [np.nan, 6, 5], [8, 8, 7]]
imputer = KNNImputer(n_neighbors=2)
imputer.fit_transform(X)""",
        """CODE.metrics.plot_roc_curve(clf, X_test, y_test)  # doctest: +SKIP
plt.show()                                   # doctest: +SKIP""" .

<DEPENDENCY.scikit-learn==0.22.2> <CONTAINS> """CODE._to_object_array([np.array([0]), np.array([1])])
_to_object_array([np.array([0]), np.array([1, 2])])
""" .

<DEPENDENCY.scikit-learn==0.24.0> <CONTAINS> """CODE.from sklearn.cluster import kmeans_plusplus
import numpy as np
X = np.array([[1, 2], [1, 4], [1, 0],
              [10, 2], [10, 4], [10, 0]])
centers, indices = kmeans_plusplus(X, n_clusters=2, random_state=0)""",
        """CODE.from sklearn.feature_selection import SequentialFeatureSelector
from sklearn.neighbors import KNeighborsClassifier
from sklearn.datasets import load_iris
X, y = load_iris(return_X_y=True)
knn = KNeighborsClassifier(n_neighbors=3)
sfs = SequentialFeatureSelector(knn, n_features_to_select=3)
sfs.fit(X, y)
sfs.get_support()
sfs.transform(X).shape
""",
        """CODE.from sklearn.metrics import det_curve
y_true = np.array([0, 0, 1, 1])
y_scores = np.array([0.1, 0.4, 0.35, 0.8])
fpr, fnr, thresholds = det_curve(y_true, y_scores)
fpr
array([0.5, 0.5, 0. ])
fnr
array([0. , 0.5, 0.5])
thresholds
array([0.35, 0.4 , 0.8 ])""",
        """CODE.from sklearn.metrics import mean_absolute_percentage_error
y_true = [3, -0.5, 2, 7]
y_pred = [2.5, 0.0, 2, 8]
mean_absolute_percentage_error(y_true, y_pred)
y_true = [[0.5, 1], [-1, 1], [7, -6]]
y_pred = [[0, 2], [-1, 2], [8, -5]]
mean_absolute_percentage_error(y_true, y_pred)
mean_absolute_percentage_error(y_true, y_pred, multioutput=[0.3, 0.7])
""",
        """CODE.from sklearn.metrics.cluster import pair_confusion_matrix
pair_confusion_matrix([0, 0, 1, 0], [1, 1, 0, 1])
""",
        """CODE.import numpy as np
from sklearn import metrics

y = np.array([0, 0, 1, 1])
pred = np.array([0.1, 0.4, 0.35, 0.8])
fpr, fnr, thresholds = metrics.det_curve(y, pred)
display = metrics.DetCurveDisplay(
    fpr=fpr, fnr=fnr, estimator_name='example estimator'
)
display.plot()
plt.show()      # doctest: +SKIP""",
        """CODE.import numpy as np
from sklearn.metrics import top_k_accuracy_score
y_true = np.array([0, 1, 2, 2])
y_score = np.array([[0.5, 0.2, 0.2],
                    [0.3, 0.4, 0.2],
                    [0.2, 0.4, 0.3],
                    [0.7, 0.2, 0.1]])
top_k_accuracy_score(y_true, y_score, k=2)
top_k_accuracy_score(y_true, y_score, k=2, normalize=False)""",
        """CODE.metrics.plot_det_curve(clf, X_test, y_test)
plt.show()""" .

<DEPENDENCY.scikit-learn==1.0> <CONTAINS> """CODE.from sklearn.model_selection import StratifiedGroupKFold
import numpy as np

X = np.ones((17, 2))
y = np.array([0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0])
groups = np.array([1, 1, 2, 2, 3, 3, 3, 4, 5, 5, 5, 5, 6, 6, 7, 8, 8])

cv = StratifiedGroupKFold(n_splits=3)
for train_idxs, test_idxs in cv.split(X, y, groups):
    print("TRAIN:", groups[train_idxs])
    print("      ", y[train_idxs])
    print(" TEST:", groups[test_idxs])
    print("      ", y[test_idxs])

plaintext
TRAIN: [1 1 2 2 4 5 5 5 5 8 8]
       [0 0 1 1 1 0 0 0 0 0 0]
 TEST: [3 3 3 6 6 7]
       [1 1 1 0 0 0]
TRAIN: [3 3 3 4 5 5 5 5 6 6 7]
       [1 1 1 1 0 0 0 0 0 0 0]
 TEST: [1 1 2 2 8 8]
       [0 0 1 1 0 0]
TRAIN: [1 1 2 2 3 3 3 6 6 7 8 8]
       [0 0 1 1 1 1 1 0 0 0...
 TEST: [4...
""",
        """CODE.import matplotlib.pyplot as plt
from sklearn.datasets import make_classification
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.calibration import CalibrationDisplay

X, y = make_classification(random_state=0)
X_train, X_test, y_train, y_test = train_test_split(
    X, y, random_state=0)
clf = LogisticRegression(random_state=0)
clf.fit(X_train, y_train)
LogisticRegression(random_state=0)
y_prob = clf.predict_proba(X_test)[:, 1]
disp = CalibrationDisplay.from_predictions(y_test, y_prob)
plt.show()""",
        """CODE.import numpy as np
from sklearn.preprocessing import SplineTransformer
X = np.arange(6).reshape(6, 1)
spline = SplineTransformer(degree=2, n_knots=3)
spline.fit_transform(X)""",
        """CODE.prob_true, prob_pred = calibration_curve(y_test, y_prob, n_bins=10)
disp = CalibrationDisplay(prob_true, prob_pred, y_prob)
disp.plot()
""" .

<DEPENDENCY.scikit-learn==1.1.0> <CONTAINS> """CODE.from sklearn.cluster import BisectingKMeans
import numpy as np
X = np.array([[1, 2], [1, 4], [1, 0],
              [10, 2], [10, 4], [10, 0],
              [10, 6], [10, 8], [10, 10]])
bisect_means = BisectingKMeans(n_clusters=3, random_state=0).fit(X)
bisect_means.labels_
bisect_means.predict([[0, 0], [12, 3]])
bisect_means.cluster_centers_""" .

<DEPENDENCY.scikit-learn==1.2.0> <CONTAINS> """CODE.import matplotlib.pyplot as plt
from sklearn.datasets import load_iris
from sklearn.model_selection import LearningCurveDisplay, learning_curve
from sklearn.tree import DecisionTreeClassifier
X, y = load_iris(return_X_y=True)
tree = DecisionTreeClassifier(random_state=0)
train_sizes, train_scores, test_scores = learning_curve(
    tree, X, y)
display = LearningCurveDisplay(train_sizes=train_sizes,
    train_scores=train_scores, test_scores=test_scores, score_name="Score")
display.plot()
plt.show()""" .

<DEPENDENCY.scikit-learn==1.3.0> <CONTAINS> """CODE.import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import make_classification
from sklearn.model_selection import ValidationCurveDisplay, validation_curve
from sklearn.linear_model import LogisticRegression

X, y = make_classification(n_samples=1_000, random_state=0)
logistic_regression = LogisticRegression()
param_name, param_range = "C", np.logspace(-8, 3, 10)
train_scores, test_scores = validation_curve(
    logistic_regression, X, y, param_name=param_name, param_range=param_range
)
display = ValidationCurveDisplay(
    param_name=param_name, param_range=param_range,
    train_scores=train_scores, test_scores=test_scores, score_name="Score"
)
display.plot()
plt.show()""" .

<DEPENDENCY.scikit-learn==1.4.0> <CONTAINS> """CODE.from sklearn.metrics import root_mean_squared_log_error
y_true = [3, 5, 2.5, 7]
y_pred = [2.5, 5, 4, 8]
root_mean_squared_log_error(y_true, y_pred)
""" .

<DEPENDENCY.seqeval==0.0.11> <CONTAINS> """CODE.def find_pad_index(array):
    for idx, num in enumerate(array):
        if num == 0:
            return idx
""",
        """CODE.def get_length(y):
    return [len([i for i in x if i != 0]) for x in y]
""" .

<DEPENDENCY.seqeval==0.0.19> <CONTAINS> """CODE.tokens = Tokens(['B-PER', 'I-PER', 'O', 'B-LOC'], IOB2)
tokens.entities
[('PER', 0, 2), ('LOC', 3, 4)]""" .

<DEPENDENCY.streamlit==0.11.0> <CONTAINS> """CODE.with menagerie.stream_to(report_id) as consume:
    async for delta_list in delta_list_iter:
        consume(delta_list)""" .

<DEPENDENCY.streamlit==0.14.2> <CONTAINS> """CODE.unflatten({
  foo_bar_baz: 123,
  foo_bar_biz: 456,
  x_bonks: 'hi',
})""" .

<DEPENDENCY.streamlit==0.17.2> <CONTAINS> """CODE._unflatten_single_dict({
  foo_bar_baz: 123,
  foo_bar_biz: 456,
  x_bonks: 'hi',
})""",
        """CODE.st.deck_gl_chart(my_data_frame)
st.deck_gl_chart(
    viewport={
        'latitude': 37.76,
        'longitude': -122.4,
        'zoom': 11,
        'pitch': 50,
    },
    layers=[{
        'type': 'HexagonLayer',
        'data': my_dataframe,
        'radius': 200,
        'elevationScale': 4,
        'elevationRange': [0, 1000],
        'pickable': True,
        'extruded': True,
    }, {
        'type': 'ScatterplotLayer',
        'data': my_other_dataframe,
        'pickable': True,
        'autoHighlight': True,
        'radiusScale': 0.02,
        'encoding': {
            'radius': 'exits',
        },
    }])""" .

<DEPENDENCY.streamlit==0.19.0> <CONTAINS> """CODE.@ConfigOption('proxy.port')
def _proxy_port():
    \"\"\"Connect to the proxy at this port.

    Defaults to 8501.
    \"\"\"
    return 8501""",
        """CODE._create_option('section.optionName',
    description = 'Put the description here.',
    default_val = 12345)

@_create_option('section.optionName')
def _section_option_name():
    \"\"\"Put the description here.\"\"\"
    return 12345

@_create_option('section.memoizedOptionName')
@util.memoize
def _section_memoized_option_name():
    \"\"\"Put the description here.\"\"\"
    return 12345""" .

<DEPENDENCY.streamlit==0.27.0> <CONTAINS> """CODE.import pandas as pd
import numpy as np
import altair as alt

df = pd.DataFrame(
    np.random.randn(200, 3),
    columns=['a', 'b', 'c'])

c = alt.Chart(df).mark_circle().encode(
    x='a', y='b', size='c', color='c')

st.altair_chart(c)""",
        "CODE.is_type(foo, 'matplotlib.figure.Figure')" .

<DEPENDENCY.streamlit==0.28.0> <CONTAINS> """CODE.chart_data = pd.DataFrame(
...     np.random.randn(20, 3),
...     columns=['a', 'b', 'c'])
...
st.line_chart(chart_data)""",
        "CODE.st.area_chart(chart_data)" .

<DEPENDENCY.streamlit==0.32.0> <CONTAINS> """CODE.@_clean_up_sig
def some_function(self, unused_element_argument, stuff):
    pass""" .

<DEPENDENCY.streamlit==0.34.0> <CONTAINS> """CODE.import streamlit as st
import plotly.plotly as py
import plotly.figure_factory as ff
import numpy as np

# Add histogram data
x1 = np.random.randn(200) - 2
x2 = np.random.randn(200)
x3 = np.random.randn(200) + 2

# Group data together
hist_data = [x1, x2, x3]

group_labels = ['Group 1', 'Group 2', 'Group 3']

# Create distplot with custom bin_size
fig = ff.create_distplot(
        hist_data, group_labels, bin_size=[.1, .25, .5])

# Plot!
st.plotly_chart(fig)""" .

<DEPENDENCY.streamlit==0.35.0> <CONTAINS> """CODE.import streamlit as st
from bokeh.plotting import figure

x = [1, 2, 3, 4, 5]
y = [6, 7, 2, 4, 5]

p = figure(
    title='simple line example',
    x_axis_label='x',
    y_axis_label='y')

p.line(x, y, legend='Trend', line_width=2)

st.bokeh_chart(p)""" .

<DEPENDENCY.streamlit==0.40.0> <CONTAINS> "CODE.extract_args(line)",
        """CODE.import graphviz as graphviz

# Create a graphlib graph object
graph = graphviz.DiGraph()
graph.edge('run', 'intr')
graph.edge('intr', 'runbl')
graph.edge('runbl', 'run')
graph.edge('run', 'kernel')
graph.edge('kernel', 'zombie')
graph.edge('kernel', 'sleep')
graph.edge('kernel', 'runmem')
graph.edge('sleep', 'swap')
graph.edge('swap', 'runswap')
graph.edge('runswap', 'new')
graph.edge('runswap', 'runmem')
graph.edge('new', 'runmem')
graph.edge('sleep', 'runmem')

st.graphviz_chart(graph)

Or you can render the chart from the graph using GraphViz's Dot
language:

st.graphviz_chart('''
    digraph {
        run -> intr
        intr -> runbl
        runbl -> run
        run -> kernel
        kernel -> zombie
        kernel -> sleep
        kernel -> runmem
        sleep -> swap
        swap -> runswap
        runswap -> new
        runswap -> runmem
        new -> runmem
        sleep -> runmem
    }
''')""" .

<DEPENDENCY.streamlit==0.43.0> <CONTAINS> """CODE.age = st.slider('Age', 25, 0, 100, 1)
st.write("I'm ", age)

values = st.slider('A range of values', (25.0, 75.0), 0.0, 100.0, 1.0)
st.write("Values:", values)""",
        """CODE.st.date_input('A date to celebrate', datetime.date(2019, 7, 6)
st.write('The date', d)""",
        """CODE.title = st.text_input('Movie title', 'Life of Brian')
st.write('The current movie title is', title)""" .

<DEPENDENCY.streamlit==0.45.0> <CONTAINS> """CODE.@_clean_up_sig
def some_function(self, unused_element_argument, stuff):
    pass""",
        """CODE.c = st.Cache()
if c:
    # Fetch data from URL here, and then clean it up. Finally assign to c.
    c.data = ...

if c := st.Cache():
    # Fetch data from URL here, and then clean it up. Finally assign to c.
    c.data = ...""" .

<DEPENDENCY.streamlit==0.46.0> <CONTAINS> """CODE.register_component('foo_bar', False)
register_component('baz', False)""" .

<DEPENDENCY.streamlit==0.49.0> <CONTAINS> """CODE.number = st.number_input('Insert a number')
st.write('The current number is ', number)""" .

<DEPENDENCY.streamlit==0.52.0> <CONTAINS> """CODE.mock_is_type.side_effect = make_is_type_mock("foo.bar.Baz")
mock_is_type(my_type, "foo.bar.Baz")""",
        """CODE.uploaded_file = st.file_uploader("Choose a CSV file", type="csv")
if uploaded_file is not None:
...     data = pd.read_csv(uploaded_file)
...     st.write(data)""" .

<DEPENDENCY.streamlit==0.53.0> <CONTAINS> """CODE.df = pd.DataFrame(
...    np.random.randn(1000, 2) / [50, 50] + [37.76, -122.4],
...    columns=['lat', 'lon'])

st.pydeck_chart(pdk.Deck(
...     map_style='mapbox://styles/mapbox/light-v9',
...     initial_view_state=pdk.ViewState(
...         latitude=37.76,
...         longitude=-122.4,
...         zoom=11,
...         pitch=50,
...     ),
...     layers=[
...         pdk.Layer(
...            'HexagonLayer',
...            data=df,
...            get_position='[lon, lat]',
...            radius=200,
...            elevation_scale=4,
...            elevation_range=[0, 1000],
...            pickable=True,
...            extruded=True,
...         ),
...         pdk.Layer(
...             'ScatterplotLayer',
...             data=df,
...             get_position='[lon, lat]',
...             get_color='[200, 30, 0, 160]',
...             get_radius=200,
...         ),
...     ],
... ))""" .

<DEPENDENCY.streamlit==0.56.0> <CONTAINS> """CODE.dataframe = pd.DataFrame({
...     'first column': [1, 2, 3, 4],
...     'second column': [10, 20, 30, 40],
... }))
st.show(dataframe)""" .

<DEPENDENCY.streamlit==0.59.0> <CONTAINS> """CODE.dataframe = pd.DataFrame({
...     'first column': [1, 2, 3, 4],
...     'second column': [10, 20, 30, 40],
... }))
st.experimental_show(dataframe)""",
        """CODE.st.beta_color_picker('Pick A Color', '#00f900')
st.write('The current color is', color)""" .

<DEPENDENCY.streamlit==0.64.0> <CONTAINS> """CODE.age = st.slider('How old are you?', 0, 130, 25)
st.write("I'm ", age, 'years old')

values = st.slider(
    'Select a range of values',
    0.0, 100.0, (25.0, 75.0))
st.write('Values:', values)

from datetime import time
appointment = st.slider(
    "Schedule your appointment:",
    value=(time(11, 30), time(12, 45))
)
st.write("You're scheduled for:", appointment)

from datetime import datetime
start_time = st.slider(
    "When do you start?",
    value=datetime(2020, 1, 1, 9, 30),
    format="MM/DD/YY - hh:mm"
)
st.write("Start time:", start_time)""",
        """CODE.agree = st.checkbox('I agree')
if agree:
    st.write('Great!')""",
        """CODE.chart_data = pd.DataFrame(
    np.random.randn(50, 3),
    columns=["a", "b", "c"])
st.bar_chart(chart_data)
""",
        """CODE.chart_data = pd.DataFrame(
...     np.random.randn(20, 3),
...     columns=['a', 'b', 'c'])
...
st.area_chart(chart_data)""",
        """CODE.df = pd.DataFrame(
...    np.random.randn(10, 5),
...    columns=('col %d' % i for i in range(5)))
...
st.table(df)""",
        """CODE.df = pd.DataFrame(
...    np.random.randn(1000, 2) / [50, 50] + [37.76, -122.4],
...    columns=['lat', 'lon'])

st.pydeck_chart(pdk.Deck(
...     map_style='mapbox://styles/mapbox/light-v9',
...     initial_view_state=pdk.ViewState(
...         latitude=37.76,
...         longitude=-122.4,
...         zoom=11,
...         pitch=50,
...     ),
...     layers=[
...         pdk.Layer(
...            'HexagonLayer',
...            data=df,
...            get_position='[lon, lat]',
...            radius=200,
...            elevation_scale=4,
...            elevation_range=[0, 1000],
...            pickable=True,
...            extruded=True,
...         ),
...         pdk.Layer(
...             'ScatterplotLayer',
...             data=df,
...             get_position='[lon, lat]',
...             get_color='[200, 30, 0, 160]',
...             get_radius=200,
...         ),
...     ],
... ))""",
        """CODE.e = RuntimeError('This is an exception of type RuntimeError')
st.exception(e)""",
        """CODE.if st.button('Say hello'):
    st.write('Why hello there')
else:
    st.write('Goodbye')""",
        """CODE.import pandas as pd
import numpy as np

df = pd.DataFrame(
    np.random.randn(200, 3),
    columns=['a', 'b', 'c'])

st.vega_lite_chart(df, {
    'mark': {'type': 'circle', 'tooltip': True},
    'encoding': {
        'x': {'field': 'a', 'type': 'quantitative'},
        'y': {'field': 'b', 'type': 'quantitative'},
        'size': {'field': 'c', 'type': 'quantitative'},
        'color': {'field': 'c', 'type': 'quantitative'},
    },
})""",
        """CODE.import streamlit as st
from bokeh.plotting import figure

x = [1, 2, 3, 4, 5]
y = [6, 7, 2, 4, 5]

p = figure(
    title='simple line example',
    x_axis_label='x',
    y_axis_label='y')

p.line(x, y, legend='Trend', line_width=2)

st.bokeh_chart(p, use_container_width=True)""",
        """CODE.import streamlit as st
import graphviz as graphviz

# Create a graphlib graph object
graph = graphviz.Digraph()
graph.edge('run', 'intr')
graph.edge('intr', 'runbl')
graph.edge('runbl', 'run')
graph.edge('run', 'kernel')
graph.edge('kernel', 'zombie')
graph.edge('kernel', 'sleep')
graph.edge('kernel', 'runmem')
graph.edge('sleep', 'swap')
graph.edge('swap', 'runswap')
graph.edge('runswap', 'new')
graph.edge('runswap', 'runmem')
graph.edge('new', 'runmem')
graph.edge('sleep', 'runmem')

st.graphviz_chart(graph)

# Or you can render the chart from the graph using GraphViz's Dot language:
st.graphviz_chart('''
    digraph {
        run -> intr
        intr -> runbl
        runbl -> run
        run -> kernel
        kernel -> zombie
        kernel -> sleep
        kernel -> runmem
        sleep -> swap
        swap -> runswap
        runswap -> new
        runswap -> runmem
        new -> runmem
        sleep -> runmem
    }
''')""",
        """CODE.import streamlit as st
import plotly.figure_factory as ff
import numpy as np

# Add histogram data
x1 = np.random.randn(200) - 2
x2 = np.random.randn(200)
x3 = np.random.randn(200) + 2

# Group data together
hist_data = [x1, x2, x3]

group_labels = ['Group 1', 'Group 2', 'Group 3']

# Create distplot with custom bin_size
fig = ff.create_distplot(
        hist_data, group_labels, bin_size=[.1, .25, .5])

# Plot!
st.plotly_chart(fig, use_container_width=True)""",
        """CODE.my_bar = st.progress(0)

for percent_complete in range(100):
...     time.sleep(0.1)
...     my_bar.progress(percent_complete + 1)""",
        """CODE.my_placeholder = st.empty()
my_placeholder.text("Hello world!")
my_placeholder.image(my_image_bytes)""",
        """CODE.number = st.number_input('Insert a number')
st.write('The current number is ', number)""",
        """CODE.option = st.selectbox(
    'How would you like to be contacted?',
    ('Email', 'Home phone', 'Mobile phone'))

st.write('You selected:', option)""",
        """CODE.options = st.multiselect(
    'What are your favorite colors',
    ['Green', 'Yellow', 'Red', 'Blue'],
    ['Yellow', 'Red'])

st.write('You selected:', options)""",
        "CODE.st.audio(audio_bytes, format='audio/ogg')",
        "CODE.st.balloons()",
        """CODE.st.beta_color_picker('Pick A Color', '#00f900')
st.write('The current color is', color)""",
        """CODE.st.date_input(
    "When's your birthday",
    datetime.date(2019, 7, 6))
st.write('Your birthday is:', d)""",
        "CODE.st.error('This is an error')",
        "CODE.st.header('This is a header')",
        """CODE.st.help(pandas.DataFrame)
x = my_poorly_documented_function()
st.help(x)""",
        "CODE.st.image(image, caption='Sunrise by the mountains', use_column_width=True)",
        "CODE.st.info('This is a purely informational message')",
        """CODE.st.json({
    'foo': 'bar',
    'baz': 'boz',
    'stuff': [
        'stuff 1',
        'stuff 2',
        'stuff 3',
        'stuff 5',
    ],
})""",
        """CODE.st.line_chart(chart_data)
chart_data = pd.DataFrame(
    np.random.randn(20, 3),
    columns=['a', 'b', 'c'])""",
        """CODE.st.radio(
    "What's your favorite movie genre",
    ('Comedy', 'Drama', 'Documentary')
)

if genre == 'Comedy':
    st.write('You selected comedy.')
else:
    st.write("You didn't select comedy.")""",
        "CODE.st.success('This is a success message!')",
        "CODE.st.warning('This is a warning')",
        """CODE.title = st.text_input('Movie title', 'Life of Brian')
st.write('The current movie title is', title)""",
        """CODE.uploaded_file = st.file_uploader("Choose a CSV file", type="csv")
if uploaded_file is not None:
...     data = pd.read_csv(uploaded_file)
...     st.write(data)""" .

<DEPENDENCY.streamlit==0.65.0> <CONTAINS> """CODE.age = st.slider('How old are you?', 0, 130, 25)
st.write("I'm ", age, 'years old')

values = st.slider(
    'Select a range of values',
    0.0, 100.0, (25.0, 75.0)
)
st.write('Values:', values)

from datetime import time
appointment = st.slider(
    "Schedule your appointment:",
    value=(time(11, 30), time(12, 45)
)
st.write("You're scheduled for:", appointment)

from datetime import datetime
start_time = st.slider(
    "When do you start?",
    value=datetime(2020, 1, 1, 9, 30),
    format="MM/DD/YY - hh:mm"
)
st.write("Start time:", start_time)""",
        """CODE.audio_file = open('myaudio.ogg', 'rb')
audio_bytes = audio_file.read()

st.audio(audio_bytes, format='audio/ogg')
""",
        """CODE.d = st.date_input(
    "When's your birthday",
    datetime.date(2019, 7, 6))
st.write('Your birthday is:', d)
""",
        """CODE.df = pd.DataFrame(
...    np.random.randn(10, 5),
...    columns=('col %d' % i for i in range(5)))
...
st.table(df)""",
        """CODE.df = pd.DataFrame(
...    np.random.randn(1000, 2) / [50, 50] + [37.76, -122.4],
...    columns=['lat', 'lon'])

st.pydeck_chart(pdk.Deck(
...     map_style='mapbox://styles/mapbox/light-v9',
...     initial_view_state=pdk.ViewState(
...         latitude=37.76,
...         longitude=-122.4,
...         zoom=11,
...         pitch=50,
...     ),
...     layers=[
...         pdk.Layer(
...            'HexagonLayer',
...            data=df,
...            get_position='[lon, lat]',
...            radius=200,
...            elevation_scale=4,
...            elevation_range=[0, 1000],
...            pickable=True,
...            extruded=True,
...         ),
...         pdk.Layer(
...             'ScatterplotLayer',
...             data=df,
...             get_position='[lon, lat]',
...             get_color='[200, 30, 0, 160]',
...             get_radius=200,
...         ),
...     ],
... ))""",
        """CODE.e = RuntimeError('This is an exception of type RuntimeError')
st.exception(e)""",
        """CODE.from PIL import Image
image = Image.open('sunrise.jpg')

st.image(image, caption='Sunrise by the mountains', use_column_width=True)
""",
        """CODE.if not name:
  st.warning('Please input a name.')
  st.stop()
st.success('Thank you for inputting a name.')""",
        """CODE.if st.button('Say hello'):
    st.write('Why hello there')
else:
    st.write('Goodbye')""",
        """CODE.import matplotlib.pyplot as plt
import numpy as np

arr = np.random.normal(1, 1, size=100)
plt.hist(arr, bins=20)

st.pyplot()
""",
        """CODE.import pandas as pd
import numpy as np

df = pd.DataFrame(
    np.random.randn(1000, 2) / [50, 50] + [37.76, -122.4],
    columns=['lat', 'lon'])

st.map(df)
""",
        """CODE.import pandas as pd
import numpy as np
import altair as alt

df = pd.DataFrame(
    np.random.randn(200, 3),
    columns=['a', 'b', 'c'])

c = alt.Chart(df).mark_circle().encode(
    x='a', y='b', size='c', color='c', tooltip=['a', 'b', 'c'])

st.altair_chart(c, use_container_width=True)""",
        """CODE.import streamlit as st
from bokeh.plotting import figure

x = [1, 2, 3, 4, 5]
y = [6, 7, 2, 4, 5]

p = figure(
    title='simple line example',
    x_axis_label='x',
    y_axis_label='y')

p.line(x, y, legend='Trend', line_width=2)

st.bokeh_chart(p, use_container_width=True)""",
        """CODE.import streamlit as st
import graphviz as graphviz

# Create a graphlib graph object
graph = graphviz.Digraph()
graph.edge('run', 'intr')
graph.edge('intr', 'runbl')
graph.edge('runbl', 'run')
graph.edge('run', 'kernel')
graph.edge('kernel', 'zombie')
graph.edge('kernel', 'sleep')
graph.edge('kernel', 'runmem')
graph.edge('sleep', 'swap')
graph.edge('swap', 'runswap')
graph.edge('runswap', 'new')
graph.edge('runswap', 'runmem')
graph.edge('new', 'runmem')
graph.edge('sleep', 'runmem')

st.graphviz_chart(graph)

Or you can render the chart from the graph using GraphViz's Dot
language:

st.graphviz_chart('''
    digraph {
        run -> intr
        intr -> runbl
        runbl -> run
        run -> kernel
        kernel -> zombie
        kernel -> sleep
        kernel -> runmem
        sleep -> swap
        swap -> runswap
        runswap -> new
        runswap -> runmem
        new -> runmem
        sleep -> runmem
    }
''')""",
        """CODE.import streamlit as st
import plotly.figure_factory as ff
import numpy as np

# Add histogram data
x1 = np.random.randn(200) - 2
x2 = np.random.randn(200)
x3 = np.random.randn(200) + 2

# Group data together
hist_data = [x1, x2, x3]

group_labels = ['Group 1', 'Group 2', 'Group 3']

# Create distplot with custom bin_size
fig = ff.create_distplot(
        hist_data, group_labels, bin_size=[.1, .25, .5])

# Plot!
st.plotly_chart(fig, use_container_width=True)""",
        """CODE.my_bar = st.progress(0)

for percent_complete in range(100):
...     time.sleep(0.1)
...     my_bar.progress(percent_complete + 1)""",
        """CODE.my_placeholder = st.empty()
my_placeholder.text("Hello world!")
my_placeholder.image(my_image_bytes)""",
        """CODE.number = st.number_input('Insert a number')
st.write('The current number is ', number)""",
        """CODE.option = st.selectbox(
    'How would you like to be contacted?',
    ('Email', 'Home phone', 'Mobile phone'))
st.write('You selected:', option)""",
        """CODE.options = st.multiselect(
    'What are your favorite colors',
    ['Green', 'Yellow', 'Red', 'Blue'],
    ['Yellow', 'Red'])

st.write('You selected:', options)""",
        "CODE.st.area_chart(chart_data)",
        "CODE.st.balloons()",
        "CODE.st.bar_chart(chart_data)",
        """CODE.st.beta_color_picker('Pick A Color', '#00f900')
st.write('The current color is', color)""",
        """CODE.st.beta_set_page_config(
    page_title="Ex-stream-ly Cool App",
    page_icon="ð§",
    layout="wide",
    initial_sidebar_state="expanded",
)""",
        "CODE.st.error('This is an error')",
        """CODE.st.experimental_get_query_params()
{"show_map": ["True"], "selected": ["asia", "america"]}""",
        """CODE.st.experimental_set_query_params(
    show_map=True,
    selected=["asia", "america"],
)""",
        """CODE.st.help(pandas.DataFrame)
x = my_poorly_documented_function()
st.help(x)""",
        "CODE.st.info('This is a purely informational message')",
        """CODE.st.json({
    'foo': 'bar',
    'baz': 'boz',
    'stuff': [
        'stuff 1',
        'stuff 2',
        'stuff 3',
        'stuff 5',
    ],
})""",
        """CODE.st.line_chart(chart_data)
chart_data = pd.DataFrame(
    np.random.randn(20, 3),
    columns=['a', 'b', 'c'])""",
        """CODE.st.radio(
    "What's your favorite movie genre",
    ('Comedy', 'Drama', 'Documentary'))

if genre == 'Comedy':
    st.write('You selected comedy.')
else:
    st.write("You didn't select comedy.")""",
        "CODE.st.success('This is a success message!')",
        """CODE.st.vega_lite_chart(df, {
    'mark': {'type': 'circle', 'tooltip': True},
    'encoding': {
        'x': {'field': 'a', 'type': 'quantitative'},
        'y': {'field': 'b', 'type': 'quantitative'},
        'size': {'field': 'c', 'type': 'quantitative'},
        'color': {'field': 'c', 'type': 'quantitative'},
    },
})""",
        "CODE.st.warning('This is a warning')",
        """CODE.title = st.text_input('Movie title', 'Life of Brian')
st.write('The current movie title is', title)""",
        """CODE.uploaded_file = st.file_uploader("Choose a CSV file", type="csv")
if uploaded_file is not None:
...     data = pd.read_csv(uploaded_file)
...     st.write(data)""" .

<DEPENDENCY.streamlit==0.68.0> <CONTAINS> """CODE.with st.beta_expander("See explanation"):
    st.write(\"\"\"
        The chart above shows some numbers I picked for you.
        I rolled actual dice for these, so they're *guaranteed* to
        be random.
    \"\"\")
    st.image("https://static.streamlit.io/examples/dice.jpg")""" .

<DEPENDENCY.streamlit==0.69.2> <CONTAINS> """CODE.st.beta_color_picker('Pick A Color', '#00f900')
st.write('The current color is', color)""",
        """CODE.st.beta_set_page_config(
    page_title="Ex-stream-ly Cool App",
    page_icon="ð§",
    layout="wide",
    initial_sidebar_state="expanded",
)""" .

<DEPENDENCY.streamlit==0.70.0> <CONTAINS> """CODE.color = st.color_picker('Pick A Color', '#00f900')
st.write('The current color is', color)""",
        """CODE.st.color_picker('Pick A Color', '#00f900')
st.write('The current color is', color)""",
        """CODE.st.set_page_config(
    page_title="Ex-stream-ly Cool App",
    page_icon="ð§",
    layout="wide",
    initial_sidebar_state="expanded",
)""" .

<DEPENDENCY.streamlit==0.73.1> <CONTAINS> """CODE.col1, col2, col3 = st.beta_columns(3)

with col1:
    st.header("A cat")
    st.image("https://static.streamlit.io/examples/cat.jpg", use_column_width=True)

with col2:
    st.header("A dog")
    st.image("https://static.streamlit.io/examples/dog.jpg", use_column_width=True)

with col3:
    st.header("An owl")
    st.image("https://static.streamlit.io/examples/owl.jpg", use_column_width=True)
""",
        """CODE.with st.beta_container():
    st.write("This is inside the container")
    st.bar_chart(np.random.randn(50, 3))

container = st.beta_container()
container.write("This is inside the container")
container.write("This is inside too")
""",
        """CODE.with st.beta_expander("See explanation"):
...     st.write(\"\"\"
...         The chart above shows some numbers I picked for you.
...         I rolled actual dice for these, so they're *guaranteed* to
...         be random.
...     \"\"\")
...     st.image("https://static.streamlit.io/examples/dice.jpg")""" .

<DEPENDENCY.streamlit==0.74.0> <CONTAINS> """CODE.col1, col2, col3 = st.beta_columns(3)

with col1:
    st.header("A cat")
    st.image("https://static.streamlit.io/examples/cat.jpg", use_column_width=True)

with col2:
    st.header("A dog")
    st.image("https://static.streamlit.io/examples/dog.jpg", use_column_width=True)

with col3:
    st.header("An owl")
    st.image("https://static.streamlit.io/examples/owl.jpg", use_column_width=True)

col1, col2 = st.beta_columns([3, 1])
data = np.random.randn(10, 1)

col1.subheader("A wide column with a chart")
col1.line_chart(data)

col2.subheader("A narrow column with the data")
col2.write(data)
""",
        """CODE.with st.beta_container():
    st.write("This is inside the container")
    st.bar_chart(np.random.randn(50, 3))

container = st.beta_container()
container.write("This is inside the container")
container.write("This is inside too")
""",
        """CODE.with st.beta_expander("See explanation"):
...     st.write(\"\"\"
...         The chart above shows some numbers I picked for you.
...         I rolled actual dice for these, so they're *guaranteed* to
...         be random.
...     \"\"\")
...     st.image("https://static.streamlit.io/examples/dice.jpg")""" .

<DEPENDENCY.streamlit==0.77.0> <CONTAINS> """CODE.with patch_config_options({"server.headless": True}):
    assert(config.get_option("server.headless") is True)
    # Other test code that relies on these options""" .

<DEPENDENCY.streamlit==0.84.2> <CONTAINS> """CODE.c = st.Cache()
if c:
    # Fetch data from URL here, and then clean it up. Finally assign to c.
    c.data = ...

if c := st.Cache():
    # Fetch data from URL here, and then clean it up. Finally assign to c.
    c.data = ...""",
        """CODE.chart_data = pd.DataFrame(
    np.random.randn(20, 3),
    columns=['a', 'b', 'c'])
st.line_chart(chart_data)
""",
        """CODE.chart_data = pd.DataFrame(
...     np.random.randn(50, 3),
...     columns=["a", "b", "c"])
...
st.bar_chart(chart_data)""",
        """CODE.df = pd.DataFrame(
...    np.random.randn(10, 5),
...    columns=('col %d' % i for i in range(5)))
...
st.table(df)""",
        """CODE.df1 = pd.DataFrame(
    np.random.randn(50, 20),
    columns=('col %d' % i for i in range(20)))

my_table = st.table(df1)

df2 = pd.DataFrame(
    np.random.randn(50, 20),
    columns=('col %d' % i for i in range(20)))

my_table.add_rows(df2)

my_chart = st.line_chart(df1)
my_chart.add_rows(df2)

my_chart = st.vega_lite_chart({
    'mark': 'line',
    'encoding': {'x': 'a', 'y': 'b'},
    'datasets': {
        'some_fancy_name': df1,  # <-- named dataset
    },
    'data': {'name': 'some_fancy_name'},
}),
my_chart.add_rows(some_fancy_name=df2)  # <-- name used as keyword
""",
        """CODE.df1 = pd.DataFrame(
   np.random.randn(50, 20),
   columns=('col %d' % i for i in range(20)))

my_table = st.table(df1)

df2 = pd.DataFrame(
   np.random.randn(50, 20),
   columns=('col %d' % i for i in range(20)))

my_table.add_rows(df2)

my_chart = st.line_chart(df1)
my_chart.add_rows(df2)

my_chart = st.vega_lite_chart({
    'mark': 'line',
    'encoding': {'x': 'a', 'y': 'b'},
    'datasets': {
      'some_fancy_name': df1,  # <-- named dataset
     },
    'data': {'name': 'some_fancy_name'},
}),
my_chart.add_rows(some_fancy_name=df2)  # <-- name used as keyword
""",
        """CODE.import pandas as pd
import numpy as np

df = pd.DataFrame(
    np.random.randn(200, 3),
    columns=['a', 'b', 'c'])

st.vega_lite_chart(df, {
    'mark': {'type': 'circle', 'tooltip': True},
    'encoding': {
        'x': {'field': 'a', 'type': 'quantitative'},
        'y': {'field': 'b', 'type': 'quantitative'},
        'size': {'field': 'c', 'type': 'quantitative'},
        'color': {'field': 'c', 'type': 'quantitative'},
    },
})""",
        """CODE.import pandas as pd
import numpy as np
import altair as alt

df = pd.DataFrame(
    np.random.randn(200, 3),
    columns=['a', 'b', 'c'])

c = alt.Chart(df).mark_circle().encode(
    x='a', y='b', size='c', color='c', tooltip=['a', 'b', 'c'])

st.altair_chart(c, use_container_width=True)""",
        "CODE.st.area_chart(chart_data)" .

<DEPENDENCY.streamlit==0.85.0> <CONTAINS> """CODE.chart_data = pd.DataFrame(
...     np.random.randn(20, 3),
...     columns=['a', 'b', 'c'])
...
st.area_chart(chart_data)""",
        """CODE.chart_data = pd.DataFrame(
...     np.random.randn(50, 3),
...     columns=["a", "b", "c"])
...
st.bar_chart(chart_data)""",
        """CODE.df = pd.DataFrame(
    np.random.randn(10, 5),
    columns=('col %d' % i for i in range(5)))
st.table(df)
""",
        """CODE.df = pd.DataFrame(
...    np.random.randn(10, 5),
...    columns=("col %d" % i for i in range(5)))
...
st._arrow_table(df)""",
        """CODE.df1 = pd.DataFrame(
    np.random.randn(50, 20),
    columns=('col %d' % i for i in range(20)))

my_table = st._legacy_table(df1)

df2 = pd.DataFrame(
    np.random.randn(50, 20),
    columns=('col %d' % i for i in range(20)))

my_table._legacy_add_rows(df2)

my_chart = st._legacy_line_chart(df1)
my_chart._legacy_add_rows(df2)

my_chart = st._legacy_vega_lite_chart({
    'mark': 'line',
    'encoding': {'x': 'a', 'y': 'b'},
    'datasets': {
        'some_fancy_name': df1,  # <-- named dataset
    },
    'data': {'name': 'some_fancy_name'},
}),
my_chart._legacy_add_rows(some_fancy_name=df2)  # <-- name used as keyword
""",
        """CODE.df1 = pd.DataFrame(
   np.random.randn(50, 20),
   columns=('col %d' % i for i in range(20)))

df2 = pd.DataFrame(
   np.random.randn(50, 20),
   columns=('col %d' % i for i in range(20)))

my_table._arrow_add_rows(df2)

my_chart = st._arrow_line_chart(df1)
my_chart._arrow_add_rows(df2)

my_chart = st._arrow_vega_lite_chart({
    'mark': 'line',
    'encoding': {'x': 'a', 'y': 'b'},
    'datasets': {
      'some_fancy_name': df1,  # <-- named dataset
     },
    'data': {'name': 'some_fancy_name'},
}),
my_chart._arrow_add_rows(some_fancy_name=df2)  # <-- name used as keyword""",
        """CODE.df1 = pd.DataFrame(
   np.random.randn(50, 20),
   columns=('col %d' % i for i in range(20)))

my_table = st._arrow_table(df1)

df2 = pd.DataFrame(
   np.random.randn(50, 20),
   columns=('col %d' % i for i in range(20)))

my_table._arrow_add_rows(df2)

my_chart = st._arrow_line_chart(df1)
my_chart._arrow_add_rows(df2)

my_chart = st._arrow_vega_lite_chart({
    'mark': 'line',
    'encoding': {'x': 'a', 'y': 'b'},
    'datasets': {
        'some_fancy_name': df1,  # <-- named dataset
    },
    'data': {'name': 'some_fancy_name'},
}),
my_chart._arrow_add_rows(some_fancy_name=df2)  # <-- name used as keyword
""",
        """CODE.df1 = pd.DataFrame(
   np.random.randn(50, 20),
   columns=('col %d' % i for i in range(20)))

my_table = st._legacy_table(df1)

df2 = pd.DataFrame(
   np.random.randn(50, 20),
   columns=('col %d' % i for i in range(20)))

my_table._legacy_add_rows(df2)

my_chart = st._legacy_line_chart(df1)
my_chart._legacy_add_rows(df2)

my_chart = st._legacy_vega_lite_chart({
    'mark': 'line',
    'encoding': {'x': 'a', 'y': 'b'},
    'datasets': {
        'some_fancy_name': df1,  # <-- named dataset
    },
    'data': {'name': 'some_fancy_name'},
}),
my_chart._legacy_add_rows(some_fancy_name=df2)  # <-- name used as keyword
""",
        """CODE.df1 = pd.DataFrame(
   np.random.randn(50, 20),
   columns=('col %d' % i for i in range(20)))
my_table = st.table(df1)

df2 = pd.DataFrame(
   np.random.randn(50, 20),
   columns=('col %d' % i for i in range(20)))
my_table.add_rows(df2)

my_chart = st.line_chart(df1)
my_chart.add_rows(df2)

my_chart = st.vega_lite_chart({
    'mark': 'line',
    'encoding': {'x': 'a', 'y': 'b'},
    'datasets': {
      'some_fancy_name': df1,  # <-- named dataset
     },
    'data': {'name': 'some_fancy_name'},
}),
my_chart.add_rows(some_fancy_name=df2)  # <-- name used as keyword
""",
        """CODE.import pandas as pd
import numpy as np

df = pd.DataFrame(
    np.random.randn(200, 3),
    columns=['a', 'b', 'c'])

st._arrow_vega_lite_chart(df, {
    'mark': {'type': 'circle', 'tooltip': True},
    'encoding': {
        'x': {'field': 'a', 'type': 'quantitative'},
        'y': {'field': 'b', 'type': 'quantitative'},
        'size': {'field': 'c', 'type': 'quantitative'},
        'color': {'field': 'c', 'type': 'quantitative'},
    },
})""",
        """CODE.import pandas as pd
import numpy as np

df = pd.DataFrame(
    np.random.randn(200, 3),
    columns=['a', 'b', 'c'])

st._legacy_vega_lite_chart(df, {
    'mark': {'type': 'circle', 'tooltip': True},
    'encoding': {
        'x': {'field': 'a', 'type': 'quantitative'},
        'y': {'field': 'b', 'type': 'quantitative'},
        'size': {'field': 'c', 'type': 'quantitative'},
        'color': {'field': 'c', 'type': 'quantitative'},
    },
})""",
        """CODE.import pandas as pd
import numpy as np

df = pd.DataFrame(
    np.random.randn(200, 3),
    columns=['a', 'b', 'c'])

st.vega_lite_chart(df, {
    'mark': {'type': 'circle', 'tooltip': True},
    'encoding': {
        'x': {'field': 'a', 'type': 'quantitative'},
        'y': {'field': 'b', 'type': 'quantitative'},
        'size': {'field': 'c', 'type': 'quantitative'},
        'color': {'field': 'c', 'type': 'quantitative'},
    },
})""",
        """CODE.import pandas as pd
import numpy as np
import altair as alt

df = pd.DataFrame(
    np.random.randn(200, 3),
    columns=['a', 'b', 'c'])

c = alt.Chart(df).mark_circle().encode(
    x='a', y='b', size='c', color='c', tooltip=['a', 'b', 'c'])

st._arrow_altair_chart(c, use_container_width=True)""",
        """CODE.import pandas as pd
import numpy as np
import altair as alt

df = pd.DataFrame(
    np.random.randn(200, 3),
    columns=['a', 'b', 'c'])

c = alt.Chart(df).mark_circle().encode(
    x='a', y='b', size='c', color='c', tooltip=['a', 'b', 'c'])

st._legacy_altair_chart(c, use_container_width=True)""",
        """CODE.import pandas as pd
import numpy as np
import altair as alt

df = pd.DataFrame(
    np.random.randn(200, 3),
    columns=['a', 'b', 'c'])

c = alt.Chart(df).mark_circle().encode(
    x='a', y='b', size='c', color='c', tooltip=['a', 'b', 'c'])

st.altair_chart(c, use_container_width=True)""",
        "CODE.st._arrow_area_chart(chart_data)",
        """CODE.st._arrow_dataframe(df)
st._arrow_dataframe(df, 200, 100)
st._arrow_dataframe(df.style.highlight_max(axis=0))""",
        "CODE.st._legacy_area_chart(chart_data)" .

<DEPENDENCY.streamlit==0.85.1> <CONTAINS> """CODE.col1, col2, col3 = st.beta_columns(3)

with col1:
    st.header("A cat")
    st.image("https://static.streamlit.io/examples/cat.jpg")

with col2:
    st.header("A dog")
    st.image("https://static.streamlit.io/examples/dog.jpg")

with col3:
    st.header("An owl")
    st.image("https://static.streamlit.io/examples/owl.jpg")

col1, col2 = st.beta_columns([3, 1])
data = np.random.randn(10, 1)

col1.subheader("A wide column with a chart")
col1.line_chart(data)

col2.subheader("A narrow column with the data")
col2.write(data)
""",
        """CODE.with st.beta_expander("See explanation"):
    st.write(\"\"\"
        The chart above shows some numbers I picked for you.
        I rolled actual dice for these, so they're *guaranteed* to
        be random.
    \"\"\")
    st.image("https://static.streamlit.io/examples/dice.jpg")""" .

<DEPENDENCY.streamlit==0.86.0> <CONTAINS> """CODE.col1, col2, col3 = st.columns(3)

with col1:
    st.header("A cat")
    st.image("https://static.streamlit.io/examples/cat.jpg")

with col2:
    st.header("A dog")
    st.image("https://static.streamlit.io/examples/dog.jpg")

with col3:
    st.header("An owl")
    st.image("https://static.streamlit.io/examples/owl.jpg")

col1, col2 = st.columns([3, 1])
data = np.random.randn(10, 1)

col1.subheader("A wide column with a chart")
col1.line_chart(data)

col2.subheader("A narrow column with the data")
col2.write(data)
""",
        """CODE.with st.expander("See explanation"):
    st.write(\"\"\"
        The chart above shows some numbers I picked for you.
        I rolled actual dice for these, so they're *guaranteed* to
        be random.
    \"\"\")
    st.image("https://static.streamlit.io/examples/dice.jpg")""" .

<DEPENDENCY.streamlit==0.88.0> <CONTAINS> """CODE.@st.cache
... def convert_df(df):
...   # Cache the conversion to prevent computation on every rerun
...       return df.to_csv().encode('utf-8')
csv = convert_df(my_large_df)
st.download_button(
...     label="Press to Download",
...     data=csv,
...     file_name='large_df.csv',
...     mime='text/csv',
... )

text_contents = '''
... Col1, Col2
... 123, 456
... 789, 000
... '''
st.download_button(
...     label='Download CSV', data=text_contents,
...     file_name='file.csv', mime='text/csv'
... )

binary_contents = b'example content'
... # Defaults to 'application/octet-stream'
st.download_button('Download binary file', binary_contents)

with open("flower.png", "rb") as file:
...     btn = st.download_button(
...             label="Download Image",
...             data=file,
...             file_name="flower.png",
...             mime="image/png"
...           )""" .

<DEPENDENCY.streamlit==1.12.0> <CONTAINS> "CODE.st.header('This is a header')",
        "CODE.st.subheader('This is a subheader')",
        "CODE.st.title('This is a title')" .

<DEPENDENCY.streamlit==1.12.1> <CONTAINS> "CODE.st.header('This is a header')",
        "CODE.st.subheader('This is a subheader')",
        "CODE.st.title('This is a title')" .

<DEPENDENCY.streamlit==1.13.0> <CONTAINS> "CODE.st.experimental_get_query_params()",
        """CODE.st.experimental_get_query_params()
{"show_map": ["True"], "selected": ["asia", "america"]}""",
        """CODE.st.experimental_set_query_params(
    show_map=True,
    selected=["asia", "america"],
)""" .

<DEPENDENCY.streamlit==1.14.0> <CONTAINS> """CODE.@st.gather_metrics
def my_command(url):
    return url

@st.gather_metrics(name="custom_name")
def my_command(url):
    return url""" .

<DEPENDENCY.streamlit==1.18.0> <CONTAINS> """CODE.import streamlit as st
import pandas as pd

df = pd.DataFrame(
    [
       {"command": "st.selectbox", "rating": 4, "is_widget": True},
       {"command": "st.balloons", "rating": 5, "is_widget": False},
       {"command": "st.time_input", "rating": 3, "is_widget": True},
   ]
)
edited_df = st.experimental_data_editor(df)

favorite_command = edited_df.loc[edited_df["rating"].idxmax()]["command"]
st.markdown(f"Your favorite command is **{favorite_command}** ð")

You can also allow the user to add and delete rows by setting `num_rows` to "dynamic":

import streamlit as st
import pandas as pd

df = pd.DataFrame(
    [
       {"command": "st.selectbox", "rating": 4, "is_widget": True},
       {"command": "st.balloons", "rating": 5, "is_widget": False},
       {"command": "st.time_input", "rating": 3, "is_widget": True},
   ]
)
edited_df = st.experimental_data_editor(df, num_rows="dynamic")

favorite_command = edited_df.loc[edited_df["rating"].idxmax()]["command"]
st.markdown(f"Your favorite command is **{favorite_command}** ð")""" .

<DEPENDENCY.streamlit==1.21.0> <CONTAINS> """CODE.import streamlit as st

st.divider()""" .

<DEPENDENCY.streamlit==1.22.0> <CONTAINS> """CODE.import streamlit as st

conn = st.experimental_connection("my_conn")

if not conn.is_healthy():
    conn.reset()""",
        """CODE.import streamlit as st

conn = st.experimental_connection("snowpark")
with conn.safe_session() as session:
    df = session.table("mytable").limit(10).to_pandas()

st.dataframe(df)""",
        """CODE.import streamlit as st

conn = st.experimental_connection("sql")
df = conn.query("select * from pet_owners where owner = :owner", ttl=3600, params={"owner":"barbara"})
st.dataframe(df)""",
        """CODE.import streamlit as st

session = st.experimental_connection("snowpark").session
df = session.table("mytable").limit(10).to_pandas()
st.dataframe(df)""",
        """CODE.import streamlit as st
conn = st.experimental_connection("snowpark")
df = conn.query("select * from pet_owners")
st.dataframe(df)""",
        """CODE.with conn.session as session:
    session.execute("INSERT INTO numbers (val) VALUES (:n);", {"n": n})
    session.commit()""" .

<DEPENDENCY.streamlit==1.23.0> <CONTAINS> """CODE.import pandas as pd
import streamlit as st

data_df = pd.DataFrame(
    {
        "apps": [
            "https://roadmap.streamlit.app",
            "https://extras.streamlit.app",
            "https://issues.streamlit.app",
            "https://30days.streamlit.app",
        ],
    }
)

st.data_editor(
    data_df,
    column_config={
        "apps": st.column_config.LinkColumn(
            "Trending apps",
            help="The top trending Streamlit apps",
            validate="^https://[a-z]+\\.streamlit\\.app$",
            max_chars=100,
        )
    },
    hide_index=True,
)
""",
        """CODE.import pandas as pd
import streamlit as st

data_df = pd.DataFrame(
    {
        "widgets": ["st.selectbox", "st.number_input", "st.text_area", "st.button"],
    }
)

st.data_editor(
    data_df,
    column_config={
        "widgets": st.column_config.Column(
            "Streamlit Widgets",
            help="Streamlit **widget** commands ð",
            width="medium",
            required=True,
        )
    },
    hide_index=True,
    num_rows="dynamic",
)
""",
        """CODE.import pandas as pd
import streamlit as st

data_df = pd.DataFrame(
    {
        "widgets": ["st.selectbox", "st.number_input", "st.text_area", "st.button"],
    }
)

st.data_editor(
    data_df,
    column_config={
        "widgets": st.column_config.TextColumn(
            "Widgets",
            help="Streamlit **widget** commands ð",
            default="st.",
            max_chars=50,
            validate="^st\\.[a-z_]+$",
        )
    },
    hide_index=True,
)""",
        """CODE.st.data_editor(
    data_df,
    column_config={
        "price": st.column_config.NumberColumn(
            "Price (in USD)",
            help="The price of the product in USD",
            min_value=0,
            max_value=1000,
            step=1,
            format="$%d",
        )
    },
    hide_index=True,
)""" .

<DEPENDENCY.streamlit==1.24.0> <CONTAINS> """CODE.import streamlit as st

prompt = st.chat_input("Say something")
if prompt:
    st.write(f"User has sent the following prompt: {prompt}")""",
        """CODE.import streamlit as st
import numpy as np

with st.chat_message("user"):
    st.write("Hello ð")
    st.line_chart(np.random.randn(30, 3))

import streamlit as st
import numpy as np

message = st.chat_message("assistant")
message.write("Hello human")
message.bar_chart(np.random.randn(30, 3))
""" .

<DEPENDENCY.streamlit==1.25.0> <CONTAINS> """CODE.import streamlit as st

st.toast('Your edited image was saved!', icon='ð')""" .

<DEPENDENCY.streamlit==1.27.2> <CONTAINS> """CODE.import streamlit as st
import pandas as pd
import numpy as np

chart_data = pd.DataFrame(
...     np.random.randn(20, 3),
...     columns=['a', 'b', 'c'])
...
st._arrow_scatter_chart(chart_data)

chart_data = pd.DataFrame(
...     np.random.randn(20, 4),
...     columns=['col1', 'col2', 'col3', 'col4'])
...
st._arrow_scatter_chart(
...     chart_data,
...     x='col1',
...     y='col2',
...     color='col3',
...     size='col4',
... )

st._arrow_scatter_chart(
...     chart_data,
...     x='col1',
...     y=['col2', 'col3'],
...     size='col4',
...     color=['#FF0000', '#0000FF'],  # Optional
... )""",
        """CODE.import streamlit as st
import pandas as pd
import numpy as np

df = pd.DataFrame(
    np.random.randn(200, 3),
    columns=['a', 'b', 'c'])

st._arrow_vega_lite_chart(df, {
    'mark': {'type': 'circle', 'tooltip': True},
    'encoding': {
        'x': {'field': 'a', 'type': 'quantitative'},
        'y': {'field': 'b', 'type': 'quantitative'},
        'size': {'field': 'c', 'type': 'quantitative'},
        'color': {'field': 'c', 'type': 'quantitative'},
    },
})""",
        """CODE.import streamlit as st
import pandas as pd
import numpy as np

df = pd.DataFrame(
   np.random.randn(10, 5),
   columns=("col %d" % i for i in range(5)))

st._arrow_table(df)""",
        """CODE.import streamlit as st
import pandas as pd
import numpy as np

df = pd.DataFrame(
...    np.random.randn(50, 20),
...    columns=('col %d' % i for i in range(20)))
...
st._arrow_dataframe(df)

st._arrow_dataframe(df, 200, 100)

You can also pass a Pandas Styler object to change the style of
the rendered DataFrame:

df = pd.DataFrame(
...    np.random.randn(10, 20),
...    columns=('col %d' % i for i in range(20)))
...
st._arrow_dataframe(df.style.highlight_max(axis=0))""",
        """CODE.import streamlit as st
import pandas as pd
import numpy as np
import altair as alt

df = pd.DataFrame(
    np.random.randn(200, 3),
    columns=['a', 'b', 'c'])

c = alt.Chart(df).mark_circle().encode(
    x='a', y='b', size='c', color='c', tooltip=['a', 'b', 'c'])

st._arrow_altair_chart(c, use_container_width=True)""" .

<DEPENDENCY.streamlit==1.28.0> <CONTAINS> """CODE.import streamlit as st

conn = st.connection("my_conn")

if not conn.is_healthy():
    conn.reset()""",
        """CODE.import streamlit as st
import pandas as pd
import numpy as np

chart_data = pd.DataFrame(np.random.randn(20, 3), columns=["a", "b", "c"])

st.scatter_chart(chart_data)

import streamlit as st
import pandas as pd
import numpy as np

chart_data = pd.DataFrame(np.random.randn(20, 3), columns=["col1", "col2", "col3"])
chart_data['col4'] = np.random.choice(['A','B','C'], 20)

st.scatter_chart(
...     chart_data,
...     x='col1',
...     y='col2',
...     color='col4',
...     size='col3',
... )

import streamlit as st
import pandas as pd
import numpy as np

chart_data = pd.DataFrame(np.random.randn(20, 4), columns=["col1", "col2", "col3", "col4"])

st.scatter_chart(
...     chart_data,
...     x='col1',
...     y=['col2', 'col3'],
...     size='col4',
...     color=['#FF0000', '#0000FF'],  # Optional
... )""",
        """CODE.import streamlit as st
import pandas as pd
import numpy as np

chart_data = pd.DataFrame(np.random.randn(20, 3), columns=["a", "b", "c"])

st.scatter_chart(chart_data)

import streamlit as st
import pandas as pd
import numpy as np

chart_data = pd.DataFrame(np.random.randn(20, 3), columns=["col1", "col2", "col3"])
chart_data['col4'] = np.random.choice(['A','B','C'], 20)

st.scatter_chart(
...     chart_data,
...     x='col1',
...     y='col2',
...     color='col4',
...     size='col3',
... )

import streamlit as st
import pandas as pd
import numpy as np

chart_data = pd.DataFrame(np.random.randn(20, 4), columns=["col1", "col2", "col3", "col4"])

st.scatter_chart(
...     chart_data,
...     x='col1',
...     y=['col2', 'col3'],
...     size='col4',
...     color=['#FF0000', '#0000FF'],  # Optional
... )
""",
        """CODE.import streamlit as st
import pandas as pd
import numpy as np

chart_data = pd.DataFrame(np.random.randn(200, 3), columns=["a", "b", "c"])

st.vega_lite_chart(
    chart_data,
    {
        "mark": {"type": "circle", "tooltip": True},
        "encoding": {
            "x": {"field": "a", "type": "quantitative"},
            "y": {"field": "b", "type": "quantitative"},
            "size": {"field": "c", "type": "quantitative"},
            "color": {"field": "c", "type": "quantitative"},
        },
    },
)""",
        """CODE.import streamlit as st
import pandas as pd
import numpy as np

df = pd.DataFrame(np.random.randn(10, 5), columns=("col %d" % i for i in range(5)))

st.table(df)""",
        """CODE.import streamlit as st
import pandas as pd
import numpy as np

df = pd.DataFrame(np.random.randn(50, 20), columns=("col %d" % i for i in range(20)))

st.dataframe(df)  # Same as st.write(df)

df = pd.DataFrame(np.random.randn(10, 20), columns=("col %d" % i for i in range(20)))

st.dataframe(df.style.highlight_max(axis=0))

df = pd.DataFrame(
    {
        "name": ["Roadmap", "Extras", "Issues"],
        "url": ["https://roadmap.streamlit.app", "https://extras.streamlit.app", "https://issues.streamlit.app"],
        "stars": [random.randint(0, 1000) for _ in range(3)],
        "views_history": [[random.randint(0, 5000) for _ in range(30)] for _ in range(3)],
    }
)
st.dataframe(
    df,
    column_config={
        "name": "App name",
        "stars": st.column_config.NumberColumn(
            "Github Stars",
            help="Number of stars on GitHub",
            format="%d â­",
        ),
        "url": st.column_config.LinkColumn("App URL"),
        "views_history": st.column_config.LineChartColumn(
            "Views (past 30 days)", y_min=0, y_max=5000
        ),
    },
    hide_index=True,
)""",
        """CODE.import streamlit as st
import pandas as pd
import numpy as np
import altair as alt

chart_data = pd.DataFrame(np.random.randn(20, 3), columns=["a", "b", "c"])

c = (
   alt.Chart(chart_data)
   .mark_circle()
   .encode(x="a", y="b", size="c", color="c", tooltip=["a", "b", "c"])
)

st.altair_chart(c, use_container_width=True)""" .

<DEPENDENCY.streamlit==1.30.0> <CONTAINS> """CODE.if st.button("Home"):
    st.switch_page("your_app.py")
if st.button("Page 1"):
    st.switch_page("pages/page_1.py")
if st.button("Page 2"):
    st.switch_page("pages/page_2.py")""" .

<DEPENDENCY.streamlit==1.4.0> <CONTAINS> """CODE.import streamlit as st

picture = st.camera_input("Take a picture")

if picture:
    st.image(picture)""" .

<DEPENDENCY.streamlit==1.7.0> <CONTAINS> "CODE.st.snow()" .

<DEPENDENCY.tensorboard==1.13.0> <CONTAINS> """CODE.DType(T)       .is_compatible_with(DType(T))        == True
DType(T)       .is_compatible_with(DType(T).as_ref) == True
DType(T).as_ref.is_compatible_with(DType(T))        == False
DType(T).as_ref.is_compatible_with(DType(T).as_ref) == True
""",
        """CODE.tf.Dimension(n)   .merge_with(tf.Dimension(n))    == tf.Dimension(n)
tf.Dimension(n)   .merge_with(tf.Dimension(None)) == tf.Dimension(n)
tf.Dimension(None).merge_with(tf.Dimension(n))    == tf.Dimension(n)
tf.Dimension(None).merge_with(tf.Dimension(None)) == tf.Dimension(None)
tf.Dimension(n)   .merge_with(tf.Dimension(m))  # raises ValueError for n != m
""" .

<DEPENDENCY.tensorboard==2.2.0> <CONTAINS> """CODE.class ExperimentalGraphsPlugin(
    graphs_plugin.GraphsPlugin,
    experimental_plugin.ExperimentalPlugin,
):
    pass

class ExperimentalDebuggerPluginLoader(
    debugger_plugin_loader.DebuggerPluginLoader,
    experimental_plugin.ExperimentalPlugin
):
    pass""" .

<DEPENDENCY.tensorboard==2.2.2> <CONTAINS> """CODE.@log_latency
def function_1():
    pass

@log_latency("custom_label")
def function_2():
    pass

def function_3():
    with log_latency("region_within_function"):
        pass""" .

<DEPENDENCY.tensorboard==2.3.0> <CONTAINS> """CODE.beholder_hook = BeholderHook(LOG_DIRECTORY)
with MonitoredSession(..., hooks=[beholder_hook]) as sess:
  sess.run(train_op)
""" .

<DEPENDENCY.tensorboardX==0.9> <CONTAINS> """CODE.import keyword
import torch
meta = []
while len(meta)<100:
    meta = meta+keyword.kwlist # get some strings
meta = meta[:100]

for i, v in enumerate(meta):
    meta[i] = v+str(i)

label_img = torch.rand(100, 3, 10, 32)
for i in range(100):
    label_img[i]*=i/100.0

writer.add_embedding(torch.randn(100, 5), metadata=meta, label_img=label_img)
writer.add_embedding(torch.randn(100, 5), label_img=label_img)
writer.add_embedding(torch.randn(100, 5), metadata=meta)""",
        """CODE.writer.add_scalars('run_14h',{'xsinx':i*np.sin(i/r),
                              'xcosx':i*np.cos(i/r),
                              'arctanx': numsteps*np.arctan(i/r)}, i)""",
        """CODE.writer.add_text('lstm', 'This is an lstm', 0)
writer.add_text('rnn', 'This is an rnn', 10)""" .

<DEPENDENCY.tensorboardX==1.5> <CONTAINS> """CODE.writer.add_scalars('run_14h',{'xsinx':i*np.sin(i/r),
                              'xcosx':i*np.cos(i/r),
                              'arctanx': numsteps*np.arctan(i/r)}, i)""" .

<DEPENDENCY.tensorboardX==1.7> <CONTAINS> """CODE.import numpy as np
dummy_data = []
for idx, value in enumerate(range(30)):
    dummy_data += [idx + 0.001] * value
values = np.array(dummy_data).astype(float).reshape(-1)
counts, limits = np.histogram(values)
sum_sq = values.dot(values)
with SummaryWriter() as summary_writer:
    summary_writer.add_histogram_raw(
            tag='hist_dummy_data',
            min=values.min(),
            max=values.max(),
            num=len(values),
            sum=values.sum(),
            sum_squares=sum_sq,
            bucket_limits=limits[1:].tolist(),
            bucket_counts=counts.tolist(),
            global_step=0)""" .

<DEPENDENCY.tensorboardX==1.8> <CONTAINS> """CODE.from tensorboardX import SummaryWriter
vertices_tensor = np.array([[
    [1, 1, 1],
    [-1, -1, 1],
    [1, -1, -1],
    [-1, 1, -1],
]], dtype=float)
colors_tensor = np.array([[
    [255, 0, 0],
    [0, 255, 0],
    [0, 0, 255],
    [255, 0, 255],
]], dtype=int)
faces_tensor = np.array([[
    [0, 2, 3],
    [0, 3, 1],
    [0, 1, 2],
    [1, 3, 2],
]], dtype=int)

writer = SummaryWriter()
writer.add_mesh('my_mesh', vertices=vertices_tensor, colors=colors_tensor, faces=faces_tensor)

writer.close()""" .

<DEPENDENCY.tensorboardX==2.1> <CONTAINS> """CODE.from tensorboardX import GlobalSummaryWriter
writer = GlobalSummaryWriter.getSummaryWriter()  # This creates a new instance.
writer.add_text('my_log', 'greeting from global1')
writer = GlobalSummaryWriter.getSummaryWriter()  # Get the instance in global1.py.
writer.add_text('my_log', 'greeting from global2')""" .

<DEPENDENCY.tensorflow==0.12.0> <CONTAINS> """CODE.  # See tf.contrib.learn.Estimator(...) for details on model_fn structure
  def my_model_fn(...):
    pass

  estimator = LogisticRegressor(model_fn=my_model_fn)

  # Input builders
  def input_fn_train:
    pass

  estimator.fit(input_fn=input_fn_train)
  estimator.predict(x=x)
""",
        """CODE.# Instantiates the `ScaleAndShift` bijector.
# This `Bijector` is initialized with `scale` and `shift` `Tensors`, giving
# the forward operation:
# Y = g(X) = matmul(scale, X) + shift

import tensorflow as tf

class ScaleAndShift(tf.Module):
    def __init__(self, shift, scale, event_ndims=0, validate_args=False, name=None):
        self.shift = shift
        self.scale = scale
        self.event_ndims = event_ndims
        self.validate_args = validate_args
        self.name = name

    def forward(self, X):
        return tf.matmul(self.scale, X) + self.shift

# Args:
shift = tf.constant(1.0)
scale = tf.constant([[2.0, 0.0], [0.0, 2.0]])
event_ndims = 1
validate_args = False
name = "scale_and_shift_bijector"

bijector = ScaleAndShift(shift, scale, event_ndims, validate_args, name)

# Args:
shift = tf.constant(0.0)
scale = tf.constant(2.0)
event_ndims = 0
validate_args = True
name = "another_scale_and_shift_bijector"

bijector2 = ScaleAndShift(shift, scale, event_ndims, validate_args, name)
""",
        """CODE.E[Y] = matmul(scale, E[X])
Cov[Y] = matmul(scale, matmul(Cov[X], scale, transpose_b=True)

mu = 0
sigma = 1
b = ScaleAndShift(shift=mu, scale=sigma)

mu = ...
sigma = ...
b = ScaleAndShift(shift=mu, scale=sigma)

mu = ...
sigma = ...
b = ScaleAndShift(shift=mu, scale=sigma, event_ndims=1)

mu = ...
sigma = ...
b = ScaleAndShift(shift=mu, scale=sigma, event_ndims=1)

mu = 1
sigma = [I, I]
b = ScaleAndShift(shift=mu, scale=sigma, event_ndims=1)
x = numpy.ones(S + sigma.shape)
b.forward(x)
""",
        """CODE.import numpy as np
import tensorflow as tf

value = [0, 1, 2, 3, 4, 5, 6, 7]
init = tf.constant_initializer(value)

print('fitting shape:')
tf.reset_default_graph()
with tf.Session():
  x = tf.get_variable('x', shape=[2, 4], initializer=init)
  x.initializer.run()
  print(x.eval())

print('larger shape:')
tf.reset_default_graph()
with tf.Session():
  x = tf.get_variable('x', shape=[3, 4], initializer=init)
  x.initializer.run()
  print(x.eval())

print('smaller shape:')
tf.reset_default_graph()
with tf.Session():
  x = tf.get_variable('x', shape=[2, 3], initializer=init)
""",
        """CODE.opt = GradientDescentOptimizer(learning_rate=0.1)
opt = tf.SyncReplicasOptimizerV2(opt, replicas_to_aggregate=50,
                                 total_num_replicas=50)
grads = opt.minimize(total_loss, global_step=self.global_step)
init_token_op = opt.get_init_tokens_op()
chief_queue_runner = opt.get_chief_queue_runner()

if is_chief:
  local_init_op = opt.chief_init_op
else:
  local_init_op = opt.local_step_init_op
ready_for_local_init_op = opt.ready_for_local_init_op
sv = tf.Supervisor(graph=g,
                   is_chief=is_chief,
                   local_init_op=local_init_op,
                   ready_for_local_init_op=ready_for_local_init_op,
                   saver=model.saver)

if is_chief and FLAGS.sync_replicas:
  sv.start_queue_runners(sess, [chief_queue_runner])
  sess.run(init_token_op)
""",
        """CODE.split0, split1, split2 = tf.split_v(1, [4, 15, 11], value)
tf.shape(split0) ==> [5, 4]
tf.shape(split1) ==> [5, 15]
tf.shape(split2) ==> [5, 11]
split0, split1, split2 = tf.split(value, 3, 1)
tf.shape(split0) ==> [5, 10]
""",
        """CODE.t1 = [[1, 2, 3], [4, 5, 6]]
t2 = [[7, 8, 9], [10, 11, 12]]
tf.concat_v2([t1, t2], 0) ==> [[1, 2, 3], [4, 5, 6], [7, 8, 9], [10, 11, 12]]
tf.concat_v2([t1, t2], 1) ==> [[1, 2, 3, 7, 8, 9], [4, 5, 6, 10, 11, 12]]

# tensor t3 with shape [2, 3]
# tensor t4 with shape [2, 3]
tf.shape(tf.concat_v2([t3, t4], 0)) ==> [4, 3]
tf.shape(tf.concat_v2([t3, t4], 1)) ==> [2, 6]
""",
        """CODE.with tf.variable_scope("foo"):
    v = tf.get_variable("v", [1])  # v.name == "foo/v:0"
    w = tf.get_variable("w", [1])  # w.name == "foo/w:0"
with tf.variable_scope("foo", reuse=True):
    v1 = tf.get_variable("v")  # The same as v above.
""" .

<DEPENDENCY.tensorflow==1.0.0> <CONTAINS> """CODE.  # See tf.contrib.learn.Estimator(...) for details on model_fn structure
  def my_model_fn(...):
    pass

  estimator = LogisticRegressor(model_fn=my_model_fn)

  # Input builders
  def input_fn_train:
    pass

  estimator.fit(input_fn=input_fn_train)
  estimator.predict(x=x)
""",
        """CODE.# Assume a BigQuery has the following schema,
#     name      STRING,
#     age       INT,
#     state     STRING

# Create the parse_examples list of features.
features = dict(
  name=tf.FixedLenFeature([1], tf.string),
  age=tf.FixedLenFeature([1], tf.int32),
  state=tf.FixedLenFeature([1], dtype=tf.string, default_value="UNK"))

# Create a Reader.
reader = bigquery_reader_ops.BigQueryReader(project_id=PROJECT,
                                            dataset_id=DATASET,
                                            table_id=TABLE,
                                            timestamp_millis=TIME,
                                            num_partitions=NUM_PARTITIONS,
                                            features=features)

# Populate a queue with the BigQuery Table partitions.
queue = tf.training.string_input_producer(reader.partitions())

# Read and parse examples.
row_id, examples_serialized = reader.read(queue)
examples = tf.parse_example(examples_serialized, features=features)

# Process the Tensors examples["name"], examples["age"], etc...
""",
        """CODE.# Create a 2 x 2 diagonal linear operator.
diag = [1., -1.]
operator = LinearOperatorDiag(diag)

operator.to_dense()
==> [[1.,  0.]
     [0., -1.]]

operator.shape
==> [2, 2]

operator.log_determinant()
==> scalar Tensor

x = ... Shape [2, 4] Tensor
operator.apply(x)
==> Shape [2, 4] Tensor

# Create a [2, 3] batch of 4 x 4 linear operators.
diag = tf.random_normal(shape=[2, 3, 4])
operator = LinearOperatorDiag(diag)

# Create a shape [2, 1, 4, 2] vector.  Note that this shape is compatible
# since the batch dimensions, [2, 1], are brodcast to
# operator.batch_shape = [2, 3].
y = tf.random_normal(shape=[2, 1, 4, 2])
x = operator.solve(y)
==> operator.apply(x) = y
""",
        """CODE.# Create a 2 x 2 identity matrix.
operator = LinearOperatorIdentity(num_rows=2, dtype=tf.float32)

operator.to_dense()
==> [[1., 0.]
     [0., 1.]]

operator.shape
==> [2, 2]

operator.log_determinant()
==> 0.

x = ... Shape [2, 4] Tensor
operator.apply(x)
==> Shape [2, 4] Tensor, same as x.

y = tf.random_normal(shape=[3, 2, 4])
# Note that y.shape is compatible with operator.shape because operator.shape
# is broadcast to [3, 2, 2].
# This broadcast does NOT require copying data, since we can infer that y
# will be passed through without changing shape.  We are always able to infer
# this if the operator has no batch_shape.
x = operator.solve(y)
==> Shape [3, 2, 4] Tensor, same as y.

# Create a 2-batch of 2x2 identity matrices
operator = LinearOperatorIdentity(num_rows=2, batch_shape=[2])
operator.to_dense()
==> [[[1., 0.]
      [0., 1.]],
     [[1., 0.]
      [0., 1.]]]

# Here, even though the operator has a batch shape, the input is the same as
# the output, so x can be passed through without a copy.  The operator is able
# to detect that no broadcast is necessary because both x and the operator
# have statically defined shape.
x = ... Shape [2, 2, 3]
operator.apply(x)
==> Shape [2, 2, 3] Tensor, same as x

# Here the operator and x have different batch_shape, and are broadcast.
# This requires a copy, since the output is different size than the input.
x = ... Shape [1, 2, 3]
operator.apply(x)
==> Shape [2, 2, 3] Tensor, equal to [x, x]
""",
        """CODE.# Create a 2 x 2 linear operator composed of two 2 x 2 operators.
operator_1 = LinearOperatorMatrix([[1., 2.], [3., 4.]])
operator_2 = LinearOperatorMatrix([[1., 0.], [0., 1.]])
operator = LinearOperatorComposition([operator_1, operator_2])

operator.to_dense()
==> [[1., 2.]
     [3., 4.]]

operator.shape
==> [2, 2]

operator.log_determinant()
==> scalar Tensor

x = ... Shape [2, 4] Tensor
operator.apply(x)
==> Shape [2, 4] Tensor

# Create a [2, 3] batch of 4 x 5 linear operators.
matrix_45 = tf.random_normal(shape=[2, 3, 4, 5])
operator_45 = LinearOperatorMatrix(matrix)

# Create a [2, 3] batch of 5 x 6 linear operators.
matrix_56 = tf.random_normal(shape=[2, 3, 5, 6])
operator_56 = LinearOperatorMatrix(matrix_56)

# Compose to create a [2, 3] batch of 4 x 6 operators.
opeartor_46 = LinearOperatorComposition([operator_45, operator_56])

# Create a shape [2, 3, 6, 2] vector.
x = tf.random_normal(shape=[2, 3, 6, 2])
operator.apply(x)
==> Shape [2, 3, 4, 2] Tensor
""",
        """CODE.# Create a 2 x 2 linear operator.
matrix = [[1., 2.], [3., 4.]]
operator = LinearOperatorMatrix(matrix)

operator.to_dense()
==> [[1., 2.]
     [3., 4.]]

operator.shape
==> [2, 2]

operator.log_determinant()
==> scalar Tensor

x = ... Shape [2, 4] Tensor
operator.apply(x)
==> Shape [2, 4] Tensor

# Create a [2, 3] batch of 4 x 4 linear operators.
matrix = tf.random_normal(shape=[2, 3, 4, 4])
operator = LinearOperatorMatrix(matrix)
""",
        """CODE.# Create a 2 x 2 lower-triangular linear operator.
tril = [[1., 2.], [3., 4.]]
operator = LinearOperatorTriL(tril)

# The upper triangle is ignored.
operator.to_dense()
==> [[1., 0.]
     [3., 4.]]

operator.shape
==> [2, 2]

operator.log_determinant()
==> scalar Tensor

x = ... Shape [2, 4] Tensor
operator.apply(x)
==> Shape [2, 4] Tensor

# Create a [2, 3] batch of 4 x 4 linear operators.
tril = tf.random_normal(shape=[2, 3, 4, 4])
operator = LinearOperatorTriL(tril)
""",
        """CODE.# Create an operator acting like a 10 x 2 x 2 matrix.
operator = LinearOperator(...)
operator.shape # = 10 x 2 x 2

# Solve one linear system (R = 1) for every member of the length 10 batch.
RHS = ... # shape 10 x 2 x 1
X = operator.solve(RHS)  # shape 10 x 2 x 1

# Solve five linear systems (R = 5) for every member of the length 10 batch.
RHS = ... # shape 10 x 2 x 5
X = operator.solve(RHS)
X[3, :, 2]  # Solution to the linear system A[3, :, :] X = RHS[3, :, 2]
""",
        """CODE.age = np.arange(4) * 1.0
height = np.arange(32, 36)
x = {'age': age, 'height': height}
y = np.arange(-32, -28)

with tf.Session() as session:
  input_fn = numpy_io.numpy_input_fn(
      x, y, batch_size=2, shuffle=False, num_epochs=1)
""",
        """CODE.features = tf.constant(["emerson", "lake", "and", "palmer"])
table = tf.contrib.lookup.string_to_index_table_from_file(
    vocabulary_file="test.txt", num_oov_buckets=1)
ids = table.lookup(features)
...
tf.tables_initializer().run()
""",
        """CODE.linalg = tf.contrib.linalg

x = [1., 2, 3]

shift = [-1., 0., 1]
diag = [1., 2, 3]
scale = linalg.LinearOperatorDiag(diag)
affine = AffineLinearOperator(shift, scale)
# In this case, `forward` is equivalent to:
# diag * scale + shift
y = affine.forward(x)  # [0., 4, 10]

shift = [2., 3, 1]
tril = [[1., 0, 0],
        [2, 1, 0],
        [3, 2, 1]]
scale = linalg.LinearOperatorTriL(tril)
affine = AffineLinearOperator(shift, scale)
# In this case, `forward` is equivalent to:
# np.squeeze(np.matmul(tril, np.expand_dims(x, -1)), -1) + shift
y = affine.forward(x)  # [3., 7, 11]
""",
        """CODE.mapping_strings = t.constant(["emerson", "lake", "palmer")
table = tf.contrib.lookup.string_to_index_table_from_tensor(
    mapping=mapping_strings, num_oov_buckets=1, default_value=-1)
features = tf.constant(["emerson", "lake", "and", "palmer"])
ids = table.lookup(features)
...
tf.tables_initializer().run()

ids.eval()  ==> [0, 1, 4, 2]
""",
        """CODE.num_oov_buckets = 3
input_tensor = tf.constant(["emerson", "lake", "palmer", "king", "crimnson"])
table = tf.IdTableWithHashBuckets(
    tf.HashTable(tf.TextFileIdTableInitializer(filename), default_value),
    num_oov_buckets)
out = table.lookup(input_tensor)
table.init.run()
print out.eval()
""",
        """CODE.saver_hook = CheckpointSaverHook(...)
summary_hook = SummaryHook(...)
with SingularMonitoredSession(hooks=[saver_hook, summary_hook]) as sess:
  while not sess.should_stop():
    sess.run(train_op)
""",
        """CODE.scale = (
  scale_identity_multiplier * tf.diag(tf.ones(d)) +
  tf.diag(scale_diag) +
  scale_tril +
  scale_perturb_factor @ diag(scale_perturb_diag) @
    tf.transpose([scale_perturb_factor])
)

b = Affine()

b = Affine(shift=[1., 2, 3])

b = Affine(shift=[1., 2, 3],
           scale_identity_multiplier=2.)

b = Affine(shift=[1., 2, 3],
           scale_diag=[-1., 2, 1])

b = Affine(shift=[1., 2, 3],
           scale_perturb_factor=[[1., 0],
                                 [0, 1],
                                 [1, 1]])

b = Affine(shift=[1., 2, 3],
           scale_diag=[1., 3, 3],
           scale_perturb_diag=[2., 1],
           scale_perturb_factor=[[1., 0],
                                 [0, 1],
                                 [1, 1]])
""",
        "CODE.tf.parallel_stack([x, y, z]) = np.asarray([x, y, z])",
        """CODE.v = tf.Variable([1, 2])
init = tf.global_variables_initializer()

with tf.Session() as sess:
    sess.run(init)
    # Usage passing the session explicitly.
    v.load([2, 3], sess)
    print(v.eval(sess)) # prints [2 3]
    # Usage with the default session.  The 'with' block
    # above makes 'sess' the default session.
    v.load([3, 4], sess)
    print(v.eval()) # prints [3 4]
""",
        """CODE.variables = tf.contrib.framework.get_model_variables()
conv_weight_variables = tf.contrib.framework.filter_variables(
    variables,
    include_patterns=['Conv'],
    exclude_patterns=['biases', 'Logits'])
""",
        """CODE.with tf.control_dependencies([tf.assert_rank_in(x, (2, 4))]):
  output = tf.reduce_sum(x)
""" .

<DEPENDENCY.tensorflow==1.0.1> <CONTAINS> """CODE.features = tf.constant(["emerson", "lake", "and", "palmer"])
table = tf.contrib.lookup.string_to_index_table_from_file(
    vocabulary_file="test.txt", num_oov_buckets=1)
ids = table.lookup(features)
...
tf.tables_initializer().run()
""",
        """CODE.mapping_strings = t.constant(["emerson", "lake", "palmer")
table = tf.contrib.lookup.string_to_index_table_from_tensor(
    mapping=mapping_strings, num_oov_buckets=1, default_value=-1)
features = tf.constant(["emerson", "lake", "and", "palmer"])
ids = table.lookup(features)
...
tf.tables_initializer().run()

ids.eval()  ==> [0, 1, 4, 2]
""" .

<DEPENDENCY.tensorflow==1.1.0> <CONTAINS> """CODE.    # dot product between tensors
    x = K.placeholder(shape=(2, 3))
    y = K.placeholder(shape=(3, 4))
    xy = K.dot(x, y)
    xy
    <tf.Tensor 'MatMul_9:0' shape=(2, 4) dtype=float32>


    # dot product between tensors
    x = K.placeholder(shape=(32, 28, 3))
    y = K.placeholder(shape=(3, 4))
    xy = K.dot(x, y)
    xy
    <tf.Tensor 'MatMul_9:0' shape=(32, 28, 4) dtype=float32>


    # Theano-like behavior example
    x = K.random_uniform_variable(shape=(2, 3), low=0, high=1)
    y = K.ones((4, 3, 5))
    xy = K.dot(x, y)
    K.int_shape(xy)
    (2, 4, 5)
""",
        """CODE.    def generate_arrays_from_file(path):
        while 1:
            f = open(path)
            for line in f:
                # create Numpy arrays of input data
                # and labels, from each line in the file
                x, y = process_line(line)
                yield (x, y)
            f.close()

    model.fit_generator(generate_arrays_from_file('/my_file.txt'),
                        samples_per_epoch=10000, epochs=10)
""",
        """CODE.    from keras import backend as K
    K.floatx()
    arr = numpy.array([1.0, 2.0], dtype='float64')
    arr.dtype
    new_arr = K.cast_to_floatx(arr)
    new_arr
    new_arr.dtype
""",
        """CODE.    from keras import backend as K
    K.set_epsilon(1e-05)
""",
        """CODE.    from keras import backend as K
    input = K.placeholder(shape=(2, 4, 5))
    val = np.array([[1, 2], [3, 4]])
    kvar = K.variable(value=val)
    K.ndim(input)
    K.ndim(kvar)
""",
        """CODE.    get_custom_objects().clear()
    get_custom_objects()["MyObject"] = MyObject
""",
        """CODE.    kvar = K.zeros((2,3))
    K.count_params(kvar)
    K.eval(kvar)
""",
        """CODE.    with CustomObjectScope({"MyObject":MyObject}):
        layer = Dense(..., W_regularizer="MyObject")
        # save, load, etc. will recognize custom object by name
""",
        """CODE.    with custom_object_scope({"MyObject":MyObject}):
        layer = Dense(..., W_regularizer="MyObject")
        # save, load, etc. will recognize custom object by name
""",
        """CODE.    x_batch = K.ones(shape=(32, 20, 1))
    y_batch = K.ones(shape=(32, 30, 20))
    xy_batch_dot = K.batch_dot(x_batch, y_batch, axes=[1, 2])
    K.int_shape(xy_batch_dot)
    (32, 1, 30)
""",
        """CODE.    x_data = HDF5Matrix('input/file.hdf5', 'data')
    model.predict(x_data)
""",
        """CODE.  def _create_my_experiment(config, hparams):
    hidden_units = [hparams.unit_per_layer] * hparams.num_hidden_layers

    return tf.contrib.learn.Experiment(
        estimator=DNNClassifier(config=config, hidden_units=hidden_units),
        train_input_fn=my_train_input,
        eval_input_fn=my_eval_input)

  tuner = create_tuner(study_configuration, objective_key)

  learn_runner.tune(experiment_fn=_create_my_experiment, tuner)
""",
        """CODE.# Create a 2 x 2 linear operator.
matrix = [[1., 2.], [3., 4.]]
operator = LinearOperatorFullMatrix(matrix)

operator.to_dense()
==> [[1., 2.]
     [3., 4.]]

operator.shape
==> [2, 2]

operator.log_determinant()
==> scalar Tensor

x = ... Shape [2, 4] Tensor
operator.apply(x)
==> Shape [2, 4] Tensor

# Create a [2, 3] batch of 4 x 4 linear operators.
matrix = tf.random_normal(shape=[2, 3, 4, 4])
operator = LinearOperatorFullMatrix(matrix)
""",
        """CODE.# Create a 3 x 3 diagonal linear operator.
diag_operator = LinearOperatorDiag(
    diag_update=[1., 2., 3.], is_non_singular=True, is_self_adjoint=True,
    is_positive_definite=True)

# Perturb with a rank 2 perturbation
operator = LinearOperatorUDVHUpdate(
    operator=diag_operator,
    u=[[1., 2.], [-1., 3.], [0., 0.]],
    diag_update=[11., 12.],
    v=[[1., 2.], [-1., 3.], [10., 10.]])

operator.shape

operator.log_determinant()

x = ... Shape [3, 4] Tensor
operator.apply(x)
""",
        """CODE.# Create a HParams object specifying names and values of the model
# hyperparameters:
hparams = HParams(learning_rate=0.1, num_hidden_units=100)

# The hyperparameter are available as attributes of the HParams object:
hparams.learning_rate ==> 0.1
hparams.num_hidden_units ==> 100

# Define a command line flag to pass name=value pairs.
# For example using argparse:
import argparse
parser = argparse.ArgumentParser(description='Train my model.')
parser.add_argument('--hparams', type=str,
                    help='Comma seperated list of "name=value" pairs.')
args = parser.parse_args()
...
def my_program():
  # Create a HParams object specifying the names and values of the
  # model hyperparameters:
  hparams = tf.HParams(learning_rate=0.1, num_hidden_units=100,
                       activations=['relu', 'tanh'])

  # Override hyperparameters values by parsing the command line
  hparams.parse(args.hparams)

  # If the user passed `--hparams=learning_rate=0.3` on the command line
  # then 'hparams' has the following attributes:
  hparams.learning_rate ==> 0.3
  hparams.num_hidden_units ==> 100
  hparams.activations ==> ['relu', 'tanh']

  # If the hyperparameters are in json format use parse_json:
  hparams.parse_json('{"learning_rate": 0.3, "activations": "relu"}')
""",
        """CODE.# Initialize a single VectorDeterministic supported at [0., 2.] in R^2.
constant = tf.contrib.distributions.Deterministic([0., 2.])
constant.prob([0., 2.])
==> 1.
constant.prob([0., 3.])
==> 0.

# Initialize a [3] batch of constants on R^2.
loc = [[0., 1.], [2., 3.], [4., 5.]]
constant = constant_lib.VectorDeterministic(loc)
constant.prob([[0., 1.], [1.9, 3.], [3.99, 5.]])
==> [1., 0., 0.]
""",
        """CODE.# Print the batch number at the beginning of every batch.
batch_print_callback = LambdaCallback(
    on_batch_begin=lambda batch,logs: print(batch))

# Plot the loss after every epoch.
import numpy as np
import matplotlib.pyplot as plt
plot_loss_callback = LambdaCallback(
    on_epoch_end=lambda epoch, logs: plt.plot(np.arange(epoch),
                                              logs['loss']))

# Terminate some processes after having finished model training.
processes = ...
cleanup_callback = LambdaCallback(
    on_train_end=lambda logs: [
        p.terminate() for p in processes if p.is_alive()])

model.fit(...,
          callbacks=[batch_print_callback,
                     plot_loss_callback,
                     cleanup_callback])
""",
        """CODE.A1 = LinearOperatorDiag(diag=[1., 1.], name="A1")
A2 = LinearOperatorDiag(diag=[2., 2.], name="A2")

# Use two tiers, the first contains an Adder that returns Diag.  Since both
# A1 and A2 are Diag, they can use this Adder.  The second tier will not be
# used.
addition_tiers = [
    [_AddAndReturnDiag()],
    [_AddAndReturnMatrix()]]
B_list = add_operators([A1, A2], addition_tiers=addition_tiers)

len(B_list)
==> 1

B_list[0].__class__.__name__
==> 'LinearOperatorDiag'

B_list[0].to_dense()
==> [[3., 0.],
     [0., 3.]]

B_list[0].name
==> 'Add/A1__A2/'
""",
        """CODE.constant = tf.contrib.distributions.Deterministic(0.)
constant.prob(0.)
==> 1.
constant.prob(2.)
==> 0.

loc = [[0., 1.], [2., 3.]]
x = [[0., 1.1], [1.99, 3.]]
constant = tf.contrib.distributions.Deterministic(loc)
constant.prob(x)
==> [[1., 0.], [0., 1.]]
""",
        """CODE.csv_logger = CSVLogger('training.log')
model.fit(X_train, Y_train, callbacks=[csv_logger])
""",
        """CODE.def antirectifier(x):
    x -= K.mean(x, axis=1, keepdims=True)
    x = K.l2_normalize(x, axis=1)
    pos = K.relu(x)
    neg = K.relu(-x)
    return K.concatenate([pos, neg], axis=1)
model.add(Lambda(antirectifier))
""",
        """CODE.dist = tf.contrib.distributions.Logistic(loc=0., scale=3.)
dist.cdf(1.)
dist = tf.contrib.distributions.Logistic(loc=[1, 2.], scale=[11, 22.])
dist.prob([0, 1.5])
dist.sample([3])
dist = tf.contrib.distributions.Logistic(loc=1., scale=[11, 22.])
dist.prob(3.0)
""",
        """CODE.ds = tf.contrib.distributions

# Initialize a single 3-variate Gaussian with covariance `cov = S @ S.T`,
# `S = diag(d) + U @ diag(m) @ U.T`. The perturbation, `U @ diag(m) @ U.T`, is
# a rank-2 update.
mu = [-0.5., 0, 0.5]   # shape: [3]
d = [1.5, 0.5, 2]      # shape: [3]
U = [[1., 2],
     [-1, 1],
     [2, -0.5]]        # shape: [3, 2]
m = [4., 5]            # shape: [2]
mvn = ds.MultivariateNormalDiagPlusLowRank(
    loc=mu
    scale_diag=d
    scale_perturb_factor=U,
    scale_perturb_diag=m)

# Evaluate this on an observation in `R^3`, returning a scalar.
mvn.prob([-1, 0, 1]).eval()  # shape: []

# Initialize a 2-batch of 3-variate Gaussians; `S = diag(d) + U @ U.T`.
mu = [[1.,  2,  3],
      [11, 22, 33]]      # shape: [b, k] = [2, 3]
U = [[[1., 2],
      [3,  4],
      [5,  6]],
     [[0.5, 0.75],
      [1,0, 0.25],
      [1.5, 1.25]]]      # shape: [b, k, r] = [2, 3, 2]
m = [[0.1, 0.2],
     [0.4, 0.5]]         # shape: [b, r] = [2, 2]

mvn = ds.MultivariateNormalDiagPlusLowRank(
    loc=mu,
    scale_perturb_factor=U,
    scale_perturb_diag=m)

mvn.covariance().eval()   # shape: [2, 3, 3]
# ==> [[[  15.63   31.57    48.51]
#       [  31.57   69.31   105.05]
#       [  48.51  105.05   162.59]]
#
#      [[   2.59    1.41    3.35]
#       [   1.41    2.71    3.34]
#       [   3.35    3.34    8.35]]]

# Compute the pdf of two `R^3` observations (one from each batch);
# return a length-2 vector.
x = [[-0.9, 0, 0.1],
     [-10, 0, 9]]     # shape: [2, 3]
mvn.prob(x).eval()    # shape: [2]
""",
        """CODE.ds = tf.contrib.distributions

# Initialize a single 3-variate Gaussian.
mu = [1., 2, 3]
cov = [[ 0.36,  0.12,  0.06],
       [ 0.12,  0.29, -0.13],
       [ 0.06, -0.13,  0.26]]
scale = tf.cholesky(cov)
# ==> [[ 0.6,  0. ,  0. ],
#      [ 0.2,  0.5,  0. ],
#      [ 0.1, -0.3,  0.4]])
mvn = ds.MultivariateNormalTriL(
    loc=mu,
    scale_tril=scale)

mvn.mean().eval()
# ==> [1., 2, 3]

# Covariance agrees with cholesky(cov) parameterization.
mvn.covariance().eval()
# ==> [[ 0.36,  0.12,  0.06],
#      [ 0.12,  0.29, -0.13],
#      [ 0.06, -0.13,  0.26]]

# Compute the pdf of an observation in `R^3` ; return a scalar.
mvn.prob([-1., 0, 1]).eval()  # shape: []

# Initialize a 2-batch of 3-variate Gaussians.
mu = [[1., 2, 3],
      [11, 22, 33]]              # shape: [2, 3]
tril = ...  # shape: [2, 3, 3], lower triangular, non-zero diagonal.
mvn = ds.MultivariateNormalTriL(
    loc=mu,
    scale_tril=tril)

# Compute the pdf of two `R^3` observations; return a length-2 vector.
x = [[-0.9, 0, 0.1],
     [-10, 0, 9]]     # shape: [2, 3]
mvn.prob(x).eval()    # shape: [2]
""",
        """CODE.ds = tf.contrib.distributions
la = tf.contrib.linalg

# Initialize a single 3-variate Gaussian.
mu = [1., 2, 3]
cov = [[ 0.36,  0.12,  0.06],
       [ 0.12,  0.29, -0.13],
       [ 0.06, -0.13,  0.26]]
scale = tf.cholesky(cov)
# ==> [[ 0.6,  0. ,  0. ],
#      [ 0.2,  0.5,  0. ],
#      [ 0.1, -0.3,  0.4]])

mvn = ds.MultivariateNormalLinearOperator(
    loc=mu,
    scale=la.LinearOperatorTriL(scale))

# Covariance agrees with cholesky(cov) parameterization.
mvn.covariance().eval()
# ==> [[ 0.36,  0.12,  0.06],
#      [ 0.12,  0.29, -0.13],
#      [ 0.06, -0.13,  0.26]]

# Compute the pdf of an`R^3` observation; return a scalar.
mvn.prob([-1., 0, 1]).eval()  # shape: []

# Initialize a 2-batch of 3-variate Gaussians.
mu = [[1., 2, 3],
      [11, 22, 33]]              # shape: [2, 3]
scale_diag = [[1., 2, 3],
              [0.5, 1, 1.5]]     # shape: [2, 3]

mvn = ds.MultivariateNormalLinearOperator(
    loc=mu,
    scale=la.LinearOperatorDiag(scale_diag))

# Compute the pdf of two `R^3` observations; return a length-2 vector.
x = [[-0.9, 0, 0.1],
     [-10, 0, 9]]     # shape: [2, 3]
mvn.prob(x).eval()    # shape: [2]
""",
        """CODE.features = tf.constant(["emerson", "lake", "and", "palmer"])
table = tf.contrib.lookup.index_table_from_file(
    vocabulary_file="test.txt", num_oov_buckets=1)
ids = table.lookup(features)
...
tf.tables_initializer().run()
""",
        """CODE.features = tf.constant(["emerson", "lake", "and", "palmer"])
table = tf.contrib.lookup.index_table_from_file(
    vocabulary_file="test.txt", num_oov_buckets=1)
ids = table.lookup(features)
...
tf.tables_initializer().run()

ids.eval()  ==> [0, 1, 3, 2]  # where 3 is the out-of-vocabulary bucket
""",
        """CODE.from keras import backend as K
K.dtype(K.placeholder(shape=(2,4,5)))
K.dtype(K.placeholder(shape=(2,4,5), dtype='float32'))
K.dtype(K.placeholder(shape=(2,4,5), dtype='float64'))
kvar = K.variable(np.array([[1, 2], [3, 4]]))
K.dtype(kvar)
kvar = K.variable(np.array([[1, 2], [3, 4]]), dtype='float32')
K.dtype(kvar)
""",
        """CODE.from keras import backend as K
K.set_floatx('float16')
""",
        """CODE.from keras import backend as K
K.set_image_data_format('channels_last')
""",
        """CODE.from keras import backend as K
a = K.placeholder((2, 2), sparse=False)
print(K.is_sparse(a))
False
b = K.placeholder((2, 2), sparse=True)
print(K.is_sparse(b))
True
""",
        """CODE.from keras import backend as K
b = K.placeholder((2, 2), sparse=True)
print(K.is_sparse(b))
c = K.to_dense(b)
print(K.is_sparse(c))
""",
        """CODE.from keras import backend as K
input = K.placeholder(shape=(2, 4, 5))
K.int_shape(input)
val = np.array([[1, 2], [3, 4]])
kvar = K.variable(value=val)
K.int_shape(kvar)
""",
        """CODE.from keras import backend as K
kvar = K.variable(np.array([[1, 2], [3, 4]]), dtype='float32')
K.eval(kvar)
""",
        """CODE.from keras.data_utils import _hash_file
_hash_file('/path/to/file.zip')
""",
        """CODE.keras.backend.epsilon()
1e-08
""",
        """CODE.keras.backend.floatx()
""",
        """CODE.keras.backend.image_data_format()
""",
        """CODE.kvar = K.random_normal_variable((2,3), 0, 1)
kvar
K.eval(kvar)
""",
        """CODE.kvar = K.random_uniform_variable((2,3), 0, 1)
K.eval(kvar)
""",
        """CODE.mapping_strings = t.constant(["emerson", "lake", "palmer")
table = tf.contrib.lookup.index_table_from_tensor(
    mapping=mapping_strings, num_oov_buckets=1, default_value=-1)
features = tf.constant(["emerson", "lake", "and", "palmer"])
ids = table.lookup(features)
...
tf.tables_initializer().run()

ids.eval()  ==> [0, 1, 4, 2]
""",
        """CODE.model = Sequential()
model.add(Bidirectional(LSTM(10, return_sequences=True), input_shape=(5, 10)))
model.add(Bidirectional(LSTM(10)))
model.add(Dense(5))
model.add(Activation('softmax'))
model.compile(loss='categorical_crossentropy', optimizer='rmsprop')
""",
        """CODE.model = Sequential()
model.add(Convolution2D(64, 3, 3,
                        border_mode='same',
                        input_shape=(3, 32, 32)))
# now: model.output_shape == (None, 64, 32, 32)

model.add(Flatten())
# now: model.output_shape == (None, 65536)
""",
        """CODE.model = Sequential()
model.add(Cropping2D(cropping=((2, 2), (4, 4)),
                     input_shape=(28, 28, 3)))
model.add(Conv2D(64, (3, 3), padding='same))
model.add(Cropping2D(cropping=((2, 2), (2, 2)))
""",
        """CODE.model = Sequential()
model.add(Dense(32, input_dim=32))
# now: model.output_shape == (None, 32)
# note: `None` is the batch dimension

model.add(RepeatVector(3))
# now: model.output_shape == (None, 3, 32)
""",
        """CODE.model = Sequential()
model.add(Dense(32, input_dim=500))
model.add(Dense(32))

model = Sequential()
model.add(Dense(32, input_shape=(500,)))
model.add(Dense(32))

model = Sequential()
model.add(Dense(32, batch_input_shape=(None, 500)))
model.add(Dense(32))
""",
        """CODE.model = Sequential()
model.add(Dense(32, input_shape=(500,)))
model.add(Dense(10, activation='softmax'))
model.compile(optimizer='rmsprop',
              loss='categorical_crossentropy',
              metrics=['accuracy'])
""",
        """CODE.model = Sequential()
model.add(Embedding(1000, 64, input_length=10))
# the model will take as input an integer matrix of size (batch,
input_length).
# the largest integer (i.e. word index) in the input should be no larger
than 999 (vocabulary size).
# now model.output_shape == (None, 10, 64), where None is the batch
dimension.

input_array = np.random.randint(1000, size=(32, 10))

model.compile('rmsprop', 'mse')
output_array = model.predict(input_array)
assert output_array.shape == (32, 10, 64)
""",
        """CODE.model = Sequential()
model.add(LocallyConnected1D(64, 3, input_shape=(10, 32)))
model.add(LocallyConnected1D(32, 3))
""",
        """CODE.model = Sequential()
model.add(LocallyConnected2D(64, (3, 3), input_shape=(32, 32, 3)))
model.add(LocallyConnected2D(32, (3, 3)))
""",
        """CODE.model = Sequential()
model.add(Masking(mask_value=0., input_shape=(timesteps, features)))
model.add(LSTM(32))
""",
        """CODE.model = Sequential()
model.add(Permute((2, 1), input_shape=(10, 64)))
# now: model.output_shape == (None, 64, 10)
# note: `None` is the batch dimension
""",
        """CODE.model = Sequential()
model.add(Reshape((3, 4), input_shape=(12,)))
model.add(Reshape((6, 2)))
model.add(Reshape((-1, 2, 2)))
""",
        """CODE.model = Sequential()
model.add(TimeDistributed(Dense(8), input_shape=(10, 16)))
model.add(TimeDistributed(Dense(32)))


model = Sequential()
model.add(TimeDistributed(Conv2D(64, (3, 3)), input_shape=(10, 299, 299, 3))
""",
        """CODE.model.save('my_model.h5')  # creates a HDF5 file 'my_model.h5'
del model  # deletes the existing model

model = load_model('my_model.h5')
""",
        """CODE.p = [0.1, 0.5, 0.4]
dist = OneHotCategorical(probs=p)

logits = [-2, 2, 0]
dist = OneHotCategorical(logits=logits)

p = [0.1, 0.4, 0.5]
dist = OneHotCategorical(probs=p)
dist.prob([0,1,0])

samples = [[0,1,0], [1,0,0]]
dist.prob(samples)
""",
        """CODE.reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2,
                              patience=5, min_lr=0.001)
model.fit(X_train, Y_train, callbacks=[reduce_lr])
""",
        """CODE.sparse_column_a = sparse_column_with_hash_bucket(...)
sparse_column_b = sparse_column_with_hash_bucket(...)

sparse_feature_a_x_sparse_feature_b = crossed_column(...)

estimator = LinearEstimator(
    feature_columns=[sparse_column_a, sparse_feature_a_x_sparse_feature_b],
    head=head_lib.poisson_regression_head())

# Input builders
def input_fn_train: # returns x, y
  ...
def input_fn_eval: # returns x, y
  ...
estimator.fit(input_fn=input_fn_train)
estimator.evaluate(input_fn=input_fn_eval)
estimator.predict(x=x)
""",
        """CODE.sparse_column_a = sparse_column_with_hash_bucket(...)
sparse_column_b = sparse_column_with_hash_bucket(...)

sparse_feature_a_x_sparse_feature_b = crossed_column(...)

estimator = SDCALogisticClassifier(
    example_id_column='example_id',
    feature_columns=[sparse_column_a, sparse_feature_a_x_sparse_feature_b]),
    weight_column_name=...,
    l2_regularization=...,
    num_loss_partitions=...,
)

# Input builders
# returns x, y (where y is the label Tensor (with 0/1 values)
def input_fn_{train, eval}:

# returns x (features dict)
def input_fn_test:
  ...
estimator.fit(input_fn=input_fn_train)
estimator.evaluate(input_fn=input_fn_eval)
estimator.predict_classes(input_fn=input_fn_test) # returns predicted classes.
estimator.predict_proba(input_fn=input_fn_test) # returns predicted prob/ties.
""",
        """CODE.sparse_column_a = sparse_column_with_hash_bucket(...)
sparse_column_b = sparse_column_with_hash_bucket(...)

sparse_feature_a_x_sparse_feature_b = crossed_column(...)

estimator = SDCARegressor(
    example_id_column='example_id',
    feature_columns=[sparse_column_a, sparse_feature_a_x_sparse_feature_b]),
    weight_column_name=...,
    l2_regularization=...,
    num_loss_partitions=...,
)

# Input builders
# returns x, y (where y is the label Tensor (with 0/1 values)
def input_fn_{train, eval}:

# returns x (features dict)
def input_fn_test:
  ...
estimator.fit(input_fn=input_fn_train)
estimator.evaluate(input_fn=input_fn_eval)
estimator.predict_scores(input_fn=input_fn_test) # returns predicted scores.""",
        """CODE.sparse_column_a = sparse_column_with_hash_bucket(...)
sparse_column_b = sparse_column_with_hash_bucket(...)

sparse_feature_a_x_sparse_feature_b = crossed_column(...)

estimator = SDCARegressor(
    example_id_column='example_id',
    feature_columns=[sparse_column_a, sparse_feature_a_x_sparse_feature_b]),
    weight_column_name=...,
    l2_regularization=...,
    num_loss_partitions=...,
)

def input_fn_{train, eval}:
  ...

def input_fn_test:
  ...

estimator.fit(input_fn=input_fn_train)
estimator.evaluate(input_fn=input_fn_eval)
estimator.predict_scores(input_fn=input_fn_test) # returns predicted scores.""",
        """CODE.sparse_feature_a = sparse_column_with_hash_bucket(...)
sparse_feature_b = sparse_column_with_hash_bucket(...)

sparse_feature_a_emb = embedding_column(sparse_id_column=sparse_feature_a,
                                        ...)
sparse_feature_b_emb = embedding_column(sparse_id_column=sparse_feature_b,
                                        ...)

estimator = DNNEstimator(
    feature_columns=[sparse_feature_a_emb, sparse_feature_b_emb],
    head=tf.contrib.learn.multi_class_head(n_classes=2),
    hidden_units=[1024, 512, 256])

head = tf.contrib.learn.multi_class_head(
    n_classes=2,
    label_name="x",
    weight_column_name="w",
    enable_centered_bias=True)
estimator = DNNEstimator(
    feature_columns=[sparse_feature_a_emb, sparse_feature_b_emb],
    head=head,
    hidden_units=[1024, 512, 256])

def input_fn_train: # returns x, y (where y represents label's class index).
  pass
estimator.fit(input_fn=input_fn_train)

def input_fn_eval: # returns x, y (where y represents label's class index).
  pass
estimator.evaluate(input_fn=input_fn_eval)
estimator.predict(x=x) # returns predicted labels (i.e. label's class index).
""",
        """CODE.temperature = 0.5
p = [0.1, 0.5, 0.4]
dist = ExpRelaxedOneHotCategorical(temperature, probs=p)
samples = dist.sample()
exp_samples = tf.exp(samples)
# exp_samples has the same distribution as samples from
# RelaxedOneHotCategorical(temperature, probs=p)


temperature = 0.5
logits = [-2, 2, 0]
dist = ExpRelaxedOneHotCategorical(temperature, logits=logits)
samples = dist.sample()
exp_samples = tf.exp(samples)
# exp_samples has the same distribution as samples from
# RelaxedOneHotCategorical(temperature, probs=p)


temperature = 1e-5
logits = [-2, 2, 0]
dist = ExpRelaxedOneHotCategorical(temperature, logits=logits)
samples = dist.sample()
exp_samples = tf.exp(samples)
# exp_samples has the same distribution as samples from
# RelaxedOneHotCategorical(temperature, probs=p)


temperature = 10
logits = [-2, 2, 0]
dist = ExpRelaxedOneHotCategorical(temperature, logits=logits)
samples = dist.sample()
exp_samples = tf.exp(samples)
# exp_samples has the same distribution as samples from
# RelaxedOneHotCategorical(temperature, probs=p)
""",
        """CODE.temperature = 0.5
p = [0.1, 0.5, 0.4]
dist = RelaxedBernoulli(temperature, probs=p)


temperature = 0.5
logits = [-2, 2, 0]
dist = RelaxedBernoulli(temperature, logits=logits)


temperature = 0.5
logits = [-2, 2, 0]
dist = Logistic(logits/temperature, 1./temperature)
samples = dist.sample()
sigmoid_samples = tf.sigmoid(samples)
# sigmoid_samples has the same distribution as samples from
# RelaxedBernoulli(temperature, logits=logits)


temperature = 1e-5
logits = [-2, 2, 0]
dist = RelaxedBernoulli(temperature, logits=logits)


temperature = 100
logits = [-2, 2, 0]
dist = RelaxedBernoulli(temperature, logits=logits)
""",
        """CODE.temperature = 0.5
p = [0.1, 0.5, 0.4]
dist = RelaxedOneHotCategorical(temperature, probs=p)


temperature = 0.5
logits = [-2, 2, 0]
dist = RelaxedOneHotCategorical(temperature, logits=logits)


temperature = 1e-5
logits = [-2, 2, 0]
dist = RelaxedOneHotCategorical(temperature, logits=logits)


temperature = 10
logits = [-2, 2, 0]
dist = RelaxedOneHotCategorical(temperature, logits=logits)
""",
        """CODE.x = Input(shape=(32,))
y = Dense(16, activation='softmax')(x)
model = Model(x, y)
""" .

<DEPENDENCY.tensorflow==1.10.0> <CONTAINS> """CODE.
ds1 = table.parallel_scan_range("row_start",
                                "row_end",
                                columns=[("cfa", "c1"),
                                         ("cfa", "c2"),
                                         ("cfb", "c3")])
ds2 = table.parallel_scan_range("row_start", "row_end",
                                cfa=["c1", "c2"], cfb="c3")
""",
        """CODE.
ds1 = table.scan_prefix("row_prefix", columns=[("cfa", "c1"),
                                               ("cfa", "c2"),
                                               ("cfb", "c3")])
ds2 = table.scan_prefix("row_prefix", cfa=["c1", "c2"], cfb="c3")
""",
        """CODE.
ds1 = table.scan_range("row_start", "row_end", columns=[("cfa", "c1"),
                                                        ("cfa", "c2"),
                                                        ("cfb", "c3")])
ds2 = table.scan_range("row_start", "row_end", cfa=["c1", "c2"], cfb="c3")
""",
        """CODE.b = tfb.FillTriangular(upper=False)
b.forward([1, 2, 3, 4, 5, 6])
# ==> [[4, 0, 0],
#      [6, 5, 0],
#      [3, 2, 1]]

b = tfb.FillTriangular(upper=True)
b.forward([1, 2, 3, 4, 5, 6])
# ==> [[1, 2, 3],
#      [0, 5, 6],
#      [0, 0, 4]]
""",
        """CODE.b = tfb.TransformDiagonal(diag_bijector=tfb.Exp())

b.forward([[1., 0.],
           [0., 1.]])""",
        """CODE.table = # ...
ds1 = table.parallel_scan_prefix("row_prefix", columns=[("cfa", "c1"),
                                                        ("cfa", "c2"),
                                                        ("cfb", "c3")])
ds2 = table.parallel_scan_prefix("row_prefix", cfa=["c1", "c2"], cfb="c3")
""",
        """CODE.table = bigtable_client.table("my_table")
key_dataset = table.get_keys_prefix("imagenet")
images = key_dataset.apply(table.lookup_columns(("cf1", "image"),
                                                ("cf2", "label"),
                                                ("cf2", "boundingbox")))
training_data = images.map(parse_and_crop, num_parallel_calls=64).batch(128)

table = bigtable_client.table("my_table")
key_dataset = table.get_keys_prefix("imagenet")
images = key_dataset.apply(table.lookup_columns(
    cf1="image", cf2=("label", "boundingbox")))
training_data = images.map(parse_and_crop, num_parallel_calls=64).batch(128)""",
        """CODE.tfb = tf.contrib.distributions.bijectors
b = tfb.ScaleTriL(
     diag_bijector=tfb.Exp(),
     diag_shift=None)
b.forward(x=[0., 0., 0.])
b.inverse(y=[[1., 0],
             [.5, 2]])
dist = tfd.TransformedDistribution(
        tfd.Normal(tf.zeros(6), tf.ones(6)),
        tfb.Chain([tfb.CholeskyOuterProduct(), tfb.ScaleTriL()]))
b = tfb.ScaleTriL(
     diag_bijector=tfb.Identity(),
     diag_shift=None)
b = tfb.ScaleTriL(
     diag_bijector=tfb.Chain([
       tfb.AffineScalar(shift=1e-3),
       tfb.Softplus(),
       tfb.AffineScalar(shift=0.5413)]),
     diag_shift=None)""" .

<DEPENDENCY.tensorflow==1.12.0> <CONTAINS> """CODE.tf.NotDifferentiable("Size")
""" .

<DEPENDENCY.tensorflow==1.12.1> <CONTAINS> """CODE.
def input_fn(input_context):
  batch_size = input_context.get_per_replica_batch_size(global_batch_size)
  d = tf.data.Dataset.from_tensors([[1.]]).repeat().batch(batch_size)
  return d.shard(input_context.num_input_pipelines,
                 input_context.input_pipeline_id)
with strategy.scope():
  iterator = strategy.make_input_fn_iterator(input_fn)
  replica_results = strategy.experimental_run(replica_fn, iterator)
""",
        """CODE.
with strategy.scope():
  var1 = tf.get_variable(...)
  with strategy.extended.colocate_vars_with(var1):
    # var2 and var3 will be created on the same device(s) as var1
    var2 = tf.get_variable(...)
    var3 = tf.get_variable(...)

  def fn(v1, v2, v3):
    # operates on v1 from var1, v2 from var2, and v3 from var3

  # `fn` runs on every device `var1` is on, `var2` and `var3` will be there
  # too.
  strategy.extended.update(var1, fn, args=(var2, var3))
""",
        """CODE.# Preprocess 4 files concurrently, and interleave blocks of 16 records from
# each file.
filenames = ["/var/data/file1.txt", "/var/data/file2.txt", ...]
dataset = (Dataset.from_tensor_slices(filenames)
           .interleave(lambda x:
               TextLineDataset(x).map(parse_fn, num_parallel_calls=1),
               cycle_length=4, block_length=16))


# NOTE: The following examples use `{ ... }` to represent the
# contents of a dataset.
a = { 1, 2, 3, 4, 5 }

# NOTE: New lines indicate "block" boundaries.
a.interleave(lambda x: Dataset.from_tensors(x).repeat(6),
             cycle_length=2, block_length=4) == {
    1, 1, 1, 1,
    2, 2, 2, 2,
    1, 1,
    2, 2,
    3, 3, 3, 3,
    4, 4, 4, 4,
    3, 3,
    4, 4,
    5, 5, 5, 5,
    5, 5,
}
""",
        """CODE.Dataset.range(5) == [0, 1, 2, 3, 4]
Dataset.range(2, 5) == [2, 3, 4]
Dataset.range(1, 5, 2) == [1, 3]
Dataset.range(1, 5, -2) == []
Dataset.range(5, 1) == []
Dataset.range(5, 1, -2) == [5, 3]
""",
        """CODE.a = { 1, 2, 3 }
b = { 4, 5, 6 }
c = { (7, 8), (9, 10), (11, 12) }
d = { 13, 14 }

Dataset.zip((a, b)) == { (1, 4), (2, 5), (3, 6) }
Dataset.zip((b, a)) == { (4, 1), (5, 2), (6, 3) }

Dataset.zip((a, b, c)) == { (1, 4, (7, 8)),
                            (2, 5, (9, 10)),
                            (3, 6, (11, 12)) }

Dataset.zip((a, d)) == { (1, 13), (2, 14) }
""",
        """CODE.a = { 1, 2, 3 }
b = { 4, 5, 6, 7 }

a.concatenate(b) == { 1, 2, 3, 4, 5, 6, 7 }
""",
        """CODE.a.flat_map(lambda x: Dataset.from_tensor_slices(x)) ==
  {[1,2,3,4,5,6,7,8,9,10]}
""",
        """CODE.class MyGraphConverter(GraphConverter):
  ...

  def get_rewriter_config(self, rewriter_config_template=None):
    my_rewriter_config = ...
    return my_rewriter_config

my_converter = MyGraphConverter(input_saved_model_dir="my_dir")
converted_graph_def = my_converter.convert()
my_converter.save(output_saved_model_dir)  # Optional

my_converter = MyGraphConverter(input_saved_model_dir="my_dir")
my_converter.convert()

# Run calibration 10 times.
converted_graph_def = my_converter.calibrate(
    fetch_names=['output:0'],
    num_runs=10,
    feed_dict_fn=lambda: {'input:0': my_next_data()})

my_converter.save(output_saved_model_dir)  # Optional
""",
        """CODE.d = tf.data.Dataset.from_tensor_slices([1, 2, 3])

d = d.filter(lambda x: x < 3) # [1, 2]

# `tf.math.equal(x, y)` is required for equality comparison
def filter_fn(x):
  return tf.math.equal(x, 1)

d = d.filter(filter_fn) # [1]
""",
        """CODE.d = tf.data.TFRecordDataset(input_file)
d = d.shard(num_workers, worker_index)
d = d.repeat(num_epochs)
d = d.shuffle(shuffle_buffer_size)
d = d.map(parser_fn, num_parallel_calls=num_map_threads)


d = Dataset.list_files(pattern)
d = d.shard(num_workers, worker_index)
d = d.repeat(num_epochs)
d = d.shuffle(shuffle_buffer_size)
d = d.interleave(tf.data.TFRecordDataset,
                 cycle_length=num_readers, block_length=1)
d = d.map(parser_fn, num_parallel_calls=num_map_threads)
""",
        """CODE.dataset = ...
iterator = dataset.make_initializable_iterator()
# ...
sess.run(iterator.initializer)
""",
        """CODE.def f(...):
  return tf.constant(37.0)
result = dataset.map(f)
result.output_classes == tf.Tensor
result.output_types == tf.float32
result.output_shapes == []  # scalar

def g(...):
  return tf.constant(37.0), tf.constant(["Foo", "Bar", "Baz"])
result = dataset.map(g)
result.output_classes == (tf.Tensor, tf.Tensor)
result.output_types == (tf.float32, tf.string)
result.output_shapes == ([], [3])

def h(...):
  return 37.0, ["Foo", "Bar", "Baz"], np.array([1.0, 2.0] dtype=np.float64)
result = dataset.map(h)
result.output_classes == (tf.Tensor, tf.Tensor, tf.Tensor)
result.output_types == (tf.float32, tf.string, tf.float64)
result.output_shapes == ([], [3], [2])

def i(...):
  return {"a": 37.0, "b": [42, 16]}, "foo"
result.output_classes == ({"a": tf.Tensor, "b": tf.Tensor}, tf.Tensor)
result.output_types == ({"a": tf.float32, "b": tf.int32}, tf.string)
result.output_shapes == ({"a": [], "b": [2]}, [])
""",
        """CODE.import itertools
tf.enable_eager_execution()

def gen():
  for i in itertools.count(1):
    yield (i, [1] * i)

ds = tf.data.Dataset.from_generator(
    gen, (tf.int64, tf.int64), (tf.TensorShape([]), tf.TensorShape([None])))

for value in ds.take(2):
  print value
# (1, array([1]))
# (2, array([1, 1]))
""",
        """CODE.import tensorflow as tf
tfgan = tf.contrib.gan

# See TFGAN's `train.py` for a description of the generator and
# discriminator API.
def generator_fn(generator_inputs):
  ...
  return generated_data

def discriminator_fn(data, conditioning):
  ...
  return logits

# Create GAN estimator.
config = tpu_config.RunConfig(model_dir='/my/dir')
gan_estimator = tfgan.estimator.TPUGANEstimator(
    generator_fn=generator_fn,
    discriminator_fn=discriminator_fn,
    generator_loss_fn=tfgan.losses.wasserstein_generator_loss,
    discriminator_loss_fn=tfgan.losses.wasserstein_discriminator_loss,
    generator_optimizer=tf.compat.v1.train.AdamOptimizer(0.1, 0.5),
    discriminator_optimizer=tf.compat.v1.train.AdamOptimizer(0.1, 0.5),
    train_batch_size=4,
    config=config)

# Train estimator.
gan_estimator.train(train_input_fn, train_steps)

# Evaluate resulting estimator.
gan_estimator.evaluate(eval_input_fn, eval_steps)

# Generate samples from generator.
predictions = np.array([
    x['generated_data'] for x in gan_estimator.predict(predict_input_fn)])""",
        """CODE.sampler = Sampler(init_args)
(initial_finished, initial_inputs) = sampler.initialize(input_tensors)
for time_step in range(time):
  cell_output, cell_state = cell.call(cell_input, previous_state)
  sample_ids = sampler.sample(time_step, cell_output, cell_state)
  (finished, next_inputs, next_state) = sampler.next_inputs(
      time_step,cell_output, cell_state)""",
        """CODE.strategy = tf.distribute.MirroredStrategy()

# Create a dataset
dataset = dataset_ops.Dataset.range(10).batch(2)

# Distribute that dataset
dist_dataset = strategy.experimental_distribute_dataset(dataset)
# Iterate over the distributed dataset
for x in dist_dataset:
  # process dataset elements
  strategy.experimental_run_v2(train_step, args=(x,))
""",
        """CODE.tf.no_gradient("Size")
""",
        """CODE.while n > 0:
    n = n - 1
    s = n

s = Undefined('s')
init_state = (s,)
s = while_loop(cond, body, init_state)
""",
        """CODE.with strategy.scope():
    v = tf.Variable(1.)
strategy.variable_created_in_scope(v)
True

v = tf.Variable(1.)
strategy.variable_created_in_scope(v)
False""" .

<DEPENDENCY.tensorflow==1.12.2> <CONTAINS> """CODE.# Build baseline multi-label classifier.
estimator = BaselineEstimator(
    head=tf.contrib.estimator.multi_label_head(n_classes=3))

# Input builders
def input_fn_train: # returns x, y (where y represents label's class index).
  pass

def input_fn_eval: # returns x, y (where y represents label's class index).
  pass

# Fit model.
estimator.train(input_fn=input_fn_train)

# Evaluates cross entropy between the test and train labels.
loss = classifier.evaluate(input_fn=input_fn_eval)["loss"]

# For each class, predicts the ratio of training examples that contain the
# class.
predictions = classifier.predict(new_samples)
""",
        """CODE.def train_input_fn():
  ...
  return train_dataset

def eval_input_fn():
  ...
  return eval_dataset

estimator = tf.estimator.DNNClassifier(...)

evaluator = tf.contrib.estimator.InMemoryEvaluatorHook(
    estimator, eval_input_fn)
estimator.train(train_input_fn, hooks=[evaluator])
""",
        """CODE.feature1 = tf.feature_column.embedding_column(
    tf.feature_column.categorical_column_with_vocabulary_list(
        key='feature1', vocabulary_list=('green', 'yellow')), dimension=1)
feature2 = tf.feature_column.numeric_column(key='feature2', default_value=0.0)

classifier = tf.estimator.DNNClassifier(
    hidden_units=[4,2], feature_columns=[feature1, feature2])

def input_fn():
  features = {'feature1': tf.constant(['green', 'green', 'yellow']),
              'feature2': tf.constant([3.5, 4.2, 6.1])}
  label = tf.constant([1., 0., 0.])
  return tf.data.Dataset.from_tensors((features, label)).repeat()

classifier.train(input_fn=input_fn, steps=10)


supervised_input_receiver_fn = (
    tf.contrib.estimator.build_raw_supervised_input_receiver_fn(
        {'feature1': tf.placeholder(dtype=tf.string, shape=[None]),
         'feature2': tf.placeholder(dtype=tf.float32, shape=[None])},
        tf.placeholder(dtype=tf.float32, shape=[None])))

serving_input_receiver_fn = (
    tf.estimator.export.build_parsing_serving_input_receiver_fn(
        tf.feature_column.make_parse_example_spec([feature1, feature2])))

export_dir = tf.contrib.estimator.export_all_saved_models(
    classifier, '/tmp/export_all',
    {tf.estimator.ModeKeys.TRAIN: supervised_input_receiver_fn,
     tf.estimator.ModeKeys.EVAL: supervised_input_receiver_fn,
     tf.estimator.ModeKeys.PREDICT: serving_input_receiver_fn})

export_dir = classifier.export_savedmodel(
    '/tmp/export_predict', serving_input_receiver_fn)

est = tf.contrib.estimator.SavedModelEstimator(export_dir)

eval_results = est.evaluate(input_fn=input_fn, steps=1)
print(eval_results)

est.train(input_fn=input_fn, steps=20)

def predict_input_fn():
  example = tf.train.Example()
  example.features.feature['feature1'].bytes_list.value.extend(['yellow'])
  example.features.feature['feature2'].float_list.value.extend([1.])
  return {'inputs':tf.constant([example.SerializeToString()])}

predictions = est.predict(predict_input_fn)
print(next(predictions))
""",
        """CODE.table = tf.contrib.lookup.MutableDenseHashTable(key_dtype=tf.int64,
                                                value_dtype=tf.int64,
                                                default_value=-1,
                                                empty_key=0)
sess.run(table.insert(keys, values))
out = table.lookup(query_keys)
print(out.eval())
""",
        """CODE.tf.NotDifferentiable("Size")
""",
        """CODE.token_sequence = sequence_categorical_column_with_hash_bucket(...)
token_emb = embedding_column(categorical_column=token_sequence, ...)

estimator = RNNClassifier(
    sequence_feature_columns=[token_emb],
    num_units=[32, 16], cell_type='lstm')

# Input builders
def input_fn_train: # returns x, y
  pass
estimator.train(input_fn=input_fn_train, steps=100)

def input_fn_eval: # returns x, y
  pass
metrics = estimator.evaluate(input_fn=input_fn_eval, steps=10)
def input_fn_predict: # returns x, None
  pass
predictions = estimator.predict(input_fn=input_fn_predict)
""",
        """CODE.token_sequence = sequence_categorical_column_with_hash_bucket(...)
token_emb = embedding_column(categorical_column=token_sequence, ...)

estimator = RNNEstimator(
    head=tf.contrib.estimator.regression_head(),
    sequence_feature_columns=[token_emb],
    num_units=[32, 16], cell_type='lstm')

# Or with custom RNN cell:
def rnn_cell_fn(mode):
  cells = [ tf.contrib.rnn.LSTMCell(size) for size in [32, 16] ]
  if mode == tf.estimator.ModeKeys.TRAIN:
    cells = [ tf.contrib.rnn.DropoutWrapper(cell, input_keep_prob=0.5)
                  for cell in cells ]
  return tf.contrib.rnn.MultiRNNCell(cells)

estimator = RNNEstimator(
    head=tf.contrib.estimator.regression_head(),
    sequence_feature_columns=[token_emb],
    rnn_cell_fn=rnn_cell_fn)

# Input builders
def input_fn_train: # returns x, y
  pass
estimator.train(input_fn=input_fn_train, steps=100)

def input_fn_eval: # returns x, y
  pass
metrics = estimator.evaluate(input_fn=input_fn_eval, steps=10)
def input_fn_predict: # returns x, None
  pass
predictions = estimator.predict(input_fn=input_fn_predict)
""" .

<DEPENDENCY.tensorflow==1.12.3> <CONTAINS> "CODE.tf.NotDifferentiable(\"Size\")" .

<DEPENDENCY.tensorflow==1.13.2> <CONTAINS> """CODE.import tensorflow as tf

# Create a tf.keras model.
model = tf.keras.Sequential()
model.add(tf.keras.layers.Dense(1, input_shape=[10]))
model.summary()

# Save the tf.keras model in the SavedModel format.
saved_to_path = tf.contrib.saved_model.save_keras_model(
      model, '/tmp/my_simple_tf_keras_saved_model')

# Load the saved keras model back.
model_prime = tf.contrib.saved_model.load_keras_model(saved_to_path)
model_prime.summary()
""" .

<DEPENDENCY.tensorflow==1.14.0> <CONTAINS> """CODE.if tf.distribute.in_cross_replica_context():
  strategy = tf.distribute.get_strategy()""",
        """CODE.import tensorflow as tf

# Create a tf.keras model.
model = tf.keras.Sequential()
model.add(tf.keras.layers.Dense(1, input_shape=[10]))
model.summary()

# Save the tf.keras model in the SavedModel format.
path = '/tmp/simple_keras_model'
tf.keras.experimental.export_saved_model(model, path)

# Load the saved keras model back.
new_model = tf.keras.experimental.load_from_saved_model(path)
new_model.summary()
""" .

<DEPENDENCY.tensorflow==1.2.0> <CONTAINS> """CODE.
  get_uid('dense')
  1
  get_uid('dense')
  2
""",
        """CODE.# Get 30th percentile with default ('nearest') interpolation.
x = [1., 2., 3., 4.]
percentile(x, q=30.)

# Get 30th percentile with 'lower' interpolation
x = [1., 2., 3., 4.]
percentile(x, q=30., interpolation='lower')

# Get 100th percentile (maximum).  By default, this is computed over every dim
x = [[1., 2.]
     [3., 4.]]
percentile(x, q=100.)

# Treat the leading dim as indexing samples, and find the 100th quantile (max)
# over all such samples.
x = [[1., 2.]
     [3., 4.]]
percentile(x, q=100., axis=[0])
""",
        """CODE.# Make an operator acting like batch matrix A.  Assume A.shape = [..., M, N]
operator = LinearOperator(...)
operator.shape = [..., M, N]

# Solve one linear system for every member of the batch.
RHS = ... # shape [..., M]

X = operator.solvevec(RHS)
# X is the solution to the linear system
# sum_j A[..., :, j] X[..., j] = RHS[..., :]

operator.matvec(X)
==> RHS
""",
        """CODE.# Make an operator acting like batch matrix A.  Assume A.shape = [..., M, N]
operator = LinearOperator(...)
operator.shape = [..., M, N]

X = ... # shape [..., N, R], batch matrix, R > 0.

Y = operator.matmul(X)
Y.shape
==> [..., M, R]

Y[..., :, r] = sum_j A[..., :, j] X[j, r]
""",
        """CODE.@batch_function(1, 2, 3)
def layer(a):
  return tf.matmul(a, a)""",
        """CODE.Dataset.range(5)
Dataset.range(2, 5)
Dataset.range(1, 5, 2)
Dataset.range(1, 5, -2)
Dataset.range(5, 1)
Dataset.range(5, 1, -2)""",
        """CODE.Dataset.range(5) == [0, 1, 2, 3, 4]
Dataset.range(2, 5) == [2, 3, 4]
Dataset.range(1, 5, 2) == [1, 3]
Dataset.range(1, 5, -2) == []
Dataset.range(5, 1) == []
Dataset.range(5, 1, -2) == [5, 3]
""",
        """CODE._unique_layer_name('dense')
_unique_layer_name('dense')""",
        """CODE.a = tf.random_normal(shape=(2, 3, 4))
b = tf.random_normal(shape=(4, 5))
result = matmul_with_broadcast(a, b)
result.shape
==> (2, 3, 5)
result[0,...]
==> tf.matmul(a[0,...], b)
result[1,...]
==> tf.matmul(a[1,...], b)
""",
        """CODE.a = { 1, 2, 3 }
b = { 4, 5, 6 }
c = { (7, 8), (9, 10), (11, 12) }
d = { 13, 14 }

Dataset.zip((a, b)) == { (1, 4), (2, 5), (3, 6) }
Dataset.zip((b, a)) == { (4, 1), (5, 2), (6, 3) }

Dataset.zip((a, b, c) == { (1, 4, (7, 8)),
                           (2, 5, (9, 10)),
                           (3, 6, (11, 12)) }

Dataset.zip((a, d)) == { (1, 13), (2, 14) }
""",
        """CODE.a = { ['a', 'b', 'c'], ['a', 'b'], ['a', 'b', 'c', 'd'] }

a.dense_to_sparse_batch(batch_size=2, row_shape=[6]) == {
    ([[0, 0], [0, 1], [0, 2], [1, 0], [1, 1]],  # indices
     ['a', 'b', 'c', 'a', 'b'],                 # values
     [2, 6]),                                   # dense_shape
    ([[2, 0], [2, 1], [2, 2], [2, 3]],
     ['a', 'b', 'c', 'd'],
     [1, 6])
}
""",
        """CODE.a.dense_to_sparse_batch(batch_size=2, row_shape=[6]) == {
    ([[0, 0], [0, 1], [0, 2], [1, 0], [1, 1]],  # indices
     ['a', 'b', 'c', 'a', 'b'],                 # values
     [2, 6]),                                   # dense_shape
    ([[2, 0], [2, 1], [2, 2], [2, 3]],
     ['a', 'b', 'c', 'd'],
     [1, 6])
}
""",
        """CODE.categorical_column = categorical_column_with_hash_bucket(
    column_name='terms', hash_bucket_size=1000)
weighted_column = weighted_categorical_column(
    categorical_column=categorical_column, weight_feature_key='frequencies')
columns = [weighted_column, ...]
features = tf.parse_example(..., features=make_parse_example_spec(columns))
linear_prediction, _, _ = linear_model(features, columns)
""",
        """CODE.colors = categorical_column_with_vocabulary_list(
    key='colors', vocabulary_list=('X', 'R', 'G', 'B', 'Y'), default_value=0)
columns = [colors, ...]
features = tf.parse_example(..., features=make_parse_example_spec(columns))
linear_prediction, _, _ = linear_model(features, columns)

columns = [embedding_column(colors, 3),...]
features = tf.parse_example(..., features=make_parse_example_spec(columns))
dense_tensor = input_layer(features, columns)
""",
        """CODE.dataset = ...
iterator = Iterator.from_dataset(dataset)
# ...
sess.run(iterator.initializer)
""",
        """CODE.def generator():
    for index in range(10):
        yield {'height': np.random.randint(32,36),
               'age': np.random.randint(18, 80),
               'label': np.ones(1)}

with tf.Session() as session:
    input_fn = generator_io.generator_input_fn(
        generator, target_key="label", batch_size=2, shuffle=False,
        num_epochs=1)
""",
        """CODE.def generator():
  for i in range(3):
    yield {"value": i}

features = {
  "value": tf.FixedLenFeature(shape=[], dtype=dtypes.int32)
}

tensor_dict = tf.contrib.training.python_input(generator, features)
batched_dict = tf.train.batch(
  tensor_dict, batch_size=2, allow_smaller_final_batch=True)

s = tf.Session()
tf.train.start_queue_runners()

batch1 = s.run(batched_dict)  # returns {"value": np.array([0, 1])}
batch2 = s.run(batched_dict)  # returns {"value": np.array([2])}
s.run(batched_dict)  # error: Queue is closed (generator finished at i==3)
""",
        """CODE.def input_layer(features, feature_columns, ...):
  outputs = [fc._get_dense_tensor(...) for fc in feature_columns]
  return tf.concat(outputs)
""",
        """CODE.ds = tf.contrib.distributions

# Initialize a single 3-variate Gaussian.
mu = [1., 2, 3]
cov = [[ 0.36,  0.12,  0.06],
       [ 0.12,  0.29, -0.13],
       [ 0.06, -0.13,  0.26]]
mvn = ds.MultivariateNormalFullCovariance(
    loc=mu,
    covariance_matrix=cov)

mvn.mean().eval()
# ==> [1., 2, 3]

# Covariance agrees with covariance_matrix.
mvn.covariance().eval()
# ==> [[ 0.36,  0.12,  0.06],
#      [ 0.12,  0.29, -0.13],
#      [ 0.06, -0.13,  0.26]]

# Compute the pdf of an observation in `R^3` ; return a scalar.
mvn.prob([-1., 0, 1]).eval()  # shape: []

# Initialize a 2-batch of 3-variate Gaussians.
mu = [[1., 2, 3],
      [11, 22, 33]]              # shape: [2, 3]
covariance_matrix = ...  # shape: [2, 3, 3], symmetric, positive definite.
mvn = ds.MultivariateNormalFullCovariance(
    loc=mu,
    covariance=covariance_matrix)

# Compute the pdf of two `R^3` observations; return a length-2 vector.
x = [[-0.9, 0, 0.1],
     [-10, 0, 9]]     # shape: [2, 3]
mvn.prob(x).eval()    # shape: [2]
""",
        """CODE.ds = tf.contrib.distributions
la = tf.contrib.linalg

# Initialize a single 3-variate VectorLaplace with some desired covariance.
mu = [1., 2, 3]
cov = [[ 0.36,  0.12,  0.06],
       [ 0.12,  0.29, -0.13],
       [ 0.06, -0.13,  0.26]]

scale = tf.cholesky(cov)
# ==> [[ 0.6,  0. ,  0. ],
#      [ 0.2,  0.5,  0. ],
#      [ 0.1, -0.3,  0.4]])

# Divide scale by sqrt(2) so that the final covariance will be what we want.
vla = ds.VectorLaplaceLinearOperator(
    loc=mu,
    scale=la.LinearOperatorTriL(scale / tf.sqrt(2)))

# Covariance agrees with cholesky(cov) parameterization.
vla.covariance().eval()
# ==> [[ 0.36,  0.12,  0.06],
#      [ 0.12,  0.29, -0.13],
#      [ 0.06, -0.13,  0.26]]

# Compute the pdf of an`R^3` observation; return a scalar.
vla.prob([-1., 0, 1]).eval()  # shape: []

# Initialize a 2-batch of 3-variate Vector Laplace's.
mu = [[1., 2, 3],
      [11, 22, 33]]              # shape: [2, 3]
scale_diag = [[1., 2, 3],
              [0.5, 1, 1.5]]     # shape: [2, 3]

vla = ds.VectorLaplaceLinearOperator(
    loc=mu,
    scale=la.LinearOperatorDiag(scale_diag))

# Compute the pdf of two `R^3` observations; return a length-2 vector.
x = [[-0.9, 0, 0.1],
     [-10, 0, 9]]     # shape: [2, 3]
vla.prob(x).eval()    # shape: [2]
""",
        """CODE.feature_columns = set(
    [feature_b, feature_c_bucketized, feature_a_x_feature_c])
features = tf.parse_example(
    serialized=serialized_examples,
    features=make_parse_example_spec(feature_columns))


{
  "feature_a": parsing_ops.VarLenFeature(tf.string),
  "feature_b": parsing_ops.FixedLenFeature([1], dtype=tf.float32),
  "feature_c": parsing_ops.FixedLenFeature([1], dtype=tf.float32)
}
""",
        """CODE.img=[[1,2,3,3,2,1],
     [1,2,3,4,5,2],
     [1,2,3,4,5,3],
     [1,2,3,4,5,4],
     [6,5,4,4,5,5]]
session = tf.InteractiveSession()
sirds = single_image_random_dot_stereograms(
    img,
    convergence_dots_size=8,
    number_colors=256,normalize=True)

out = sirds.eval()
png = tf.image.encode_png(out).eval()
with open('picture_out.png', 'wb') as f:
  f.write(png)
""",
        """CODE.initial_state = attention_wrapper.zero_state(dtype=..., batch_size=...)
initial_state = initial_state.clone(cell_state=encoder_state)""",
        """CODE.initializer = RandomUniform(-1, 1)
config = initializer.get_config()
initializer = RandomUniform.from_config(config)""",
        """CODE.iterator = Iterator.from_structure(tf.int64, tf.TensorShape([]))

dataset_range = Dataset.range(10)
range_initializer = iterator.make_initializer(dataset_range)

dataset_evens = dataset_range.filter(lambda x: x % 2 == 0)
evens_initializer = iterator.make_initializer(dataset_evens)

# Define a model based on the iterator; in this example, the model_fn
# is expected to take scalar tf.int64 Tensors as input (see
# the definition of 'iterator' above).
prediction, loss = model_fn(iterator.get_next())

# Train for `num_epochs`, where for each epoch, we first iterate over
# dataset_range, and then iterate over dataset_evens.
for _ in range(num_epochs):
  # Initialize the iterator to `dataset_range`
  sess.run(range_initializer)
  while True:
    try:
      pred, loss_val = sess.run([prediction, loss])
    except tf.errors.OutOfRangeError:
      break

  # Initialize the iterator to `dataset_evens`
  sess.run(evens_initializer)
  while True:
    try:
      pred, loss_val = sess.run([prediction, loss])
    except tf.errors.OutOfRangeError:
      break
""",
        """CODE.keywords = categorical_column_with_hash_bucket("keywords", 10K)
columns = [keywords, ...]
features = tf.parse_example(..., features=make_parse_example_spec(columns))
linear_prediction = linear_model(features, columns)

keywords_embedded = embedding_column(keywords, 16)
columns = [keywords_embedded, ...]
features = tf.parse_example(..., features=make_parse_example_spec(columns))
dense_tensor = input_layer(features, columns)
""",
        """CODE.name = indicator_column(categorical_column_with_vocabulary_list('name',
    ['bob', 'george', 'wanda'])
columns = [name, ...]
features = tf.parse_example(..., features=make_parse_example_spec(columns))
dense_tensor = input_layer(features, columns)

dense_tensor == [[1, 0, 0]]  # If "name" bytes_list is ["bob"]
dense_tensor == [[1, 0, 1]]  # If "name" bytes_list is ["bob", "wanda"]
dense_tensor == [[2, 0, 0]]  # If "name" bytes_list is ["bob", "bob"]
""",
        """CODE.operator = LinearOperator(...)

X = ... # shape [..., N], batch vector

Y = operator.matvec(X)
Y.shape
==> [..., M]

Y[..., :] = sum_j A[..., :, j] X[..., j]
""",
        """CODE.pcm = tf.placeholder(tf.float32, [None, 9152])
frames = tf.contrib.signal.frames(pcm, 512, 180)
magspec = tf.abs(tf.spectral.rfft(frames, [512]))
image = tf.expand_dims(magspec, 3)
""",
        """CODE.price = numeric_column('price')
columns = [price, ...]
features = tf.parse_example(..., features=make_parse_example_spec(columns))
dense_tensor = input_layer(features, columns)

bucketized_price = bucketized_column(price, boundaries=[...])
columns = [bucketized_price, ...]
features = tf.parse_example(..., features=make_parse_example_spec(columns))
linear_prediction = linear_model(features, columns)
""",
        """CODE.price = numeric_column('price')
keywords_embedded = embedding_column(
    categorical_column_with_hash_bucket("keywords", 10K), dimensions=16)
columns = [price, keywords_embedded, ...]
features = tf.parse_example(..., features=make_parse_example_spec(columns))
dense_tensor = input_layer(features, columns)
for units in [128, 64, 32]:
  dense_tensor = tf.layers.dense(dense_tensor, units, tf.nn.relu)
prediction = tf.layers.dense(dense_tensor, 1)
""",
        """CODE.price = numeric_column('price')
price_buckets = bucketized_column(price, boundaries=[0., 10., 100., 1000.])
keywords = categorical_column_with_hash_bucket("keywords", 10K)
keywords_price = crossed_column('keywords', price_buckets, ...)
columns = [price_buckets, keywords, keywords_price ...]
features = tf.parse_example(..., features=make_parse_example_spec(columns))
prediction = linear_model(features, columns)
""",
        """CODE.real_column_a = real_valued_column(...)
sparse_column_b = sparse_column_with_hash_bucket(...)

regressor = SDCALinearRegressor(
    example_id_column='example_id',
    feature_columns=[real_column_a, sparse_column_b]),
    weight_column_name=...,
    l2_regularization=...,
    num_loss_partitions=...,
)

# Input builders
# returns x, y (where y is the label Tensor (with 0/1 values)
def input_fn_{train, eval}:

# returns x (features dict)
def input_fn_test:
  ...
regressor.fit(input_fn=input_fn_train)
regressor.evaluate(input_fn=input_fn_eval)
regressor.predict_scores(input_fn=input_fn_test) # returns predicted scores.
""",
        """CODE.states = categorical_column_with_vocabulary_file(
    key='states', vocabulary_file='/us/states.txt', vocabulary_size=50,
    num_oov_buckets=5)
columns = [states, ...]
features = tf.parse_example(..., features=make_parse_example_spec(columns))
linear_prediction = linear_model(features, columns)


states = categorical_column_with_vocabulary_file(
    key='states', vocabulary_file='/us/states.txt', vocabulary_size=51,
    default_value=0)
columns = [states, ...]
features = tf.parse_example(..., features=make_parse_example_spec(columns))
linear_prediction, _, _ = linear_model(features, columns)


columns = [embedding_column(states, 3),...]
features = tf.parse_example(..., features=make_parse_example_spec(columns))
dense_tensor = input_layer(features, columns)
""",
        """CODE.tf.matrix_adjoint(x) ==> [[1 4]
                          [2 5]
                          [-3j 6j]]
tf.matmul(matrix, b, adjoint_b=True)
""",
        """CODE.video_id = categorical_column_with_identity(
    key='video_id', num_buckets=1000000, default_value=0)
columns = [video_id, ...]
features = tf.parse_example(..., features=make_parse_example_spec(columns))
linear_prediction, _, _ = linear_model(features, columns)

columns = [embedding_column(video_id, 9),...]
features = tf.parse_example(..., features=make_parse_example_spec(columns))
dense_tensor = input_layer(features, columns)
""",
        """CODE.with tf.name_scope('scope1'):
    with tf.name_scope('scope2'):
      print(tf.contrib.framework.get_name_scope())
""",
        """CODE.with tf.name_scope('scope1'):
  with tf.name_scope('scope2'):
    print(tf.get_default_graph().get_name_scope())
""",
        """CODE.x = [[1, 2],
     [3, 4]]  # Shape [2, 2], no batch dims

y = [[[1]]]   # Shape [1, 1, 1], 1 batch dim of shape [1]

x_bc, y_bc = broadcast_matrix_batch_dims([x, y])

x_bc
==> [[[1, 2],
      [3, 4]]]  # Shape [1, 2, 2], 1 batch dim of shape [1].

y_bc
==> same as y


x = tf.random_normal(shape=(2, 3, 1, 4, 4))
y = tf.random_normal(shape=(1, 3, 2, 5, 5))
x_bc, y_bc = broadcast_matrix_batch_dims([x, y])

x_bc.shape
==> (2, 3, 2, 4, 4)

y_bc.shape
==> (2, 3, 2, 5, 5)
""" .

<DEPENDENCY.tensorflow==1.3.0> <CONTAINS> """CODE.# Build DebugClassifier
classifier = DebugClassifier()

# Input builders
def input_fn_train(): # returns x, y (where y represents label's class index).
  pass

def input_fn_eval(): # returns x, y (where y represents label's class index).
  pass

# Fit model.
classifier.fit(input_fn=input_fn_train)

# Evaluate cross entropy between the test and train labels.
loss = classifier.evaluate(input_fn=input_fn_eval)["loss"]

# predict_class outputs the most commonly seen class in training.
predicted_label = classifier.predict_class(new_samples)

# predict_proba outputs the class distribution from training.
label_distribution = classifier.predict_proba(new_samples)
""",
        """CODE.# Build DebugRegressor
regressor = DebugRegressor()

# Input builders
def input_fn_train(): # returns x, y (where y represents label's class index).
  pass

def input_fn_eval(): # returns x, y (where y represents label's class index).
  pass

# Fit model.
regressor.fit(input_fn=input_fn_train)

# Evaluate squared-loss between the test and train targets.
loss = regressor.evaluate(input_fn=input_fn_eval)["loss"]

# predict_scores outputs mean value seen during training.
predicted_targets = regressor.predict_scores(new_samples)
""",
        """CODE.ds = tf.contrib.distributions
la = tf.contrib.linalg

# Initialize a single 2-variate VectorExponential, supported on
# {(x, y) in R^2 : x > 0, y > 0}.

# The first component has pdf exp{-x}, the second 0.5 exp{-x / 2}
vex = ds.VectorExponentialDiag(scale_diag=[1., 2.])

# Compute the pdf of an`R^2` observation; return a scalar.
vex.prob([3., 4.]).eval()  # shape: []

# Initialize a 2-batch of 3-variate Vector Exponential's.
loc = [[1., 2, 3],
       [1., 0, 0]]              # shape: [2, 3]
scale_diag = [[1., 2, 3],
              [0.5, 1, 1.5]]     # shape: [2, 3]

vex = ds.VectorExponentialDiag(loc, scale_diag)

# Compute the pdf of two `R^3` observations; return a length-2 vector.
x = [[1.9, 2.2, 3.1],
     [10., 1.0, 9.0]]     # shape: [2, 3]
vex.prob(x).eval()    # shape: [2]
""",
        """CODE.ds = tf.contrib.distributions
la = tf.contrib.linalg

# Initialize a single 2-variate VectorExponential, supported on
# {(x, y) in R^2 : x > 0, y > 0}.
mat = [[1.0, 0.1],
       [0.1, 1.0]]

vex = ds.VectorExponentialLinearOperator(
    scale=la.LinearOperatorFullMatrix(mat))

# Compute the pdf of an`R^2` observation; return a scalar.
vex.prob([1., 2.]).eval()  # shape: []

# Initialize a 2-batch of 3-variate Vector Exponential's.
mu = [[1., 2, 3],
      [1., 0, 0]]              # shape: [2, 3]
scale_diag = [[1., 2, 3],
              [0.5, 1, 1.5]]     # shape: [2, 3]

vex = ds.VectorExponentialLinearOperator(
    loc=mu,
    scale=la.LinearOperatorDiag(scale_diag))

# Compute the pdf of two `R^3` observations; return a length-2 vector.
x = [[1.9, 2.2, 3.1],
     [10., 1.0, 9.0]]     # shape: [2, 3]
vex.prob(x).eval()    # shape: [2]
""",
        """CODE.enqueuer = SequenceEnqueuer(...)
enqueuer.start()
datas = enqueuer.get()
for data in datas:
    # Use the inputs; training, evaluating, predicting.
    # ... stop sometime.
enqueuer.close()
""" .

<DEPENDENCY.tensorflow==1.4.0> <CONTAINS> """CODE.abs = ds.bijectors.AbsoluteValue()

abs.forward([-1., 0., 1.])
==> [1., 0.,  1.]

abs.inverse(1.)
==> [-1., 1.]

# The |dX/dY| is constant, == 1.  So Log|dX/dY| == 0.
abs.inverse_log_det_jacobian(1.)
==> [0., 0.]

# Special case handling of 0.
abs.inverse(0.)
==> [0., 0.]

abs.inverse_log_det_jacobian(0.)
==> [0., 0.]
""",
        """CODE.ds = tf.contrib.distributions

# Make independent distribution from a 2-batch Normal.
ind = ds.Independent(
    distribution=ds.Normal(loc=[-1., 1], scale=[0.1, 0.5]),
    reduce_batch_ndims=1)

# All batch dims have been "absorbed" into event dims.
ind.batch_shape  # ==> []
ind.event_shape  # ==> [2]

# Make independent distribution from a 2-batch bivariate Normal.
ind = ds.Independent(
    distribution=ds.MultivariateNormalDiag(
        loc=[[-1., 1], [1, -1]],
        scale_identity_multiplier=[1., 0.5]),
    reduce_batch_ndims=1)

# All batch dims have been "absorbed" into event dims.
ind.batch_shape  # ==> []
ind.event_shape  # ==> [2, 2]
""",
        """CODE.ds = tf.contrib.distributions
la = tf.contrib.linalg

# Create two batches of VectorDiffeomixtures, one with mix_loc=[0.] and
# another with mix_loc=[1]. In both cases, `K=2` and the affine
# transformations involve:
# k=0: loc=zeros(dims)  scale=LinearOperatorScaledIdentity
# k=1: loc=[2.]*dims    scale=LinOpDiag
dims = 5
vdm = ds.VectorDiffeomixture(
    mix_loc=[[0.], [1]],
    mix_scale=[1.],
    distribution=ds.Normal(loc=0., scale=1.),
    loc=[
        None,  # Equivalent to `np.zeros(dims, dtype=np.float32)`.
        np.float32([2.]*dims),
    ],
    scale=[
        la.LinearOperatorScaledIdentity(
          num_rows=dims,
          multiplier=np.float32(1.1),
          is_positive_definite=True),
        la.LinearOperatorDiag(
          diag=np.linspace(2.5, 3.5, dims, dtype=np.float32),
          is_positive_definite=True),
    ],
    validate_args=True)
""",
        """CODE.ds = tf.contrib.distributions
pln = ds.PoissonLogNormalQuadratureCompound(
    loc=[0., -0.5],
    scale=1.,
    quadrature_polynomial_degree=10,
    validate_args=True)
""",
        """CODE.import matplotlib.pyplot as plt
ds = tf.contrib.distributions

### Create a mixture of two scalar Gaussians:

gm = ds.MixtureSameFamily(
    mixture_distribution=ds.Categorical(
        probs=[0.3, 0.7]),
    components_distribution=ds.Normal(
      loc=[-1., 1],       # One for each component.
      scale=[0.1, 0.5]))  # And same here.

gm.mean()
# ==> 0.4

gm.variance()
# ==> 1.018

# Plot PDF.
x = np.linspace(-2., 3., int(1e4), dtype=np.float32)
plt.plot(x, gm.prob(x).eval());

### Create a mixture of two Bivariate Gaussians:

gm = ds.MixtureSameFamily(
    mixture_distribution=ds.Categorical(
        probs=[0.3, 0.7]),
    components_distribution=ds.MultivariateNormalDiag(
        loc=[[-1., 1],  # component 1
             [1, -1]],  # component 2
        scale_identity_multiplier=[.3, .6]))

gm.mean()
# ==> array([ 0.4, -0.4], dtype=float32)

gm.covariance()
# ==> array([[ 1.119, -0.84],
#            [-0.84,  1.119]], dtype=float32)

# Plot PDF contours.
def meshgrid(x, y=x):
  [gx, gy] = np.meshgrid(x, y, indexing='ij')
  gx, gy = np.float32(gx), np.float32(gy)
  grid = np.concatenate([gx.ravel()[None, :], gy.ravel()[None, :]], axis=0)
  return grid.T.reshape(x.size, y.size, 2)
grid = meshgrid(np.linspace(-2, 2, 100, dtype=np.float32)
plt.contour(grid[..., 0], grid[..., 1], gm.prob(grid).eval());""" .

<DEPENDENCY.tensorflow==1.6.0> <CONTAINS> """CODE.tfp = tf.contrib.bayesflow

net = tfp.layers.dense_flipout(
    features, 512, activation=tf.nn.relu)
logits = tfp.layers.dense_flipout(net, 10)
neg_log_likelihood = tf.nn.softmax_cross_entropy_with_logits(
    labels=labels, logits=logits)
kl = sum(tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES))
loss = neg_log_likelihood + kl
train_op = tf.train.AdamOptimizer().minimize(loss)
""",
        """CODE.tfp = tf.contrib.bayesflow

net = tfp.layers.dense_local_reparameterization(
    features, 512, activation=tf.nn.relu)
logits = tfp.layers.dense_local_reparameterization(net, 10)
neg_log_likelihood = tf.nn.softmax_cross_entropy_with_logits(
    labels=labels, logits=logits)
kl = sum(tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES))
loss = neg_log_likelihood + kl
train_op = tf.train.AdamOptimizer().minimize(loss)
""",
        """CODE.tfp = tf.contrib.bayesflow

net = tfp.layers.dense_reparameterization(
    features, 512, activation=tf.nn.relu)
logits = tfp.layers.dense_reparameterization(net, 10)
neg_log_likelihood = tf.nn.softmax_cross_entropy_with_logits(
    labels=labels, logits=logits)
kl = sum(tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES))
loss = neg_log_likelihood + kl
train_op = tf.train.AdamOptimizer().minimize(loss)
""" .

<DEPENDENCY.tensorflow==1.7.0> <CONTAINS> """CODE.a.apply(tf.contrib.data.sliding_window_batch(window_size=3, stride=2)) ==
{
    [[1], [2], [3]],
    [[3], [4], [5]],
}
""",
        """CODE.colors = sequence_categorical_column_with_vocabulary_list(
    key='colors', vocabulary_list=('R', 'G', 'B', 'Y'))
colors_indicator = _sequence_indicator_column(colors)
columns = [colors]

features = tf.parse_example(..., features=make_parse_example_spec(columns))
input_layer, sequence_length = sequence_input_layer(features, columns)

rnn_cell = tf.nn.rnn_cell.BasicRNNCell(hidden_size)
outputs, state = tf.nn.dynamic_rnn(
    rnn_cell, inputs=input_layer, sequence_length=sequence_length)
""",
        """CODE.colors = sequence_categorical_column_with_vocabulary_list(
    key='colors', vocabulary_list=('R', 'G', 'B', 'Y'),
    num_oov_buckets=2)
colors_embedding = embedding_column(colors, dimension=3)
columns = [colors_embedding]

features = tf.parse_example(..., features=make_parse_example_spec(columns))
input_layer, sequence_length = sequence_input_layer(features, columns)

rnn_cell = tf.nn.rnn_cell.BasicRNNCell(hidden_size)
outputs, state = tf.nn.dynamic_rnn(
    rnn_cell, inputs=input_layer, sequence_length=sequence_length)
""",
        """CODE.import tensorflow as tf
import tensorflow.contrib.eager as tfe
import os

checkpoint_directory = "/tmp/training_checkpoints"
checkpoint_prefix = os.path.join(checkpoint_directory, "ckpt")

root = tfe.Checkpoint(optimizer=optimizer, model=model)
root.restore(tf.train.latest_checkpoint(checkpoint_directory))
for _ in range(num_training_steps):
  optimizer.minimize( ... )
root.save(file_prefix=checkpoint_prefix)
""",
        """CODE.params = PlacerParams(hidden_size=128, decay_steps=50)
hparams.hidden_size ==> 128
hparams.decay_steps ==> 50
""",
        """CODE.rating = sequence_numeric_column('rating')
watches = sequence_categorical_column_with_identity(
    'watches', num_buckets=1000)
watches_embedding = embedding_column(watches, dimension=10)
columns = [rating, watches]

features = tf.parse_example(..., features=make_parse_example_spec(columns))
input_layer, sequence_length = sequence_input_layer(features, columns)

rnn_cell = tf.nn.rnn_cell.BasicRNNCell(hidden_size)
outputs, state = tf.nn.dynamic_rnn(
    rnn_cell, inputs=input_layer, sequence_length=sequence_length)
""",
        """CODE.saver = Saver(root)
saver.restore(path).assert_consumed()

saver.restore(path).assert_consumed().run_restore_ops()
""",
        """CODE.spec = tensor_spec.BoundedTensorSpec((1, 2, 3), tf.float32, 0, (5, 5, 5))
tf_minimum = tf.convert_to_tensor(spec.minimum, dtype=spec.dtype)
tf_maximum = tf.convert_to_tensor(spec.maximum, dtype=spec.dtype)

spec = tensor_spec.BoundedTensorSpec((3, 5), tf.int32, 0, 2)
""",
        """CODE.states = sequence_categorical_column_with_vocabulary_file(
    key='states', vocabulary_file='/us/states.txt', vocabulary_size=50,
    num_oov_buckets=5)
states_embedding = embedding_column(states, dimension=10)
columns = [states_embedding]

features = tf.parse_example(..., features=make_parse_example_spec(columns))
input_layer, sequence_length = sequence_input_layer(features, columns)

rnn_cell = tf.nn.rnn_cell.BasicRNNCell(hidden_size)
outputs, state = tf.nn.dynamic_rnn(
    rnn_cell, inputs=input_layer, sequence_length=sequence_length)
""",
        """CODE.temperature = sequence_numeric_column('temperature')
columns = [temperature]

features = tf.parse_example(..., features=make_parse_example_spec(columns))
input_layer, sequence_length = sequence_input_layer(features, columns)

rnn_cell = tf.nn.rnn_cell.BasicRNNCell(hidden_size)
outputs, state = tf.nn.dynamic_rnn(
    rnn_cell, inputs=input_layer, sequence_length=sequence_length)""",
        """CODE.tfd = tf.contrib.distributions

def make_likelihood(true_variances):
  return tfd.MultivariateNormalDiag(
      scale_diag=tf.sqrt(true_variances))

dims = 10
dtype = np.float32
true_variances = tf.linspace(dtype(1), dtype(3), dims)
likelihood = make_likelihood(true_variances)

states, kernel_results = hmc.sample_chain(
    num_results=1000,
    target_log_prob_fn=likelihood.log_prob,
    current_state=tf.zeros(dims),
    step_size=0.5,
    num_leapfrog_steps=2,
    num_burnin_steps=500)

# Compute sample stats.
sample_mean = tf.reduce_mean(states, axis=0)
sample_var = tf.reduce_mean(
    tf.squared_difference(states, sample_mean),
    axis=0)


tfd = tf.contrib.distributions

def make_prior(dims, dtype):
  return tfd.MultivariateNormalDiag(
      loc=tf.zeros(dims, dtype))

def make_likelihood(weights, factors):
  return tfd.MultivariateNormalDiag(
      loc=tf.tensordot(weights, factors, axes=[[0], [-1]]))

# Setup data.
num_weights = 10
num_factors = 4
num_chains = 100
dtype = np.float32

prior = make_prior(num_weights, dtype)
weights = prior.sample(num_chains)
factors = np.random.randn(num_factors, num_weights).astype(dtype)
x = make_likelihood(weights, factors).sample(num_chains)

def target_log_prob(w):
  # Target joint is: `f(w) = p(w, x | factors)`.
  return prior.log_prob(w) + make_likelihood(w, factors).log_prob(x)

# Get `num_results` samples from `num_chains` independent chains.
chains_states, kernels_results = hmc.sample_chain(
    num_results=1000,
    target_log_prob_fn=target_log_prob,
    current_state=tf.zeros([num_chains, dims], dtype),
    step_size=0.1,
    num_leapfrog_steps=2,
    num_burnin_steps=500)

# Compute sample stats.
sample_mean = tf.reduce_mean(chains_states, axis=[0, 1])
sample_var = tf.reduce_mean(
    tf.squared_difference(chains_states, sample_mean),
    axis=[0, 1])
""",
        """CODE.tfd = tf.contrib.distributions

dims = 10
num_iter = int(1e3)
dtype = np.float32

position = tf.placeholder(np.float32)
momentum = tf.placeholder(np.float32)

[
    next_momentums,
    next_positions,
] = hmc._leapfrog_integrator(
    current_momentums=[momentum],
    target_log_prob_fn=tfd.MultivariateNormalDiag(
        loc=tf.zeros(dims, dtype)).log_prob,
    current_state_parts=[position],
    step_sizes=0.1,
    num_leapfrog_steps=3)[:2]

sess.graph.finalize()  # No more graph building.

momentum_ = np.random.randn(dims).astype(dtype)
position_ = np.random.randn(dims).astype(dtype)

positions = np.zeros([num_iter, dims], dtype)
for i in xrange(num_iter):
  position_, momentum_ = sess.run(
      [next_momentums[0], next_position[0]],
      feed_dict={position: position_, momentum: momentum_})
  positions[i] = position_

plt.plot(positions[:, 0]);  # Sinusoidal.
""",
        """CODE.tokens = sequence_categorical_column_with_hash_bucket(
    'tokens', hash_bucket_size=1000)
tokens_embedding = embedding_column(tokens, dimension=10)
columns = [tokens_embedding]

features = tf.parse_example(..., features=make_parse_example_spec(columns))
input_layer, sequence_length = sequence_input_layer(features, columns)

rnn_cell = tf.nn.rnn_cell.BasicRNNCell(hidden_size)
outputs, state = tf.nn.dynamic_rnn(
    rnn_cell, inputs=input_layer, sequence_length=sequence_length)""",
        """CODE.watches = sequence_categorical_column_with_identity(
    'watches', num_buckets=1000)
watches_embedding = _sequence_embedding_column(watches, dimension=10)
columns = [watches]

features = tf.parse_example(..., features=make_parse_example_spec(columns))
input_layer, sequence_length = sequence_input_layer(features, columns)

rnn_cell = tf.nn.rnn_cell.BasicRNNCell(hidden_size)
outputs, state = tf.nn.dynamic_rnn(
    rnn_cell, inputs=input_layer, sequence_length=sequence_length)
""",
        """CODE.watches = sequence_categorical_column_with_identity(
    'watches', num_buckets=1000)
watches_embedding = embedding_column(watches, dimension=10)
columns = [watches_embedding]

features = tf.parse_example(..., features=make_parse_example_spec(columns))
input_layer, sequence_length = sequence_input_layer(features, columns)

rnn_cell = tf.nn.rnn_cell.BasicRNNCell(hidden_size)
outputs, state = tf.nn.dynamic_rnn(
    rnn_cell, inputs=input_layer, sequence_length=sequence_length)""" .

<DEPENDENCY.tensorflow==1.8.0> <CONTAINS> """CODE.# Create a 4 x 4 linear operator combined of two 2 x 2 operators.
operator_1 = LinearOperatorFullMatrix([[1., 2.], [3., 4.]])
operator_2 = LinearOperatorFullMatrix([[1., 0.], [0., 1.]])
operator = LinearOperatorBlockDiag([operator_1, operator_2])

operator.to_dense()
==> [[1., 2., 0., 0.],
     [3., 4., 0., 0.],
     [0., 0., 1., 0.],
     [0., 0., 0., 1.]]

operator.shape
==> [4, 4]

operator.log_abs_determinant()
==> scalar Tensor

x1 = ... # Shape [2, 2] Tensor
x2 = ... # Shape [2, 2] Tensor
x = tf.concat([x1, x2], 0)  # Shape [2, 4] Tensor
operator.matmul(x)
==> tf.concat([operator_1.matmul(x1), operator_2.matmul(x2)])

# Create a [2, 3] batch of 4 x 4 linear operators.
matrix_44 = tf.random_normal(shape=[2, 3, 4, 4])
operator_44 = LinearOperatorFullMatrix(matrix)

# Create a [1, 3] batch of 5 x 5 linear operators.
matrix_55 = tf.random_normal(shape=[1, 3, 5, 5])
operator_55 = LinearOperatorFullMatrix(matrix_55)

# Combine to create a [2, 3] batch of 9 x 9 operators.
operator_99 = LinearOperatorBlockDiag([operator_44, operator_55])

# Create a shape [2, 3, 9] vector.
x = tf.random_normal(shape=[2, 3, 9])
operator_99.matmul(x)
==> Shape [2, 3, 9] Tensor
""",
        """CODE._unique_layer_name('dense')  # dense_1
_unique_layer_name('dense')  # dense_2
""",
        """CODE.b = AffineScalar()
b = AffineScalar(shift=[1., 2, 3])
b = AffineScalar(
  shift=[1., 2, 3],
  scale=2.)""",
        """CODE.bijector.Square().forward(x=[[1., 0], [2, 1]])
bijector.Square().inverse(y=[[1., 4], [9, 1]])
""",
        """CODE.def broken_beta(shape, alpha, beta, seed):
  x = tf.random_gamma(shape, alpha, seed=seed)
  y = tf.random_gamma(shape, beta, seed=seed)
  return x / (x + y)

def random_beta(shape, alpha, beta, seed):
  seed = SeedStream(seed, salt="random_beta")
  x = tf.random_gamma(shape, alpha, seed=seed())
  y = tf.random_gamma(shape, beta, seed=seed())
  return x / (x + y)
""",
        """CODE.from keras.preprocessing.sequence import TimeseriesGenerator
import numpy as np

data = np.array([[i] for i in range(50)])
targets = np.array([[i] for i in range(50)])

data_gen = TimeseriesGenerator(data, targets,
                               length=10, sampling_rate=2,
                               batch_size=2)
assert len(data_gen) == 20

batch_0 = data_gen[0]
x, y = batch_0
assert np.array_equal(x,
                      np.array([[[0], [2], [4], [6], [8]],
                                [[1], [3], [5], [7], [9]]]))
assert np.array_equal(y,
                      np.array([[10], [11]]))
""",
        """CODE.step = step_fn.StandardSingleLossStep(dataset, loss_fn, optimizer)
step.initialize(distribution)
step(distribution)
""",
        """CODE.tfd = tf.contrib.distributions

dtype = np.float32
dims = 2
new_batch_shape = [1, 2, 3]
old_batch_shape = [6]

scale = np.ones(old_batch_shape + [dims], dtype)
mvn = tfd.MultivariateNormalDiag(scale_diag=scale)
reshape_mvn = tfd.BatchReshape(
    distribution=mvn,
    batch_shape=new_batch_shape,
    validate_args=True)

reshape_mvn.batch_shape

x = reshape_mvn.sample(sample_shape=[4, 5])
x.shape

reshape_mvn.log_prob(x).shape
""",
        """CODE.token_sequence = sequence_categorical_column_with_hash_bucket(...)
token_emb = embedding_column(categorical_column=token_sequence, ...)

estimator = RNNClassifier(
    num_units=[32, 16], cell_type='lstm',
    sequence_feature_columns=[token_emb])

# Input builders
def input_fn_train: # returns x, y
  pass
estimator.train(input_fn=input_fn_train, steps=100)

def input_fn_eval: # returns x, y
  pass
metrics = estimator.evaluate(input_fn=input_fn_eval, steps=10)
def input_fn_predict: # returns x, None
  pass
predictions = estimator.predict(input_fn=input_fn_predict)
""" .

<DEPENDENCY.tensorflow==1.9.0> <CONTAINS> """CODE.
# Build autoencoder.
x = tf.placeholder(tf.float32, shape=[None, 16, 16, 1])
y = forward_transform(x)
entropy_bottleneck = EntropyBottleneck()
y_, likelihoods = entropy_bottleneck(y, training=True)
x_ = backward_transform(y_)

# Information content (= predicted codelength) in bits of each batch element
# (note that taking the natural logarithm and dividing by `log(2)` is
# equivalent to taking base-2 logarithms):
bits = tf.reduce_sum(tf.log(likelihoods), axis=(1, 2, 3)) / -np.log(2)

# Squared difference of each batch element:
squared_error = tf.reduce_sum(tf.squared_difference(x, x_), axis=(1, 2, 3))

# The loss is a weighted sum of mean squared error and entropy (average
# information content), where the weight controls the trade-off between
# approximation error and entropy.
main_loss = 0.5 * tf.reduce_mean(squared_error) + tf.reduce_mean(bits)

# Minimize loss and auxiliary loss, and execute update op.
main_optimizer = tf.train.AdamOptimizer(learning_rate=1e-4)
main_step = optimizer.minimize(main_loss)
# 1e-2 is a good starting point for the learning rate of the auxiliary loss,
# assuming Adam is used.
aux_optimizer = tf.train.AdamOptimizer(learning_rate=1e-2)
aux_step = optimizer.minimize(entropy_bottleneck.losses[0])
step = tf.group(main_step, aux_step, entropy_bottleneck.updates[0])



# Build autoencoder.
x = tf.placeholder(tf.float32, shape=[None, 16, 16, 1])
y = forward_transform(x)
strings = EntropyBottleneck().compress(y)
shape = tf.shape(y)[1:]



strings = tf.placeholder(tf.string, shape=[None])
shape = tf.placeholder(tf.int32, shape=[3])
entropy_bottleneck = EntropyBottleneck(dtype=tf.float32)
y_ = entropy_bottleneck.decompress(strings, shape, channels=5)
x_ = backward_transform(y_)
""",
        """CODE.
opt = tf.AdamOptimizer(learning_rate=...)

loss_scale_manger = tf.contrib.mixed_precision.FixedLossScaleManager(5000)

loss_scale_optimizer = LossScaleOptimizer(opt, loss_scale_manager)

train_op = loss_scale_optimizer.minimize(loss)
""",
        """CODE.# Build baseline multi-label classifier.
estimator = BaselineEstimator(
    head=tf.contrib.estimator.multi_label_head(n_classes=3))

# Input builders
def input_fn_train: # returns x, y (where y represents label's class index).
  pass

def input_fn_eval: # returns x, y (where y represents label's class index).
  pass

# Fit model.
estimator.train(input_fn=input_fn_train)

# Evaluates cross entropy between the test and train labels.
loss = classifier.evaluate(input_fn=input_fn_eval)["loss"]

# For each class, predicts the ratio of training examples that contain the
# class.
predictions = classifier.predict(new_samples)
""",
        """CODE.# Create a 4 x 4 linear operator composed of two 2 x 2 operators.
operator_1 = LinearOperatorFullMatrix([[1., 2.], [3., 4.]])
operator_2 = LinearOperatorFullMatrix([[1., 0.], [2., 1.]])
operator = LinearOperatorKronecker([operator_1, operator_2])

operator.to_dense()
==> [[1., 2., 0., 0.],
     [3., 4., 0., 0.],
     [2., 4., 1., 2.],
     [6., 8., 3., 4.]]

operator.shape
==> [4, 4]

operator.log_abs_determinant()
==> scalar Tensor

x = ... Shape [4, 2] Tensor
operator.matmul(x)
==> Shape [4, 2] Tensor

# Create a [2, 3] batch of 4 x 5 linear operators.
matrix_45 = tf.random_normal(shape=[2, 3, 4, 5])
operator_45 = LinearOperatorFullMatrix(matrix)

# Create a [2, 3] batch of 5 x 6 linear operators.
matrix_56 = tf.random_normal(shape=[2, 3, 5, 6])
operator_56 = LinearOperatorFullMatrix(matrix_56)

# Compose to create a [2, 3] batch of 20 x 30 operators.
operator_large = LinearOperatorKronecker([operator_45, operator_56])

# Create a shape [2, 3, 20, 2] vector.
x = tf.random_normal(shape=[2, 3, 6, 2])
operator_large.matmul(x)
==> Shape [2, 3, 30, 2] Tensor
""",
        """CODE.# Create the Y = softsign(X) transform.
softsign = Softsign()
x = [[[1., 2],
      [3, 4]],
     [[5, 6],
      [7, 8]]]
x / (1 + abs(x)) == softsign.forward(x)
x / (1 - abs(x)) == softsign.inverse(x)
""",
        """CODE.# spectrum is real ==> operator is self-adjoint
# spectrum is positive ==> operator is positive definite
spectrum = [6., 4, 2]

operator = LinearOperatorCirculant(spectrum)

# IFFT[spectrum]
operator.convolution_kernel()
==> [4 + 0j, 1 + 0.58j, 1 - 0.58j]

operator.to_dense()
==> [[4 + 0.0j, 1 - 0.6j, 1 + 0.6j],
     [1 + 0.6j, 4 + 0.0j, 1 - 0.6j],
     [1 - 0.6j, 1 + 0.6j, 4 + 0.0j]]

# Example of defining in terms of a real convolution kernel

# convolution_kernel is real ==> spectrum is Hermitian.
convolution_kernel = [1., 2., 1.]]
spectrum = tf.fft(tf.cast(convolution_kernel, tf.complex64))

# spectrum is Hermitian ==> operator is real.
# spectrum is shape [3] ==> operator is shape [3, 3]
# We force the input/output type to be real, which allows this to operate
# like a real matrix.
operator = LinearOperatorCirculant(spectrum, input_output_dtype=tf.float32)

operator.to_dense()
==> [[ 1, 1, 2],
     [ 2, 1, 1],
     [ 1, 2, 1]]

# Example of Hermitian spectrum

# spectrum is shape [3] ==> operator is shape [3, 3]
# spectrum is Hermitian ==> operator is real.
spectrum = [1, 1j, -1j]

operator = LinearOperatorCirculant(spectrum)

operator.to_dense()
==> [[ 0.33 + 0j,  0.91 + 0j, -0.24 + 0j],
     [-0.24 + 0j,  0.33 + 0j,  0.91 + 0j],
     [ 0.91 + 0j, -0.24 + 0j,  0.33 + 0j]]

# Example of forcing real `dtype` when spectrum is Hermitian

# spectrum is shape [4] ==> operator is shape [4, 4]
# spectrum is real ==> operator is self-adjoint
# spectrum is Hermitian ==> operator is real
# spectrum has positive real part ==> operator is positive-definite.
spectrum = [6., 4, 2, 4]

# Force the input dtype to be float32.
# Cast the output to float32. This is fine because the operator will be
# real due to Hermitian spectrum.
operator = LinearOperatorCirculant(spectrum, input_output_dtype=tf.float32)

operator.shape
==> [4, 4]

operator.to_dense()
==> [[4, 1, 0, 1],
     [1, 4, 1, 0],
     [0, 1, 4, 1],
     [1, 0, 1, 4]]

# convolution_kernel = tf.ifft(spectrum)
operator.convolution_kernel()
==> [4, 1, 0, 1]
""",
        """CODE.a = Input(shape=(32,))
b = Dense(32)(a)
model = Model(inputs=a, outputs=b)
model = keras_support.tpu_model(model)
model.compile(
    optimizer=tf.train.GradientDescentOptimizer(learning_rate=1.0),
    ...)
a = Input(shape=(32,))
b = Dense(32)(a)
model = Model(inputs=a, outputs=b)
model = keras_support.tpu_model(model, replicas=2)
model.compile(
    optimizer=tf.train.GradientDescentOptimizer(learning_rate=1.0),
    ...)""",
        """CODE.bijector.Ordered().forward([2, 3, 4])
# Result: [2., 0., 0.]

bijector.Ordered().inverse([0.06428002, -1.07774478, -0.71530371])
# Result: [0.06428002, 0.40464228, 0.8936858]
""",
        """CODE.class SlotManager(tf.contrib.checkpoint.Checkpointable):

  def __init__(self):
    # Create a dependency named "slotdeps" on the container.
    self.slotdeps = tf.contrib.checkpoint.UniqueNameTracker()
    slotdeps = self.slotdeps
    slots = []
    slots.append(slotdeps.track(tfe.Variable(3.), "x"))  # Named "x"
    slots.append(slotdeps.track(tfe.Variable(4.), "y"))
    slots.append(slotdeps.track(tfe.Variable(5.), "x"))  # Named "x_1"
""",
        """CODE.classifier = tf.estimator.LinearClassifier(
    feature_columns=[age, language])
classifier.train(input_fn=input_fn)

feature_spec = {
    'age': tf.placeholder(dtype=tf.int64),
    'language': array_ops.placeholder(dtype=tf.string)
}
label_spec = tf.placeholder(dtype=dtypes.int64)

train_rcvr_fn = tf.contrib.estimator.build_raw_supervised_input_receiver_fn(
    feature_spec, label_spec)

serve_rcvr_fn = tf.estimator.export.build_parsing_serving_input_receiver_fn(
    feature_spec)

rcvr_fn_map = {
    model_fn_lib.ModeKeys.TRAIN: train_rcvr_fn,
    model_fn_lib.ModeKeys.PREDICT: serve_rcvr_fn,
}

export_dir = tf.contrib.estimator.export_all_saved_models(
    classifier,
    export_dir_base='my_model/',
    input_receiver_fn_map=rcvr_fn_map)

# export_dirs is a dict of directories with SavedModels, which
# can be used for serving, analysis with TFMA, or directly loaded in.
with ops.Graph().as_default() as graph:
  with session.Session(graph=graph) as sess:
    loader.load(sess, [tag_constants.TRAINING], export_dir)
    weights = graph.get_tensor_by_name('linear/linear_model/age/weights')
    ...
""",
        """CODE.classifier = tf.estimator.LinearClassifier(
    feature_columns=[age, language])
classifier.train(input_fn=input_fn, steps=1000)

feature_spec = {
    'age': tf.placeholder(dtype=tf.int64),
    'language': array_ops.placeholder(dtype=tf.string)
}
label_spec = tf.placeholder(dtype=dtypes.int64)

train_rcvr_fn = tf.contrib.estimator.build_raw_supervised_input_receiver_fn(
    feature_spec, label_spec)

export_dir = tf.contrib.estimator.export_saved_model_for_mode(
    classifier,
    export_dir_base='my_model/',
    input_receiver_fn=train_rcvr_fn,
    mode=model_fn_lib.ModeKeys.TRAIN)

# export_dir is a timestamped directory with the SavedModel, which
# can be used for serving, analysis with TFMA, or directly loaded in.
with ops.Graph().as_default() as graph:
  with session.Session(graph=graph) as sess:
    loader.load(sess, [tag_constants.TRAINING], export_dir)
    weights = graph.get_tensor_by_name(''linear/linear_model/age/weights')
    ...
""",
        """CODE.datasets = [tf.data.Dataset.from_tensors("foo").repeat(),
            tf.data.Dataset.from_tensors("bar").repeat(),
            tf.data.Dataset.from_tensors("baz").repeat()]

choice_dataset = tf.data.Dataset.range(3).repeat(3)

result = tf.contrib.data.choose_from_datasets(datasets, choice_dataset)
""",
        """CODE.def train_input_fn():
  ...
  return train_dataset

def eval_input_fn():
  ...
  return eval_dataset

estimator = tf.estimator.DNNClassifier(...)

evaluator = tf.contrib.estimator.InMemoryEvaluatorHook(
    estimator, eval_input_fn)
estimator.train(train_input_fn, hooks=[evaluator])
""",
        """CODE.est = tf.estimator.Estimator(model_fn)
while True:
  est.train(
      train_input_fn,
      hooks=[tf.contrib.data.CheckpointInputPipelineHook(est)],
      steps=train_steps_per_eval)
  # Note: We do not pass the hook here.
  metrics = est.evaluate(eval_input_fn)
  if should_stop_the_training(metrics):
    break
""",
        """CODE.fill_triangular_inverse(
  [[4, 0, 0],
   [6, 5, 0],
   [3, 2, 1]])

fill_triangular_inverse(
  [[1, 2, 3],
   [0, 5, 6],
   [0, 0, 4]], upper=True)
""",
        """CODE.import tensorflow as tf
import pydot

dot_string = tf.contrib.checkpoint.dot_graph_from_checkpoint('/path/to/ckpt')
parsed, = pydot.graph_from_dot_data(dot_string)
parsed.write_svg('/tmp/tensorflow/visualized_checkpoint.svg')
""",
        """CODE.my_head = tf.contrib.estimator.logistic_regression_head()
my_estimator = tf.contrib.estimator.DNNEstimator(
    head=my_head,
    hidden_units=...,
    feature_columns=...)


def _my_model_fn(features, labels, mode):
  my_head = tf.contrib.estimator.logistic_regression_head()
  logits = tf.keras.Model(...)(features)

  return my_head.create_estimator_spec(
      features=features,
      mode=mode,
      labels=labels,
      optimizer=tf.AdagradOptimizer(learning_rate=0.1),
      logits=logits)
my_estimator = tf.estimator.Estimator(model_fn=_my_model_fn)
""",
        """CODE.obj = Checkpointable()
obj.has_dependency = tf.Variable(0., name="dep")
obj.no_dependency = NoDependency(tf.Variable(1., name="nodep"))
assert obj.no_dependency.name == "nodep:0"
""",
        """CODE.object_graph = tf.contrib.checkpoint.object_metadata(
    tf.train.latest_checkpoint(checkpoint_directory))
ckpt_variable_names = set()
for node in object_graph.nodes:
  for attribute in node.attributes:
    ckpt_variable_names.add(attribute.full_name)""",
        """CODE.tfd.bijectors.MatrixInverseTriL().forward(x=[[1., 0], [2, 1]])
# Result: [[1., 0], [-2, 1]], i.e., inv(x)

tfd.bijectors.MatrixInverseTriL().inverse(y=[[1., 0], [-2, 1]])
# Result: [[1., 0], [2, 1]], i.e., inv(y).
""",
        """CODE.token_sequence = sequence_categorical_column_with_hash_bucket(...)
token_emb = embedding_column(categorical_column=token_sequence, ...)

estimator = RNNEstimator(
    head=tf.contrib.estimator.regression_head(),
    sequence_feature_columns=[token_emb],
    num_units=[32, 16], cell_type='lstm')

# Or with custom RNN cell:
def rnn_cell_fn(mode):
  cells = [ tf.contrib.rnn.LSTMCell(size) for size in [32, 16] ]
  if mode == tf.estimator.ModeKeys.TRAIN:
    cells = [ tf.contrib.rnn.DropoutWrapper(cell, input_keep_prob=0.5)
                  for cell in cells ]
  return tf.contrib.rnn.MultiRNNCell(cells)

estimator = RNNEstimator(
    head=tf.contrib.estimator.regression_head(),
    sequence_feature_columns=[token_emb],
    rnn_cell_fn=rnn_cell_fn)

# Input builders
def input_fn_train: # returns x, y
  pass
estimator.train(input_fn=input_fn_train, steps=100)

def input_fn_eval: # returns x, y
  pass
metrics = estimator.evaluate(input_fn=input_fn_eval, steps=10)
def input_fn_predict: # returns x, None
  pass
predictions = estimator.predict(input_fn=input_fn_predict)
""" .

<DEPENDENCY.tensorflow==2.0.4> <CONTAINS> """CODE.class SerializationExampleFeatureColumn(
    FeatureColumn, collections.namedtuple(
        'SerializationExampleFeatureColumn',
        ('dimension', 'parent', 'dtype', 'normalizer_fn'))):

  def _get_config(self):
    # Create a dict from the namedtuple.
    # Python attribute literals can be directly copied from / to the config.
    # For example 'dimension', assuming it is an integer literal.
    config = dict(zip(self._fields, self))

    # (De)serialization of parent FeatureColumns should use the provided
    # (de)serialize_feature_column() methods that take care of de-duping.
    config['parent'] = serialize_feature_column(self.parent)

    # Many objects provide custom (de)serialization e.g: for tf.DType
    # tf.DType.name, tf.as_dtype() can be used.
    config['dtype'] = self.dtype.name

    # Non-trivial dependencies should be Keras-(de)serializable.
    config['normalizer_fn'] = generic_utils.serialize_keras_object(
        self.normalizer_fn)

    return config

  @classmethod
  def _from_config(cls, config, custom_objects=None, columns_by_name=None):
    # This should do the inverse transform from `_get_config` and construct
    # the namedtuple.
    kwargs = config.copy()
    kwargs['parent'] = deserialize_feature_column(
        config['parent'], custom_objects, columns_by_name)
    kwargs['dtype'] = dtypes.as_dtype(config['dtype'])
    kwargs['normalizer_fn'] = generic_utils.deserialize_keras_object(
      config['normalizer_fn'], custom_objects=custom_objects)
    return cls(**kwargs)
""" .

<DEPENDENCY.tensorflow==2.1.0> <CONTAINS> """CODE.class SerializationExampleFeatureColumn(
    FeatureColumn, collections.namedtuple(
        'SerializationExampleFeatureColumn',
        ('dimension', 'parent', 'dtype', 'normalizer_fn'))):

  def get_config(self):
    # Create a dict from the namedtuple.
    # Python attribute literals can be directly copied from / to the config.
    # For example 'dimension', assuming it is an integer literal.
    config = dict(zip(self._fields, self))

    # (De)serialization of parent FeatureColumns should use the provided
    # (de)serialize_feature_column() methods that take care of de-duping.
    config['parent'] = serialize_feature_column(self.parent)

    # Many objects provide custom (de)serialization e.g: for tf.DType
    # tf.DType.name, tf.as_dtype() can be used.
    config['dtype'] = self.dtype.name

    # Non-trivial dependencies should be Keras-(de)serializable.
    config['normalizer_fn'] = generic_utils.serialize_keras_object(
        self.normalizer_fn)

    return config

  @classmethod
  def from_config(cls, config, custom_objects=None, columns_by_name=None):
    # This should do the inverse transform from `get_config` and construct
    # the namedtuple.
    kwargs = config.copy()
    kwargs['parent'] = deserialize_feature_column(
        config['parent'], custom_objects, columns_by_name)
    kwargs['dtype'] = dtypes.as_dtype(config['dtype'])
    kwargs['normalizer_fn'] = generic_utils.deserialize_keras_object(
      config['normalizer_fn'], custom_objects=custom_objects)
    return cls(**kwargs)
""" .

<DEPENDENCY.tensorflow==2.11.1> <CONTAINS> """CODE.class FruitTraceType:
  def _placeholder_value():
    return Fruit()


@tf.function
def foo(x):
  # Here `x` can be the placeholder value
  ...
""" .

<DEPENDENCY.tensorflow==2.12.0> <CONTAINS> """CODE.class FruitTraceType:
  def placeholder_value(self, placeholder_context=None):
    return Fruit()


@tf.function
def foo(x):
  # Here `x` is be the placeholder value
  ...
""" .

<DEPENDENCY.tensorflow==2.13.0> <CONTAINS> """CODE.strategy = tf.distribute.TPUStrategy(tpu_cluster_resolver)
# initialization omitted

with strategy.scope():
  # Save in the checkpoint.
  trained_step = tf.Variable(initial_value=tf.constant(0, dtype=tf.dtypes.int64), name='trained_step', aggregation=tf.VariableAggregation.ONLY_FIRST_REPLICA)

  checkpoint_manager = tf.train.CheckpointManager(checkpoint, directory, max_to_keep=1)
  preemption_handler = tf.distribute.experimental.PreemptionCheckpointHandler(cluster_resolver, checkpoint_manager)

while trained_step.numpy() < NUM_STEPS:
  # Train STEPS_IN_FUNCTION steps at once.
  train_multi_step_function()
  trained_step.assign_add(STEPS_IN_FUNCTION)
  preemption_handler.save_checkpoint_if_preempted()""",
        """CODE.with preemption_checkpoint_handler.watch_preemption_scope():
  while trained_step.numpy() < NUM_STEPS:

    # distributed_train_function contains a call to strategy.run.
    loss += preemption_checkpoint_handler.run(distributed_train_function, args=(next(iterator),))
    trained_step.assign_add(STEPS_PER_TRAIN_FUNCTION)""" .

<DEPENDENCY.tensorflow==2.14.0> <CONTAINS> """CODE.
ds = ...
# Round up to the next full batch.
target_cardinality = -(-ds.cardinality() // batch_size) * batch_size
ds = ds.apply(tf.data.experimental.pad_to_cardinality(target_cardinality))
# Set `drop_remainder` so that batch shape will be known statically. No data
# will actually be dropped since the batch size divides the cardinality.
ds = ds.batch(batch_size, drop_remainder=True)
""",
        """CODE.@ops.RegisterGradient("Relayout")
def _relayout_gradient(op, grad):
  return relayout_like(grad, layout_input=op.inputs[0])

@tf.function
def func(x):
  z = tf.ones(x.shape)
  z = dtensor.relayout_like(z, x)
  return x + z""",
        """CODE.def normalize_each_split(split):
  return split - tf.math.reduce_mean(split)

def tpu_computation(x):
  x_split = strategy.experimental_split_to_logical_devices(
              x, [num_cores_per_replica, 1])
  y = experimental_map_outside_compilation(
        normalize_each_split, x_split)
  y_split = strategy.experimental_split_to_logical_devices(
              x, [num_cores_per_replica, 1])
  return y_split
""",
        """CODE.seed = [1, 2]
new_seeds = tf.random.experimental.stateless_split(seed, num=3)
print(new_seeds)
tf.random.stateless_normal(shape=[3], seed=new_seeds[0, :])
""",
        """CODE.with strategy.scope():
  var = tf.Variable(initial_value=0.0, per_worker_variable=True)

All per-worker values can be retrieved and read into a list via
PerWorkerVariable.read_all()
""" .

<DEPENDENCY.tensorflow==2.14.1> <CONTAINS> """CODE.@linear_operator_algebra.RegisterAdjoint(lin_op.LinearOperatorIdentity)
def _adjoint_identity(lin_op_a):
  # Return the identity matrix.""",
        """CODE.@linear_operator_algebra.RegisterCholesky(lin_op.LinearOperatorIdentity)
def _cholesky_identity(lin_op_a):
  # Return the identity matrix.""",
        """CODE.@linear_operator_algebra.RegisterInverse(lin_op.LinearOperatorIdentity)
def _inverse_identity(lin_op_a):
  # Return the identity matrix.""",
        """CODE.@linear_operator_algebra.RegisterMatmul(
  lin_op.LinearOperatorIdentity,
  lin_op.LinearOperatorIdentity)
def _matmul_identity(a, b):
  # Return the identity matrix.""",
        """CODE.@linear_operator_algebra.RegisterSolve(
  lin_op.LinearOperatorIdentity,
  lin_op.LinearOperatorIdentity)
def _solve_identity(a, b):
  # Return the identity matrix.""",
        """CODE.@tf.function
def f(x):
  return x

f_concrete = f.get_concrete_function(tf.constant(1.0))
f_concrete = f.get_concrete_function(x=tf.constant(1.0))

@tf.function
def f(x):
  return x

f_concrete = f.get_concrete_function(tf.TensorSpec([], tf.float64))

@tf.function(input_signature=[tf.TensorSpec(None, tf.float32)])
def f(x):
  return x

f_concrete = f.get_concrete_function()""",
        """CODE.@tf.function(jit_compile=True)
def f(x):
    return x + 1

f.experimental_get_compiler_ir(tf.random.normal([10, 10])(stage='hlo')


y = tf.Variable(tf.zeros([10, 20], dtype=tf.float32))

@tf.function(jit_compile=True)
def f(x):
    return x + y

hlo_str = f.experimental_get_compiler_ir(tf.TensorSpec(shape=(10, 20)))(stage='hlo')
""" .

<DEPENDENCY.tensorflow==2.15.0> <CONTAINS> """CODE.@tf.function
def f(x):
  return x

f_concrete = f.get_concrete_function(tf.constant(1.0))
f_concrete = f.get_concrete_function(x=tf.constant(1.0))

@tf.function
def f(x):
  return x

f_concrete = f.get_concrete_function(tf.TensorSpec([], tf.float64))

@tf.function(input_signature=[tf.TensorSpec(None, tf.float32)])
def f(x):
  return x

f_concrete = f.get_concrete_function()

f_concrete(tf.constant(1.0))
f_concrete(x=tf.constant(1.0))""",
        """CODE.@tf.function(jit_compile=True)
def f(x):
    return x + 1

f.experimental_get_compiler_ir(tf.random.normal([10, 10])(stage='hlo')


y = tf.Variable(tf.zeros([10, 20], dtype=tf.float32))

@tf.function(jit_compile=True)
def f(x):
    return x + y

hlo_str = f.experimental_get_compiler_ir(tf.TensorSpec(shape=(10, 20)))(stage='hlo')
""",
        """CODE.representative_dataset = [{"input": tf.random.uniform(shape=(3, 3))}
                      for _ in range(256)]

dataset_file_map = (
  tf.quantization.experimental.TfRecordRepresentativeDatasetSaver(
        path_map={'serving_default': '/tmp/representative_dataset_path'}
    ).save({'serving_default': representative_dataset})
)

quantization_options = tf.quantization.experimental.QuantizationOptions(
    signature_keys=['serving_default'],
    representative_datasets=dataset_file_map,
)
tf.quantization.experimental.quantize_saved_model(
    '/tmp/input_model',
    '/tmp/output_model',
    quantization_options=quantization_options,
)
""" .

<DEPENDENCY.tensorflow==2.2.0> <CONTAINS> """CODE.  layer = Hashing(num_bins=3)
  inp = np.asarray([['A'], ['B'], ['C'], ['D'], ['E']])
  layer(inputs)
  [[1], [0], [1], [1], [2]]


  layer = Hashing(num_bins=3, salt=[133, 137])
  inp = np.asarray([['A'], ['B'], ['C'], ['D'], ['E']])
  layer(inputs)
  [[1], [2], [1], [0], [2]]
""",
        """CODE.# Initializing TPU system with 2 logical devices and 4 replicas.
resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='')
tf.config.experimental_connect_to_cluster(resolver)
topology = tf.tpu.experimental.initialize_tpu_system(resolver)
device_assignment = tf.tpu.experimental.DeviceAssignment.build(
    topology,
    computation_shape=[1, 1, 2],
    num_replicas=4)
strategy = tf.distribute.experimental.TPUStrategy(
    resolver, device_assignment=device_assignment)

iterator = iter(inputs)

@tf.function()
def step_fn(inputs):
  images, labels = inputs
  images = strategy.experimental_split_to_logical_devices(
    inputs, [1, 2, 4, 1])

  // model() function will be executed on 8 logical devices with `inputs`
  // split 2 * 4  ways.
  output = model(inputs)

  // For loss calculation, all logical devices share the same logits
  // and labels.
  labels = strategy.experimental_replicate_to_logical_devices(labels)
  output = strategy.experimental_replicate_to_logical_devices(output)
  loss = loss_fn(labels, output)

  return loss

strategy.run(step_fn, args=(next(iterator),))
""",
        """CODE.# Initializing TPU system with 2 logical devices and 4 replicas.
resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='')
tf.config.experimental_connect_to_cluster(resolver)
topology = tf.tpu.experimental.initialize_tpu_system(resolver)
device_assignment = tf.tpu.experimental.DeviceAssignment.build(
    topology,
    computation_shape=[1, 1, 2],
    num_replicas=4)
strategy = tf.distribute.experimental.TPUStrategy(
    resolver, device_assignment=device_assignment)
iterator = iter(inputs)

@tf.function()
def step_fn(inputs):
  output = tf.add(inputs, inputs)

  // Add operation will be executed on logical device 0.
  output = strategy.experimental_assign_to_logical_device(output, 0)
  return output

strategy.run(step_fn, args=(next(iterator),))
""",
        """CODE.@def_function.function
def f(x):
  return x + constant_op.constant(1.)

with context.collect_graphs() as graphs:
  with ops.device("CPU:0"):
    f(constant_op.constant(1.))

graph, = graphs  # `graph` contains a single GraphDef for inspection
""",
        """CODE.@register_acd_resource_resolver
def ResolveIdentity(op, resource_reads, resource_writes):
  # op: The `Operation` being processed by ACD currently.
  # resource_reads: An `ObjectIdentitySet` of read-only resources.
  # resource_writes: An `ObjectIdentitySet` of read-write resources.
  if not resource_reads or resource_writes:
    return False
  def update(resource_inputs):
    to_add = []
    to_remove = []
    for t in resource_inputs:
      if t.op.type == "Identity":
        to_remove.append(t)
        to_add.append(t.op.inputs[0])
    if not to_add and not to_remove:
      return False
    for t in to_remove:
      resource_inputs.discard(t)
    resource_inputs.update(to_add)
    return True
  return update(resource_reads) or update(resource_writes)
""",
        """CODE.@testing_utils.run_with_all_saved_model_formats
def test_foo(self):
    save_format = testing_utils.get_save_format()
    saved_model_dir = '/tmp/saved_model/'
    model = keras.models.Sequential()
    model.add(keras.layers.Dense(2, input_shape=(3,)))
    model.add(keras.layers.Dense(3))
    model.compile(loss='mse', optimizer='sgd', metrics=['acc'])

    keras.models.save_model(model, saved_model_dir, save_format=save_format)
    model = keras.models.load_model(saved_model_dir)""",
        """CODE.@tf.function
def compare_round_trip():
  samples = 1000
  frame_length = 400
  halflen = frame_length // 2
  waveform = tf.random.normal(dtype=tf.float32, shape=[samples])
  waveform_pad = tf.pad(waveform, [[halflen, 0],])
  mdct = tf.signal.mdct(waveform_pad, frame_length, pad_end=True,
                        window_fn=tf.signal.vorbis_window)
  inverse_mdct = tf.signal.inverse_mdct(mdct,
                                        window_fn=tf.signal.vorbis_window)
  inverse_mdct = inverse_mdct[halflen: halflen + samples]
  return waveform, inverse_mdct

waveform, inverse_mdct = compare_round_trip()
np.allclose(waveform.numpy(), inverse_mdct.numpy(), rtol=1e-3, atol=1e-4)""",
        """CODE.@tf.function
def replica_fn(input):
    return input*2.0

strategy = tf.distribute.MirroredStrategy()
tensor_input = tf.constant(3.0)
result = strategy.run(replica_fn, args=(tensor_input,))

@tf.function
def run():
    def value_fn(value_context):
        return value_context.num_replicas_in_sync
    distributed_values = (
        strategy.experimental_distribute_values_from_function(
            value_fn))
    def replica_fn2(input):
        return input*2
    return strategy.run(replica_fn2, args=(distributed_values,))

result = run()""",
        """CODE.a = tf.constant([True])
b = tf.constant([False])
tf.math.logical_and(a, b)


c = tf.constant([True])
x = tf.constant([False, True, True, False])
tf.math.logical_and(c, x)


y = tf.constant([False, False, True, True])
z = tf.constant([False, True, False, True])
tf.math.logical_and(y, z)
""",
        """CODE.a = tf.experimental.dlpack.from_dlpack(dlcapsule)
# `a` uses the memory shared by dlpack
""",
        """CODE.a = tf.tensor([1, 10])
dlcapsule = tf.experimental.dlpack.to_dlpack(a)
# dlcapsule represents the dlpack data structure
""",
        """CODE.cache = ContextValueCache(int)
cache[None] += 2
cache[None] += 4
assert cache[None] == 6

with tf.Graph().as_default() as g:
  cache[None] += 5
  cache[g] += 3
assert cache[g] == 8

cache = ContextValueCache(lambda x: x + 1)
g = tf.get_default_graph()

value = cache.setdefault(key=g, kwargs={'x': 3})
assert cache[g] == 4""",
        """CODE.cdf = tf.config.experimental.ClusterDeviceFilters()
for i in range(num_workers):
  cdf.set_device_filters('worker', i, ['/job:ps'])
for i in range(num_ps):
  cdf.set_device_filters('ps', i, ['/job:worker'])

tf.config.experimental_connect_to_cluster(cluster_def,
                                          cluster_device_filters=cdf)
""",
        """CODE.dataset = tf.data.TFRecordDataset("examples.tfrecord")
cardinality = tf.data.experimental.cardinality(dataset)
print((cardinality == tf.data.experimental.UNKNOWN_CARDINALITY).numpy())
True
dataset = dataset.apply(tf.data.experimental.assert_cardinality(42))
print(tf.data.experimental.cardinality(dataset).numpy())
42""",
        """CODE.def dataset_fn(input_context):
  batch_size = input_context.get_per_replica_batch_size(global_batch_size)
  d = tf.data.Dataset.from_tensors([[1.]]).repeat().batch(batch_size)
  return d.shard(
      input_context.num_input_pipelines, input_context.input_pipeline_id)

inputs = strategy.experimental_distribute_datasets_from_function(dataset_fn)

for batch in inputs:
  replica_results = strategy.run(replica_fn, args=(batch,))

# If you want to specify `input_signature` for a `tf.function` you must
# first create the iterator.
iterator = iter(inputs)

@tf.function(input_signature=[iterator.element_spec])
def replica_fn_with_signature(inputs):
  # train the model with inputs
  return

for _ in range(steps):
  strategy.run(replica_fn_with_signature,
      args=(next(iterator),))
""",
        """CODE.def setdiff1d(x, y):
    out = [val for val in x if val not in y]
    idx = [x.index(val) for val in out]
    return out, idx

x = [1, 2, 3, 4, 5, 6]
y = [1, 3, 5]
out, idx = setdiff1d(x, y)
print("ListDiff(out={}, idx={})".format(out, idx))
""",
        """CODE.def value_fn(context):
    return context.replica_id_in_sync_group/context.num_replicas_in_sync

context = tf.distribute.experimental.ValueContext(
    replica_id_in_sync_group=2, num_replicas_in_sync=4)
per_replica_value = value_fn(context)
per_replica_value

strategy = tf.distribute.MirroredStrategy()
def value_fn(value_context):
    return value_context.num_replicas_in_sync

distributed_values = (
     strategy.experimental_distribute_values_from_function(
       value_fn))
local_result = strategy.experimental_local_results(distributed_values)
local_result""",
        """CODE.estimator = tf.estimator.tpu.TPUEstimator(
    ...
    embedding_config_spec=tf.estimator.tpu.experimental.EmbeddingConfigSpec(
        ...
        optimization_parameters=tf.tpu.experimental.FtrlParameters(0.1),
        ...))""",
        """CODE.loss = 100 * mean(abs(y_true - y_pred) / y_true, axis=-1)
y_true = np.random.random(size=(2, 3))
y_true = np.maximum(y_true, 1e-7)  # Prevent division by zero
y_pred = np.random.random(size=(2, 3))
loss = tf.keras.losses.mean_absolute_percentage_error(y_true, y_pred)
assert loss.shape == (2,)
assert np.array_equal(
    loss.numpy(),
    100. * np.mean(np.abs((y_true - y_pred) / y_true), axis=-1)""",
        """CODE.m = tf.keras.metrics.RecallAtPrecision(0.8, num_thresholds=1)
_ = m.update_state([0, 0, 1, 1], [0, 0.5, 0.3, 0.9])
m.result().numpy()

m.reset_states()
_ = m.update_state([0, 0, 1, 1], [0, 0.5, 0.3, 0.9],
                   sample_weight=[1, 0, 0, 1])
m.result().numpy()
""",
        """CODE.model = tf.keras.Sequential()
model.add(tf.keras.layers.Dense(16))
model.add(tf.keras.layers.experimental.SyncBatchNormalization())""",
        """CODE.operator_0 = tf.linalg.LinearOperatorFullMatrix([[1., 2.], [3., 4.]])
operator_1 = tf.linalg.LinearOperatorFullMatrix([[1., 0.], [0., 1.]])
operator_2 = tf.linalg.LinearOperatorLowerTriangular([[5., 6.], [7., 8]])
operator = LinearOperatorBlockLowerTriangular(
  [[operator_0], [operator_1, operator_2]])

operator.to_dense()
operator.shape
operator.log_abs_determinant()
x0 = [[1., 6.], [-3., 4.]]
x1 = [[0., 2.], [4., 0.]]
x = tf.concat([x0, x1], 0)  # Shape [2, 4] Tensor
operator.matmul(x)
tf.concat([operator_0.matmul(x0),
  operator_1.matmul(x0) + operator_2.matmul(x1)], axis=0)
matrix_44 = tf.random.normal(shape=[2, 3, 4, 4])
operator_44 = tf.linalg.LinearOperatorFullMatrix(matrix_44)
matrix_54 = tf.random.normal(shape=[1, 3, 5, 4])
operator_54 = tf.linalg.LinearOperatorFullMatrix(matrix_54)
matrix_55 = tf.random.normal(shape=[1, 3, 5, 5])
operator_55 = tf.linalg.LinearOperatorFullMatrix(matrix_55)
operator_99 = LinearOperatorBlockLowerTriangular(
  [[operator_44], [operator_54, operator_55]])
operator_99.shape
x = tf.random.normal(shape=[2, 1, 9])
y = operator_99.matvec(x)
y.shape
""",
        """CODE.reader = tf_record_random_reader(file_path)

record_1, offset_1 = reader.read(0)  # 0 is the initial offset.
# offset_1 is the ending offset of the 1st record and the starting offset of
# the next.

record_2, offset_2 = reader.read(offset_1)
# offset_2 is the ending offset of the 2nd record and the starting offset of
# the next.
# We can jump back and read the first record again if so desired.
reader.read(0)
""",
        """CODE.resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='')
tf.config.experimental_connect_to_cluster(resolver)
tf.tpu.experimental.initialize_tpu_system(resolver)
strategy = tf.distribute.experimental.TPUStrategy(tpu='')

options = tf.distribute.RunOptions()
options.experimental_bucketizing_dynamic_shape = True

iterator = iter(inputs)

@tf.function()
def step_fn(inputs):
  output = tf.reduce_sum(inputs)
  return output

strategy.run(step_fn, args=(next(iterator),),
                           options=options)
""",
        """CODE.resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='')
tf.config.experimental_connect_to_cluster(resolver)
topology = tf.tpu.experimental.initialize_tpu_system(resolver)
device_assignment = tf.tpu.experimental.DeviceAssignment.build(
    topology,
    computation_shape=[2, 2, 2],
    num_replicas=1)
strategy = tf.distribute.experimental.TPUStrategy(
    resolver, device_assignment=device_assignment)

iterator = iter(inputs)

@tf.function()
def step_fn(inputs):
  inputs = strategy.experimental_split_to_logical_devices(
    inputs, [1, 2, 4, 1])

  // model() function will be executed on 8 logical devices with `inputs`
  // split 2 * 4  ways.
  output = model(inputs)
  return output

strategy.run(step_fn, args=(next(iterator),))
""",
        """CODE.sequence = [[1], [2, 3], [4, 5, 6]]
tf.keras.preprocessing.sequence.pad_sequences(sequence)
array([[0, 0, 1],
       [0, 2, 3],
       [4, 5, 6]], dtype=int32)

tf.keras.preprocessing.sequence.pad_sequences(sequence, value=-1)
array([[-1, -1,  1],
       [-1,  2,  3],
       [ 4,  5,  6]], dtype=int32)

tf.keras.preprocessing.sequence.pad_sequences(sequence, padding='post')
array([[1, 0, 0],
       [2, 3, 0],
       [4, 5, 6]], dtype=int32)

tf.keras.preprocessing.sequence.pad_sequences(sequence, maxlen=2)
array([[0, 1],
       [2, 3],
       [5, 6]], dtype=int32)
""",
        """CODE.strategy = tf.distribute.MirroredStrategy()

# Create a dataset
dataset = dataset_ops.Dataset.TFRecordDataset([
  "/a/1.tfr", "/a/2.tfr", "/a/3.tfr", "/a/4.tfr"])

# Distribute that dataset
dist_dataset = strategy.experimental_distribute_dataset(dataset)

# Iterate over the distributed dataset
for x in dist_dataset:
  # process dataset elements
  strategy.run(train_step, args=(x,))


strategy = tf.distribute.MirroredStrategy()

# Create a dataset
dataset = dataset_ops.Dataset.TFRecordDataset([
  "/a/1.tfr", "/a/2.tfr", "/a/3.tfr", "/a/4.tfr"])

# Distribute that dataset
dist_dataset = strategy.experimental_distribute_dataset(dataset)

@tf.function(input_signature=[dist_dataset.element_spec])
def train_step(inputs):
  # train model with inputs
  return

# Iterate over the distributed dataset
for x in dist_dataset:
  # process dataset elements
  strategy.run(train_step, args=(x,))
""",
        """CODE.strategy = tf.distribute.MirroredStrategy()
def value_fn(ctx):
  return tf.constant(1.)
distributed_values = (
     strategy.experimental_distribute_values_from_function(
       value_fn))
local_result = strategy.experimental_local_results(distributed_values)
local_result

strategy = tf.distribute.MirroredStrategy()
array_value = np.array([3., 2., 1.])
def value_fn(ctx):
  return array_value[ctx.replica_id_in_sync_group]
distributed_values = (
     strategy.experimental_distribute_values_from_function(
       value_fn))
local_result = strategy.experimental_local_results(distributed_values)
local_result

strategy = tf.distribute.MirroredStrategy()
def value_fn(ctx):
  return ctx.num_replicas_in_sync
distributed_values = (
     strategy.experimental_distribute_values_from_function(
       value_fn))
local_result = strategy.experimental_local_results(distributed_values)
local_result

strategy = tf.distribute.TPUStrategy()
worker_devices = strategy.extended.worker_devices
multiple_values = []
for i in range(strategy.num_replicas_in_sync):
  with tf.device(worker_devices[i]):
    multiple_values.append(tf.constant(1.0))

def value_fn(ctx):
  return multiple_values[ctx.replica_id]

distributed_values = strategy.
  experimental_distribute_values_from_function(
  value_fn)""",
        """CODE.superdiag = [3., 4., 5.]
diag = [1., -1., 2.]
subdiag = [6., 7., 8]
operator = tf.linalg.LinearOperatorTridiag(
   [superdiag, diag, subdiag],
   diagonals_format='sequence')
operator.to_dense()
operator.shape
operator.log_abs_determinant()

diagonals = tf.random.normal(shape=[2, 3, 3, 4])
operator = tf.linalg.LinearOperatorTridiag(
   diagonals,
   diagonals_format='compact')

y = tf.random.normal(shape=[2, 1, 4, 2])
x = operator.solve(y)
x
""",
        """CODE.text_parts, floats = _FloatExtractor()("Text 1.0 Text")
text_parts
["Text ", " Text"]
floats
np.array([1.0])""",
        "CODE.tf.math.ceil([-1.7, -1.5, -0.2, 0.2, 1.5, 1.7, 2.0])",
        "CODE.tf.math.special.dawsn([-1., -0.5, 0.5, 1.]).numpy()",
        "CODE.tf.math.special.expint([1., 1.1, 2.1, 4.1]).numpy()",
        "CODE.tf.math.special.fresnel_cos([-1., -0.1, 0.1, 1.]).numpy()",
        "CODE.tf.math.special.fresnel_sin([-1., -0.1, 0.1, 1.]).numpy()",
        """CODE.tf.math.special.spence([0.5, 1., 2., 3.]).numpy()
array([ 0.58224034,  0.        , -0.82246685, -1.4367464], dtype=float32)""",
        """CODE.tf.math.xlog1py(0., 1.)
tf.math.xlog1py(1., 1.)
tf.math.xlog1py(2., 2.)
tf.math.xlog1py(0., -1.)""",
        """CODE.try:
  with tf.experimental.async_scope():
    for _ in range(num_steps):
      # Step function updates the metric `loss` internally
      train_step_fn()
except tf.errors.OutOfRangeError:
  tf.experimental.async_clear_error()
logging.info('loss =', loss.numpy())
""",
        """CODE.updater = tf.compat.v1.saved_model.MethodNameUpdater(export_dir)
updater.replace_method_name(signature_key="foo", method_name="regress")
updater.replace_method_name(signature_key="bar", method_name="classify", tags="serve")
updater.save(new_export_dir)
""",
        """CODE.with MemoryChecker() as memory_checker:
  tensors = []
  for _ in range(10):
    tensors.append(tf.constant(1))
    memory_checker.record_snapshot()

memory_checker.report()
memory_checker.assert_no_leak_if_all_possibly_except_one()""",
        """CODE.with tf.profiler.experimental.Profile("/path/to/logdir"):
  # do some work
""",
        """CODE.x = tf.constant(5)
y = tf.constant(10)
z = tf.constant(10)
tensor_set = {x.ref(), y.ref(), z.ref()}
x.ref() in tensor_set
True
tensor_dict = {x.ref(): 'five', y.ref(): 'ten', z.ref(): 'ten'}
tensor_dict[y.ref()]
'ten'
x = tf.constant(5)
x.ref().deref()
<tf.Tensor: shape=(), dtype=int32, numpy=5>""",
        """CODE.x = tf.constant([2., 0., -2.])
tf.math.rsqrt(x)""",
        """CODE.y_true = np.random.randint(0, 2, size=(2, 3))
y_pred = np.random.random(size=(2, 3))
loss = tf.keras.losses.mean_squared_logarithmic_error(y_true, y_pred)
assert loss.shape == (2,)
y_true = np.maximum(y_true, 1e-7)
y_pred = np.maximum(y_pred, 1e-7)
assert np.array_equal(
...     loss.numpy(),
...     np.mean(
...         np.square(np.log(y_true + 1.) - np.log(y_pred + 1.)), axis=-1))""" .

<DEPENDENCY.tensorflow==2.3.0> <CONTAINS> """CODE.
dataset = tf.data.Dataset.range(5)
dataset = dataset.map(lambda x: x*x)
dataset = dataset.apply(
    tf.data.experimental.service.distribute("parallel_epochs",
                                            "grpc://dataservice:5000"))
dataset = dataset.map(lambda x: x+1)

for element in dataset:
  print(element)  # prints { 1, 2, 5, 10, 17 }

""",
        """CODE.
dispatcher = tf.data.experimental.service.DispatchServer(port=5050)
dispatcher.join()
""",
        """CODE.
worker_server = tf.data.experimental.service.WorkerServer(
    port=5051, dispatcher_address="grpc://localhost:5050")
worker_server.join()
""",
        """CODE.  # Populating a metadata file (or a metadta buffer) and associated files to
  a model file:
  populator = MetadataPopulator.with_model_file(model_file)
  # For metadata buffer (bytearray read from the metadata file), use:
  # populator.load_metadata_buffer(metadata_buf)
  populator.load_metadata_file(metadata_file)
  populator.load_associated_files([label.txt])
  populator.populate()

  # Populating a metadata file (or a metadta buffer) and associated files to
  a model buffer:
  populator = MetadataPopulator.with_model_buffer(model_buf)
  populator.load_metadata_file(metadata_file)
  populator.load_associated_files([label.txt])
  populator.populate()
  # Writing the updated model buffer into a file.
  updated_model_buf = populator.get_model_buffer()
  with open("updated_model.tflite", "wb") as f:
    f.write(updated_model_buf)
""",
        """CODE.  class FooType(object):

    def __init__(self):
      self.foo_property = None

  class DummyTransformer(NodeStateTracker, ast.NodeTransformer):

    def visit_If(self, node):
      self.state[FooType].enter()
      self.state[FooType].foo_property = node
      node = self.veneric_visit(node)
      self.state[FooType].exit()
      return node

    def visit_Name(self, node):
      self.state[FooType].foo_property  # will hold the innermost enclosing if

    def visit_If(self, node):
      with self.state[FooType] as foo:
        foo.foo_property = node
        return self.generic_visit(node)
""",
        """CODE.  def call(function):
    with tf.profiler.experimental.Trace("call",
         function_name=function.name) as tm:
      binary, in_cache = jit_compile(function)
      tm.set_metadata(in_cache=in_cache)
      execute(binary)
""",
        """CODE.# Setup TPUStrategy
resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='')
tf.config.experimental_connect_to_cluster(resolver)
tf.tpu.experimental.initialize_tpu_system(resolver)
strategy = tf.distribute.TPUStrategy(resolver)

dataset = tf.data.Dataset.range(16)
distributed_dataset_on_host = (
    strategy.experimental_distribute_dataset(
        dataset,
        tf.distribute.InputOptions(
            experimental_prefetch_to_device=False)))
""",
        """CODE.@tf.function
def one_step(input):
    return input

for _ in range(step_num):
    strategy.run(one_step, args=(dist_dataset_iterator.get_next(),))""",
        """CODE.@tf.function
def run():
  def value_fn(value_context):
    return value_context.num_replicas_in_sync
  distributed_values = (
      strategy.experimental_distribute_values_from_function(value_fn))
  def replica_fn(input):
    return input * 2
  return strategy.run(replica_fn, args=(distributed_values,))""",
        """CODE.@tf.function
def sqrt(x):
    y = x / 2
    d = y
    for _ in range(10):
        d /= 2
        if y * y < x:
            y += d
        else:
            y -= d
        ys.append(y.numpy())
    return y

tf.config.experimental_run_functions_eagerly(True)""",
        """CODE.c = tf.MatMul(a, b)
tensor_tracer.trace_tensor(c)
d = tf.add(c, 1)""",
        """CODE.class InterruptingCallback(tf.keras.callbacks.Callback):
  def on_epoch_begin(self, epoch, logs=None):
    if epoch == 4:
      raise RuntimeError('Interrupting!')

callback = tf.keras.callbacks.experimental.BackupAndRestore(
backup_dir="/tmp")

model = tf.keras.models.Sequential([tf.keras.layers.Dense(10)])
model.compile(tf.keras.optimizers.SGD(), loss='mse')

try:
  model.fit(np.arange(100).reshape(5, 20), np.zeros(5), epochs=10,
            batch_size=1, callbacks=[callback, InterruptingCallback()],
            verbose=0)
except:
  pass

history = model.fit(np.arange(100).reshape(5, 20), np.zeros(5), epochs=10,
            batch_size=1, callbacks=[callback], verbose=0)
# Only 6 more epochs are run, since first trainning got interrupted at
# zero-indexed epoch 4, second training will continue from 4 to 9.
len(history.history['loss'])
""",
        """CODE.dataset = tf.data.Dataset.from_tensors(42)
iterator = iter(dataset)
iterator.element_spec
tf.TensorSpec(shape=(), dtype=tf.int32, name=None)""",
        """CODE.dataset = tf.data.Dataset.from_tensors(42)
iterator = iter(dataset)
optional = iterator.get_next_as_optional()
print(optional.has_value())
tf.Tensor(True, shape=(), dtype=bool)
print(optional.get_value())
tf.Tensor(42, shape=(), dtype=int32)
optional = iterator.get_next_as_optional()
print(optional.has_value())
tf.Tensor(False, shape=(), dtype=bool)""",
        """CODE.dataset = tf.data.Dataset.from_tensors(42)
iterator = iter(dataset)
print(iterator.get_next())""",
        """CODE.dataset = tf.data.Dataset.range(42)
print(dataset.cardinality().numpy())
dataset = dataset.repeat()
cardinality = dataset.cardinality()
print((cardinality == tf.data.INFINITE_CARDINALITY).numpy())
dataset = dataset.filter(lambda x: True)
cardinality = dataset.cardinality()
print((cardinality == tf.data.UNKNOWN_CARDINALITY).numpy())""",
        """CODE.def f1(): return tf.constant(1)
def f2(): return tf.constant(2)
r = tf.execute_fn_for_device({"CPU": f1, "GPU": f2}, default_fn=f1)
""",
        """CODE.def proc_func():
  # user code to be run
  import pdb; pdb.set_trace()

def follow_ups():
  time.sleep(5)
  mpr.start_single_process(
      task_type='evaluator',
      task_id=0)

mpr = multi_process_runner.MultiProcessRunner(
    proc_func,
    multi_worker_test_base.create_cluster_spec(
        has_chief=True, num_workers=1))
threading.Thread(target=follow_ups).start()
mpr.start_in_process_as(as_task_type='chief', as_task_id=0)
mpr.join()
""",
        """CODE.def step_fn(x):
...   return x
@tf.function
... def train_fn(distributed_iterator):
...   for _ in tf.range(steps_per_loop):
...     optional_data = distributed_iterator.get_next_as_optional()
...     if not optional_data.has_value():
...       break
...     tf.print(strategy.run(step_fn, args=(optional_data.get_value(),)))
train_fn(distributed_iterator)""",
        """CODE.dispatcher = tf.data.experimental.service.DispatchServer(port=0)
dataset = tf.data.Dataset.range(10)
dataset = dataset.apply(tf.data.experimental.service.distribute(
    processing_mode="parallel_epochs", service=dispatcher.target))""",
        """CODE.dispatcher = tf.data.experimental.service.DispatchServer(port=0)
dispatcher_address = dispatcher.target.split("://")[1]
worker = tf.data.experimental.service.WorkerServer(
    port=0, dispatcher_address=dispatcher_address)
dataset = tf.data.Dataset.range(10)
dataset = dataset.apply(tf.data.experimental.service.distribute(
    processing_mode="parallel_epochs", service=dispatcher.target))
print(list(dataset.as_numpy_iterator()))

dispatcher = tf.data.experimental.service.DispatchServer(port=5050)
dispatcher.join()""",
        """CODE.dispatcher = tf.data.experimental.service.DispatchServer(port=0, start=False)
dispatcher.start()""",
        """CODE.from keras.models import load_model

model.save('my_model.h5')  # creates a HDF5 file 'my_model.h5'
del model  # deletes the existing model

# returns a compiled model
# identical to the previous one
model = load_model('my_model.h5')
""",
        """CODE.global_batch_size = 16
strategy = tf.distribute.MirroredStrategy()
dataset = tf.data.Dataset.from_tensors(([1.],[2])).repeat(100).batch(global_batch_size)
dist_dataset = strategy.experimental_distribute_dataset(dataset)
dist_dataset.element_spec
(TensorSpec(shape=(None, 1), dtype=tf.float32, name=None),
 TensorSpec(shape=(None, 1), dtype=tf.int32, name=None))

strategy = tf.distribute.MirroredStrategy(['/gpu:0', '/gpu:1'])
(PerReplicaSpec(TensorSpec(shape=(None, 1), dtype=tf.float32, name=None),
                TensorSpec(shape=(None, 1), dtype=tf.float32, name=None)),
 PerReplicaSpec(TensorSpec(shape=(None, 1), dtype=tf.int32, name=None),
                TensorSpec(shape=(None, 1), dtype=tf.int32, name=None))""",
        """CODE.global_batch_size = 16
strategy = tf.distribute.MirroredStrategy()
dataset = tf.data.Dataset.from_tensors(([1.],[2])).repeat(100).batch(global_batch_size)
distributed_iterator = iter(strategy.experimental_distribute_dataset(dataset))
distributed_iterator.element_spec
(TensorSpec(shape=(None, 1), dtype=tf.float32, name=None),
 TensorSpec(shape=(None, 1), dtype=tf.int32, name=None))

strategy = tf.distribute.MirroredStrategy(['/gpu:0', '/gpu:1'])
(PerReplicaSpec(TensorSpec(shape=(None, 1), dtype=tf.float32, name=None),
                TensorSpec(shape=(None, 1), dtype=tf.float32, name=None)),
 PerReplicaSpec(TensorSpec(shape=(None, 1), dtype=tf.int32, name=None),
                TensorSpec(shape=(None, 1), dtype=tf.int32, name=None))""",
        """CODE.gpu_devices = tf.config.list_physical_devices('GPU')
if gpu_devices:
...   details = tf.config.experimental.get_device_details(gpu_devices[0])
...   details.get('device_name', 'Unknown GPU')""",
        """CODE.image = tf.keras.preprocessing.image.load_img(image_path)
input_arr = keras.preprocessing.image.img_to_array(image)
input_arr = np.array([input_arr])  # Convert single image to a batch.
predictions = model.predict(input_arr)
""",
        """CODE.initializer = tf.keras.initializers.HeUniform()
values = initializer(shape=(2, 2))

initializer = tf.keras.initializers.HeUniform()
layer = tf.keras.layers.Dense(3, kernel_initializer=initializer)""",
        """CODE.initializer = tf.keras.initializers.LecunNormal()
values = initializer(shape=(2, 2))

initializer = tf.keras.initializers.LecunNormal()
layer = tf.keras.layers.Dense(3, kernel_initializer=initializer)""",
        """CODE.initializer = tf.keras.initializers.LecunUniform()
values = initializer(shape=(2, 2))

initializer = tf.keras.initializers.LecunUniform()
layer = tf.keras.layers.Dense(3, kernel_initializer=initializer)""",
        """CODE.input_data = data[:-10]
targets = data[10:]
dataset = tf.keras.preprocessing.timeseries_dataset_from_array(
    input_data, targets, sequence_length=10)
for batch in dataset:
  inputs, targets = batch
  assert np.array_equal(inputs[0], data[:10])  # First sequence: steps [0-9]
  assert np.array_equal(targets[0], data[10])  # Corresponding target: step 10
  break
""",
        """CODE.keys = tf.data.Dataset.range(100)
values = tf.data.Dataset.range(100).map(
    lambda x: string_ops.as_string(x * 2))
ds = tf.data.Dataset.zip((keys, values))
init = tf.lookup.experimental.DatasetInitializer(ds)
table = tf.lookup.StaticHashTable(init, "")
output = table.lookup([0, 1, 2])
assertEquals(outputs, ["0", "2", "4"])""",
        """CODE.layer = tf.keras.layers.experimental.preprocessing.CategoryEncoding(
          max_tokens=4, output_mode="count")
layer([[0, 1], [0, 0], [1, 2], [3, 1]])

layer = tf.keras.layers.experimental.preprocessing.CategoryEncoding(
          max_tokens=4, output_mode="count")
count_weights = np.array([[.1, .2], [.1, .1], [.2, .3], [.4, .2]])
layer([[0, 1], [0, 0], [1, 2], [3, 1]], count_weights=count_weights)
""",
        """CODE.layer = tf.keras.layers.experimental.preprocessing.CategoryEncoding(
    max_tokens=4, output_mode="count")
count_weights = np.array([[.1, .2], [.1, .1], [.2, .3], [.4, .2]])
layer([[0, 1], [0, 0], [1, 2], [3, 1]], count_weights=count_weights)""",
        """CODE.means = 0.
stddevs = tf.math.exp(tf.random.uniform(shape=[2, 3]))
minvals = [-1., -2., -1000.]
maxvals = [[10000.], [1.]]
y = tf.random.stateless_parameterized_truncated_normal(
  shape=[10, 2, 3], seed=[7, 17],
  means=means, stddevs=stddevs, minvals=minvals, maxvals=maxvals)
y.shape""",
        """CODE.nest.flatten(data, expand_composites=True)
nest.flatten(nest.map(
    data, lambda x: _flatten_and_filter_composite(x, False, True)))
""",
        """CODE.optional = tf.experimental.Optional.empty(
  tf.TensorSpec(shape=(), dtype=tf.int32, name=None))
print(optional.has_value())
""",
        """CODE.optional = tf.experimental.Optional.from_value(42)
print(optional.element_spec)
tf.TensorSpec(shape=(), dtype=tf.int32, name=None)""",
        """CODE.os.environ['TF_CONFIG'] = json.dumps({
  'cluster': {
      'worker': ["localhost:12345", "localhost:23456"],
      'ps': ["localhost:34567"]
  },
  'task': {'type': 'worker', 'index': 0}
})

# This implicitly uses TF_CONFIG for the cluster and current task info.
strategy = tf.distribute.experimental.MultiWorkerMirroredStrategy()

...

if strategy.cluster_resolver.task_type == 'worker':
  # Perform something that's only applicable on workers. Since we set this
  # as a worker above, this block will run on this particular instance.
elif strategy.cluster_resolver.task_type == 'ps':
  # Perform something that's only applicable on parameter servers. Since we
  # set this as a worker above, this block will not run on this particular
  # instance.
""",
        """CODE.p1 = RowPartition.from_row_lengths([4, 0, 3, 1, 0])
p2 = RowPartition.from_row_splits([0, 4, 4, 7, 8, 8])
p3 = RowPartition.from_row_starts([0, 4, 4, 7, 8], nvals=8)
p4 = RowPartition.from_row_limits([4, 4, 7, 8, 8])
p5 = RowPartition.from_value_rowids([0, 0, 0, 0, 2, 2, 2, 3], nrows=5)
""",
        """CODE.partition = RowPartition.from_row_lengths([2, 0, 1])
_partition_outer_dimension(tf.constant([1, 2, 3]), partition)

struct_value = StructuredTensor.from_pyval(
    [{'x': 1}, {'x': 2}, {'x': 3}])
_partition_outer_dimension(struct_value, partition)
""",
        """CODE.resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='')
tpu_system_medata = resolver.get_tpu_system_metadata()
num_hosts = tpu_system_medata.num_hosts
""",
        """CODE.rt1 = tf.ragged.constant([[1, 2, 3], [4, 5]])
ragged_tensor_to_string(rt1).numpy()
b'[[1, 2, 3], [4, 5]]'

rt2 = tf.ragged.constant([[['a'], ['b', 'c']], [['d', 'e', 'f'], []]])
ragged_tensor_to_string(rt2).numpy()
b"[[['a'], ['b', 'c']], [['d', 'e', 'f'], []]]"

rt3 = tf.ragged.constant([[1], [2, 3, 4, 5, 6], [], [], [7], [8, 9]])
ragged_tensor_to_string(rt3, summarize=2).numpy()
b'[[1], [2, 3, ..., 5, 6], ..., [7], [8, 9]]'""",
        """CODE.sample_text = 'This is a sample sentence.'
tf.keras.preprocessing.text.text_to_word_sequence(sample_text)""",
        """CODE.self.row_lengths().shape == [self.static_nrows]
self.row_starts().shape == [self.static_nrows]
self.row_limits().shape == [self.static_nrows]
self.row_splits().shape == [self.static_nrows + 1]
""",
        """CODE.size = (200, 200)
ds = ds.map(lambda img: tf.image.resize(img, size))


size = (200, 200)
ds = ds.map(lambda img: smart_resize(img, size))
""",
        """CODE.step = tf.Variable(0, name="step")
checkpoint = tf.Checkpoint(step=step)
options = tf.CheckpointOptions(experimental_io_device="/job:localhost")
checkpoint.save("/tmp/ckpt", options=options)""",
        """CODE.tf.math.special.bessel_i0e([-1., -0.5, 0.5, 1.]).numpy()
array([0.46575961, 0.64503527, 0.64503527, 0.46575961], dtype=float32)""",
        "CODE.tf.math.special.bessel_i1e([-1., -0.5, 0.5, 1.]).numpy()",
        "CODE.tf.math.special.bessel_j0([0.5, 1., 2., 4.]).numpy()",
        "CODE.tf.math.special.bessel_j1([0.5, 1., 2., 4.]).numpy()",
        "CODE.tf.math.special.bessel_k0([0.5, 1., 2., 4.]).numpy()",
        """CODE.tf.math.special.bessel_k0e([0.5, 1., 2., 4.]).numpy()
array([1.52410939, 1.14446308, 0.84156822, 0.60929767], dtype=float32)""",
        "CODE.tf.math.special.bessel_k1([0.5, 1., 2., 4.]).numpy()",
        "CODE.tf.math.special.bessel_k1e([0.5, 1., 2., 4.]).numpy()",
        "CODE.tf.math.special.bessel_y0([0.5, 1., 2., 4.]).numpy()",
        "CODE.tf.math.special.bessel_y1([0.5, 1., 2., 4.]).numpy()",
        """CODE.tf.profiler.experimental.start('logdir')
for step in range(num_steps):
  # Creates a trace event for each training step with the step number.
  with tf.profiler.experimental.Trace("Train", step_num=step):
    train_fn()
tf.profiler.experimental.stop()
""",
        """CODE.tf.ragged.cross([tf.ragged.constant([['a'], ['b', 'c']]),
                  tf.ragged.constant([['d'], ['e']]),
                  tf.ragged.constant([['f'], ['g']])])""",
        """CODE.tf.ragged.cross_hashed([tf.ragged.constant([['a'], ['b', 'c']]),
                         tf.ragged.constant([['d'], ['e']]),
                         tf.ragged.constant([['f'], ['g']])],
                        num_buckets=100)""",
        """CODE.worker = tf.data.experimental.service.WorkerServer(
    port=0, dispatcher_address=dispatcher_address)
dataset = tf.data.Dataset.range(10)
dataset = dataset.apply(tf.data.experimental.service.distribute(
    processing_mode="parallel_epochs", service=dispatcher.target))
print(list(dataset.as_numpy_iterator()))

worker = tf.data.experimental.service.WorkerServer(
    port=5051, dispatcher_address="grpc://localhost:5050")
worker.join()""",
        """CODE.x = [[2., 3., 4.], [1., 2., 3.]]
x2 = [[2., 3., 4.], [10000., 2., 3.]]
y = tf.zeros([3, 3])
z = tf.linalg.set_diag(y, x, align='LEFT_RIGHT', k=(-1, 0))
soln = tf.linalg.banded_triangular_solve(x, tf.ones([3, 1]))
are_equal = soln == tf.linalg.banded_triangular_solve(x2, tf.ones([3, 1]))
are_equal = soln == tf.linalg.triangular_solve(z, tf.ones([3, 1]))
x = [[2., 3., 4., 5.], [-1., -2., -3., -4.]]
y = tf.zeros([4, 4])
z = tf.linalg.set_diag(y, x, align='LEFT_RIGHT', k=(0, 1))
soln = tf.linalg.banded_triangular_solve(x, tf.ones([4, 1]), lower=False)
are_equal = (soln == tf.linalg.triangular_solve(z, tf.ones([4, 1]), lower=False))
""" .

<DEPENDENCY.tensorflow==2.3.4> <CONTAINS> """CODE.def dataset_fn(input_context):
  batch_size = input_context.get_per_replica_batch_size(global_batch_size)
  d = tf.data.Dataset.from_tensors([[1.]]).repeat().batch(batch_size)
  return d.shard(
      input_context.num_input_pipelines, input_context.input_pipeline_id)

inputs = strategy.experimental_distribute_datasets_from_function(dataset_fn)

for batch in inputs:
  replica_results = strategy.run(replica_fn, args=(batch,))""" .

<DEPENDENCY.tensorflow==2.4.0> <CONTAINS> """CODE.@composite.Composite('AddN')
def _compose_add_n(inputs, N):
    if N == 1:
      ....""",
        """CODE.class MyTransformer(PyToPy):

  def transform_ast(self, node, ctx):
    node = <<transform node, usually using ast.NodeTransformer classes>>
    return node

transformer = MyTransfomer()

new_f, module, source_map = transformer.transform_function(f, ...)
# new_f is a function with signature identical to f""",
        """CODE.def dataset_fn(input_context):
  batch_size = input_context.get_per_replica_batch_size(global_batch_size)
  d = tf.data.Dataset.from_tensors([[1.]]).repeat().batch(batch_size)
  return d.shard(
      input_context.num_input_pipelines, input_context.input_pipeline_id)

inputs = strategy.distribute_datasets_from_function(dataset_fn)

for batch in inputs:
  replica_results = strategy.run(replica_fn, args=(batch,))""",
        """CODE.ds = tf.data.Dataset.range(8)
ds = ds.batch(4)
ds = _LegacyRebatchDataset(ds, num_replicas=3)
for elem in ds:
  print(elem)
""" .

<DEPENDENCY.tensorflow==2.4.4> <CONTAINS> """CODE.if not tf.is_tensor(t):
  t = tf.convert_to_tensor(t)
return t.dtype
""" .

<DEPENDENCY.tensorflow==2.7.0> <CONTAINS> """CODE.class CustomBatchEncoder(ExtensionTypeBatchEncoder):
...   pass # Override batch(), unbatch(), encode(), and decode().

class CustomType(BatchableExtensionType):
...   x: tf.Tensor
...   y: tf.Tensor
...   shape: tf.TensorShape
...   __batch_encoder__ = CustomBatchEncoder()""",
        """CODE.class Vehicle(BatchableExtensionType):
    top_speed: tf.Tensor
    mpg: tf.Tensor

batch = Vehicle([120, 150, 80], [30, 40, 12])
tf.map_fn(lambda vehicle: vehicle.top_speed * vehicle.mpg, batch,
          fn_output_signature=tf.int32).numpy()""",
        """CODE.estimator = tf.estimator.tpu.TPUEstimator(
    ...
    embedding_spec=tf.estimator.tpu.experimental.EmbeddingConfigSpec(
        ...
        optimization_parameters=tf.tpu.experimental.AdagradMomentumParameters(0.1),
        ...))""",
        """CODE.tf.keras.mixed_precision.set_global_policy('mixed_float16')
tf.keras.mixed_precision.global_policy()
tf.keras.layers.Dense(10).dtype_policy
tf.keras.layers.Dense(10, dtype='float64').dtype_policy
tf.keras.mixed_precision.set_global_policy('float32')
""" .

<DEPENDENCY.tensorflow==2.7.4> <CONTAINS> """CODE.# Retrieve the training sequences.
(x_train, _), _ = keras.datasets.imdb.load_data()
# Retrieve the word index file mapping words to indices
word_index = keras.datasets.imdb.get_word_index()
# Reverse the word index to obtain a dict mapping indices to words
inverted_word_index = dict((i, word) for (word, i) in word_index.items())
# Decode the first sequence in the dataset
decoded_sequence = " ".join(inverted_word_index[i] for i in x_train[0])
""",
        """CODE.(x_train, y_train), (x_test, y_test) = cifar10.load_data()
y_train = utils.to_categorical(y_train, num_classes)
y_test = utils.to_categorical(y_test, num_classes)
datagen = ImageDataGenerator(
    featurewise_center=True,
    featurewise_std_normalization=True,
    rotation_range=20,
    width_shift_range=0.2,
    height_shift_range=0.2,
    horizontal_flip=True,
    validation_split=0.2)
datagen.fit(x_train)
model.fit(datagen.flow(x_train, y_train, batch_size=32, subset='training'), validation_data=datagen.flow(x_train, y_train, batch_size=8, subset='validation'), steps_per_epoch=len(x_train) / 32, epochs=epochs)
for e in range(epochs):
    print('Epoch', e)
    batches = 0
    for x_batch, y_batch in datagen.flow(x_train, y_train, batch_size=32):
        model.fit(x_batch, y_batch)
        batches += 1
        if batches >= len(x_train) / 32:
            break

train_datagen = ImageDataGenerator(
        rescale=1./255,
        shear_range=0.2,
        zoom_range=0.2,
        horizontal_flip=True)
test_datagen = ImageDataGenerator(rescale=1./255)
train_generator = train_datagen.flow_from_directory(
        'data/train',
        target_size=(150, 150),
        batch_size=32,
        class_mode='binary')
validation_generator = test_datagen.flow_from_directory(
        'data/validation',
        target_size=(150, 150),
        batch_size=32,
        class_mode='binary')
model.fit(
        train_generator,
        steps_per_epoch=2000,
        epochs=50,
        validation_data=validation_generator,
        validation_steps=800)

data_gen_args = dict(featurewise_center=True,
                     featurewise_std_normalization=True,
                     rotation_range=90,
                     width_shift_range=0.1,
                     height_shift_range=0.1,
                     zoom_range=0.2)
image_datagen = ImageDataGenerator(**data_gen_args)
mask_datagen = ImageDataGenerator(**data_gen_args)
seed = 1
image_datagen.fit(images, augment=True, seed=seed)
mask_datagen.fit(masks, augment=True, seed=seed)
image_generator = image_datagen.flow_from_directory(
    'data/images',
    class_mode=None,
    seed=seed)
mask_generator = mask_datagen.flow_from_directory(
    'data/masks',
    class_mode=None,
    seed=seed)
train_generator = zip(image_generator, mask_generator)
model.fit(
    train_generator,
    steps_per_epoch=2000,
    epochs=50)
""",
        """CODE.adapt_data = np.array([[1.], [2.], [3.], [4.], [5.]], dtype=np.float32)
input_data = np.array([[1.], [2.], [3.]], np.float32)
layer = Normalization()
layer.adapt(adapt_data)
layer(input_data)
<tf.Tensor: shape=(3, 1), dtype=float32, numpy=
array([[-1.4142135 ],
       [-0.70710677],
       [ 0.        ]], dtype=float32)>

input_data = np.array([[1.], [2.], [3.]], np.float32)
layer = Normalization(mean=3., variance=2.)
layer(input_data)
<tf.Tensor: shape=(3, 1), dtype=float32, numpy=
array([[-1.4142135 ],
       [-0.70710677],
       [ 0.        ]], dtype=float32)>""",
        """CODE.data = tf.constant(np.arange(10).reshape(5, 2) * 10, dtype=tf.float32)
print(data)


layer = tf.keras.layers.LayerNormalization(axis=1)
output = layer(data)
print(output)


mean_i = sum(x_i[j] for j in range(k)) / k
var_i = sum((x_i[j] - mean_i) ** 2 for j in range(k)) / k


x_i_normalized = (x_i - mean_i) / sqrt(var_i + epsilon)


output_i = x_i_normalized * gamma + beta
""",
        """CODE.ds1 = tf.data.Dataset.from_tensor_slices([1, 2, 3])
ds2 = tf.data.Dataset.from_tensor_slices([4, 5, 6])
ds_zipped_tuple = tf.data.Dataset.zip((ds1, ds2))
ds_unzipped_tuple = _unzip_dataset(ds_zipped_tuple)
ds_zipped_dict = tf.data.Dataset.zip({'ds1': ds1, 'ds2': ds2})
ds_unzipped_dict = _unzip_dataset(ds_zipped_dict)""",
        """CODE.from PIL import Image
img = np.random.random(size=(100, 100, 3))
pil_img = tf.keras.preprocessing.image.array_to_img(img)
""",
        """CODE.from PIL import Image
img_data = np.random.random(size=(100, 100, 3))
img = tf.keras.preprocessing.image.array_to_img(img_data)
array = tf.keras.preprocessing.image.img_to_array(img)
""",
        """CODE.from keras.preprocessing.sequence import TimeseriesGenerator
import numpy as np
data = np.array([[i] for i in range(50)])
targets = np.array([[i] for i in range(50)])
data_gen = TimeseriesGenerator(data, targets,
                               length=10, sampling_rate=2,
                               batch_size=2)
assert len(data_gen) == 20
batch_0 = data_gen[0]
x, y = batch_0
assert np.array_equal(x,
                      np.array([[[0], [2], [4], [6], [8]],
                                [[1], [3], [5], [7], [9]]]))
assert np.array_equal(y,
                      np.array([[10], [11]]))
""",
        """CODE.image = tf.keras.preprocessing.image.load_img(image_path)
input_arr = tf.keras.preprocessing.image.img_to_array(image)
input_arr = np.array([input_arr])  # Convert single image to a batch.
predictions = model.predict(input_arr)
""",
        """CODE.import tensorflow as tf

training = True
rating = tf.feature_column.sequence_numeric_column('rating')
watches = tf.feature_column.sequence_categorical_column_with_identity(
    'watches', num_buckets=1000)
watches_embedding = tf.feature_column.embedding_column(watches,
                                            dimension=10)
columns = [rating, watches_embedding]

features = {
 'rating': tf.sparse.from_dense([[1.0,1.1, 0, 0, 0],
                                             [2.0,2.1,2.2, 2.3, 2.5]]),
 'watches': tf.sparse.from_dense([[2, 85, 0, 0, 0],[33,78, 2, 73, 1]])
}

sequence_input_layer = tf.keras.experimental.SequenceFeatures(columns)
sequence_input, sequence_length = sequence_input_layer(
   features, training=training)
sequence_length_mask = tf.sequence_mask(sequence_length)
hidden_size = 32
rnn_cell = tf.keras.layers.SimpleRNNCell(hidden_size)
rnn_layer = tf.keras.layers.RNN(rnn_cell)
outputs, state = rnn_layer(sequence_input, mask=sequence_length_mask)
""",
        """CODE.inp_1 = ['a', 'b', 'c']
inp_2 = ['d', 'e', 'f']
layer = tf.keras.layers.experimental.preprocessing.CategoryCrossing()
layer([inp_1, inp_2])

inp_1 = ['a', 'b', 'c']
inp_2 = ['d', 'e', 'f']
layer = tf.keras.layers.experimental.preprocessing.CategoryCrossing(
   separator='-')
layer([inp_1, inp_2])""",
        """CODE.input_data = data[:-10]
targets = data[10:]
dataset = tf.keras.preprocessing.timeseries_dataset_from_array(
    input_data, targets, sequence_length=10)
for batch in dataset:
  inputs, targets = batch
  assert np.array_equal(inputs[0], data[:10])  # First sequence: steps [0-9]
  assert np.array_equal(targets[0], data[10])  # Corresponding target: step 10
  break


X = np.arange(100)
Y = X*2

sample_length = 20
input_dataset = tf.keras.preprocessing.timeseries_dataset_from_array(
  X, None, sequence_length=sample_length, sequence_stride=sample_length)
target_dataset = tf.keras.preprocessing.timeseries_dataset_from_array(
  Y, None, sequence_length=sample_length, sequence_stride=sample_length)

for batch in zip(input_dataset, target_dataset):
  inputs, targets = batch
  assert np.array_equal(inputs[0], X[:sample_length])

  # second sample equals output timestamps 20-40
  assert np.array_equal(targets[1], Y[sample_length:2*sample_length])
  break
""",
        """CODE.input_img = np.random.random((32, 224, 224, 3))
layer = tf.keras.layers.experimental.preprocessing.RandomZoom(.5, .2)
out_img = layer(input_img)
out_img.shape
""",
        """CODE.inputs = tf.keras.Input(shape=(10, 128, 128, 3)
conv_2d_layer = tf.keras.layers.Conv2D(64, (3, 3))
outputs = tf.keras.layers.TimeDistributed(conv_2d_layer)(inputs)
outputs.shape
TensorShape([None, 10, 126, 126, 64])""",
        """CODE.inputs = {'x2': tf.keras.Input(shape=(5,)),
...           'x1': tf.keras.Input(shape=(1,))}
norm_layer = tf.keras.layers.experimental.preprocessing.Normalization()
y = norm_layer(inputs['x2'])
y, z = tf.keras.layers.Lambda(lambda x: (x, x))(inputs['x1'])
outputs = [inputs['x1'], [y, z]]
stage = FunctionalPreprocessingStage(inputs, outputs)""",
        """CODE.keras_model = tf.keras.Model(...)
keras_model.compile(...)

estimator = tf.keras.estimator.model_to_estimator(keras_model)

def input_fn():
  return dataset_ops.Dataset.from_tensors(
      ({'features': features, 'sample_weights': sample_weights},
       targets))

estimator.train(input_fn, steps=1)


inputs = {'a': tf.keras.Input(..., name='a'),
          'b': tf.keras.Input(..., name='b')}
outputs = {'c': tf.keras.layers.Dense(..., name='c')(inputs['a']),
           'd': tf.keras.layers.Dense(..., name='d')(inputs['b'])}
keras_model = tf.keras.Model(inputs, outputs)
keras_model.compile(...)
export_outputs = {'c': tf.estimator.export.RegressionOutput,
                  'd': tf.estimator.export.ClassificationOutput}

estimator = tf.keras.estimator.model_to_estimator(
    keras_model, export_outputs=export_outputs)

def input_fn():
  return dataset_ops.Dataset.from_tensors(
      ({'features': features, 'sample_weights': sample_weights},
       targets))

estimator.train(input_fn, steps=1)
""",
        """CODE.layer = tf.keras.layers.experimental.preprocessing.CategoryEncoding(
          num_tokens=4, output_mode="one_hot")
layer([3, 2, 0, 1])


layer = tf.keras.layers.experimental.preprocessing.CategoryEncoding(
          num_tokens=4, output_mode="multi_hot")
layer([[0, 1], [0, 0], [1, 2], [3, 1]])


layer = tf.keras.layers.experimental.preprocessing.CategoryEncoding(
          num_tokens=4, output_mode="count")
count_weights = np.array([[.1, .2], [.1, .1], [.2, .3], [.4, .2]])
layer([[0, 1], [0, 0], [1, 2], [3, 1]], count_weights=count_weights)
""",
        """CODE.layer = tf.keras.layers.experimental.preprocessing.Hashing(num_bins=3)
inp = [['A'], ['B'], ['C'], ['D'], ['E']]
layer(inp)


layer = tf.keras.layers.experimental.preprocessing.Hashing(num_bins=3, mask_value='')
inp = [['A'], ['B'], [''], ['C'], ['D']]
layer(inp)


layer = tf.keras.layers.experimental.preprocessing.Hashing(num_bins=3, salt=[133, 137])
inp = [['A'], ['B'], ['C'], ['D'], ['E']]
layer(inp)


layer = tf.keras.layers.experimental.preprocessing.Hashing(num_bins=3, salt=133)
inp = [['A'], ['B'], ['C'], ['D'], ['E']]
layer(inp)
""",
        """CODE.linear_model = LinearModel()
dnn_model = keras.Sequential([keras.layers.Dense(units=64),
                             keras.layers.Dense(units=1)])
combined_model = WideDeepModel(linear_model, dnn_model)
combined_model.compile(optimizer=['sgd', 'adam'], 'mse', ['mse'])
combined_model.fit([linear_inputs, dnn_inputs], y, epochs)

linear_model = LinearModel()
linear_model.compile('adagrad', 'mse')
linear_model.fit(linear_inputs, y, epochs)
dnn_model = keras.Sequential([keras.layers.Dense(units=1)])
dnn_model.compile('rmsprop', 'mse')
dnn_model.fit(dnn_inputs, y, epochs)
combined_model = WideDeepModel(linear_model, dnn_model)
combined_model.compile(optimizer=['sgd', 'adam'], 'mse', ['mse'])
combined_model.fit([linear_inputs, dnn_inputs], y, epochs)""",
        """CODE.model = LinearModel()
model.compile(optimizer='sgd', loss='mse')
model.fit(x, y, epochs=epochs)


model = LinearModel()
opt = tf.keras.optimizers.Adam()
loss_fn = tf.keras.losses.MeanSquaredError()
with tf.GradientTape() as tape:
  output = model(sparse_input)
  loss = tf.reduce_mean(loss_fn(target, output))
grads = tape.gradient(loss, model.weights)
opt.apply_gradients(zip(grads, model.weights))
""",
        """CODE.model = Sequential()
model.add(Bidirectional(LSTM(10, return_sequences=True), input_shape=(5, 10)))
model.add(Bidirectional(LSTM(10)))
model.add(Dense(5))
model.add(Activation('softmax'))
model.compile(loss='categorical_crossentropy', optimizer='rmsprop')

# With custom backward layer
model = Sequential()
forward_layer = LSTM(10, return_sequences=True)
backward_layer = LSTM(10, activation='relu', return_sequences=True,
                      go_backwards=True)
model.add(Bidirectional(forward_layer, backward_layer=backward_layer,
                      input_shape=(5, 10)))
model.add(Dense(5))
model.add(Activation('softmax'))
model.compile(loss='categorical_crossentropy', optimizer='rmsprop')
""",
        """CODE.model.add(tf.keras.layers.experimental.SyncBatchNormalization())
""",
        """CODE.opt = tf.keras.optimizers.SGD(1.0)
model_loss_scale = tf.mixed_precision.experimental.DynamicLossScale()

for step in training_steps:
  with LossScaleGradientTape(model_loss_scale) as tape:
    logits = ...  # Run model and get logits
    loss = tf.nn.softmax_cross_entropy_with_logits(logits=logits,
                                                   labels=labels)
    loss = tf.reduce_mean(loss)
  vars = tape.watched_variables()
  grads = tape.gradient(loss, vars)
  opt.apply_gradients(zip(grads, vars))
""",
        """CODE.price = tf.feature_column.numeric_column('price')
keywords_embedded = tf.feature_column.embedding_column(
    tf.feature_column.categorical_column_with_hash_bucket("keywords", 10K),
    dimension=16)
columns = [price, keywords_embedded, ...]
partitioner = tf.compat.v1.fixed_size_partitioner(num_shards=4)
feature_layer = tf.compat.v1.keras.layers.DenseFeatures(
    feature_columns=columns, partitioner=partitioner)

features = tf.io.parse_example(
    ..., features=tf.feature_column.make_parse_example_spec(columns))
dense_tensor = feature_layer(features)
for units in [128, 64, 32]:
  dense_tensor = tf.compat.v1.keras.layers.Dense(
                     units, activation='relu')(dense_tensor)
prediction = tf.compat.v1.keras.layers.Dense(1)(dense_tensor)
""",
        """CODE.sample_text = 'This is a sample sentence.'
tf.keras.preprocessing.text.text_to_word_sequence(sample_text)""",
        """CODE.sequence = [[1], [2, 3], [4, 5, 6]]
tf.keras.preprocessing.sequence.pad_sequences(sequence)
array([[0, 0, 1],
       [0, 2, 3],
       [4, 5, 6]], dtype=int32)

tf.keras.preprocessing.sequence.pad_sequences(sequence, value=-1)
array([[-1, -1,  1],
       [-1,  2,  3],
       [ 4,  5,  6]], dtype=int32)

tf.keras.preprocessing.sequence.pad_sequences(sequence, padding='post')
array([[1, 0, 0],
       [2, 3, 0],
       [4, 5, 6]], dtype=int32)

tf.keras.preprocessing.sequence.pad_sequences(sequence, maxlen=2)
array([[0, 1],
       [2, 3],
       [5, 6]], dtype=int32)
""",
        """CODE.size = (200, 200)
ds = ds.map(lambda img: tf.image.resize(img, size))


size = (200, 200)
ds = ds.map(lambda img: smart_resize(img, size))
""" .

<DEPENDENCY.tensorflow==2.8.0> <CONTAINS> """CODE.@set_xla_env_flag(flag='--xla_gpu_enable_fast_min_max=false')
def testFoo(self):
    ...""",
        """CODE.shape = RaggedShape._from_inner_shape([2, 3, 4])
shape._num_slices_in_dimension(0) = 2
shape._num_slices_in_dimension(1) = 6
shape._num_slices_in_dimension(2) = 24
shape._num_slices_in_dimension(-1) = 24
shape._num_slices_in_dimension(-2) = 6
shape._num_slices_in_dimension(-2) = 2""" .

<DEPENDENCY.tensorflow_datasets==1.0.2> <CONTAINS> """CODE.tfds.SequenceDict({
    'frame': tfds.features.Image(shape=(64, 64, 3))
    'action': tfds.features.ClassLabel(['up', 'down', 'left', 'right'])
}, length=NB_FRAME)

yield {
    'frame': np.ones(shape=(NB_FRAME, 64, 64, 3)),
    'action': ['left', 'left', 'up', ...],
}

{
    'frame': tf.Tensor(shape=(NB_FRAME, 64, 64, 3), dtype=tf.uint8),
    'action': tf.Tensor(shape=(NB_FRAME,), dtype=tf.int64),
}""" .

<DEPENDENCY.tensorflow_datasets==1.1.0> <CONTAINS> """CODE.
ds = tf.data.TFRecordDataset(filepath)
ds = ds.map(file_adapter.parse_example)
""",
        """CODE.@tfds.decode.make_decoder(output_dtype=tf.string)
def no_op_decoder(example, feature):
  \"\"\"Decoder simply decoding feature normally.\"\"\"
  return feature.decode_example(example)

tfds.load('mnist', split='train', decoders: {
    'image': no_op_decoder(),
})""",
        """CODE.class Experiment(enum.Enum):
  EXP_A = enum.auto()  # Short description of experiment.

class MyBuilder(...):
  VERSION = tfds.core.Version('1.2.3', experiments={
      tfds.core.Experiment.EXP_A: True,
      })""",
        """CODE.ds = ds.load(
    'imagenet2012',
    split='train',
    decoders={
        'image': tfds.decode.SkipDecoding(),
    }
)

for ex in ds.take(1):
  assert ex['image'].dtype == tf.string
""",
        """CODE.ds = tfds.load('mnist', split='test[:33%]')
ds = tfds.load('mnist', split=ReadInstruction.from_spec('test[:33%]'))
ds = tfds.load('mnist', split=ReadInstruction('test', to=33, unit='%'))
ds = tfds.load('mnist', split=ReadInstruction(
    'test', from_=0, to=33, unit='%'))

ds = tfds.load('mnist', split='test[:33%]+train[1:-1]')
ds = tfds.load('mnist', split=ReadInstruction.from_spec(
    'test[:33%]+train[1:-1]'))
ds = tfds.load('mnist', split=(
    ReadInstruction.('test', to=33, unit='%') +
    ReadInstruction.('train', from_=1, to=-1, unit='abs')))

tests = tfds.load(
    'mnist',
    [ReadInstruction('train', from_=k, to=k+10, unit='%')
     for k in range(0, 100, 10)])
trains = tfds.load(
    'mnist',
    [RI('train', to=k, unit='%') + RI('train', from_=k+10, unit='%')
     for k in range(0, 100, 10)])""",
        """CODE.feature = FeatureDict({
    'a': w,
    'b': x,
    'c': {
        'd': y,
        'e': z,
    },
})

feature._flatten({
    'b': X,
    'c': {
        'd': Y,
    },
})
""" .

<DEPENDENCY.tensorflow_datasets==1.2.0> <CONTAINS> """CODE.
# Set-up the folder containing the 'my_dataset.txt' checksums.
checksum_dir = os.path.join(os.path.dirname(__file__), 'checksums/')
checksum_dir = os.path.normpath(checksum_dir)

# Add the checksum dir (will be executed when the user import your dataset)
tfds.download.add_checksums_dir(checksum_dir)

class MyDataset(tfds.core.DatasetBuilder):
  ...
""",
        """CODE.def as_dataset(self, *args, **kwargs):
  return tf.data.Dataset.from_generator(
      lambda: ({
          'image': np.ones(shape=(28, 28, 1), dtype=np.uint8),
          'label': i % 10,
      } for i in range(num_examples)),
      output_types=self.info.features.dtype,
      output_shapes=self.info.features.shape,
  )

with mock_data(as_dataset_fn=as_dataset):
  ds = tfds.load('some_dataset', split='train')

  for ex in ds:  # ds will yield the fake data example of 'as_dataset'.
    ex
""",
        """CODE.ds, ds_info = tfds.load('cifar10', split='train', with_info=True)
fig = tfds.show_examples(ds_info, ds)
""" .

<DEPENDENCY.tensorflow_datasets==1.3.2> <CONTAINS> """CODE.example_data = [
    [1, 2, 3],
    [],
    [4, 5]
]
tensor_info = TensorInfo(shape=(None, None,), sequence_rank=2, ...)
out = _add_ragged_fields(example_data, tensor_info)
out == {
    'ragged_flat_values': ([0, 1, 2, 3, 4, 5], TensorInfo(shape=(), ...)),
    'ragged_row_length_0': ([3, 0, 2], TensorInfo(shape=(None,), ...))
}
""" .

<DEPENDENCY.tensorflow_datasets==3.1.0> <CONTAINS> """CODE.
downloader = KaggleCompetitionDownloader(competition_name)
for fname in downloader.competition_files:
  downloader.download_file(fname, make_file_output_path(fname))
""" .

<DEPENDENCY.tensorflow_datasets==3.2.0> <CONTAINS> """CODE.
builder = tfds.TranslateFolder(root_dir='path/to/manual_dir/')
print(builder.info)  # Splits, num examples,... are automatically calculated
ds = builder.as_dataset(split='train', shuffle_files=True)
""",
        """CODE.builder = tfds.ImageFolder('path/to/image_dir/')
print(builder.info)
ds = builder.as_dataset(split='train', shuffle_files=True)
tfds.show_examples(ds, builder.info)
""",
        """CODE.builder = tfds.builder('mnist')
tfds.show_statistics(builder.info)

ds, ds_info = tfds.load('mnist', with_info)
tfds.show_statistics(ds_info)
""",
        """CODE.ds = tfds.load('mnist', split='train').batch(32).prefetch()
tfds.core.benchmark(ds, batch_size=32)
""",
        """CODE.fs = MockFs()
with fs.mock():

  fs.add_file('/path/to/file1', 'Content of file 1')

  assert tf.io.gfile.exists('/path/to/file1')
  with tf.io.gfile.GFile('/path/to/file2', 'w') as f:
    f.write('Content of file 2')
  tf.io.gfile.rename('/path/to/file1', '/path/to/file1_moved')

  assert fs.files == {
      '/path/to/file2': 'Content of file 2',
      '/path/to/file1_moved': 'Content of file 1',
  }
""" .

<DEPENDENCY.tensorflow_datasets==4.0.0> <CONTAINS> """CODE.df = StyledDataFrame(...)
df.current_style.apply(...)  # Configure the style
df  # The data-frame is displayed using ` pandas.io.formats.style.Styler`""" .

<DEPENDENCY.tensorflow_datasets==4.0.1> <CONTAINS> """CODE.
def _build_pcollection(pipeline, extracted_dir):
  return (
      pipeline
      | beam.Create(gfile.io.listdir(extracted_dir))
      | beam.Map(_process_file)
  )
""" .

<DEPENDENCY.tensorflow_datasets==4.1.0> <CONTAINS> """CODE.def _split_generators(self, dl_manager):
  path = dl_manager.download_and_extract('http://dataset.org/my_data.zip')
  return {
      'train': self._generate_examples(path=f'{path}/train/'),
      'test': self._generate_examples(path=f'{path}/test/'),
  }
""",
        """CODE.flatten_with_path({'a': {'b': v}}) == [(('a', 'b'), v)]
""",
        """CODE.path = tfds.core.tfds_path() / 'path/to/data.txt'
path = tfds.core.tfds_path('path/to/data.txt')
path = tfds.core.tfds_path('path', 'to', 'data.txt')
""",
        """CODE.split_builder = SplitBuilder(...)

with split_builder.maybe_beam_pipeline():
  split_info_future = split_builder.submit_split_generation(...)

split_info = split_info_future.result()
""" .

<DEPENDENCY.tensorflow_datasets==4.2.0> <CONTAINS> """CODE.  features=tfds.features.FeatureDict({
   'agent_id': tf.string,
    'episode': tfds.features.Dataset({
      'observation': tfds.features.Image(),
      'reward': tfds.features.Image(),
    }),
  })


yield _, {
  'agent_id': agent_name
  'episode': ({'observation': ..., 'reward': ...} for _ in range(10)),
}
""",
        """CODE.ds_name, builder_kwargs = parse_builder_name_kwargs(
    'kaggle:ds/cfg:1.2.3', data_dir='...'
)
ds_name.namespace == 'kaggle'
ds_name.name == 'ds'
builder_kwargs == {'config': 'cfg', 'version': '1.2.3', 'data_dir': '...'}
""",
        """CODE.logger = logging.getLogger()
logger.addHandler(logging.StreamHandler(TqdmStream()))

for _ in tqdm.tqdm(range(10)):
  logger.info('No visual artifacts')
""",
        """CODE.register = DataDirRegister(path='/path/to/namespaces.toml')

# List all registered datasets: ['kaggle:ds0', 'kaggle:ds1',...]
register.list_builders()

# Load a specific dataset
builder = register.builder('tensorflow_graphics:shapenet')
""" .

<DEPENDENCY.tensorflow_datasets==4.3.0> <CONTAINS> """CODE.
tfds.enable_progress_bar()
""",
        """CODE.import tensorflow_datasets as tfds

with tfds.core.community.mock_huggingface_import():
  import datasets  # `datasets` is a _MockedHFDatasets

# Using `datasets.Xyz` uses the corresponding `tfds.Xyz` API
class MyDataset(datasets.GeneratorBasedBuilder):
  version = datasets.Version('1.0.0')
  ...

# This works !!
ds = tfds.load('my_dataset')
""",
        """CODE.path = GithubPath.from_repo('tensorflow/datasets')
path = path / 'docs' / 'catalog'
assert path.is_dir()
datasets = [
    p.name for p in path.iterdir() if p.match('*.md')
]

path = GithubPath('github://tensorflow/datasets/tree/master/docs/README.md')
assert path.subpath == 'docs/README.md'
assert path.repo == 'tensorflow/datasets'
assert path.branch == 'master'""",
        """CODE.register = PackageRegister(path='/path/to/datasets-source-list.jsonl')

# List all registered datasets: ['kaggle:ds0', 'kaggle:ds1',...]
register.list_builders()

# Load a specific dataset
builder = register.builder('tensorflow_graphics:shapenet')
""",
        """CODE.with tfds.core.community.mock_builtin_to_use_gfile():
  files = os.listdir('gs://some-bucket')
""" .

<DEPENDENCY.tensorflow_datasets==4.5.1> <CONTAINS> """CODE.tfds.load(..., split=tfds.split_for_jax_process('train'))


tfds.even_splits(split, n=jax.process_count())[jax.process_index()]
""" .

<DEPENDENCY.tensorflow_datasets==4.9.2> <CONTAINS> """CODE.
df = StyledDataFrame(...)
df.current_style.apply(...)  # Configure the style
df  # The data-frame is displayed using ` pandas.io.formats.style.Styler`
""" .

<DEPENDENCY.torch==1.0.1> <CONTAINS> """CODE.a = torch.randn(4, 4)
a
tensor([[ 1.3398,  0.2663, -0.2686,  0.2450],
        [-0.7401, -0.8805, -0.3402, -1.1936],
        [ 0.4907, -1.3948, -1.0691, -0.3132],
        [-1.6092,  0.5419, -0.2993,  0.3195]])

torch.argmax(a, dim=1)
tensor([ 0,  2,  0,  1])""",
        """CODE.a = torch.randn(4, 4)
torch.argmin(a, dim=1)
""",
        "CODE.state_dict = torch.utils.model_zoo.load_url('https://s3.amazonaws.com/pytorch/models/resnet18-5c106cde.pth')",
        """CODE.torch.argsort(a, dim=1)
tensor([[2, 0, 3, 1],
        [3, 2, 1, 0],
        [2, 1, 0, 3],
        [3, 2, 1, 0]])""",
        """CODE.torch.isnan(torch.tensor([1, float('nan'), 2]))
tensor([ 0,  1,  0], dtype=torch.uint8)""" .

<DEPENDENCY.torch==1.1.0> <CONTAINS> """CODE.@skipIfNotRegistered('MyOp', 'MyOp is not linked!')
        This will check if 'MyOp' is in the caffe2.python.core""",
        """CODE.A = torch.randn(2, 3, 3)
A_LU, pivots = A.lu()
P, A_L, A_U = torch.lu_unpack(A_LU, pivots)

# can recover A from factorization
A_ = torch.bmm(P, torch.bmm(A_L, A_U))
""",
        """CODE.A = torch.randn(2, 3, 3)
A_LU, pivots = torch.lu(A)
A_LU
tensor([[[ 1.3506,  2.5558, -0.0816],
         [ 0.1684,  1.1551,  0.1940],
         [ 0.1193,  0.6189, -0.5497]],

        [[ 0.4526,  1.2526, -0.3285],
         [-0.7988,  0.7175, -0.9701],
         [ 0.2634, -0.9255, -0.3459]]])
pivots
tensor([[ 3,  3,  3],
        [ 3,  3,  3]], dtype=torch.int32)
A_LU, pivots, info = torch.lu(A, get_infos=True)
if info.nonzero().size(0) == 0:
    print('LU factorization succeeded for all samples!')
""",
        """CODE.a = [1, 2, 3]
b = [4, 5]
tensor_a = torch.tensor(a)
tensor_b = torch.tensor(b)
torch.cartesian_prod(tensor_a, tensor_b)
tensor([[1, 4],
        [1, 5],
        [2, 4],
        [2, 5],
        [3, 4],
        [3, 5]])""",
        """CODE.a = torch.randn(3, 3)
a = torch.mm(a, a.t()) # make symmetric positive definite
a
tensor([[ 3.5405, -0.4577,  0.8342],
        [-0.4577,  1.8244, -0.1996],
        [ 0.8342, -0.1996,  3.7493]])
u,piv = torch.pstrf(a)
u
tensor([[ 1.9363,  0.4308, -0.1031],
        [ 0.0000,  1.8316, -0.2256],
        [ 0.0000,  0.0000,  1.3277]])
piv
tensor([ 2,  0,  1], dtype=torch.int32)
p = torch.eye(3).index_select(0,piv.long()).index_select(0,piv.long()).t() # make pivot permutation
torch.mm(torch.mm(p.t(),torch.mm(u.t(),u)),p) # reconstruct
tensor([[ 3.5405, -0.4577,  0.8342],
        [-0.4577,  1.8244, -0.1996],
        [ 0.8342, -0.1996,  3.7493]])""",
        """CODE.def torch.unique_consecutive(input, return_inverse=False, return_counts=False, dim=None):
    # Eliminates all but the first element from every consecutive group of equivalent elements.
    # This function is different from torch.unique in the sense that it only eliminates consecutive duplicate values.
    # Arguments:
    # input (Tensor): the input tensor
    # return_inverse (bool): Whether to also return the indices for where elements in the original input ended up in the returned unique list.
    # return_counts (bool): Whether to also return the counts for each unique element.
    # dim (int): the dimension to apply unique. If None, the unique of the flattened input is returned. default: None
    # Returns:
    # (Tensor, Tensor (optional), Tensor (optional)): A tensor or a tuple of tensors containing
    # - output (Tensor): the output list of unique scalar elements.
    # - inverse_indices (Tensor): (optional) if return_inverse is True, there will be an additional returned tensor (same shape as input) representing the indices for where elements in the original input map to in the output;
    # otherwise, this function will only return a single tensor.
    # - counts (Tensor): (optional) if return_counts is True, there will be an additional returned tensor (same shape as output or output.size(dim), if dim was specified) representing the number of occurrences for each unique value or tensor.

    # Example:
    x = torch.tensor([1, 1, 2, 2, 3, 1, 1, 2])
    output = torch.unique_consecutive(x)
    output
    output, inverse_indices = torch.unique_consecutive(x, return_inverse=True)
    output
    inverse_indices
    output, counts = torch.unique_consecutive(x, return_counts=True)
    output
    counts
""",
        """CODE.import keyword
import torch
meta = []
while len(meta)<100:
    meta = meta+keyword.kwlist # get some strings
meta = meta[:100]

for i, v in enumerate(meta):
    meta[i] = v+str(i)

label_img = torch.rand(100, 3, 10, 32)
for i in range(100):
    label_img[i]*=i/100.0

writer.add_embedding(torch.randn(100, 5), metadata=meta, label_img=label_img)
writer.add_embedding(torch.randn(100, 5), label_img=label_img)
writer.add_embedding(torch.randn(100, 5), metadata=meta)""",
        "CODE.torch.hub.list('pytorch/vision', force_reload=True)",
        """CODE.torch.nn.Sequential(
            torch.nn.Linear(20, 100),
            torch.nn.BatchNorm1d(100)
          ).cuda()

process_group = torch.distributed.new_group(process_ids)
sync_bn_module = convert_sync_batchnorm(module, process_group)""",
        """CODE.weight = torch.FloatTensor([[1, 2.3, 3], [4, 5.1, 6.3]])
embeddingbag = nn.EmbeddingBag.from_pretrained(weight)
input = torch.LongTensor([[1, 0]])
embeddingbag(input)""",
        "CODE.writer.add_custom_scalars_multilinechart(['twse/0050', 'twse/2330'])",
        """CODE.writer.add_scalars('run_14h', {'xsinx':i*np.sin(i/r),
                               'xcosx':i*np.cos(i/r),
                               'arctanx': numsteps*np.arctan(i/r)}, i)""",
        """CODE.writer.add_text('lstm', 'This is an lstm', 0)
writer.add_text('rnn', 'This is an rnn', 10)""" .

<DEPENDENCY.torch==1.10.0> <CONTAINS> """CODE.    @staticmethod
    def forward(ctx, x: torch.Tensor, y: torch.Tensor, z: int):
        w = x * y * z
        out = x * y + y * z + w
        ctx.save_for_backward(x, y, out)
        ctx.z = z  # z is not a tensor
        ctx.w = w  # w is neither input nor output
        return out

    @staticmethod
    def backward(ctx, grad_out):
        x, y, out = ctx.saved_tensors
        z = ctx.z
        gx = grad_out * (y + y * z)
        gy = grad_out * (x + z + x * z)
        gz = None
        return gx, gy, gz
""",
        """CODE.    class Func(Function):
        @staticmethod
        def forward(ctx, x):
            sorted, idx = x.sort()
            ctx.mark_non_differentiable(idx)
            ctx.save_for_backward(x, idx)
            return sorted, idx

        @staticmethod
        @once_differentiable
        def backward(ctx, g1, g2):  # still need to accept g2
            x, idx = ctx.saved_tensors
            grad_input = torch.zeros_like(x)
            grad_input.index_add_(0, idx, g1)
            return grad_input""",
        """CODE.# Create intra-machine subgroups.
cur_subgroup, subgroups = dist.new_subgroups()
# Allreduce within the machine.
rank = dist.get_rank()
tensor = torch.ones(1, device=rank) * rank
dist.all_reduce(tensor, group=cur_subgroup)
tensor
tensor([8])     # Assume 8 is the number of CUDA devices per machine.
# Cleanup.
for subgroup in subgroups:
    dist.destroy_process_group(subgroup)
""",
        """CODE.:class:`CollatorIterDataPipe`.

Iterable DataPipe to collate samples from datapipe to Tensor(s) by `util_.collate.default_collate`,
or customized Data Structure by collate_fn.

Args:
    datapipe: Iterable DataPipe being collated
    collate_fn: Customized collate function to collect and combine data or a batch of data.
        Default function collates to Tensor(s) based on data type.
    fn_args: Positional arguments for `collate_fn`
    fn_kwargs: Keyword arguments for `collate_fn`

Example: Convert integer data to float Tensor
    class MyIterDataPipe(torch.utils.data.IterDataPipe):
    ...     def __init__(self, start, end):
    ...         super(MyIterDataPipe).__init__()
    ...         assert end > start, "this example code only works with end >= start"
    ...         self.start = start
    ...         self.end = end
    ...
    ...     def __iter__(self):
    ...         return iter(range(self.start, self.end))
    ...
    ...     def __len__(self):
    ...         return self.end - self.start
    ...
    ds = MyIterDataPipe(start=3, end=7)
    print(list(ds))
    [3, 4, 5, 6]

    def collate_fn(batch):
    ...     return torch.tensor(batch, dtype=torch.float)
    ...
    collated_ds = CollateIterDataPipe(ds, collate_fn=collate_fn)
    print(list(collated_ds))
    [tensor(3.), tensor(4.), tensor(5.), tensor(6.)]""",
        """CODE.D = Dispatcher('add')
D.add((int, int), lambda x, y: x + y)
D.add((float, float), lambda x, y: x + y)
D(1, 2)""",
        """CODE.Foo.with_callable_args = classmethod(_with_callable_args)
Foo.with_args = classmethod(_with_args)
foo_builder = Foo.with_callable_args(cur_time=get_time_func).with_args(name="dan")
foo_instance1 = foo_builder()
wait 50
foo_instance2 = foo_builder()
id(foo_instance1.creation_time) == id(foo_instance2.creation_time)
""",
        """CODE._create_op_prefix('add')
'AddBackward'""",
        """CODE.class BaseSparsifier:
    def __init__(self, model, config, defaults):
        self.model = model
        self.config = config
        self.defaults = defaults

    def update_mask(self):
        # Function to compute a new mask for all keys in the `module_groups`
        pass

# Example code
config = [model.layer1, {'module': model.linear2, 'sparsity_level': 0.5}]
defaults = {'sparsity_level': 0.7}
sparsifier = BaseSparsifier(config, defaults)

class BaseSparsifier:
    def __init__(self, model, config, defaults):
        self.model = model
        self.config = config
        self.defaults = defaults

    def update_mask(self):
        # Function to compute a new mask for all keys in the `module_groups`
        pass

# Example code
config = [model.layer1, {'module': model.linear2, 'sparsity_level': 0.5}]
defaults = {'sparsity_level': 0.7}
sparsifier = BaseSparsifier(config, defaults)
""",
        """CODE.class Inplace(Function):
    @staticmethod
    def forward(ctx, x):
        x_npy = x.numpy() # x_npy shares storage with x
        x_npy += 1
        ctx.mark_dirty(x)
        return x

    @staticmethod
    @once_differentiable
    def backward(ctx, grad_output):
        return grad_output
""",
        """CODE.class M(torch.nn.Module):
    def forward(self, x:TensorType((1,2,3, Dyn)), y:TensorType((1,2,3, Dyn))):
        return torch.add(x, y)""",
        """CODE.class SimpleFunc(Function):
    @staticmethod
    def forward(ctx, x):
        return x.clone(), x.clone()

    @staticmethod
    @once_differentiable
    def backward(ctx, g1, g2):
        return g1 + g2  # No check for None necessary

# We modify SimpleFunc to handle non-materialized grad outputs
class Func(Function):
    @staticmethod
    def forward(ctx, x):
        ctx.set_materialize_grads(False)
        ctx.save_for_backward(x)
        return x.clone(), x.clone()

    @staticmethod
    @once_differentiable
    def backward(ctx, g1, g2):
        x, = ctx.saved_tensors
        grad_input = torch.zeros_like(x)
        if g1 is not None:  # We must check for None now
            grad_input += g1
        if g2 is not None:
            grad_input += g2
        return grad_input

a = torch.tensor(1., requires_grad=True)
b, _ = Func.apply(a)  # induces g2 to be undefined""",
        """CODE.def f(a, b, c):
    prod_1 = a * b
    with torch.autograd.graph.save_on_cpu():
        prod_2 = prod_1 * c
    y = prod_2 * a
    return y

y = f(a, b, c)

del a, b, c

y.sum().backward()
""",
        """CODE.def pack_hook(tensor: Tensor) -> Any
    print("Packing", tensor)
    return tensor

def unpack_hook(x)
    print("Unpacking", x)
    return x

a = torch.ones(5, requires_grad=True)
b = torch.ones(5, requires_grad=True) * 2
with torch.autograd.graph.saved_tensors_hooks(pack_hook, unpack_hook):
    y = a * b
    Packing tensor([1., 1., 1., 1., 1.])
    Packing tensor([2., 2., 2., 2., 2.])
y.sum().backward()
Unpacking tensor([1., 1., 1., 1., 1.])
Unpacking tensor([2., 2., 2., 2., 2.])""",
        """CODE.dense = torch.randn(5, 5)
sparse = dense.to_sparse_csr()
sparse._nnz()""",
        """CODE.f = Dispatcher('f')
@f.register(int)
def inc(x):
    return x + 1
@f.register(float)
def dec(x):
    return x - 1
@f.register(list)
@f.register(tuple)
def reverse(x):
    return x[::-1]
f(1)
f(1.0)
f([1, 2, 3])""",
        """CODE.fc1 = nn.Linear(16, 8).cuda(0)
fc2 = nn.Linear(8, 4).cuda(1)
dropout = nn.Dropout()

# Dropout does not have any parameters/buffers, but we want to
# run it on cuda:1 to avoid any GPU to CPU transfers.
model = nn.Sequential(fc1, fc2, WithDevice(dropout, 'cuda:1'))
model = Pipe(model, chunks=8)""",
        """CODE.from multipledispatch import dispatch
@dispatch(int)
... def inc(x):
...     return x + 1
implementation = inc.dispatch(int)
implementation(3)
4
print(inc.dispatch(float))
None""",
        """CODE.import os
import torch
import torch.distributed as dist
import torch.multiprocessing as mp
import torch.nn.parallel.DistributedDataParallel as DDP
import torch.distributed.optim.ZeroRedundancyOptimizer as ZeRO
from torch.distributed.algorithms.join import Join

# On each spawned worker
def worker(rank):
    dist.init_process_group("nccl", rank=rank, world_size=2)
    model = DDP(torch.nn.Linear(1, 1).to(rank), device_ids=[rank])
    optim = ZeRO(model.parameters(), torch.optim.Adam, lr=0.01)
    # Rank 1 gets one more input than rank 0
    inputs = [torch.tensor([1.]).to(rank) for _ in range(10 + rank)]
    with Join([model, optim]):
        for input in inputs:
            loss = model(input).sum()
            loss.backward()
            optim.step()
    # All ranks reach here without hanging/erroring
""",
        """CODE.import torch
import torch.distributed as dist
import torch.distributed.algorithms.ddp_comm_hooks.post_localSGD_hook as post_localSGD
import torch.distributed.algorithms.model_averaging.averagers as averagers
import torch.nn as nn

dist.init_process_group("nccl", rank=rank, world_size=16)
torch.cuda.set_device(rank)
module = nn.Linear(1, 1, bias=False).to(rank)
model = nn.parallel.DistributedDataParallel(
    module, device_ids=[rank], output_device=rank
)
subgroup, subgroups = dist.new_subgroups()
state = PostLocalSGDState(subgroup=subgroup, start_localSGD_iter=100)
model.register_comm_hook(state, post_localSGD_hook)

averager = averagers.PeriodicModelAverager(period=4, warmup_steps=100)
for step in range(0, 20):
    optimizer.zero_grad()
    loss = loss_fn(output, labels)
    loss.backward()
    optimizer.step()
    averager.average_parameters(model.parameters())""",
        """CODE.import torch
import torch.distributed as dist
import torch.distributed.algorithms.model_averaging.averagers as averagers
import torch.nn as nn
from torch.distributed.optim import PostLocalSGDOptimizer

model = nn.parallel.DistributedDataParallel(
    module, device_ids=[rank], output_device=rank
)

# Register a post-localSGD communication hook.
subgroup, subgroups = dist.new_subgroups()
state = PostLocalSGDState(subgroup=subgroup, start_localSGD_iter=100)
model.register_comm_hook(state, post_localSGD_hook)

# Create a post-localSGD optimizer that wraps a local optimizer.
# Note that ``warmup_steps`` used in ``PostLocalSGDOptimizer`` must be the same as
# ``start_localSGD_iter`` used in ``PostLocalSGDState``.
local_optim = torch.optim.SGD(params=model.parameters(), lr=0.01)
opt = PostLocalSGDOptimizer(
    optim=local_optim,
    averager=averagers.PeriodicModelAverager(period=4, warmup_steps=100)
)

# In the first 100 steps, DDP runs global gradient averaging at every step.
# After 100 steps, DDP runs gradient averaging within each subgroup (intra-node by default),
# and post-localSGD optimizer runs global model averaging every 4 steps after applying the local optimizer.
for step in range(0, 20):
    opt.zero_grad()
    loss = loss_fn(output, labels)
    loss.backward()
    opt.step()""",
        """CODE.lambda1 = lambda epoch: epoch // 30
lambda2 = lambda epoch: 0.95 ** epoch
scheduler = LambdaSL(sparsifier, sl_lambda=[lambda1, lambda2])
for epoch in range(100):
    train(...)
    validate(...)
    scheduler.step()""",
        """CODE.m = nn.ReflectionPad3d(1)
input = torch.arange(8, dtype=torch.float).reshape(1, 1, 2, 2, 2)
m(input)""",
        """CODE.scheduler = ConstantLR(optimizer=self.opt, factor=0.5, total_iters=4)
for epoch in range(100):
    train(...)
    validate(...)
    scheduler.step()""",
        """CODE.scheduler = LinearLR(optimizer, start_factor=0.5, total_iters=4)
for epoch in range(100):
    train(...)
    validate(...)
    scheduler.step()""",
        """CODE.scheduler1 = ConstantLR(self.opt, factor=0.1, total_iters=2)
scheduler2 = ExponentialLR(self.opt, gamma=0.9)
scheduler = ChainedScheduler([scheduler1, scheduler2])
for epoch in range(100):
    train(...)
    validate(...)
    scheduler.step()""",
        """CODE.scheduler1 = ConstantLR(self.opt, factor=0.1, total_iters=2)
scheduler2 = ExponentialLR(self.opt, gamma=0.9)
scheduler = SequentialLR(self.opt, schedulers=[scheduler1, scheduler2], milestones=[2])
for epoch in range(100):
    train(...)
    validate(...)
    scheduler.step()""",
        """CODE.torch.distributed.init_process_group(
    backend='gloo', world_size=N, init_method='...'
)
pg = dist.distributed_c10d._get_default_group()
async_reduction = True
module = ToyModel()
ddp_model = PythonDDP(module, pg, async_reduction)
loss_fn = nn.MSELoss()
optimizer = optim.SGD(ddp_model.parameters(), lr=0.001)
outputs = ddp_model(torch.randn(20, 10).to(rank))
labels = torch.randn(20, 10).to(rank)
loss_fn(outputs, labels).backward()

# Reduce param grads
ddp_model.all_reduce_grads()
optimizer.step()""",
        """CODE.ts = TensorSpec(
    shape = [1, 3, 224, 224],
    dtype = ScalarType.Float
)""" .

<DEPENDENCY.torch==1.12.0> <CONTAINS> """CODE.@custom_sharded_op_impl(torch.nn.functional.linear)
def my_custom_sharded_linear(types, args, kwargs, process_group):
  ....

input = torch.rand(10, 32)
weight = sharded_tensor.rand(32, 16)
bias = torch.rand(16)
# This will call 'my_custom_sharded_linear'
torch.nn.functional.linear(input, weight, bias)""" .

<DEPENDENCY.torch==1.3.0> <CONTAINS> """CODE.@torch.jit.unused
def memory_efficient(self, x):
    import pdb
    pdb.set_trace()
    return x + 10

m = torch.jit.script(MyModule(use_memory_efficent=False))
m.save("m.pt")

m = torch.jit.script(MyModule(use_memory_efficient=True))
# exception raised
m(torch.rand(100))""",
        """CODE.Foo.with_args = classmethod(_with_args)
Foo.with_args(x=1).with_args(y=2)""",
        """CODE.def linear(x):
   if not torch.jit.is_scripting():
      return torch.linear(x)
   else:
      return unsupported_linear_op(x)""",
        """CODE.f_add = FloatFunctional()
a = torch.tensor(3.0)
b = torch.tensor(4.0)
f_add.add(a, b)  # Equivalent to ``torch.add(3, 4)
""",
        """CODE.import torch.distributed as dist
dist.init_process_group(backend='gloo', rank=0, world_size=2)
dist.init_rpc("worker0")
worker1 = dist.get_worker_id("worker1")
rref1 = dist.remote(worker1, torch.add, args=(torch.ones(2), 3))
rref2 = dist.remote(worker1, torch.add, args=(torch.ones(2), 1))
x = rref1.to_here() + rref2.to_here()
dist.join_rpc()

import torch.distributed as dist
dist.init_process_group(backend='gloo', rank=1, world_size=2)
dist.init_rpc("worker1")
dist.join_rpc()""",
        """CODE.import torch.distributed.autograd as dist_autograd
with dist_autograd.context() as context_id:
     forward pass...
     backward pass...
     optimizer step...""",
        """CODE.q_add = QFunctional('add')
a = torch.quantize_per_tensor(torch.tensor(3.0), 1.0, 0, torch.qint32)
b = torch.quantize_per_tensor(torch.tensor(4.0), 1.0, 0, torch.qint32)
q_add.add(a, b)  # Equivalent to ``torch.ops.quantized.add(3, 4)""",
        "CODE.torch.hub.download_url_to_file('https://s3.amazonaws.com/pytorch/models/resnet18-5c106cde.pth', '/tmp/temporary_file')",
        """CODE.with torch.autograd.profiler.profile() as prof:
    y = x ** 2
    with torch.autograd.profiler.record_function("label-z"): # label the block
        z = y ** 3
    y.backward()

print(prof.key_averages().table(sort_by="self_cpu_time_total"))
""" .

<DEPENDENCY.torch==1.3.1> <CONTAINS> """CODE.def bar():
    cb = createResolutionCallback(1)
    print(cb("foo"))

def baz():
    foo = 2
    bar()

baz()""" .

<DEPENDENCY.torch==1.4.0> <CONTAINS> """CODE.def bar():
    cb = createResolutionCallbackFromFrame(1)
    print(cb("foo"))

def baz():
    foo = 2
    bar()

baz()""" .

<DEPENDENCY.torch==1.4.1> <CONTAINS> """CODE.A = torch.randn(2, 3, 3)
    A_LU, pivots = torch.lu(A)
    A_LU
    tensor([[[ 1.3506,  2.5558, -0.0816],
             [ 0.1684,  1.1551,  0.1940],
             [ 0.1193,  0.6189, -0.5497]],

            [[ 0.4526,  1.2526, -0.3285],
             [-0.7988,  0.7175, -0.9701],
             [ 0.2634, -0.9255, -0.3459]]])
    pivots
    tensor([[ 3,  3,  3],
            [ 3,  3,  3]], dtype=torch.int32)
    A_LU, pivots, info = torch.lu(A, get_infos=True)
    if info.nonzero().size(0) == 0:
    ...   print('LU factorization succeeded for all samples!')""",
        """CODE.t4d = torch.empty(3, 3, 4, 2)
    p1d = (1, 1) # pad last dim by 1 on each side
    out = F.pad(t4d, p1d, "constant", 0)  # effectively zero padding
    print(out.data.size())
    torch.Size([3, 3, 4, 4])
    p2d = (1, 1, 2, 2) # pad last dim by (1, 1) and 2nd to last by (2, 2)
    out = F.pad(t4d, p2d, "constant", 0)
    print(out.data.size())
    torch.Size([3, 3, 8, 4])
    t4d = torch.empty(3, 3, 4, 2)
    p3d = (0, 1, 2, 1, 3, 3) # pad by (0, 1), (2, 1), and (3, 3)
    out = F.pad(t4d, p3d, "constant", 0)
    print(out.data.size())
    torch.Size([3, 9, 7, 3])""" .

<DEPENDENCY.torch==1.5.0> <CONTAINS> """CODE.output = torch.unique(torch.tensor([1, 3, 2, 3], dtype=torch.long))
    output
    tensor([ 2,  3,  1])

    output, inverse_indices = torch.unique(
            torch.tensor([1, 3, 2, 3], dtype=torch.long), sorted=True, return_inverse=True)
    output
    tensor([ 1,  2,  3])
    inverse_indices
    tensor([ 0,  2,  1,  2])

    output, inverse_indices = torch.unique(
            torch.tensor([[1, 3], [2, 3]], dtype=torch.long), sorted=True, return_inverse=True)
    output
    tensor([ 1,  2,  3])
    inverse_indices
    tensor([[ 0,  2],
            [ 1,  2]])""",
        """CODE.output = torch.unique(torch.tensor([1, 3, 2, 3], dtype=torch.long))
output
tensor([ 2,  3,  1])

output, inverse_indices = torch.unique(
        torch.tensor([1, 3, 2, 3], dtype=torch.long), sorted=True, return_inverse=True)
output
tensor([ 1,  2,  3])
inverse_indices
tensor([ 0,  2,  1,  2])

output, inverse_indices = torch.unique(
        torch.tensor([[1, 3], [2, 3]], dtype=torch.long), sorted=True, return_inverse=True)
output
tensor([ 1,  2,  3])
inverse_indices
tensor([[ 0,  2],
        [ 1,  2]])
""" .

<DEPENDENCY.torch==1.5.1> <CONTAINS> """CODE.x = torch.tensor([1, 1, 2, 2, 3, 1, 1, 2])
    output = torch.unique_consecutive(x)
    output
    tensor([1, 2, 3, 1, 2])

    output, inverse_indices = torch.unique_consecutive(x, return_inverse=True)
    output
    tensor([1, 2, 3, 1, 2])
    inverse_indices
    tensor([0, 0, 1, 1, 2, 3, 3, 4])

    output, counts = torch.unique_consecutive(x, return_counts=True)
    output
    tensor([1, 2, 3, 1, 2])
    counts
    tensor([2, 2, 1, 2, 1])""" .

<DEPENDENCY.torch==1.6.0> <CONTAINS> """CODE.# Creates model and optimizer in default precision
model = Net().cuda()
optimizer = optim.SGD(model.parameters(), ...)

for input, target in data:
    optimizer.zero_grad()

    # Enables autocasting for the forward pass (model + loss)
    with autocast():
        output = model(input)
        loss = loss_fn(output, target)

    # Exits the context manager before backward()
    loss.backward()
    optimizer.step()


class AutocastModel(nn.Module):
    ...
    @autocast()
    def forward(self, input):
        ...
""",
        """CODE.@rpc.functions.async_execution
def async_add_chained(to, x, y, z):
    return rpc.rpc_async(to, torch.add, args=(x, y)).then(
        lambda fut: fut.wait() + z
    )

@torch.jit.script
def script_add(x, y):
    return x + y

@rpc.functions.async_execution
@torch.jit.script
def async_add(to, x, y):
    return rpc.rpc_async(to, script_add, (x, y))

class AsyncExecutionClass:

    @staticmethod
    @rpc.functions.async_execution
    def static_async_add(to, x, y, z):
        return rpc.rpc_async(to, torch.add, args=(x, y)).then(
            lambda fut: fut.wait() + z
        )

    @classmethod
    @rpc.functions.async_execution
    def class_async_add(cls, to, x, y, z):
        ret_fut = torch.futures.Future()
        rpc.rpc_async(to, torch.add, args=(x, y)).then(
            lambda fut: ret_fut.set_result(fut.wait() + z)
        )
        return ret_fut""",
        """CODE.channel_shuffle = nn.ChannelShuffle(2)
input = torch.randn(1, 4, 2, 2)
output = channel_shuffle(input)""",
        """CODE.class MyModule(nn.Module):
    def forward(input):
        return input + 1

module_cls = MyModule

import torch
import torch.distributed.rpc as rpc
from torch import nn, Tensor
from torch.distributed.nn.api.remote_module import RemoteModule

rpc.init_rpc("worker0", rank=0, world_size=2)
remote_linear_module = RemoteModule(
    "worker1", nn.Linear, args=(20, 30),
)
input = torch.randn(128, 20)
ret_fut = remote_linear_module.forward_async(input)
ret = ret_fut.wait()
rpc.shutdown()

import torch
import torch.distributed.rpc as rpc

rpc.init_rpc("worker1", rank=1, world_size=2)
rpc.shutdown()""",
        "CODE.custom_fwd = torch.jit.script(custom_fwd) if not torch.jit.is_scripting() else custom_fwd",
        """CODE.from torch.nn.quantized import functional as qF
filters = torch.randn(33, 16, 3, dtype=torch.float)
inputs = torch.randn(20, 16, 50, dtype=torch.float)
bias = torch.randn(33, dtype=torch.float)

scale, zero_point = 1.0, 0
dtype_inputs = torch.quint8
dtype_filters = torch.qint8

q_filters = torch.quantize_per_tensor(filters, scale, zero_point, dtype_filters)
q_inputs = torch.quantize_per_tensor(inputs, scale, zero_point, dtype_inputs)
qF.conv1d(q_inputs, q_filters, bias, padding=1, scale=scale, zero_point=zero_point)
""",
        """CODE.import threading
import time
import torch

def slow_set_future(fut, value):
    time.sleep(0.5)
    fut.set_result(value)

fut = torch.futures.Future()
t = threading.Thread(
    target=slow_set_future,
    args=(fut, torch.ones(2) * 3)
)
t.start()

print(fut.wait())  # tensor([3., 3.])
t.join()""",
        """CODE.import torch

fut0 = torch.futures.Future()
fut1 = torch.futures.Future()

fut = torch.futures.collect_all([fut0, fut1])

fut0.set_result(0)
fut1.set_result(1)

fut_list = fut.wait()
print(f"fut0 result = {fut_list[0].wait()}")
print(f"fut1 result = {fut_list[1].wait()}")""",
        """CODE.import torch
A = torch.tensor([[0, 1], [1, 0]])
B = torch.tensor([[3, 4, 5], [6, 7, 8]])
C = torch.tensor(7)
D = torch.tensor([1, 2, 3])
E = torch.tensor([[4], [5], [6]])
torch.block_diag(A, B, C, D, E)""",
        """CODE.import torch
class MyModule(torch.nn.Module):
    def __init__(self, N, M):
        super(MyModule, self).__init__()
        self.weight = torch.nn.Parameter(torch.rand(N, M))
        self.linear = torch.nn.Linear(N, M)

    def forward(self, input):
        output = self.weight.mm(input)
        output = self.linear(output)
        return output

scripted_module = torch.jit.script(MyModule(2, 3).eval())
frozen_module = torch.jit.freeze(scripted_module)
# parameters have been removed and inlined into the Graph as constants
assert len(list(frozen_module.named_parameters())) == 0
# See the compiled graph as Python code
print(frozen_module.code)

import torch
class MyModule2(torch.nn.Module):
    def __init__(self):
        super(MyModule2, self).__init__()
        self.modified_tensor = torch.tensor(10.)
        self.version = 1

    def forward(self, input):
        self.modified_tensor += 1
        return input + self.modified_tensor

scripted_module = torch.jit.script(MyModule2().eval())
frozen_module = torch.jit.freeze(scripted_module, preserved_attrs=["version"])
# we've manually preserved `version`, so it still exists on the frozen module and can be modified
assert frozen_module.version == 1
frozen_module.version = 2
# `modified_tensor` is detected as being mutated in the forward, so freezing preserves
# it to retain model semantics
assert frozen_module(torch.tensor(1)) == torch.tensor(12)
# now that we've run it once, the next result will be incremented by one
assert frozen_module(torch.tensor(1)) == torch.tensor(13)""",
        """CODE.import torch
from torch import Tensor
def foo(a : Tensor, b : int) -> Tensor:
    return a + b
def bar(a):
    fut : torch.jit.Future[Tensor] = torch.jit.fork(foo, a, b=2)
    return torch.jit.wait(fut)
script_bar = torch.jit.script(bar)
input = torch.tensor(2)
# only the scripted version executes asynchronously
assert script_bar(input) == bar(input)
# trace is not run asynchronously, but fork is captured in IR
graph = torch.jit.trace(bar, (input,)).graph
assert "fork" in str(graph)

import torch
from torch import Tensor
class SubMod(torch.nn.Module):
    def forward(self, a: Tensor, b : int):
        return a + b
class Mod(torch.nn.Module):
    def __init__(self):
        super(self).__init__()
        self.mod = SubMod()
    def forward(self, input):
        fut = torch.jit.fork(self.mod, a, b=2)
        return torch.jit.wait(fut)
input = torch.tensor(2)
mod = Mod()
assert mod(input) == torch.jit.script(mod).forward(input)
""",
        """CODE.import torch
from torch.quantization import get_default_qconfig
from torch.quantization import quantize_jit

ts_model = torch.jit.script(float_model.eval())  # or torch.jit.trace(float_model, input)
qconfig = get_default_qconfig('fbgemm')
def calibrate(model, data_loader):
    model.eval()
    with torch.no_grad():
        for image, target in data_loader:
            model(image)

quantized_model = quantize_jit(
    ts_model,
    {'': qconfig},
    calibrate,
    [data_loader_test])
""",
        """CODE.import torch
from torch.quantization import per_channel_dynamic_qconfig
from torch.quantization import quantize_dynamic_jit

ts_model = torch.jit.script(float_model.eval())  # or torch.jit.trace(float_model, input)
qconfig = get_default_qconfig('fbgemm')
def calibrate(model, data_loader):
    model.eval()
    with torch.no_grad():
        for image, target in data_loader:
            model(image)

quantized_model = quantize_dynamic_jit(
    ts_model,
    {'': qconfig},
    calibrate,
    [data_loader_test])
""",
        """CODE.import torch
import io

# Load LiteScriptModule from saved file path
torch.jit._load_for_lite_interpreter('lite_script_module.pt')

# Load LiteScriptModule from io.BytesIO object
with open('lite_script_module.pt', 'rb') as f:
    buffer = io.BytesIO(f.read())

# Load all tensors to the original device
torch.jit.mobile._load_for_lite_interpreter(buffer)""",
        """CODE.import torch
import torch.distributed.rpc as rpc
rpc.init_rpc("worker0", rank=0, world_size=2)
x, y = torch.tensor(1), torch.tensor(2)
outer_profile_rref = rpc.remote(dst_worker_name, rpc._server_process_global_profile)
outer_profile_rref.rpc_sync().__enter__()
rpc.rpc_sync(dst_worker_name, torch.add, (x, y))
inner_profile_rref = rpc.remote(dst_worker_name, rpc._server_process_global_profile)
inner_profile_rref.rpc_sync().__enter__()
rpc.rpc_sync(dst_worker_name, torch.sub, (x, y))
inner_profile_rref.rpc_sync().__exit__(None, None, None)
outer_profile_rref.rpc_sync().__exit__(None, None, None
rpc.shutdown()""",
        """CODE.input = torch.arange(4) + rank * 4
input = list(input.chunk(4))
input
[tensor([0]), tensor([1]), tensor([2]), tensor([3])]     # Rank 0
[tensor([4]), tensor([5]), tensor([6]), tensor([7])]     # Rank 1
[tensor([8]), tensor([9]), tensor([10]), tensor([11])]   # Rank 2
[tensor([12]), tensor([13]), tensor([14]), tensor([15])] # Rank 3
output = list(torch.empty([4], dtype=torch.int64).chunk(4))
dist.all_to_all(output, input)
output
[tensor([0]), tensor([4]), tensor([8]), tensor([12])]    # Rank 0
[tensor([1]), tensor([5]), tensor([9]), tensor([13])]    # Rank 1
[tensor([2]), tensor([6]), tensor([10]), tensor([14])]   # Rank 2
[tensor([3]), tensor([7]), tensor([11]), tensor([15])]   # Rank 3

input
tensor([0, 1, 2, 3, 4, 5])                                       # Rank 0
tensor([10, 11, 12, 13, 14, 15, 16, 17, 18])                     # Rank 1
tensor([20, 21, 22, 23, 24])                                     # Rank 2
tensor([30, 31, 32, 33, 34, 35, 36])                             # Rank 3
input_splits
[2, 2, 1, 1]                                                     # Rank 0
[3, 2, 2, 2]                                                     # Rank 1
[2, 1, 1, 1]                                                     # Rank 2
[2, 2, 2, 1]                                                     # Rank 3
output_splits
[2, 3, 2, 2]                                                     # Rank 0
[2, 2, 1, 2]                                                     # Rank 1
[1, 2, 1, 2]                                                     # Rank 2
[1, 2, 1, 1]                                                     # Rank 3
input = list(input.split(input_splits))
input
[tensor([0, 1]), tensor([2, 3]), tensor([4]), tensor([5])]                   # Rank 0
[tensor([10, 11, 12]), tensor([13, 14]), tensor([15, 16]), tensor([17, 18])] # Rank 1
[tensor([20, 21]), tensor([22]), tensor([23]), tensor([24])]                 # Rank 2
[tensor([30, 31]), tensor([32, 33]), tensor([34, 35]), tensor([36])]         # Rank 3
output = ...
dist.all_to_all(output, input)
output
[tensor([0, 1]), tensor([10, 11, 12]), tensor([20, 21]), tensor([30, 31])]   # Rank 0
[tensor([2, 3]), tensor([13, 14]), tensor([22]), tensor([32, 33])]           # Rank 1
[tensor([4]), tensor([15, 16]), tensor([23]), tensor([34, 35])]              # Rank 2
[tensor([5]), tensor([17, 18]), tensor([24]), tensor([36])]                  # Rank 3""",
        """CODE.loader, optimizer, model, loss_fn = ...
swa_model = torch.optim.swa_utils.AveragedModel(model)
scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer,
                                    T_max=300)
swa_start = 160
swa_scheduler = SWALR(optimizer, swa_lr=0.05)
for i in range(300):
     for input, target in loader:
         optimizer.zero_grad()
         loss_fn(model(input), target).backward()
         optimizer.step()
     if i > swa_start:
         swa_model.update_parameters(model)
         swa_scheduler.step()
     else:
         scheduler.step()

# Update bn statistics for the swa_model at the end
torch.optim.swa_utils.update_bn(loader, swa_model)""",
        """CODE.lr_lambda = lambda epoch: 0.9
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lr_lambda)
swa_scheduler = torch.optim.swa_utils.SWALR(optimizer, anneal_strategy="linear", anneal_epochs=20, swa_lr=0.05)
swa_start = 160
for i in range(300):
     for input, target in loader:
         optimizer.zero_grad()
         loss_fn(model(input), target).backward()
         optimizer.step()
     if i > swa_start:
         swa_scheduler.step()
     else:
         scheduler.step()""",
        "CODE.nn.init.trunc_normal_(w)",
        """CODE.prepare_model_with_stubs(float_model, q_model, module_swap_list, Logger)
q_model(data)
ob_dict = get_logger_dict(q_model)""" .

<DEPENDENCY.torch==1.7.0> <CONTAINS> """CODE.Fuzzer(
    parameters=[
        FuzzedParameter("x_len", 4, 1024, distribution="uniform"),

        # `y` will either be size one, or match the size of `x`.
        FuzzedParameter("y_len", distribution={
            0.5: 1,
            0.5: ParameterAlias("x_len")
        }),
    ],
    tensors=[
        FuzzedTensor("x", size=("x_len",)),
        FuzzedTensor("y", size=("y_len",)),
    ],
)""",
        "CODE._del_nested_attr(obj, ['conv', 'weight'])",
        "CODE._load_local(path, 'resnet50', pretrained=True)",
        """CODE.copy a node from one graph into another. arg_transform needs to transform arguments from the graph of node
to the graph of self. Example:

g : torch._fx.Graph = ...
new_graph = torch._fx.graph()
value_remap = {}
for node in g.nodes:
    value_remap[node] = new_graph.node_copy(node, lambda n : value_remap[n])""",
        "CODE.ddp_model._register_comm_hook(process_group, quantization_pertensor_hook)",
        """CODE.def noop(state: object, bucket: dist._GradBucket) -> torch.futures.Future
    fut = torch.futures.Future()
    fut.set_result(bucket.get_tensors())
    return fut

ddp._register_comm_hook(state = None, hook = noop)

def encode_and_decode(state: object, bucket: dist._GradBucket) -> torch.futures.Future
    tensors = [t / process_group.world_size for t in bucket.get_tensors()]
    encoded_tensors = encode(tensors) # encode gradients
    fut = process_group.allreduce(encoded_tensors).get_future()
    # Define the then callback to decode.
    def decode(fut):
        decoded_tensors = decode(fut.value()) # decode gradients
        return decoded_tensors
    return fut.then(decode)

ddp._register_comm_hook(state = None, hook = encode_and_decode)""",
        """CODE.import torch
from torch.quantization import per_channel_dynamic_qconfig
from torch.quantization import quantize_dynamic_fx

graph_module = torch._fx.symbolic_trace(float_model.eval())
qconfig = get_default_qconfig('fbgemm')
def calibrate(model, data_loader):
    model.eval()
    with torch.no_grad():
        for image, target in data_loader:
            model(image)

quantized_model = quantize_dynamic_fx(
    graph_module,
    {'': qconfig},
    calibrate,
    [data_loader_test])
""",
        """CODE.import torch
import torch.distributed as dist
import os
import torch.multiprocessing as mp
import torch.nn as nn

# On each spawned worker
def worker(rank):
    dist.init_process_group("nccl", rank=rank, world_size=2)
    torch.cuda.set_device(rank)
    model = nn.Linear(1, 1, bias=False).to(rank)
    model = torch.nn.parallel.DistributedDataParallel(
        model, device_ids=[rank], output_device=rank
    )
    # Rank 1 gets one more input than rank 0.
    inputs = [torch.tensor([1]).float() for _ in range(10 + rank)]
    with model.join():
        for _ in range(5):
            for inp in inputs:
                loss = model(inp).sum()
                loss.backward()

# Without the join() API, the below synchronization will hang
# blocking for rank 1's allreduce to complete.
torch.cuda.synchronize(device=rank)""",
        """CODE.m = nn.Sequential(
    nn.Linear(50, 50),
    nn.Unflatten(1, (2, 5, 5))
)
output = m(output)
output.size()

m = nn.Sequential(
    nn.Linear(50, 50),
    nn.Unflatten(1, torch.Size([2, 5, 5]))
)
output = m(output)
output.size()

m = nn.Sequential(
    nn.Linear(50, 50),
    nn.Unflatten('features', (('C', 2), ('H', 50), ('W',50)))
)
output = m(output)
output.size()
""",
        """CODE.m = nn.SiLU()
input = torch.randn(2)
output = m(input)
""",
        "CODE.register_ddp_comm_hook(DDPCommHookType.FP16_COMPRESS, model, state)",
        """CODE.torch.dot                            # [D], [D] -> []
batched_dot = torch.vmap(torch.dot)  # [N, D], [N, D] -> [N]
x, y = torch.randn(2, 5), torch.randn(2, 5)
batched_dot(x, y)

batch_size, feature_size = 3, 5
weights = torch.randn(feature_size, requires_grad=True)

def model(feature_vec):
    # Very simple linear model with activation
    return feature_vec.dot(weights).relu()

examples = torch.randn(batch_size, feature_size)
result = torch.vmap(model)(examples)

N = 5
f = lambda x: x ** 2
x = torch.randn(N, requires_grad=True)
y = f(x)
I_N = torch.eye(N)

jacobian_rows = [torch.autograd.grad(y, x, v, retain_graph=True)[0]
                 for v in I_N.unbind()]
jacobian = torch.stack(jacobian_rows)

def get_vjp(v):
    return torch.autograd.grad(y, x, v)
jacobian = torch.vmap(get_vjp)(I_N)""",
        """CODE.x = torch.randn(2)
x
tensor([1.4584, 0.7583])
torch.atleast_1d(x)
tensor([1.4584, 0.7583])
x = torch.tensor(1.)
x
tensor(1.)
torch.atleast_1d(x)
tensor([1.])
x = torch.tensor(0.5)
y = torch.tensor(1.)
torch.atleast_1d((x,y))
(tensor([0.5000]), tensor([1.]))""",
        """CODE.x = torch.tensor(0.5)
x
tensor(0.5000)
torch.atleast_3d(x)
tensor([[[0.5000]]])
y = torch.randn(2,2)
y
tensor([[-0.8079,  0.7460],
        [-1.1647,  1.4734]])
torch.atleast_3d(y)
tensor([[[-0.8079],
        [ 0.7460]],
        <BLANKLINE>
        [[-1.1647],
        [ 1.4734]]])
x = torch.randn(1,1,1)
x
tensor([[[-1.5689]]])
torch.atleast_3d(x)
tensor([[[-1.5689]]])
x = torch.tensor(0.5)
y = torch.tensor(1.)
torch.atleast_3d((x,y))
(tensor([[[0.5000]]]), tensor([[[1.]]]))""",
        """CODE.x = torch.tensor(1.)
torch.atleast_2d(x)
x = torch.randn(2,2)
torch.atleast_2d(x)
x = torch.tensor(0.5)
y = torch.tensor(1.)
torch.atleast_2d((x,y))""" .

<DEPENDENCY.torch==1.7.1> <CONTAINS> """CODE.def noop(state: object, bucket: dist._GradBucket) -> torch.futures.Future
    fut = torch.futures.Future()
    fut.set_result(bucket.get_tensors())
    return fut

ddp._register_comm_hook(state = None, hook = noop)

def encode_and_decode(state: object, bucket: dist._GradBucket) -> torch.futures.Future
    tensors = [t / process_group.world_size for t in bucket.get_tensors()]
    encoded_tensors = encode(tensors) # encode gradients
    fut = process_group.allreduce(encoded_tensors).get_future()
    # Define the then callback to decode.
    def decode(fut):
        decoded_tensors = decode(fut.value()) # decode gradients
        return decoded_tensors
    return fut.then(decode)

ddp._register_comm_hook(state = None, hook = encode_and_decode)""" .

<DEPENDENCY.torch==1.8.0> <CONTAINS> """CODE.class Foo(Managed):
    pass

with Foo() as f:
    assert f == Foo.current()""",
        """CODE.def noop(state: object, bucket: dist._GradBucket):
    fut = torch.futures.Future()
    fut.set_result(bucket.get_tensors())
    return fut

ddp.register_comm_hook(state = None, hook = noop)

def encode_and_decode(state: object, bucket: dist._GradBucket):
    tensors = [t / process_group.world_size for t in bucket.get_tensors()]
    encoded_tensors = encode(tensors) # encode gradients
    fut = process_group.allreduce(encoded_tensors).get_future()

    # Define the then callback to decode.
    def decode(fut):
        decoded_tensors = decode(fut.value()) # decode gradients
        return decoded_tensors

    return fut.then(decode)

ddp.register_comm_hook(state = None, hook = encode_and_decode)""" .

<DEPENDENCY.torch==1.9.0> <CONTAINS> """CODE.import torch

    def callback(fut):
        print(f"This will run after the future has finished.")
        print(fut.wait())

    fut = torch.futures.Future()
    fut.add_done_callback(callback)
    fut.set_result(5)

    # Outputs are:
    This will run after the future has finished.
    5""" .

<DEPENDENCY.torch==1.9.1> <CONTAINS> """CODE.dense = torch.randn(5, 5)
sparse = dense._to_sparse_csr()
sparse._nnz()
æªæ¾å°pythonä»£ç """ .

<DEPENDENCY.torch==2.1.2> <CONTAINS> """CODE.# an operator with data-dependent output shape
    @custom_op("mylibrary::custom_nonzero")
    def custom_nonzero(x: Tensor) -> Tensor:
        ...

    @custom_nonzero.impl_abstract():
    def custom_nonzero_abstract(x):
        # Number of nonzero-elements is data-dependent
        ctx = torch._custom_op.get_ctx()
        nnz = ctx.create_unbacked_symint()
        shape = [x.dim(), nnz]
        result = x.new_empty(shape, dtype=torch.long)
        return result

    @numpy_nonzero.impl(['cpu', 'cuda'])
    def custom_nonzero_impl(x):
        x_np = to_numpy(x)
        res = np.stack(np.nonzero(x_np), axis=1)
        # the size associated with ctx.create_unbacked_symint()
        # must be constrained in the same way, so we add an assertion here.
        if res.shape[0] < 2 or res.shape[0] > x.numel():
            raise RuntimeError("not supported")
        return torch.tensor(res, device=x.device)""",
        """CODE.# xdoctest: +SKIP("distributed")
    from torch.distributed.tensor.parallel import parallelize_module, PairwiseParallel
    from torch.nn.parallel import DistributedDataParallel as DDP
    from torch.distributed.tensor.parallel.ddp import pre_dp_module_transform

    # Define the module.
    m = module(...)
    parallelize_module(m, PairwiseParallel())
    m = pre_dp_module_transform(m)
    m = DDP(m)""" .

<DEPENDENCY.torch==2.2.0> <CONTAINS> """CODE.# An operator with data-dependent output shape
    lib = torch.library.Library("mymodule", "FRAGMENT")
    lib.define("mymodule::custom_nonzero(Tensor x) -> Tensor")

    @torch.library.impl_abstract("mymodule::custom_nonzero")
    def custom_nonzero_abstract(x):
        # Number of nonzero-elements is data-dependent.
        # Since we cannot peek at the data in an abstract impl,
        # we use the ctx object to construct a new symint that
        # represents the data-dependent size.
        ctx = torch.library.get_ctx()
        nnz = ctx.new_dynamic_size()
        shape = [nnz, x.dim()]
        result = x.new_empty(shape, dtype=torch.int64)
        return result

    @torch.library.impl(lib, "custom_nonzero", "CPU")
    def custom_nonzero_cpu(x):
        x_np = x.numpy()
        res = np.stack(np.nonzero(x_np), axis=1)
        return torch.tensor(res, device=x.device)""",
        """CODE.@dataclass
class InputDataClass:
    feature: torch.Tensor
    bias: int

class OutputDataClass:
    res: torch.Tensor

torch.export.register_dataclass(InputDataClass)
torch.export.register_dataclass(OutputDataClass)

def fn(o: InputDataClass) -> torch.Tensor:
    res = res=o.feature + o.bias
    return OutputDataClass(res=res)

ep = torch.export.export(fn, (InputDataClass(torch.ones(2, 2), 1), ))
print(ep)""",
        """CODE.@decorateIf(unittest.skip, lambda params: params["x"] == 2)
@parametrize("x", range(5))
def test_foo(self, x):
    ...

@parametrize("x,y", [(1, 'foo'), (2, 'bar'), (3, 'baz')])
@decorateIf(
    unittest.expectedFailure,
    lambda params: params["x"] == 3 and params["y"] == "baz"
)
def test_bar(self, x, y):
    ...

@decorateIf(
    unittest.expectedFailure,
    lambda params: params["op"].name == "add" and params["dtype"] == torch.float16
)
@ops(op_db)
def test_op_foo(self, device, dtype, op):
    ...

@decorateIf(
    unittest.skip,
    lambda params: params["module_info"].module_cls is torch.nn.Linear and params["device"] == "cpu"
)
@modules(module_db)
def test_module_foo(self, device, dtype, module_info):
    ...""",
        """CODE.@torch.jit.interface
class InterfaceType:
    def run(self, x: torch.Tensor) -> torch.Tensor:
        pass

@torch.jit.script
class Impl1:
    def run(self, x: torch.Tensor) -> torch.Tensor:
        return x.relu()

class Impl2(torch.nn.Module):
    def __init__(self):
        super().__init__()
        self.val = torch.rand(())

    @torch.jit.export
    def run(self, x: torch.Tensor) -> torch.Tensor:
        return x + self.val

def user_fn(impls: List[InterfaceType], idx: int, val: torch.Tensor) -> torch.Tensor:
    return impls[idx].run(val)

user_fn_jit = torch.jit.script(user_fn)

impls = [Impl1(), torch.jit.script(Impl2())]
val = torch.rand(4, 4)
user_fn_jit(impls, 0, val)
user_fn_jit(impls, 1, val)""",
        """CODE.Version("1.2.3").base_version
Version("1.2.3+abc").base_version
Version("1!1.2.3+abc.dev1").base_version""",
        """CODE.Version("1.2.3").is_devrelease
Version("1.2.3.dev1").is_devrelease""",
        """CODE.Version("1.2.3").is_postrelease
Version("1.2.3.post1").is_postrelease""",
        """CODE.Version("1.2.3").is_prerelease
Version("1.2.3a1").is_prerelease
Version("1.2.3b1").is_prerelease
Version("1.2.3rc1").is_prerelease
Version("1.2.3dev1").is_prerelease""",
        "CODE.Version(\"1.2.3\").major",
        """CODE.Version("1.2.3").micro
3
Version("1").micro""",
        """CODE.Version("1.2.3").minor
Version("1").minor""",
        """CODE.Version("1.2.3").public
Version("1.2.3+abc").public
Version("1.2.3+abc.dev1").public""",
        """CODE.Version("1.2.3").release
Version("2.0.0").release
Version("1!2.0.0.post0").release""",
        "CODE.Version(\"1.2.3.dev1\").dev",
        "CODE.Version(\"1.2.3.post1\").post",
        """CODE.Version("2.0.0").epoch
Version("1!2.0.0").epoch""",
        """CODE.a = torch.arange(3, dtype=torch.float, requires_grad=True)
b = torch.arange(5, dtype=torch.float, requires_grad=True)
nt = torch.nested.nested_tensor([a, b], requires_grad=True)
nt.is_leaf
True
""",
        """CODE.class ProtobufONNXProgramSerializer:
    def serialize(
        self, onnx_program: torch.onnx.ONNXProgram, destination: io.BufferedIOBase
    ) -> None:
        destination.write(onnx_program.model_proto.SerializeToString())

model = MyModel()
arg1 = torch.randn(2, 2, 2)  # positional input 1
torch.onnx.dynamo_export(model, arg1).save(
    destination="exported_model.onnx",
    serializer=ProtobufONNXProgramSerializer(),
)""",
        """CODE.def broadcast_prefix(prefix_tree, full_tree, is_leaf=None):
    # Check if prefix_tree is a prefix of full_tree
    if not is_prefix(prefix_tree, full_tree):
        raise ValueError("Prefix tree is not a prefix of full tree")

    # Get the number of leaves in full_tree
    num_leaves_full = count_leaves(full_tree)

    # Replicate leaves from prefix_tree to match the number of leaves in full_tree
    replicated_leaves = replicate_leaves(prefix_tree, num_leaves_full)

    return replicated_leaves

def is_prefix(prefix_tree, full_tree):
    # Check if prefix_tree is a prefix of full_tree
    # Implementation details omitted for brevity
    pass

def count_leaves(tree):
    # Count the number of leaves in a tree
    # Implementation details omitted for brevity
    pass

def replicate_leaves(tree, num_replicas):
    # Replicate leaves in a tree to match the specified number of replicas
    # Implementation details omitted for brevity
    pass
""",
        """CODE.def get_custom_policy():
    no_recompute_list = [
        torch.ops.aten.mm.default,
    ]
    def custom_policy(mode, func, *args, **kwargs):
        return func in no_recompute_list
    return custom_policy

def selective_checkpointing_context_fn():
    return _pt2_selective_checkpoint_context_fn_gen(get_custom_policy())

def gn(x, y):
    return torch.sigmoid(torch.matmul(torch.matmul(x, y), y)) * y

def fn(x, y):
    return torch.utils.checkpoint.checkpoint(
        gn, x, y,
        use_reentrant=False,
        context_fn=selective_checkpointing_context_fn,
    )

x = torch.randn(4, 4, requires_grad=True)
y = torch.randn(4, 4, requires_grad=True)

compiled_fn = torch.compile(fn)""",
        """CODE.def pattern(x, weight):
    conv = F.conv2d(x, weight)
    relu = F.relu(conv)
    return relu, {"conv": conv, "relu": relu}

def target_graph(x, weight):
    conv = F.conv2d(x, weight)
    relu = F.relu(conv)
    relu *= 2
    return relu

pattern_gm = capture_pre_autograd_graph(pattern, example_inputs)
target_gm = capture_pre_autograd_graph(target_graph, example_inputs)
matcher = SubgraphMatcherWithNameNodeMap(pattern_gm)
matches = matcher.match(target_gm)
for match in matches:
    match.name_node_map["conv"].meta["annotation"] = ...""",
        """CODE.dt_mesh = DeviceMesh("xla", [[1, 2, 3, 4]])
mesh = convert_to_xla_mesh(dt_mesh)""",
        """CODE.import torch
from torch.ao.quantization.quantize_pt2e import prepare_pt2e
from torch._export import capture_pre_autograd_graph
from torch.ao.quantization.quantizer import (
    XNNPACKQuantizer,
    get_symmetric_quantization_config,
)

class M(torch.nn.Module):
    def __init__(self):
        super().__init__()
        self.linear = torch.nn.Linear(5, 10)

   def forward(self, x):
       return self.linear(x)

# initialize a floating point model
float_model = M().eval()

# define calibration function
def calibrate(model, data_loader):
    model.eval()
    with torch.no_grad():
        for image, target in data_loader:
            model(image)

# Step 1. program capture
# NOTE: this API will be updated to torch.export API in the future, but the captured
# result shoud mostly stay the same
m = capture_pre_autograd_graph(m, *example_inputs)
# we get a model with aten ops

# Step 2. quantization
# backend developer will write their own Quantizer and expose methods to allow
# users to express how they
# want the model to be quantized
quantizer = XNNPACKQuantizer().set_global(get_symmetric_quantization_config())
m = prepare_pt2e(m, quantizer)

# run calibration
# calibrate(m, sample_inference_data)""",
        """CODE.import torch
from torch.ao.quantization.quantize_pt2e import prepare_qat_pt2e
from torch._export import capture_pre_autograd_graph
from torch.ao.quantization.quantizer import (
    XNNPACKQuantizer,
    get_symmetric_quantization_config,
)

class M(torch.nn.Module):
    def __init__(self):
        super().__init__()
        self.linear = torch.nn.Linear(5, 10)

   def forward(self, x):
       return self.linear(x)

# initialize a floating point model
float_model = M().eval()

# define the training loop for quantization aware training
def train_loop(model, train_data):
    model.train()
    for image, target in data_loader:
        ...

# Step 1. program capture
# NOTE: this API will be updated to torch.export API in the future, but the captured
# result shoud mostly stay the same
m = capture_pre_autograd_graph(m, *example_inputs)
# we get a model with aten ops

# Step 2. quantization
# backend developer will write their own Quantizer and expose methods to allow
# users to express how they
# want the model to be quantized
quantizer = XNNPACKQuantizer().set_global(get_symmetric_quantization_config())
m = prepare_qat_pt2e(m, quantizer)

# run quantization aware training
train_loop(prepared_model, train_loop)""",
        """CODE.import torch
from torch.distributed.fsdp import FullyShardedDataParallel as FSDP
from torch.nn.parallel import DistributedDataParallel as DDP
from torch.distributed.checkpoint.state_dict import get_state_dict

fsdp_model = FSDP(copy.deepcopy(model))
fsdp_optim = torch.optim.Adam(model.parameters(), lr=1e-3)
ddp_model = DDP(copy.deepcopy(model))
ddp_optim = torch.optim.Adam(model.parameters(), lr=1e-3)


ddp_state_dict, ddp_optim_state_dict = get_state_dict(ddp_model, ddp_optim)
fsdp_state_dict, fsdp_optim_state_dict = get_state_dict(fsdp_model, fsdp_optim)

# if we simply call ddp_model.state_dict() and fsdp_model.state_dict(),
# the asserts will fail.
assert ddp_state_dict == fsdp_state_dict
assert ddp_optim_state == fsdp_optim_state_dict""",
        """CODE.import torch
import torch.onnx
def func_returning_tuples(x, y, z):
...     x = x + y
...     y = y + z
...     z = x + y
...     return (x, (y, z))
x = torch.tensor(1.)
y = torch.tensor(2.)
z = torch.tensor(3.)
onnx_program = torch.onnx.dynamo_export(func_returning_tuples, x, y, z)
pt_output = func_returning_tuples(x, y, z)
print(pt_output)
(tensor(3.), (tensor(5.), tensor(8.)))
print(onnx_program.adapt_torch_outputs_to_onnx(pt_output, model_with_state_dict=func_returning_tuples))
[tensor(3.), tensor(5.), tensor(8.)]
""",
        """CODE.import torch
import torch.onnx
from typing import Dict, Tuple
def func_nested_input(
...     x_dict: Dict[str, torch.Tensor],
...     y_tuple: Tuple[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]
... ):
...     if "a" in x_dict:
...         x = x_dict["a"]
...     elif "b" in x_dict:
...         x = x_dict["b"]
...     else:
...         x = torch.randn(3)
...
...     y1, (y2, y3) = y_tuple
...
...     return x + y1 + y2 + y3
x_dict = {"a": torch.tensor(1.)}
y_tuple = (torch.tensor(2.), (torch.tensor(3.), torch.tensor(4.)))
onnx_program = torch.onnx.dynamo_export(func_nested_input, x_dict, y_tuple)
print(x_dict, y_tuple)
{'a': tensor(1.)} (tensor(2.), (tensor(3.), tensor(4.)))
print(onnx_program.adapt_torch_inputs_to_onnx(x_dict, y_tuple, model_with_state_dict=func_nested_input))
(tensor(1.), tensor(2.), tensor(3.), tensor(4.))
""",
        """CODE.import torch
torch.unravel_index(torch.tensor(4), (3, 2))
torch.unravel_index(torch.tensor([4, 1]), (3, 2))
torch.unravel_index(torch.tensor([0, 1, 2, 3, 4, 5]), (3, 2))
torch.unravel_index(torch.tensor([1234, 5678]), (10, 10, 10, 10))
torch.unravel_index(torch.tensor([[1234], [5678]]), (10, 10, 10, 10))
torch.unravel_index(torch.tensor([[1234], [5678]]), (100, 100))
""",
        """CODE.input = torch.tensor([-100.0, -200, -300])
input
tensor([-100., -200., -300.])
other = torch.tensor([-1.0, -2, -3])
other
tensor([-1., -2., -3.])
mask = torch.tensor([True, False, True])
mask
tensor([ True, False,  True])
torch.masked._ops.logaddexp(input, other, input_mask=mask, other_mask=mask)
tensor([-1., -inf, -3.])
""",
        """CODE.mod = ...
comm_mode = CommDebugMode()
with comm_mode:
    mod.sum().backward()
""",
        """CODE.out = F.pad(t4d, p1d, "constant", 0)  # effectively zero padding
out = F.pad(t4d, p2d, "constant", 0)
out = F.pad(t4d, p3d, "constant", 0)""",
        """CODE.print(Version("1.2.3").local)
None
Version("1.2.3+abc").local
'abc'""",
        """CODE.print(Version("1.2.3").pre)
Version("1.2.3a1").pre
Version("1.2.3b1").pre
Version("1.2.3rc1").pre""",
        "CODE.quantized_model = convert_pt2e(prepared_model)",
        """CODE.register_pytree_node(
    set,
    lambda s: (sorted(s), None, None),
    lambda children, _: set(children),
)""",
        """CODE.t = torch.randn(4, 8, 8)
dt_mesh = DeviceMesh("xla", torch.arange(8).reshape(2,4))
placements = [Replicate(), Shard(0)]
my_dtensor = distribute_tensor(t, dt_mesh, placements)

partition_spec = convert_to_xla_partition_spec(t, placements)
""",
        """CODE.torch.library.define("mylib::sin", "(Tensor x) -> Tensor")
@torch.library.impl("mylibrary::sin", "cpu")
def f(x):
    return torch.from_numpy(np.sin(x.numpy()))
x = torch.randn(3)
y = torch.ops.mylib.sin(x)
assert torch.allclose(y, x)""",
        """CODE.tree = {'b': (2, [3, 4]), 'a': 1, 'c': None, 'd': 5}
tree_leaves(tree)
[1, 2, 3, 4, None, 5]
tree_leaves(1)
[1]
tree_leaves(None)
[None]""",
        """CODE.tree = {'b': (2, [3, 4]), 'a': 1, 'c': None, 'd': 5}
tree_structure(tree)
PyTreeSpec({'a': *, 'b': (*, [*, *]), 'c': *, 'd': *}, NoneIsLeaf)
tree_structure(1)
PyTreeSpec(*, NoneIsLeaf)
tree_structure(None)
PyTreeSpec(*, NoneIsLeaf)""",
        """CODE.tree_map(lambda x: x + 1, {'x': 7, 'y': (42, 64)})
{'x': 8, 'y': (43, 65)}
tree_map(lambda x: x is None, {'x': 7, 'y': (42, 64), 'z': None})
{'x': False, 'y': (False, False), 'z': True}
""",
        """CODE.v1 = Version("1.0a5")
v2 = Version("1.0")
v1
<Version('1.0a5')>
v2
<Version('1.0')>
v1 < v2
True
v1 == v2
False
v1 > v2
False
v1 >= v2
False
v1 <= v2
True""" .

<DEPENDENCY.torchaudio==0.13.1> <CONTAINS> """CODE.torchaudio.datasets.utils.download_from_url(url, from_path)
torchaudio.datasets.utils.extract_archive(from_path, to_path)""" .

<DEPENDENCY.torchaudio==0.4.0> <CONTAINS> """CODE.from torchaudio.datasets.utils import unicode_csv_reader
import io
with io.open(data_path, encoding="utf8") as f:
    reader = unicode_csv_reader(f)""",
        "CODE.torchaudio.datasets.utils.extract_archive(from_path, to_path)" .

<DEPENDENCY.torchaudio==0.8.1> <CONTAINS> """CODE.ei = torchaudio.sox_encodinginfo_t()
ei.encoding = torchaudio.get_sox_encoding_t(1)
ei.bits_per_sample = 16
ei.compression = 0
ei.reverse_bytes = torchaudio.get_sox_option_t(2)
ei.reverse_nibbles = torchaudio.get_sox_option_t(2)
ei.reverse_bits = torchaudio.get_sox_option_t(2)
ei.opposite_endian = torchaudio.get_sox_bool(0)
""" .

<DEPENDENCY.torchaudio==0.9.0> <CONTAINS> """CODE.from torchaudio.models.wav2vec2.utils import import_fairseq_model

# Load model using fairseq
model_file = 'wav2vec_small.pt'
model, _, _ = fairseq.checkpoint_utils.load_model_ensemble_and_task([model_file])
original = model[0]
imported = import_fairseq_model(original, num_out=28)

# Perform feature extraction
waveform, _ = torchaudio.load('audio.wav')
features, _ = imported.extract_features(waveform)

# Compare result with the original model from fairseq
reference = original.feature_extractor(waveform).transpose(1, 2)
torch.testing.assert_allclose(features, reference)

from torchaudio.models.wav2vec2.utils import import_fairseq_model

# Load model using fairseq
model_file = 'wav2vec_small_960h.pt'
model, _, _ = fairseq.checkpoint_utils.load_model_ensemble_and_task([model_file])
original = model[0]
imported = import_fairseq_model(original.w2v_encoder)

# Perform encoding
waveform, _ = torchaudio.load('audio.wav')
emission, _ = imported(waveform)

# Compare result with the original model from fairseq
mask = torch.zeros_like(waveform)
reference = original(waveform, mask)['encoder_out'].transpose(0, 1)
torch.testing.assert_allclose(emission, reference)""",
        """CODE.from torchaudio.models.wav2vec2.utils import import_huggingface_model

original = Wav2Vec2ForCTC.from_pretrained("facebook/wav2vec2-base-960h")
model = import_huggingface_model(original)

waveforms, _ = torchaudio.load("audio.wav")
logits, _ = model(waveforms)
""",
        """CODE.from torchaudio.models.wav2vec2.utils import import_huggingface_model

original = Wav2Vec2ForCTC.from_pretrained("facebook/wav2vec2-large-960h")
model = import_huggingface_model(original)
torch.save(model.state_dict(), "wav2vec2-base-960h.pt")

model = wav2vec2_large(num_out=32)
model.load_state_dict(torch.load("wav2vec2-base-960h.pt"))
""",
        """CODE.model = wav2vec2_large_lv60k(num_out=32)
model.load_state_dict(torch.load("wav2vec2-base-960h.pt"))
""",
        """CODE.wav2vec2_base(num_out=32)
model.load_state_dict(torch.load("wav2vec2-base-960h.pt"))""" .

<DEPENDENCY.torchaudio==2.0.1> <CONTAINS> """CODE.for k, v in get_audio_decoders().items():
    print(f"{k}: {v}")""",
        """CODE.for k, v in get_audio_encoders().items():
    print(f"{k}: {v}")
comfortnoise: RFC 3389 comfort noise generator
s302m: SMPTE 302M
aac: AAC (Advanced Audio Coding)
ac3: ATSC A/52A (AC-3)
ac3_fixed: ATSC A/52A (AC-3)
alac: ALAC (Apple Lossless Audio Codec)""",
        """CODE.for k, v in get_demuxers().items():
    print(f"{k}: {v}")
aa: Audible AA format files
aac: raw ADTS AAC (Advanced Audio Coding)
aax: CRI AAX
ac3: raw AC-3""",
        """CODE.for k, v in get_input_devices().items():
    print(f"{k}: {v}")
avfoundation: AVFoundation input device
lavfi: Libavfilter virtual input device""",
        """CODE.for k, v in get_output_devices().items():
    print(f"{k}: {v}")""",
        """CODE.for k, v in get_video_decoders().items():
    print(f"{k}: {v}")
aasc: Autodesk RLE
aic: Apple Intermediate Codec
alias_pix: Alias/Wavefront PIX image
agm: Amuse Graphics Movie
amv: AMV Video
anm: Deluxe Paint Animation""",
        "CODE.print(get_input_protocols())",
        "CODE.print(get_output_protocols())" .

<DEPENDENCY.torchaudio==2.1.0> <CONTAINS> """CODE.decoder = cuda_ctc_decoder(
    vocab_file="tokens.txt",
    blank_skip_threshold=0.95,
)
results = decoder(log_probs, encoder_out_lens) # List of shape (B, nbest) of Hypotheses""",
        """CODE.decoder = torchaudio.models.decoder.ctc_decoder(...)
decoder.decode_begin()
decoder.decode_step(emission1)
decoder.decode_step(emission2)
decoder.decode_end()
result = decoder.get_final_hypothesis()""",
        """CODE.from torchaudio.pipelines import MMS_FA as bundle
bundle.get_dict()
{'-': 0, 'a': 1, 'i': 2, 'e': 3, 'n': 4, 'o': 5, 'u': 6, 't': 7, 's': 8, 'r': 9, 'm': 10, 'k': 11, 'l': 12, 'd': 13, 'g': 14, 'h': 15, 'y': 16, 'b': 17, 'p': 18, 'w': 19, 'c': 20, 'v': 21, 'j': 22, 'z': 23, 'f': 24, "'": 25, 'q': 26, 'x': 27, '*': 28}
bundle.get_dict(star=None)
{'-': 0, 'a': 1, 'i': 2, 'e': 3, 'n': 4, 'o': 5, 'u': 6, 't': 7, 's': 8, 'r': 9, 'm': 10, 'k': 11, 'l': 12, 'd': 13, 'g': 14, 'h': 15, 'y': 16, 'b': 17, 'p': 18, 'w': 19, 'c': 20, 'v': 21, 'j': 22, 'z': 23, 'f': 24, "'": 25, 'q': 26, 'x': 27}
""",
        """CODE.from torchaudio.pipelines import MMS_FA as bundle
bundle.get_labels()
bundle.get_labels(star=None)""" .

<DEPENDENCY.torchaudio==2.2.0> <CONTAINS> """CODE.n_chroma = 12
tuning = 0.0
ctroct = 5.0
octwidth = 2.0
norm = 2
base_c = True

waveform, sample_rate = torchaudio.load("test.wav", normalize=True)
spectrogram_transform = transforms.Spectrogram(n_fft=1024)
spectrogram = spectrogram_transform(waveform)
chroma_transform = transforms.ChromaScale(sample_rate=sample_rate, n_freqs=1024 // 2 + 1)
chroma_spectrogram = chroma_transform(spectrogram)
""",
        """CODE.transform = transforms.ChromaSpectrogram(sample_rate=sample_rate, n_fft=400)
chromagram = transform(waveform)  # (channel, n_chroma, time)
""" .

<DEPENDENCY.torchvision==0.4.0> <CONTAINS> """CODE.model = MNASNet(1000, 1.0)
x = torch.rand(1, 3, 224, 224)
y = model(x)
y.dim()
y.nelement()""" .

<DEPENDENCY.torchvision==0.5.0> <CONTAINS> """CODE.input = torch.rand(1, 3, 10, 10)
kh, kw = 3, 3
weight = torch.rand(5, 3, kh, kw)
offset = torch.rand(5, 2 * kh * kw, 8, 8)
out = deform_conv2d(input, offset, weight)
print(out.shape)
""" .

<DEPENDENCY.torchvision==0.8.0> <CONTAINS> """CODE.from torchvision.models.detection.backbone_utils import resnet_fpn_backbone
backbone = resnet_fpn_backbone('resnet50', pretrained=True, trainable_layers=3)
x = torch.rand(1,3,64,64)
output = backbone(x)
print([(k, v.shape) for k, v in output.items()])
""",
        """CODE.import torch
import torchvision
from torchvision.models.detection import RetinaNet
from torchvision.models.detection.anchor_utils import AnchorGenerator

backbone = torchvision.models.mobilenet_v2(pretrained=True).features
backbone.out_channels = 1280

anchor_generator = AnchorGenerator(
    sizes=((32, 64, 128, 256, 512),),
    aspect_ratios=((0.5, 1.0, 2.0),)
)

model = RetinaNet(backbone,
                  num_classes=2,
                  anchor_generator=anchor_generator)
model.eval()
x = [torch.rand(3, 300, 400), torch.rand(3, 500, 400)]
predictions = model(x)""",
        """CODE.import torchvision
video_path = "path_to_a_test_video"

reader = torchvision.io.VideoReader(video_path, "video")
reader.seek(2.0)
frame = next(reader)

reader.seek(2)
for frame in reader:
    frames.append(frame['data'])
# additionally, `seek` implements a fluent API, so we can do
for frame in reader.seek(2):
    frames.append(frame['data'])

for frame in itertools.takewhile(lambda x: x['pts'] <= 5, reader.seek(2)):
    frames.append(frame['data'])

for frame in itertools.islice(reader.seek(2), 10):
    frames.append(frame['data'])""" .

<DEPENDENCY.torchvision==0.9.0> <CONTAINS> """CODE.@test_all_configs
def test_foo(self, config):
    pass""",
        """CODE.combinations_grid(foo=("bar", "baz"), spam=("eggs", "ham"))
[
    {'foo': 'bar', 'spam': 'eggs'},
    {'foo': 'bar', 'spam': 'ham'},
    {'foo': 'baz', 'spam': 'eggs'},
    {'foo': 'baz', 'spam': 'ham'}
]""",
        """CODE.from torchvision import models as M

# Classification
model = M.mobilenet_v3_large(pretrained=False)
print(store_model_weights(model, './class.pth'))

# Quantized Classification
model = M.quantization.mobilenet_v3_large(pretrained=False, quantize=False)
model.fuse_model()
model.qconfig = torch.quantization.get_default_qat_qconfig('qnnpack')
_ = torch.quantization.prepare_qat(model, inplace=True)
print(store_model_weights(model, './qat.pth'))

# Object Detection
model = M.detection.fasterrcnn_mobilenet_v3_large_fpn(pretrained=False, pretrained_backbone=False)
print(store_model_weights(model, './obj.pth'))

# Segmentation
model = M.segmentation.deeplabv3_mobilenet_v3_large(pretrained=False, pretrained_backbone=False, aux_loss=True)
print(store_model_weights(model, './segm.pth', strict=False))
""",
        """CODE.model = torchvision.models.detection.fasterrcnn_mobilenet_v3_large_320_fpn(pretrained=True)
model.eval()
x = [torch.rand(3, 300, 400), torch.rand(3, 500, 400)]
predictions = model(x)""",
        """CODE.model = torchvision.models.detection.fasterrcnn_mobilenet_v3_large_fpn(pretrained=True)
model.eval()
x = [torch.rand(3, 300, 400), torch.rand(3, 500, 400)]
predictions = model(x)
""" .

<DEPENDENCY.tqdm==3.3.0> <CONTAINS> """CODE.df = pd.DataFrame(np.random.randint(0, 100, (100000, 6)))
tqdm_pandas(tqdm())
df.groupby(0).progress_apply(lambda x: x**2)""",
        """CODE.with tqdm(...) as t:
    reporthook = my_hook(t)
    urllib.urlretrieve(..., reporthook=reporthook)""" .

<DEPENDENCY.tqdm==4.35.0> <CONTAINS> """CODE.a = FormatReplace('something')
"{:5d}".format(a)""" .

<DEPENDENCY.tqdm==4.40.0> <CONTAINS> """CODE.with tqdm.wrapattr(file_obj, "read", total=file_obj.size) as fobj:
    while True:
        chunk = fobj.read(chunk_size)
        if not chunk:
            break""" .

<DEPENDENCY.tqdm==4.60.0> <CONTAINS> """CODE.import logging
from tqdm import trange
from tqdm.contrib.logging import logging_redirect_tqdm

LOG = logging.getLogger(__name__)

if __name__ == '__main__':
    logging.basicConfig(level=logging.INFO)
    with logging_redirect_tqdm():
        for i in trange(9):
            if i == 4:
                LOG.info("console logging redirected to `tqdm.write()`")
    # logging restored
""" .

<DEPENDENCY.tqdm==4.66.0> <CONTAINS> """CODE.from tqdm.utils import envwrap
@envwrap("FOO_")
def test(a=1, b=2, c=3):
    print(f"received: a={a}, b={b}, c={c}")

""" .

<DEPENDENCY.traitlets==4.1.0> <CONTAINS> """CODE.@default('bar')
def get_bar_default(self):
    return 11

@default('bar')
def some_other_default(self):
    return 3.0""",
        """CODE.@observe('name')
@observe_compat
def _foo_changed(self, change):
    ...
super()._foo_changed(self, name, old, new)""",
        "CODE.Int(0).tag(config=True, sync=True)",
        """CODE.import warnings
def foo():
    warnings.warn(RuntimeWarning("bar"))

with warnings.catch_warnings():
    warnings.simplefilter('once')
    foo()

from numpy.testing import assert_warns
foo()

with all_warnings():
    assert_warns(RuntimeWarning, foo)""" .

<DEPENDENCY.traitlets==4.3.0> <CONTAINS> """CODE.    import enum
    from traitlets import HasTraits, UseEnum

    class Color(enum.Enum):
        red = 1         # -- IMPLICIT: default_value
        blue = 2
        green = 3

    class MyEntity(HasTraits):
        color = UseEnum(Color, default_value=Color.blue)

    entity = MyEntity(color=Color.red)
    entity.color = Color.green    # USE: Enum-value (preferred)
    entity.color = "green"        # USE: name (as string)
    entity.color = "Color.green"  # USE: scoped-name (as string)
    entity.color = 3              # USE: number (as int)
    assert entity.color is Color.green""" .

<DEPENDENCY.traitlets==5.0.0> <CONTAINS> """CODE... code-block:: python
    class MyClass(HasTraits):
        i = Int()

    mc = MyClass()
    assert not mc.trait_has_value("i")
    mc.i # generates a default value
    assert mc.trait_has_value("i")""" .

<DEPENDENCY.traitlets==5.1.0> <CONTAINS> """CODE.os.environ['FOO']='test'
expand_path('variable FOO is $FOO')""" .

<DEPENDENCY.traitlets==5.2.0> <CONTAINS> """CODE.nested_update(
    {'a': 1, 'b': 2},
    {'b': 3, 'c': 4}
)

nested_update(
    {'x': {'a': 1, 'b': 2}, 'y': 5, 'z': 6},
    {'x': {'b': 3, 'c': 4}, 'z': 7, '0': 8},
)""" .

<DEPENDENCY.transformers==2.1.0> <CONTAINS> """CODE.import tensorflow as tf
from transformers import CTRLTokenizer, TFCTRLModel

tokenizer = CTRLTokenizer.from_pretrained('ctrl')
model = TFCTRLModel.from_pretrained('ctrl')
input_ids = tf.constant(tokenizer.encode("Hello, my dog is cute"))[None, :]  # Batch size 1
outputs = model(input_ids)
last_hidden_states = outputs[0]  # The last hidden-state is the first element of the output tuple""",
        """CODE.import torch
from transformers import CTRLTokenizer, CTRLLMHeadModel

tokenizer = CTRLTokenizer.from_pretrained('ctrl')
model = CTRLLMHeadModel.from_pretrained('ctrl')

input_ids = torch.tensor(tokenizer.encode("Links Hello, my dog is cute")).unsqueeze(0)  # Batch size 1
outputs = model(input_ids, labels=input_ids)
loss, logits = outputs[:2]""",
        """CODE.import torch
from transformers import CTRLTokenizer, TFCTRLLMHeadModel

tokenizer = CTRLTokenizer.from_pretrained('ctrl')
model = TFCTRLLMHeadModel.from_pretrained('ctrl')

input_ids = torch.tensor(tokenizer.encode("Links Hello, my dog is cute")).unsqueeze(0)  # Batch size 1
outputs = model(input_ids, labels=input_ids)
loss, logits = outputs[:2]""" .

<DEPENDENCY.transformers==2.11.0> <CONTAINS> """CODE.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
    model = BertModel.from_pretrained('bert-base-uncased')

    num_added_toks = tokenizer.add_tokens(['new_tok1', 'my_new-tok2'])
    print('We have added', num_added_toks, 'tokens')
    model.resize_token_embeddings(len(tokenizer))""" .

<DEPENDENCY.transformers==2.2.0> <CONTAINS> """CODE.import tensorflow as tf
from transformers import AlbertTokenizer, TFAlbertForMaskedLM

tokenizer = AlbertTokenizer.from_pretrained('albert-base-v2')
model = TFAlbertForMaskedLM.from_pretrained('albert-base-v2')
input_ids = tf.constant(tokenizer.encode("Hello, my dog is cute"))[None, :]  # Batch size 1
outputs = model(input_ids)
prediction_scores = outputs[0]""",
        """CODE.import tensorflow as tf
from transformers import AlbertTokenizer, TFAlbertForSequenceClassification

tokenizer = AlbertTokenizer.from_pretrained('albert-base-v2')
model = TFAlbertForSequenceClassification.from_pretrained('albert-base-v2')
input_ids = tf.constant(tokenizer.encode("Hello, my dog is cute"))[None, :]  # Batch size 1
outputs = model(input_ids)
logits = outputs[0]""",
        """CODE.import tensorflow as tf
from transformers import AlbertTokenizer, TFAlbertModel

tokenizer = AlbertTokenizer.from_pretrained('bert-base-uncased')
model = TFAlbertModel.from_pretrained('bert-base-uncased')
input_ids = tf.constant(tokenizer.encode("Hello, my dog is cute"))[None, :]  # Batch size 1
outputs = model(input_ids)
last_hidden_states = outputs[0]  # The last hidden-state is the first element of the output tuple""",
        """CODE.import tensorflow as tf
from transformers import RobertaTokenizer, TFRobertaForTokenClassification

tokenizer = RobertaTokenizer.from_pretrained('roberta-base')
model = TFRobertaForTokenClassification.from_pretrained('roberta-base')
input_ids = tf.constant(tokenizer.encode("Hello, my dog is cute", add_special_tokens=True))[None, :]  # Batch size 1
outputs = model(input_ids)
scores = outputs[0]""",
        """CODE.import tensorflow as tf
from transformers import XxxTokenizer, TFXxxForMaskedLM

tokenizer = XxxTokenizer.from_pretrained('xxx-base-uncased')
model = TFXxxForMaskedLM.from_pretrained('xxx-base-uncased')
input_ids = tf.constant(tokenizer.encode("Hello, my dog is cute"))[None, :]  # Batch size 1
outputs = model(input_ids)
prediction_scores = outputs[0]""",
        """CODE.import tensorflow as tf
from transformers import XxxTokenizer, TFXxxForQuestionAnswering

tokenizer = XxxTokenizer.from_pretrained('xxx-base-uncased')
model = TFXxxForQuestionAnswering.from_pretrained('xxx-base-uncased')
input_ids = tf.constant(tokenizer.encode("Hello, my dog is cute"))[None, :]  # Batch size 1
outputs = model(input_ids)
start_scores, end_scores = outputs[:2]""",
        """CODE.import tensorflow as tf
from transformers import XxxTokenizer, TFXxxForSequenceClassification

tokenizer = XxxTokenizer.from_pretrained('xxx-base-uncased')
model = TFXxxForSequenceClassification.from_pretrained('xxx-base-uncased')
input_ids = tf.constant(tokenizer.encode("Hello, my dog is cute"))[None, :]  # Batch size 1
outputs = model(input_ids)
logits = outputs[0]""",
        """CODE.import tensorflow as tf
from transformers import XxxTokenizer, TFXxxForTokenClassification

tokenizer = XxxTokenizer.from_pretrained('xxx-base-uncased')
model = TFXxxForTokenClassification.from_pretrained('xxx-base-uncased')
input_ids = tf.constant(tokenizer.encode("Hello, my dog is cute"))[None, :]  # Batch size 1
outputs = model(input_ids)
scores = outputs[0]""",
        """CODE.import tensorflow as tf
from transformers import XxxTokenizer, TFXxxModel

tokenizer = XxxTokenizer.from_pretrained('xxx-base-uncased')
model = TFXxxModel.from_pretrained('xxx-base-uncased')
input_ids = tf.constant(tokenizer.encode("Hello, my dog is cute"))[None, :]  # Batch size 1
outputs = model(input_ids)
last_hidden_states = outputs[0]  # The last hidden-state is the first element of the output tuple""",
        """CODE.tile(ex, 2, 0)
torch.Tensor([[1,2],[1,2],[3,4],[3,4]])""",
        """CODE.tokenizer = AlbertTokenizer.from_pretrained('albert-base-v2')
model = AlbertForQuestionAnswering.from_pretrained('albert-base-v2')
question, text = "Who was Jim Henson?", "Jim Henson was a nice puppet"
input_text = "[CLS] " + question + " [SEP] " + text + " [SEP]"
input_ids = tokenizer.encode(input_text)
token_type_ids = [0 if i <= input_ids.index(102) else 1 for i in range(len(input_ids))]
start_scores, end_scores = model(torch.tensor([input_ids]), token_type_ids=torch.tensor([token_type_ids]))
all_tokens = tokenizer.convert_ids_to_tokens(input_ids)
print(' '.join(all_tokens[torch.argmax(start_scores) : torch.argmax(end_scores)+1]))""",
        """CODE.tokenizer = AlbertTokenizer.from_pretrained('albert-base-v2')
model = AlbertForSequenceClassification.from_pretrained('albert-base-v2')
input_ids = torch.tensor(tokenizer.encode("Hello, my dog is cute")).unsqueeze(0)  # Batch size 1
labels = torch.tensor([1]).unsqueeze(0)  # Batch size 1
outputs = model(input_ids, labels=labels)
loss, logits = outputs[:2]
""",
        """CODE.tokenizer = CamembertTokenizer.from_pretrained('camembert-base')
model = CamembertForMaskedLM.from_pretrained('camembert-base')
input_ids = torch.tensor(tokenizer.encode("J'aime le camembert !")).unsqueeze(0)  # Batch size 1
outputs = model(input_ids, masked_lm_labels=input_ids)
loss, prediction_scores = outputs[:2]""",
        """CODE.tokenizer = CamembertTokenizer.from_pretrained('camembert-base')
model = CamembertForMultipleChoice.from_pretrained('camembert-base')
choices = ["J'aime le camembert !", "Je deteste le camembert !"]
input_ids = torch.tensor([tokenizer.encode(s, add_special_tokens=True) for s in choices]).unsqueeze(0)  # Batch size 1, 2 choices
labels = torch.tensor(1).unsqueeze(0)  # Batch size 1
outputs = model(input_ids, labels=labels)
loss, classification_scores = outputs[:2]""",
        """CODE.tokenizer = CamembertTokenizer.from_pretrained('camembert-base')
model = CamembertForTokenClassification.from_pretrained('camembert-base')
input_ids = torch.tensor(tokenizer.encode("J'aime le camembert !", add_special_tokens=True)).unsqueeze(0)  # Batch size 1
labels = torch.tensor([1] * input_ids.size(1)).unsqueeze(0)  # Batch size 1
outputs = model(input_ids, labels=labels)
loss, scores = outputs[:2]
""",
        """CODE.tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')
model = DistilBertForTokenClassification.from_pretrained('distilbert-base-uncased')
input_ids = torch.tensor(tokenizer.encode("Hello, my dog is cute")).unsqueeze(0)  # Batch size 1
labels = torch.tensor([1] * input_ids.size(1)).unsqueeze(0)  # Batch size 1
outputs = model(input_ids, labels=labels)
loss, scores = outputs[:2]
""",
        """CODE.tokenizer = RobertaTokenizer.from_pretrained('roberta-base')
model = RobertaForTokenClassification.from_pretrained('roberta-base')
input_ids = torch.tensor(tokenizer.encode("Hello, my dog is cute", add_special_tokens=True)).unsqueeze(0)  # Batch size 1
labels = torch.tensor([1] * input_ids.size(1)).unsqueeze(0)  # Batch size 1
outputs = model(input_ids, labels=labels)
loss, scores = outputs[:2]
""",
        """CODE.tokenizer = XxxTokenizer.from_pretrained('xxx-base-uncased')
model = XxxForMaskedLM.from_pretrained('xxx-base-uncased')
input_ids = torch.tensor(tokenizer.encode("Hello, my dog is cute")).unsqueeze(0)  # Batch size 1
outputs = model(input_ids, masked_lm_labels=input_ids)
loss, prediction_scores = outputs[:2]""",
        """CODE.tokenizer = XxxTokenizer.from_pretrained('xxx-base-uncased')
model = XxxForQuestionAnswering.from_pretrained('xxx-large-uncased-whole-word-masking-finetuned-squad')
question, text = "Who was Jim Henson?", "Jim Henson was a nice puppet"
input_text = "[CLS] " + question + " [SEP] " + text + " [SEP]"
input_ids = tokenizer.encode(input_text)
token_type_ids = [0 if i <= input_ids.index(102) else 1 for i in range(len(input_ids))]
start_scores, end_scores = model(torch.tensor([input_ids]), token_type_ids=torch.tensor([token_type_ids]))
all_tokens = tokenizer.convert_ids_to_tokens(input_ids)
print(' '.join(all_tokens[torch.argmax(start_scores) : torch.argmax(end_scores)+1]))""",
        """CODE.tokenizer = XxxTokenizer.from_pretrained('xxx-base-uncased')
model = XxxForSequenceClassification.from_pretrained('xxx-base-uncased')
input_ids = torch.tensor(tokenizer.encode("Hello, my dog is cute")).unsqueeze(0)  # Batch size 1
labels = torch.tensor([1]).unsqueeze(0)  # Batch size 1
outputs = model(input_ids, labels=labels)
loss, logits = outputs[:2]
""",
        """CODE.tokenizer = XxxTokenizer.from_pretrained('xxx-base-uncased')
model = XxxForTokenClassification.from_pretrained('xxx-base-uncased')
input_ids = torch.tensor(tokenizer.encode("Hello, my dog is cute")).unsqueeze(0)  # Batch size 1
labels = torch.tensor([1] * input_ids.size(1)).unsqueeze(0)  # Batch size 1
outputs = model(input_ids, labels=labels)
loss, scores = outputs[:2]""" .

<DEPENDENCY.transformers==2.2.2> <CONTAINS> """CODE.import tensorflow as tf
from transformers import DistilBertTokenizer, TFDistilBertForTokenClassification

tokenizer = DistilBertTokenizer.from_pretrained('bert-base-uncased')
model = TFDistilBertForTokenClassification.from_pretrained('bert-base-uncased')
input_ids = tf.constant(tokenizer.encode("Hello, my dog is cute"))[None, :]  # Batch size 1
outputs = model(input_ids)
scores = outputs[0]""",
        """CODE.import tensorflow as tf
from transformers import XLNetTokenizer, TFXLNetForTokenClassification

tokenizer = XLNetTokenizer.from_pretrained('xlnet-large-cased')
model = TFXLNetForSequenceClassification.from_pretrained('xlnet-large-cased')
input_ids = tf.constant(tokenizer.encode("Hello, my dog is cute"))[None, :]  # Batch size 1
outputs = model(input_ids)
scores = outputs[0]""" .

<DEPENDENCY.transformers==2.9.0> <CONTAINS> """CODE.def forward_chunk(self, hidden_states):
    hidden_states = self.decoder(hidden_states)
    return hidden_states

def forward(self, hidden_states):
    return apply_chunking_to_forward(self.chunk_size_lm_head, self.seq_len_dim, self.forward_chunk, hidden_states)""",
        """CODE.from transformers import BertConfig, EncoderDecoderConfig, EncoderDecoderModel

# Initializing a BERT bert-base-uncased style configuration
config_encoder = BertConfig()
config_decoder = BertConfig()

config = EncoderDecoderConfig.from_encoder_decoder_configs(config_encoder, config_decoder)

# Initializing a Bert2Bert model from the bert-base-uncased style configurations
model = EncoderDecoderModel(config=config)

# Accessing the model configuration
config_encoder = model.config.encoder
config_decoder  = model.config.decoder""",
        """CODE.tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')
model = BertModel.from_pretrained('bert-base-uncased')

num_added_toks = tokenizer.add_tokens(['new_tok1', 'my_new-tok2'])
print('We have added', num_added_toks, 'tokens')
model.resize_token_embeddings(len(tokenizer))  # Notice: resize_token_embeddings expect to receive the full size of the new vocabulary, i.e. the length of the tokenizer.""" .

<DEPENDENCY.transformers==2.9.1> <CONTAINS> """CODE.config = BertConfig.from_pretrained('bert-base-uncased')
model = AutoModelForMulitpleChoice.from_config(config)""",
        """CODE.from transformers import MarianTokenizer
tok = MarianTokenizer.from_pretrained('Helsinki-NLP/opus-mt-en-de')
src_texts = [ "I am a small frog.", "Tom asked his teacher for advice."]
tgt_texts = ["Ich bin ein kleiner Frosch.", "Tom bat seinen Lehrer um Rat."]  # optional
batch_enc: BatchEncoding = tok.prepare_translation_batch(src_texts, tgt_texts=tgt_texts)
# keys  [input_ids, attention_mask, decoder_input_ids,  decoder_attention_mask].
# model(**batch) should work
""" .

<DEPENDENCY.transformers==3.0.0> <CONTAINS> """CODE.from transformers import MBartTokenizer
tokenizer = MBartTokenizer.from_pretrained('facebook/mbart-large-en-ro')
example_english_phrase = " UN Chief Says There Is No Military Solution in Syria"
expected_translation_romanian = "Åeful ONU declarÄ cÄ nu existÄ o soluÅ£ie militarÄ Ã®n Siria"
batch: dict = tokenizer.prepare_translation_batch(
...     example_english_phrase, src_lang="en_XX", tgt_lang="ro_RO", tgt_texts=expected_translation_romanian
... )""" .

<DEPENDENCY.transformers==3.4.0> <CONTAINS> """CODE.from transformers import MBartTokenizerFast
tokenizer = MBartTokenizerFast.from_pretrained('facebook/mbart-large-en-ro')
example_english_phrase = " UN Chief Says There Is No Military Solution in Syria"
expected_translation_romanian = "Åeful ONU declarÄ cÄ nu existÄ o soluÅ£ie militarÄ Ã®n Siria"
batch: dict = tokenizer.prepare_seq2seq_batch(
    example_english_phrase, src_lang="en_XX", tgt_lang="ro_RO", tgt_texts=expected_translation_romanian
)
""",
        """CODE.from transformers import MarianTokenizer, MarianMTModel
from typing import List
src = 'fr'  # source language
trg = 'en'  # target language
sample_text = "oÃ¹ est l'arrÃªt de bus ?"
mname = f'Helsinki-NLP/opus-mt-{src}-{trg}'

model = MarianMTModel.from_pretrained(mname)
tok = MarianTokenizer.from_pretrained(mname)
batch = tok.prepare_seq2seq_batch(src_texts=[sample_text])  # don't need tgt_text for inference
gen = model.generate(**batch)  # for forward pass: model(**batch)
words: List[str] = tok.batch_decode(gen, skip_special_tokens=True)  # returns "Where is the bus stop ?\"""",
        """CODE.from transformers import PegasusTokenizer, PegasusForConditionalGeneration
from typing import List
PGE_ARTICLE = "PG&E stated it scheduled the blackouts in response to forecasts for high winds amid dry conditions. The aim is to reduce the risk of wildfires. Nearly 800 thousand customers were scheduled to be affected by the shutoffs which were expected to last through at least midday tomorrow."
mname = "google/pegasus-xsum"

model = PegasusForConditionalGeneration.from_pretrained(mname)
tok = PegasusTokenizer.from_pretrained(mname)
batch = tok.prepare_seq2seq_batch(src_texts=[PGE_ARTICLE])  # don't need tgt_text for inference
gen = model.generate(**batch)  # for forward pass: model(**batch)
summary: List[str] = tok.batch_decode(gen, skip_special_tokens=True)
assert summary == "California's largest electricity provider has turned off power to tens of thousands of customers.\"""",
        """CODE.from transformers import XLMProphetNetTokenizer, XLMProphetNetDecoder
import torch

tokenizer = XLMProphetNetTokenizer.from_pretrained('microsoft/xprophetnet-large-wiki100-cased')
model = XLMProphetNetDecoder.from_pretrained('patrickvonplaten/xprophetnet-large-uncased-standalone', add_cross_attention=False, return_dict=True)
assert model.config.is_decoder, f"{model.__class__} has to be configured as a decoder."
inputs = tokenizer("Hello, my dog is cute", return_tensors="pt")
outputs = model(**inputs)

last_hidden_states = outputs.last_hidden_state
""",
        """CODE.from transformers import XLMProphetNetTokenizer, XLMProphetNetEncoder
import torch

tokenizer = XLMProphetNetTokenizer.from_pretrained('microsoft/xprophetnet-large-wiki100-cased')
model = XLMProphetNetEncoder.from_pretrained('patrickvonplaten/xprophetnet-large-uncased-standalone', return_dict=True)
assert model.config.is_decoder, f"{model.__class__} has to be configured as a decoder."
inputs = tokenizer("Hello, my dog is cute", return_tensors="pt")
outputs = model(**inputs)

last_hidden_states = outputs.last_hidden_state""",
        """CODE.from transformers import XLMProphetNetTokenizer, XLMProphetNetForCausalLM
import torch

tokenizer = XLMProphetNetTokenizer.from_pretrained('microsoft/xprophetnet-large-wiki100-cased')
model = XLMProphetNetForCausalLM.from_pretrained('patrickvonplaten/xprophetnet-decoder-clm-large-uncased', return_dict=True)
assert model.config.is_decoder, f"{model.__class__} has to be configured as a decoder."
inputs = tokenizer("Hello, my dog is cute", return_tensors="pt")
outputs = model(**inputs)

logits = outputs.logits

from transformers import BertTokenizer, EncoderDecoderModel
import torch

tokenizer = BertTokenizer.from_pretrained('bert-uncased-large')
model = EncoderDecoderModel.from_encoder_decoder_pretrained("bert-uncased-large", "patrickvonplaten/xprophetnet-decoder-clm-large-uncased")

inputs = tokenizer("Hello, my dog is cute", return_tensors="pt")
outputs = model(input_ids=inputs["input_ids"], labels=inputs["input_ids"])

loss = outputs.loss""",
        """CODE.from transformers import XLMProphetNetTokenizer, XLMProphetNetForConditionalGeneration
tokenizer = XLMProphetNetTokenizer.from_pretrained('microsoft/xprophetnet-large-wiki100-cased')
model =  XLMProphetNetForConditionalGeneration.from_pretrained('microsoft/xprophetnet-large-wiki100-cased')
input_ids = tokenizer("Studies have been shown that owning a dog is good for you", return_tensors="pt").input_ids
decoder_input_ids = tokenizer("Studies show that", return_tensors="pt").input_ids
outputs = model(input_ids=input_ids, decoder_input_ids=decoder_input_ids, return_dict=True)
logits_next_token = outputs.logits
logits_ngram_next_tokens = outputs.logits_ngram""",
        """CODE.from transformers import XLMProphetNetTokenizer, XLMProphetNetModel
tokenizer = XLMProphetNetTokenizer.from_pretrained('microsoft/xprophetnet-large-wiki100-cased')
model = XLMProphetNetModel.from_pretrained('microsoft/xprophetnet-large-wiki100-cased')
input_ids = tokenizer("Studies have been shown that owning a dog is good for you", return_tensors="pt").input_ids
decoder_input_ids = tokenizer("Studies show that", return_tensors="pt").input_ids
outputs = model(input_ids=input_ids, decoder_input_ids=decoder_input_ids, return_dict=True)
last_hidden_states = outputs.last_hidden_state
last_hidden_states_ngram = outputs.last_hidden_state_ngram""",
        """CODE.import time

pbar = NotebookProgressBar(100)
for val in range(100):
    pbar.update(val)
    time.sleep(0.07)
pbar.update(100)""",
        """CODE.import torch
from transformers.modeling_deroberta import XSoftmax

# Make a tensor
x = torch.randn([4,20,100])

# Create a mask
mask = (x>0).int()

y = XSoftmax.apply(x, mask, dim=-1)""",
        """CODE.tokenizer = GPT2Tokenizer()
text = "Hello world!"
tokens = tokenizer.tokenize(text)
print(tokens)""",
        """CODE.tokenizer = GPT2Tokenizer()
text = "Hello world!"
tokens = tokenizer.tokenize(text)
print(tokens)
tokenizer.decode(tokens)""" .

<DEPENDENCY.transformers==3.5.0> <CONTAINS> """CODE.model = AutoModelForNextSentencePrediction.from_pretrained('bert-base-uncased')
model = AutoModelForNextSentencePrediction.from_pretrained('bert-base-uncased', output_attentions=True)
config = AutoConfig.from_json_file('./tf_model/bert_tf_model_config.json')
model = AutoModelForNextSentencePrediction.from_pretrained('./tf_model/bert_tf_checkpoint.ckpt.index', from_tf=True, config=config)""" .

<DEPENDENCY.transformers==4.0.0> <CONTAINS> """CODE.from transformers import MT5ForConditionalGeneration, T5Tokenizer
model = MT5ForConditionalGeneration.from_pretrained("google/mt5-small")
tokenizer = T5Tokenizer.from_pretrained("google/mt5-small")
article = "UN Offizier sagt, dass weiter verhandelt werden muss in Syrien."
summary = "Weiter Verhandlung in Syrien."
batch = tokenizer.prepare_seq2seq_batch(src_texts=[article], tgt_texts=[summary], return_tensors="pt")
outputs = model(**batch)
loss = outputs.loss""",
        """CODE.from transformers import MT5Model, T5Tokenizer
model = MT5Model.from_pretrained("google/mt5-small")
tokenizer = T5Tokenizer.from_pretrained("google/mt5-small")
article = "UN Offizier sagt, dass weiter verhandelt werden muss in Syrien."
summary = "Weiter Verhandlung in Syrien."
batch = tokenizer.prepare_seq2seq_batch(src_texts=[article], tgt_texts=[summary], return_tensors="pt")
outputs = model(input_ids=batch.input_ids, decoder_input_ids=batch.labels)
hidden_states = outputs.last_hidden_state""",
        """CODE.from transformers import TFMT5ForConditionalGeneration, T5Tokenizer
model = TFMT5ForConditionalGeneration.from_pretrained("google/mt5-small")
tokenizer = T5Tokenizer.from_pretrained("google/mt5-small")
article = "UN Offizier sagt, dass weiter verhandelt werden muss in Syrien."
summary = "Weiter Verhandlung in Syrien."
batch = tokenizer.prepare_seq2seq_batch(src_texts=[article], tgt_texts=[summary], return_tensors="tf")
outputs = model(batch)
loss = outputs.loss""",
        """CODE.from transformers import TFMT5Model, T5Tokenizer
model = TFMT5Model.from_pretrained("google/mt5-small")
tokenizer = T5Tokenizer.from_pretrained("google/mt5-small")
article = "UN Offizier sagt, dass weiter verhandelt werden muss in Syrien."
summary = "Weiter Verhandlung in Syrien."
batch = tokenizer.prepare_seq2seq_batch(src_texts=[article], tgt_texts=[summary], return_tensors="tf")
batch["decoder_input_ids"] = batch["labels"]
del batch["labels"]
outputs = model(batch)
hidden_states = outputs.last_hidden_state""",
        """CODE.model = TFAutoModelForNextSentencePrediction.from_pretrained('bert-base-uncased')
model = TFAutoModelForNextSentencePrediction.from_pretrained('bert-base-uncased', output_attentions=True)
config = AutoConfig.from_json_file('./pt_model/bert_pt_model_config.json')
model = TFAutoModelForNextSentencePrediction.from_pretrained('./pt_model/bert_pytorch_model.bin', from_pt=True, config=config)""" .

<DEPENDENCY.transformers==4.1.0> <CONTAINS> """CODE.from transformers import (
...    AutoTokenizer,
...    AutoModelForSeq2SeqLM,
...    LogitsProcessorList,
...    MinLengthLogitsProcessor,
...    HammingDiversityLogitsProcessor,
...    BeamSearchScorer,
... )
import torch

tokenizer = AutoTokenizer.from_pretrained("t5-base")
model = AutoModelForSeq2SeqLM.from_pretrained("t5-base")

encoder_input_str = "translate English to German: How old are you?"
encoder_input_ids = tokenizer(encoder_input_str, return_tensors="pt").input_ids


# lets run diverse beam search using 6 beams
num_beams = 6
# define decoder start token ids
input_ids = torch.ones((num_beams, 1), device=model.device, dtype=torch.long)
input_ids = input_ids * model.config.decoder_start_token_id

# add encoder_outputs to model keyword arguments
model_kwargs = {
...     "encoder_outputs": model.get_encoder()(encoder_input_ids.repeat_interleave(num_beams, dim=0), return_dict=True)
... }

# instantiate beam scorer
beam_scorer = BeamSearchScorer(
...     batch_size=1,
...     max_length=model.config.max_length,
...     num_beams=num_beams,
...     device=model.device,
...     num_beam_groups=3
... )

# instantiate logits processors
logits_processor = LogitsProcessorList([
...     HammingDiversityLogitsProcessor(5.5, num_beams=6, num_beam_groups=3),
...     MinLengthLogitsProcessor(5, eos_token_id=model.config.eos_token_id),
... ])

outputs = model.group_beam_search(input_ids, beam_scorer, logits_processor=logits_processor, **model_kwargs)

print("Generated:", tokenizer.batch_decode(outputs, skip_special_tokens=True))""",
        """CODE.from transformers import MT5EncoderModel, T5Tokenizer
model = MT5EncoderModel.from_pretrained("google/mt5-small")
tokenizer = T5Tokenizer.from_pretrained("google/mt5-small")
article = "UN Offizier sagt, dass weiter verhandelt werden muss in Syrien."
input_ids = tokenizer(article, return_tensors="pt").input_ids
outputs = model(input_ids)
hidden_state = outputs.last_hidden_state""",
        """CODE.from transformers import TFMT5EncoderModel, T5Tokenizer
model = TFMT5EncoderModel.from_pretrained("google/mt5-small")
tokenizer = T5Tokenizer.from_pretrained("google/mt5-small")
article = "UN Offizier sagt, dass weiter verhandelt werden muss in Syrien."
input_ids = tokenizer(article, return_tensors="tf").input_ids
outputs = model(input_ids)
hidden_state = outputs.last_hidden_state""",
        """CODE.model = AutoModelForTableQuestionAnswering.from_pretrained('google/tapas-base-finetuned-wtq')
model = AutoModelForTableQuestionAnswering.from_pretrained('google/tapas-base-finetuned-wtq', output_attentions=True)
config = AutoConfig.from_json_file('./tf_model/tapas_tf_checkpoint.json')
model = AutoModelForQuestionAnswering.from_pretrained('./tf_model/tapas_tf_checkpoint.ckpt.index', from_tf=True, config=config)""" .

<DEPENDENCY.transformers==4.10.0> <CONTAINS> """CODE.from transformers import FlaxEncoderDecoderModel
model = FlaxEncoderDecoderModel.from_encoder_decoder_pretrained('bert-base-cased', 'gpt2')
model.save_pretrained("./bert2gpt2")
model = FlaxEncoderDecoderModel.from_pretrained("./bert2gpt2")
""",
        """CODE.from transformers import FlaxEncoderDecoderModel, BertTokenizer
import jax.numpy as jnp

model = FlaxEncoderDecoderModel.from_encoder_decoder_pretrained('bert-base-cased', 'gpt2')

tokenizer = BertTokenizer.from_pretrained('bert-base-cased')

text = "My friends are cool but they eat too many carbs."
input_ids = tokenizer.encode(text, max_length=1024, return_tensors='np')
encoder_outputs = model.encode(input_ids)

decoder_start_token_id = model.config.decoder.bos_token_id
decoder_input_ids = jnp.ones((input_ids.shape[0], 1), dtype="i4") * decoder_start_token_id

outputs = model.decode(decoder_input_ids, encoder_outputs)
logits = outputs.logits""",
        """CODE.from transformers import FlaxMT5ForConditionalGeneration, T5Tokenizer
model = FlaxMT5ForConditionalGeneration.from_pretrained("google/mt5-small")
tokenizer = T5Tokenizer.from_pretrained("google/mt5-small")
article = "UN Offizier sagt, dass weiter verhandelt werden muss in Syrien."
summary = "Weiter Verhandlung in Syrien."
inputs = tokenizer(article, return_tensors="np")
with tokenizer.as_target_tokenizer():
...     decoder_input_ids = tokenizer(summary, return_tensors="np").input_ids
outputs = model(**inputs, decoder_input_ids=decoder_input_ids)
logits = outputs.logits""",
        """CODE.from transformers import FlaxMT5Model, T5Tokenizer
model = FlaxMT5Model.from_pretrained("google/mt5-small")
tokenizer = T5Tokenizer.from_pretrained("google/mt5-small")
article = "UN Offizier sagt, dass weiter verhandelt werden muss in Syrien."
summary = "Weiter Verhandlung in Syrien."
inputs = tokenizer(article, return_tensors="np")
with tokenizer.as_target_tokenizer():
...     decoder_input_ids = tokenizer(summary, return_tensors="np").input_ids
outputs = model(input_ids=inputs["input_ids"], decoder_input_ids=decoder_input_ids)
hidden_states = outputs.last_hidden_state""",
        """CODE.model = FlaxEncoderDecoderModel.from_encoder_decoder_pretrained('bert-base-cased', 'gpt2')
tokenizer = BertTokenizer.from_pretrained('bert-base-cased')
text = "My friends are cool but they eat too many carbs."
input_ids = tokenizer.encode(text, return_tensors='np')
encoder_outputs = model.encode(input_ids)""" .

<DEPENDENCY.transformers==4.11.0> <CONTAINS> """CODE.:class:`~transformers.SpeechEncoderDecoderConfig` is the configuration class to store the configuration of a
:class:`~transformers.SpeechEncoderDecoderModel`. It is used to instantiate an Encoder Decoder model according to
the specified arguments, defining the encoder and decoder configs.

Configuration objects inherit from :class:`~transformers.PretrainedConfig` and can be used to control the model
outputs. Read the documentation from :class:`~transformers.PretrainedConfig` for more information.

Args:
    kwargs (`optional`):
        Dictionary of keyword arguments. Notably:

            - **encoder** (:class:`~transformers.PretrainedConfig`, `optional`) -- An instance of a configuration
              object that defines the encoder config.
            - **decoder** (:class:`~transformers.PretrainedConfig`, `optional`) -- An instance of a configuration
              object that defines the decoder config.

Examples::

    from transformers import BertConfig, Wav2Vec2Config, SpeechEncoderDecoderConfig, SpeechEncoderDecoderModel

    # Initializing a Wav2Vec2 & BERT style configuration
    config_encoder = Wav2Vec2Config()
    config_decoder = BertConfig()

    config = SpeechEncoderDecoderConfig.from_encoder_decoder_configs(config_encoder, config_decoder)

    # Initializing a Wav2Vec2Bert model from a Wav2Vec2 & bert-base-uncased style configurations
    model = SpeechEncoderDecoderModel(config=config)

    # Accessing the model configuration
    config_encoder = model.config.encoder
    config_decoder  = model.config.decoder
    # set decoder config to causal lm
    config_decoder.is_decoder = True
    config_decoder.add_cross_attention = True

    # Saving the model, including its configuration
    model.save_pretrained('my-model')

    # loading model and config from pretrained folder
    encoder_decoder_config = SpeechEncoderDecoderConfig.from_pretrained('my-model')
    model = SpeechEncoderDecoderModel.from_pretrained('my-model', config=encoder_decoder_config)""",
        """CODE.model = FlaxPegasusForConditionalGeneration.from_pretrained('google/pegasus-large')
tokenizer = PegasusTokenizer.from_pretrained('google/pegasus-large')
inputs = tokenizer(text, max_length=1024, return_tensors='np')
encoder_outputs = model.encode(**inputs)""",
        """CODE.model = FlaxPegasusForConditionalGeneration.from_pretrained('google/pegasus-large')
tokenizer = PegasusTokenizer.from_pretrained('google/pegasus-large')
text = "My friends are cool but they eat too many carbs."
inputs = tokenizer(text, max_length=1024, return_tensors='np')
encoder_outputs = model.encode(**inputs)
decoder_start_token_id = model.config.decoder_start_token_id
decoder_input_ids = jnp.ones((inputs.input_ids.shape[0], 1), dtype="i4") * decoder_start_token_id
outputs = model.decode(decoder_input_ids, encoder_outputs)
last_decoder_hidden_states = outputs.last_hidden_state""",
        """CODE.model = FlaxPegasusForConditionalGeneration.from_pretrained('google/pegasus-large')
tokenizer = PegasusTokenizer.from_pretrained('google/pegasus-large')
text = "My friends are cool but they eat too many carbs."
inputs = tokenizer(text, max_length=1024, return_tensors='np')
encoder_outputs = model.encode(**inputs)
decoder_start_token_id = model.config.decoder_start_token_id
decoder_input_ids = jnp.ones((inputs.input_ids.shape[0], 1), dtype="i4") * decoder_start_token_id
outputs = model.decode(decoder_input_ids, encoder_outputs)
logits = outputs.logits""",
        """CODE.trie = Trie()
trie.add("Hello åé")
trie.data
{"H": {"e": {"l": {"l": {"o": {" ": {"å": {"é": {"": 1}}}}}}}}

trie.add("Hello")
trie.data
{"H": {"e": {"l": {"l": {"o": {"": 1, " ": {"å": {"é": {"": 1}}}}}}}}
""",
        """CODE.trie = Trie()
trie.split("[CLS] This is a extra_id_100")
["[CLS] This is a extra_id_100"]
trie.add("[CLS]")
trie.add("extra_id_1")
trie.add("extra_id_100")
trie.split("[CLS] This is a extra_id_100")
["[CLS]", " This is a ", "extra_id_100"]
""" .

<DEPENDENCY.transformers==4.12.0> <CONTAINS> """CODE.:class:`~transformers.VisionEncoderDecoderConfig` is the configuration class to store the configuration of a
:class:`~transformers.VisionEncoderDecoderModel`. It is used to instantiate an Encoder Decoder model according to
the specified arguments, defining the encoder and decoder configs.

Configuration objects inherit from :class:`~transformers.PretrainedConfig` and can be used to control the model
outputs. Read the documentation from :class:`~transformers.PretrainedConfig` for more information.

Args:
    kwargs (`optional`):
        Dictionary of keyword arguments. Notably:

            - **encoder** (:class:`~transformers.PretrainedConfig`, `optional`) -- An instance of a configuration
              object that defines the encoder config.
            - **decoder** (:class:`~transformers.PretrainedConfig`, `optional`) -- An instance of a configuration
              object that defines the decoder config.

Examples::

    from transformers import BertConfig, ViTConfig, VisionEncoderDecoderConfig, VisionEncoderDecoderModel

    # Initializing a ViT & BERT style configuration
    config_encoder = ViTConfig()
    config_decoder = BertConfig()

    config = VisionEncoderDecoderConfig.from_encoder_decoder_configs(config_encoder, config_decoder)

    # Initializing a ViTBert model from a ViT & bert-base-uncased style configurations
    model = VisionEncoderDecoderModel(config=config)

    # Accessing the model configuration
    config_encoder = model.config.encoder
    config_decoder  = model.config.decoder
    # set decoder config to causal lm
    config_decoder.is_decoder = True
    config_decoder.add_cross_attention = True

    # Saving the model, including its configuration
    model.save_pretrained('my-model')

    # loading model and config from pretrained folder
    encoder_decoder_config = VisionEncoderDecoderConfig.from_pretrained('my-model')
    model = VisionEncoderDecoderModel.from_pretrained('my-model', config=encoder_decoder_config)""",
        """CODE._model = EncoderDecoderModel.from_pretrained("patrickvonplaten/bert2bert-cnn_dailymail-fp16")
_model.encoder.save_pretrained("./encoder")
_model.decoder.save_pretrained("./decoder")
model = TFEncoderDecoderModel.from_encoder_decoder_pretrained(
...     "./encoder", "./decoder", encoder_from_pt=True, decoder_from_pt=True
... )
model.config = _model.config""" .

<DEPENDENCY.transformers==4.13.0> <CONTAINS> """CODE.from PIL import Image
import requests
from transformers import VisionTextDualEncoderModel, AutoFeatureExtractor

model = VisionTextDualEncoderModel.from_pretrained("clip-italian/clip-italian")
feature_extractor = AutoFeatureExtractor.from_pretrained("google/vit-base-patch16-224")

url = "http://images.cocodataset.org/val2017/000000039769.jpg"
image = Image.open(requests.get(url, stream=True).raw)

inputs = feature_extractor(images=image, return_tensors="pt")

image_features = model.get_image_features(**inputs)""",
        """CODE.from transformers import BlenderbotTokenizer, FlaxBlenderbotForConditionalGeneration
model = FlaxBlenderbotForConditionalGeneration.from_pretrained('facebook/blenderbot-400M-distill')
tokenizer = BlenderbotTokenizer.from_pretrained('facebook/blenderbot-400M-distill')
text = "My friends are cool but they eat too many carbs."
inputs = tokenizer(text, max_length=1024, return_tensors='jax')
encoder_outputs = model.encode(**inputs)""",
        """CODE.from transformers import FlaxVisionEncoderDecoderModel
from PIL import Image
import requests

url = 'http://images.cocodataset.org/val2017/000000039769.jpg'
image = Image.open(requests.get(url, stream=True).raw)

feature_extractor = ViTFeatureExtractor.from_pretrained('google/vit-base-patch16-224-in21k')

# initialize a vit-gpt2 from pretrained ViT and GPT2 models. Note that the cross-attention layers will be randomly initialized
model = FlaxVisionEncoderDecoderModel.from_encoder_decoder_pretrained('vit', 'gpt2')

pixel_values = feature_extractor(images=image, return_tensors="np").pixel_values
encoder_outputs = model.encode(pixel_values)
""",
        """CODE.from transformers import FlaxVisionEncoderDecoderModel
import jax.numpy as jnp
from PIL import Image
import requests

url = 'http://images.cocodataset.org/val2017/000000039769.jpg'
image = Image.open(requests.get(url, stream=True).raw)

feature_extractor = ViTFeatureExtractor.from_pretrained('google/vit-base-patch16-224-in21k')

# initialize a vit-gpt2 from pretrained ViT and GPT2 models. Note that the cross-attention layers will be randomly initialized
model = FlaxVisionEncoderDecoderModel.from_encoder_decoder_pretrained('vit', 'gpt2')

pixel_values = feature_extractor(images=image, return_tensors="np").pixel_values
encoder_outputs = model.encode(pixel_values)

decoder_start_token_id = model.config.decoder.bos_token_id
decoder_input_ids = jnp.ones((pixel_values.shape[0], 1), dtype="i4") * decoder_start_token_id

outputs = model.decode(decoder_input_ids, encoder_outputs)
logits = outputs.logits""",
        """CODE.from transformers import FlaxVisionEncoderDecoderModel
model = FlaxVisionEncoderDecoderModel.from_encoder_decoder_pretrained('google/vit-base-patch16-224-in21k', 'gpt2')
model.save_pretrained("./vit-gpt2")
model = FlaxVisionEncoderDecoderModel.from_pretrained("./vit-gpt2")
""",
        """CODE.from transformers import VisionTextDualEncoderModel, AutoTokenizer

model = VisionTextDualEncoderModel.from_pretrained("clip-italian/clip-italian")
tokenizer = AutoTokenizer.from_pretrained("clip-italian/clip-italian")

inputs = tokenizer(["una foto di un gatto", "una foto di un cane"],  padding=True, return_tensors="pt")
text_features = model.get_text_features(**inputs)""",
        """CODE.model = FlaxBlenderbotForConditionalGeneration.from_pretrained('facebook/blenderbot-400M-distill')
tokenizer = BlenderbotTokenizer.from_pretrained('facebook/blenderbot-400M-distill')
inputs = tokenizer(text, max_length=1024, return_tensors='jax')
encoder_outputs = model.encode(**inputs)
decoder_start_token_id = model.config.decoder_start_token_id
decoder_input_ids = jnp.ones((inputs.input_ids.shape[0], 1), dtype="i4") * decoder_start_token_id
outputs = model.decode(decoder_input_ids, encoder_outputs)
last_decoder_hidden_states = outputs.last_hidden_state""",
        """CODE.model = FlaxBlenderbotForConditionalGeneration.from_pretrained('facebook/blenderbot-400M-distill')
tokenizer = BlenderbotTokenizer.from_pretrained('facebook/blenderbot-400M-distill')
text = "My friends are cool but they eat too many carbs."
inputs = tokenizer(text, max_length=1024, return_tensors='jax')
encoder_outputs = model.encode(**inputs)
decoder_start_token_id = model.config.decoder_start_token_id
decoder_input_ids = jnp.ones((inputs.input_ids.shape[0], 1), dtype="i4") * decoder_start_token_id
outputs = model.decode(decoder_input_ids, encoder_outputs)
logits = outputs.logits""",
        """CODE.model = FlaxBlenderbotSmallForConditionalGeneration.from_pretrained('facebook/blenderbot_small-90M')
tokenizer = BlenderbotSmallTokenizer.from_pretrained('facebook/blenderbot_small-90M')
text = "My friends are cool but they eat too many carbs."
inputs = tokenizer(text, max_length=1024, return_tensors='np')
encoder_outputs = model.encode(**inputs)""",
        """CODE.model = FlaxBlenderbotSmallForConditionalGeneration.from_pretrained('facebook/blenderbot_small-90M')
tokenizer = BlenderbotSmallTokenizer.from_pretrained('facebook/blenderbot_small-90M')
text = "My friends are cool but they eat too many carbs."
inputs = tokenizer(text, max_length=1024, return_tensors='np')
encoder_outputs = model.encode(**inputs)
decoder_start_token_id = model.config.decoder_start_token_id
decoder_input_ids = jnp.ones((inputs.input_ids.shape[0], 1), dtype="i4") * decoder_start_token_id
outputs = model.decode(decoder_input_ids, encoder_outputs)
last_decoder_hidden_states = outputs.last_hidden_state""",
        """CODE.model = FlaxBlenderbotSmallForConditionalGeneration.from_pretrained('facebook/blenderbot_small-90M')
tokenizer = BlenderbotSmallTokenizer.from_pretrained('facebook/blenderbot_small-90M')
text = "My friends are cool but they eat too many carbs."
inputs = tokenizer(text, max_length=1024, return_tensors='np')
encoder_outputs = model.encode(**inputs)
decoder_start_token_id = model.config.decoder_start_token_id
decoder_input_ids = jnp.ones((inputs.input_ids.shape[0], 1), dtype="i4") * decoder_start_token_id
outputs = model.decode(decoder_input_ids, encoder_outputs)
logits = outputs.logits""",
        """CODE.model.params = model.to_bf16(model.params)
flat_params = traverse_util.flatten_dict(model.params)
mask = {path: (path[-2] != ("LayerNorm", "bias") and path[-2:] != ("LayerNorm", "scale")) for path in flat_params}
mask = traverse_util.unflatten_dict(mask)
model.params = model.to_bf16(model.params, mask)
""",
        """CODE.model.params = model.to_f16(model.params)
model.params = model.to_fp32(model.params)
""" .

<DEPENDENCY.transformers==4.16.0> <CONTAINS> """CODE.# Download a tokenizer configuration from huggingface.co and cache.
tokenizer_config = get_file_from_repo("bert-base-uncased", "tokenizer_config.json")
# This model does not have a tokenizer config so the result will be None.
tokenizer_config = get_file_from_repo("xlm-roberta-base", "tokenizer_config.json")
""",
        """CODE.# a workaround to load from pytorch checkpoint
_model = VisionEncoderDecoderModel.from_pretrained("ydshieh/vit-gpt2-coco-en")
_model.encoder.save_pretrained("./encoder")
_model.decoder.save_pretrained("./decoder")
model = TFVisionEncoderDecoderModel.from_encoder_decoder_pretrained(
...     "./encoder", "./decoder", encoder_from_pt=True, decoder_from_pt=True
... )
# This is only for copying some specific attributes of this particular model.
model.config = _model.config

from transformers import TFVisionEncoderDecoderModel, ViTFeatureExtractor, GPT2Tokenizer
from PIL import Image
import requests

feature_extractor = ViTFeatureExtractor.from_pretrained("ydshieh/vit-gpt2-coco-en")
decoder_tokenizer = GPT2Tokenizer.from_pretrained("ydshieh/vit-gpt2-coco-en")
model = TFVisionEncoderDecoderModel.from_pretrained("ydshieh/vit-gpt2-coco-en")

url = "http://images.cocodataset.org/val2017/000000039769.jpg"
img = Image.open(requests.get(url, stream=True).raw)
pixel_values = feature_extractor(images=img, return_tensors="tf").pixel_values  # Batch size 1

output_ids = model.generate(
...     pixel_values, max_length=16, num_beams=4, return_dict_in_generate=True
).sequences

preds = decoder_tokenizer.batch_decode(output_ids, skip_special_tokens=True)
preds = [pred.strip() for pred in preds]

assert preds == ["a cat laying on top of a couch next to another cat"]
""",
        """CODE.from PIL import Image
import requests
from transformers import CLIPProcessor, TFCLIPModel

model = TFCLIPModel.from_pretrained("openai/clip-vit-base-patch32")
processor = CLIPProcessor.from_pretrained("openai/clip-vit-base-patch32")

url = "http://images.cocodataset.org/val2017/000000039769.jpg"
image = Image.open(requests.get(url, stream=True).raw)

inputs = processor(images=image, return_tensors="tf")

image_features = model.get_image_features(**inputs)
""",
        """CODE.from transformers import CLIPTokenizer, TFCLIPModel

model = TFCLIPModel.from_pretrained("openai/clip-vit-base-patch32")
tokenizer = CLIPTokenizer.from_pretrained("openai/clip-vit-base-patch32")

inputs = tokenizer(["a photo of a cat", "a photo of a dog"], padding=True, return_tensors="tf")
text_features = model.get_text_features(**inputs)
""",
        """CODE.from transformers import NystromformerModel, NystromformerConfig

# Initializing a Nystromformer uw-madison/nystromformer-512 style configuration
configuration = NystromformerConfig()

# Initializing a model from the uw-madison/nystromformer-512 style configuration
model = NystromformerModel(configuration)

# Accessing the model configuration
configuration = model.config
""",
        """CODE.from transformers import RealmEmbedder, RealmConfig

# Initializing a REALM realm-cc-news-pretrained-* style configuration
configuration = RealmConfig()

# Initializing a model from the qqaatw/realm-cc-news-pretrained-embedder style configuration
model = RealmEmbedder(configuration)

# Accessing the model configuration
configuration = model.config
""",
        """CODE.from transformers import RealmTokenizer

# batch_size = 2, num_candidates = 2
text = [["Hello world!", "Nice to meet you!"], ["The cute cat.", "The adorable dog."]]

tokenizer = RealmTokenizer.from_pretrained("qqaatw/realm-cc-news-pretrained-encoder")
tokenized_text = tokenizer.batch_encode_candidates(text, max_length=10, return_tensors="pt")
""",
        """CODE.from transformers import RealmTokenizerFast

# batch_size = 2, num_candidates = 2
text = [["Hello world!", "Nice to meet you!"], ["The cute cat.", "The adorable dog."]]

tokenizer = RealmTokenizerFast.from_pretrained("qqaatw/realm-cc-news-pretrained-encoder")
tokenized_text = tokenizer.batch_encode_candidates(text, max_length=10, return_tensors="pt")
""",
        """CODE.from transformers import SwinModel, SwinConfig

# Initializing a Swin microsoft/swin-tiny-patch4-window7-224 style configuration
configuration = SwinConfig()

# Initializing a model from the microsoft/swin-tiny-patch4-window7-224 style configuration
model = SwinModel(configuration)

# Accessing the model configuration
configuration = model.config
""",
        """CODE.from transformers import TFVisionEncoderDecoderModel

model = TFVisionEncoderDecoderModel.from_encoder_decoder_pretrained(
    "google/vit-base-patch16-224-in21k", "bert-base-uncased"
)
model.save_pretrained("./vit-bert")
model = TFVisionEncoderDecoderModel.from_pretrained("./vit-bert")
""",
        """CODE.from transformers import ViTMAEModel, ViTMAEConfig

# Initializing a ViT MAE vit-mae-base style configuration
configuration = ViTMAEConfig()

# Initializing a model from the vit-mae-base style configuration
model = ViTMAEModel(configuration)

# Accessing the model configuration
configuration = model.config
""",
        """CODE.from transformers import YosoModel, YosoConfig

# Initializing a YOSO uw-madison/yoso-4096 style configuration
configuration = YosoConfig()

# Initializing a model from the uw-madison/yoso-4096 style configuration
model = YosoModel(configuration)

# Accessing the model configuration
configuration = model.config
""" .

<DEPENDENCY.transformers==4.17.0> <CONTAINS> """CODE.# Let's see how to retrieve time steps for a model
from transformers import AutoTokenizer, AutoFeatureExtractor, AutoModelForCTC
from datasets import load_dataset
import datasets
import torch

# import model, feature extractor, tokenizer
model = AutoModelForCTC.from_pretrained("facebook/wav2vec2-base-960h")
tokenizer = AutoTokenizer.from_pretrained("facebook/wav2vec2-base-960h")
feature_extractor = AutoFeatureExtractor.from_pretrained("facebook/wav2vec2-base-960h")

# load first sample of English common_voice
dataset = load_dataset("common_voice", "en", split="train", streaming=True)
dataset = dataset.cast_column("audio", datasets.Audio(sampling_rate=16_000))
dataset_iter = iter(dataset)
sample = next(dataset_iter)

# forward sample through model to get greedily predicted transcription ids
input_values = feature_extractor(sample["audio"]["array"], return_tensors="pt").input_values
logits = model(input_values).logits[0]
pred_ids = torch.argmax(logits, axis=-1)

# retrieve word stamps (analogous commands for `output_char_offsets`)
outputs = tokenizer.decode(pred_ids, output_word_offsets=True)
# compute `time_offset` in seconds as product of downsampling ratio and sampling_rate
time_offset = model.config.inputs_to_logits_ratio / feature_extractor.sampling_rate

word_offsets = [
...     {
...         "word": d["word"],
...         "start_time": round(d["start_offset"] * time_offset, 2),
...         "end_time": round(d["end_offset"] * time_offset, 2),
...     }
...     for d in outputs.word_offsets
... ]
# compare word offsets with audio `common_voice_en_100038.mp3` online on the dataset viewer:
# https://huggingface.co/datasets/common_voice/viewer/en/train
word_offsets[:3]
[{'word': 'WHY', 'start_time': 1.42, 'end_time': 1.54}, {'word': 'DOES', 'start_time': 1.64, 'end_time': 1.9}, {'word': 'MILISANDRA', 'start_time': 2.26, 'end_time': 2.9}]
""",
        """CODE.from transformers import (
...     AutoTokenizer,
...     AutoModelForSeq2SeqLM,
...     LogitsProcessorList,
...     MinLengthLogitsProcessor,
...     ConstrainedBeamSearchScorer,
...     PhrasalConstraint,
... )
import torch

tokenizer = AutoTokenizer.from_pretrained("t5-base")
model = AutoModelForSeq2SeqLM.from_pretrained("t5-base")

encoder_input_str = "translate English to German: How old are you?"
encoder_input_ids = tokenizer(encoder_input_str, return_tensors="pt").input_ids


# lets run beam search using 3 beams
num_beams = 3
# define decoder start token ids
input_ids = torch.ones((num_beams, 1), device=model.device, dtype=torch.long)
input_ids = input_ids * model.config.decoder_start_token_id

# add encoder_outputs to model keyword arguments
model_kwargs = {
...     "encoder_outputs": model.get_encoder()(
...         encoder_input_ids.repeat_interleave(num_beams, dim=0), return_dict=True
...     )
... }

constraint_str = "sind"
constraint_token_ids = tokenizer.encode(constraint_str)[:-1]  # slice to remove eos token
constraints = [PhrasalConstraint(token_ids=constraint_token_ids)]


# instantiate beam scorer
beam_scorer = ConstrainedBeamSearchScorer(
...     batch_size=1, num_beams=num_beams, device=model.device, constraints=constraints
... )

# instantiate logits processors
logits_processor = LogitsProcessorList(
...     [
...         MinLengthLogitsProcessor(5, eos_token_id=model.config.eos_token_id),
...     ]
... )

outputs = model.constrained_beam_search(
...     input_ids, beam_scorer, constraints=constraints, logits_processor=logits_processor, **model_kwargs
... )

print("Generated:", tokenizer.batch_decode(outputs, skip_special_tokens=True))
# => ['Wie alter sind Sie?']
""",
        """CODE.from transformers import (
...     AutoTokenizer,
...     TFAutoModelForCausalLM,
...     TFLogitsProcessorList,
...     TFMinLengthLogitsProcessor,
...     TFTopKLogitsWarper,
...     TFTemperatureLogitsWarper,
... )

tokenizer = AutoTokenizer.from_pretrained("gpt2")
model = TFAutoModelForCausalLM.from_pretrained("gpt2")

# set pad_token_id to eos_token_id because GPT2 does not have a EOS token
model.config.pad_token_id = model.config.eos_token_id

input_prompt = "Today is a beautiful day, and"
input_ids = tokenizer(input_prompt, return_tensors="tf").input_ids

# instantiate logits processors
logits_processor = TFLogitsProcessorList(
...     [
...         TFMinLengthLogitsProcessor(15, eos_token_id=model.config.eos_token_id),
...     ]
... )
# instantiate logits processors
logits_warper = TFLogitsProcessorList(
...     [
...         TFTopKLogitsWarper(50),
...         TFTemperatureLogitsWarper(0.7),
...     ]
... )

outputs = model.sample(input_ids, logits_processor=logits_processor, logits_warper=logits_warper)

print("Generated:", tokenizer.batch_decode(outputs, skip_special_tokens=True))
""",
        """CODE.from transformers import (
...     AutoTokenizer,
...     TFAutoModelForCausalLM,
...     TFLogitsProcessorList,
...     TFMinLengthLogitsProcessor,
... )

tokenizer = AutoTokenizer.from_pretrained("gpt2")
model = TFAutoModelForCausalLM.from_pretrained("gpt2")

# set pad_token_id to eos_token_id because GPT2 does not have a EOS token
model.config.pad_token_id = model.config.eos_token_id

input_prompt = "Today is a beautiful day, and"
input_ids = tokenizer(input_prompt, return_tensors="tf").input_ids

# instantiate logits processors
logits_processor = TFLogitsProcessorList(
...     [
...         TFMinLengthLogitsProcessor(15, eos_token_id=model.config.eos_token_id),
...     ]
... )

outputs = model.greedy_search(input_ids, logits_processor=logits_processor)

print("Generated:", tokenizer.batch_decode(outputs, skip_special_tokens=True))
""",
        """CODE.from transformers import ConvNextModel, ConvNextConfig
configuration = ConvNextConfig()
model = ConvNextModel(configuration)
configuration = model.config
""",
        """CODE.from transformers import Data2VecAudioModel, Data2VecAudioConfig

# Initializing a Data2VecAudio facebook/data2vec-audio-base-960h style configuration
configuration = Data2VecAudioConfig()

# Initializing a model from the facebook/data2vec-audio-base-960h style configuration
model = Data2VecAudioModel(configuration)

# Accessing the model configuration
configuration = model.config
""",
        """CODE.from transformers import Data2VecTextModel, Data2VecTextConfig
configuration = Data2VecTextConfig()
model = Data2VecTextModel(configuration)
configuration = model.config
""",
        """CODE.from transformers import FlaxSpeechEncoderDecoderModel

# initialize a wav2vec2-2-bart from pretrained wav2vec2 and bart models. Note that the cross-attention layers will be randomly initialized
model = FlaxSpeechEncoderDecoderModel.from_encoder_decoder_pretrained(
...     "facebook/wav2vec2-large-lv60", "facebook/bart-large"
... )
# saving model after fine-tuning
model.save_pretrained("./wav2vec2-2-bart-large")
# load fine-tuned model
model = FlaxSpeechEncoderDecoderModel.from_pretrained("./wav2vec2-2-bart-large")
""",
        """CODE.from transformers import FlaxSpeechEncoderDecoderModel

model = FlaxSpeechEncoderDecoderModel.from_encoder_decoder_pretrained(
    "facebook/wav2vec2-large-lv60", "facebook/bart-large"
)

inputs = jnp.ones((2, 5000), dtype=jnp.float32)
encoder_outputs = model.encode(inputs)
""",
        """CODE.from transformers import FlaxSpeechEncoderDecoderModel
import jax.numpy as jnp

model = FlaxSpeechEncoderDecoderModel.from_encoder_decoder_pretrained(
    "facebook/wav2vec2-large-lv60", "facebook/bart-large"
)

inputs = jnp.ones((2, 5000), dtype=jnp.float32)
encoder_outputs = model.encode(inputs)

decoder_start_token_id = model.config.decoder.bos_token_id
decoder_input_ids = jnp.ones((inputs.shape[0], 1), dtype="i4") * decoder_start_token_id

outputs = model.decode(decoder_input_ids, encoder_outputs)
logits = outputs.logits""",
        """CODE.from transformers import MaskFormerConfig, MaskFormerModel

# Initializing a MaskFormer facebook/maskformer-swin-base-ade configuration
configuration = MaskFormerConfig()

# Initializing a model from the facebook/maskformer-swin-base-ade style configuration
model = MaskFormerModel(configuration)

# Accessing the model configuration
configuration = model.config
""",
        """CODE.from transformers import PLBartModel, PLBartConfig

# Initializing a PLBART uclanlp/plbart-base style configuration
configuration = PLBartConfig()
# Initializing a model from the uclanlp/plbart-base style configuration
model = PLBartModel(configuration)
# Accessing the model configuration
configuration = model.config
""",
        """CODE.from transformers import PLBartTokenizer

tokenizer = PLBartTokenizer.from_pretrained("uclanlp/plbart-python-en_XX", src_lang="python", tgt_lang="en_XX")
example_python_phrase = "def maximum(a,b,c):NEW_LINE_INDENTreturn max([a,b,c])"
expected_translation_english = "Returns the maximum value of a b c."
inputs = tokenizer(example_python_phrase, return_tensors="pt")
with tokenizer.as_target_tokenizer():
...     labels = tokenizer(expected_translation_english, return_tensors="pt")
inputs["labels"] = labels["input_ids"]
""",
        """CODE.from transformers import PoolFormerModel, PoolFormerConfig

# Initializing a PoolFormer sail/poolformer_s12 style configuration
configuration = PoolFormerConfig()

# Initializing a model from the sail/poolformer_s12 style configuration
model = PoolFormerModel(configuration)

# Accessing the model configuration
configuration = model.config
""",
        """CODE.from transformers import XGLMModel, XGLMConfig

# Initializing a XGLM facebook/xglm-564M style configuration
configuration = XGLMConfig()

# Initializing a model from the facebook/xglm-564M style configuration
model = XGLMModel(configuration)

# Accessing the model configuration
configuration = model.config
""",
        """CODE.from transformers import XLMRobertaXLModel, XLMRobertaXLConfig

# Initializing a XLM_ROBERTA_XL bert-base-uncased style configuration
configuration = XLMRobertaXLConfig()

# Initializing a model from the bert-base-uncased style configuration
model = XLMRobertaXLModel(configuration)

# Accessing the model configuration
configuration = model.config
""" .

<DEPENDENCY.transformers==4.18.0> <CONTAINS> """CODE.convert_file_size_to_int("1MB")
1048576
""",
        """CODE.dtype_byte_size(torch.float32)
4
""",
        """CODE.from transformers import AutoModel

model = AutoModel.from_pretrained("bert-base-cased")

model.push_to_hub("my-finetuned-bert")

model.push_to_hub("my-finetuned-bert", use_temp_dir=True)

model.push_to_hub("my-finetuned-bert", organization="huggingface")

model.push_to_hub("my-finetuned-bert", repo_url="https://huggingface.co/sgugger/my-finetuned-bert")
""",
        """CODE.from transformers import DPTModel, DPTConfig

# Initializing a DPT dpt-large style configuration
configuration = DPTConfig()

# Initializing a model from the dpt-large style configuration
model = DPTModel(configuration)

# Accessing the model configuration
configuration = model.config
""",
        """CODE.from transformers import DecisionTransformerModel, DecisionTransformerConfig

# Initializing a DecisionTransformer configuration
configuration = DecisionTransformerConfig()

# Initializing a model from the configuration
model = DecisionTransformerConfig(configuration)

# Accessing the model configuration
configuration = model.config
""",
        """CODE.from transformers import GLPNModel, GLPNConfig

# Initializing a GLPN kaist/gdpdepth-kitti style configuration
configuration = GLPNConfig()

# Initializing a model from the kaist/gdpdepth-kitti style configuration
model = GLPNModel(configuration)

# Accessing the model configuration
configuration = model.config
""",
        """CODE.from transformers import ResNetConfig, ResNetModel
configuration = ResNetConfig()
model = ResNetModel(configuration)
configuration = model.config
""",
        """CODE.from transformers import VanModel, VanConfig
configuration = VanConfig()
model = VanModel(configuration)
configuration = model.config
""" .

<DEPENDENCY.transformers==4.19.0> <CONTAINS> """CODE.from transformers import Data2VecVisionModel, Data2VecVisionConfig

# Initializing a Data2VecVision data2vec_vision-base-patch16-224-in22k style configuration
configuration = Data2VecVisionConfig()

# Initializing a model from the data2vec_vision-base-patch16-224-in22k style configuration
model = Data2VecVisionModel(configuration)

# Accessing the model configuration
configuration = model.config
""",
        """CODE.from transformers import FlavaModel, FlavaForPreTraining, FlavaConfig

# Initializing a FlavaConfig with style configuration
configuration = FlavaConfig()

# Initializing a FlavaModel and FlavaForPreTraining model from the style configuration
model = FlavaModel(configuration)
model_pre = FlavaForPreTraining(configuration)

# Accessing the model configuration
configuration = model.config
configuration_pre = model_pre.config
""",
        """CODE.from transformers import FlavaMultimodalModel, FlavaMultimodalConfig
configuration = FlavaMultimodalConfig()
model = FlavaMultimodalModel(configuration)
configuration = model.config
""",
        """CODE.from transformers import FlavaTextModel, FlavaTextConfig

# Initializing a FlavaTextModel with  style configuration
configuration = FlavaTextConfig()

# Initializing a FlavaTextConfig from the  style configuration
model = FlavaTextModel(configuration)

# Accessing the model configuration
configuration = model.config
""",
        """CODE.from transformers import OPTModel, OPTConfig

# Initializing a OPT facebook/opt-large style configuration
configuration = OPTConfig()

# Initializing a model from the facebook/opt-large style configuration
model = OPTModel(configuration)

# Accessing the model configuration
configuration = model.config
""",
        """CODE.from transformers import RegNetConfig, RegNetModel
configuration = RegNetConfig()
model = RegNetModel(configuration)
configuration = model.config
""" .

<DEPENDENCY.transformers==4.20.0> <CONTAINS> """CODE.from transformers import BloomModel, BloomConfig

# Initializing a Bloom configuration
configuration = BloomConfig()

# Initializing a model from the configuration
model = BloomModel(configuration)

# Accessing the model configuration
configuration = model.config
""",
        """CODE.from transformers import BloomTokenizerFast
tokenizer = BloomTokenizerFast.from_pretrained("bigscience/bloom")
tokenizer("Hello world")['input_ids']
[15496, 995]
tokenizer(" Hello world")['input_ids']
[18435, 995]
""",
        """CODE.from transformers import CvtModel, CvtConfig

# Initializing a Cvt msft/cvt style configuration
configuration = CvtConfig()

# Initializing a model from the msft/cvt style configuration
model = CvtModel(configuration)

# Accessing the model configuration
configuration = model.config
""",
        """CODE.from transformers import GPTNeoXModel, GPTNeoXConfig

# Initializing a GPTNeoX gpt-neox-20b style configuration
configuration = GPTNeoXConfig()

# Initializing a model from the gpt-neox-20b style configuration
model = GPTNeoXModel(configuration)

# Accessing the model configuration
configuration = model.config
""",
        """CODE.from transformers import GPTNeoXTokenizerFast
tokenizer = GPTNeoXTokenizerFast.from_pretrained("gpt2")
tokenizer("Hello world")['input_ids']
[15496, 995]
tokenizer(" Hello world")['input_ids']
[18435, 995]
""",
        """CODE.from transformers import LayoutLMv3Model, LayoutLMv3Config

# Initializing a LayoutLMv3 microsoft/layoutlmv3-base style configuration
configuration = LayoutLMv3Config()

# Initializing a model from the microsoft/layoutlmv3-base style configuration
model = LayoutLMv3Model(configuration)

# Accessing the model configuration
configuration = model.config
""",
        """CODE.from transformers import LevitModel, LevitConfig

# Initializing a LeViT levit-base-192 style configuration
configuration = LevitConfig()

# Initializing a model from the levit-base-192 style configuration
model = LevitModel(configuration)

# Accessing the model configuration
configuration = model.config
""",
        """CODE.from transformers import MCTCTModel, MCTCTConfig

# Initializing a M-CTC-T mctct-large style configuration
configuration = MCTCTConfig()

# Initializing a model from the mctct-large style configuration
model = MCTCTModel(configuration)

# Accessing the model configuration
configuration = model.config
""",
        """CODE.from transformers import T5Tokenizer, FlaxLongT5ForConditionalGeneration
import jax.numpy as jnp

tokenizer = T5Tokenizer.from_pretrained("t5-base")
model = FlaxLongT5ForConditionalGeneration.from_pretrained("google/long-t5-local-base")

text = "My friends are cool but they eat too many carbs."
inputs = tokenizer(text, return_tensors="np")
encoder_outputs = model.encode(**inputs)

decoder_start_token_id = model.config.decoder_start_token_id
decoder_input_ids = jnp.ones((inputs.input_ids.shape[0], 1), dtype="i4") * decoder_start_token_id

outputs = model.decode(decoder_input_ids, encoder_outputs)
logits = outputs.logits""",
        """CODE.from transformers import T5Tokenizer, FlaxLongT5ForConditionalGeneration
import jax.numpy as jnp

tokenizer = T5Tokenizer.from_pretrained("t5-base")
model = FlaxLongT5ForConditionalGeneration.from_pretrained("google/long-t5-local-base")

text = "summarize: My friends are cool but they eat too many carbs."
inputs = tokenizer(text, return_tensors="np")
encoder_outputs = model.encode(**inputs)

decoder_start_token_id = model.config.decoder_start_token_id
decoder_input_ids = jnp.ones((inputs.input_ids.shape[0], 1), dtype="i4") * decoder_start_token_id

outputs = model.decode(decoder_input_ids, encoder_outputs)
logits = outputs.logits""",
        """CODE.from transformers import T5Tokenizer, FlaxLongT5ForConditionalGeneration
tokenizer = T5Tokenizer.from_pretrained("t5-base")
model = FlaxLongT5ForConditionalGeneration.from_pretrained("google/long-t5-local-base")
text = "My friends are cool but they eat too many carbs."
inputs = tokenizer(text, return_tensors="np")
encoder_outputs = model.encode(**inputs)""",
        """CODE.from transformers import TrajectoryTransformerModel, TrajectoryTransformerConfig

# Initializing a TrajectoryTransformer CarlCochet/trajectory-transformer-halfcheetah-medium-v2 style configuration
configuration = TrajectoryTransformerConfig()

# Initializing a model from the CarlCochet/trajectory-transformer-halfcheetah-medium-v2 style configuration
model = TrajectoryTransformerModel(configuration)

# Accessing the model configuration
configuration = model.config
""",
        """CODE.from transformers import Wav2Vec2ConformerModel, Wav2Vec2ConformerConfig

# Initializing a Wav2Vec2Conformer facebook/wav2vec2-conformer-large-rel-pos style configuration
configuration = Wav2Vec2ConformerConfig()

# Initializing a model from the facebook/wav2vec2-conformer-large-rel-pos style configuration
model = Wav2Vec2ConformerModel(configuration)

# Accessing the model configuration
configuration = model.config
""" .

<DEPENDENCY.transformers==4.21.0> <CONTAINS> """CODE.from PIL import Image
import requests
from transformers import AutoProcessor, GroupViTModel

model = GroupViTModel.from_pretrained("nvidia/groupvit-gcc-yfcc")
processor = AutoProcessor.from_pretrained("nvidia/groupvit-gcc-yfcc")

url = "http://images.cocodataset.org/val2017/000000039769.jpg"
image = Image.open(requests.get(url, stream=True).raw)

inputs = processor(images=image, return_tensors="pt")

image_features = model.get_image_features(**inputs)
""",
        """CODE.from transformers import AutoTokenizer, TFBertTokenizer

tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")
tf_tokenizer = TFBertTokenizer.from_tokenizer(tokenizer)
""",
        """CODE.from transformers import CLIPTokenizer, GroupViTModel

model = GroupViTModel.from_pretrained("nvidia/groupvit-gcc-yfcc")
tokenizer = CLIPTokenizer.from_pretrained("nvidia/groupvit-gcc-yfcc")

inputs = tokenizer(["a photo of a cat", "a photo of a dog"], padding=True, return_tensors="pt")
text_features = model.get_text_features(**inputs)
""",
        """CODE.from transformers import CodeGenModel, CodeGenConfig

# Initializing a CodeGen 6B configuration
configuration = CodeGenConfig()

# Initializing a model from the configuration
model = CodeGenModel(configuration)

# Accessing the model configuration
configuration = model.config
""",
        """CODE.from transformers import CodeGenTokenizer
tokenizer = CodeGenTokenizer.from_pretrained("Salesforce/codegen-350M-mono")
tokenizer("Hello world")['input_ids']
tokenizer(" Hello world")['input_ids']""",
        """CODE.from transformers import CodeGenTokenizerFast
tokenizer = CodeGenTokenizerFast.from_pretrained("Salesforce/codegen-350M-mono")
tokenizer("Hello world")['input_ids']
tokenizer(" Hello world")['input_ids']""",
        """CODE.from transformers import GroupViTTextConfig, GroupViTTextModel
configuration = GroupViTTextConfig()
model = GroupViTTextModel(configuration)
configuration = model.config
""",
        """CODE.from transformers import GroupViTVisionConfig, GroupViTVisionModel
configuration = GroupViTVisionConfig()
model = GroupViTVisionModel(configuration)
configuration = model.config
""",
        """CODE.from transformers import MobileViTConfig, MobileViTModel

# Initializing a mobilevit-small style configuration
configuration = MobileViTConfig()

# Initializing a model from the mobilevit-small style configuration
model = MobileViTModel(configuration)

# Accessing the model configuration
configuration = model.config
""",
        """CODE.from transformers import TFBertTokenizer

tf_tokenizer = TFBertTokenizer.from_pretrained("bert-base-uncased")
""" .

<DEPENDENCY.transformers==4.22.0> <CONTAINS> """CODE.from transformers import DonutSwinConfig, DonutSwinModel

# Initializing a Donut naver-clova-ix/donut-base style configuration
configuration = DonutSwinConfig()

# Randomly initializing a model from the naver-clova-ix/donut-base style configuration
model = DonutSwinModel(configuration)

# Accessing the model configuration
configuration = model.config
""" .

<DEPENDENCY.transformers==4.23.0> <CONTAINS> """CODE.from transformers import ConditionalDetrModel, ConditionalDetrConfig

# Initializing a Conditional DETR microsoft/conditional-detr-resnet-50 style configuration
configuration = ConditionalDetrConfig()

# Initializing a model from the microsoft/conditional-detr-resnet-50 style configuration
model = ConditionalDetrModel(configuration)

# Accessing the model configuration
configuration = model.config
""",
        """CODE.from transformers import EsmModel, EsmConfig

# Initializing a ESM esm-base-uncased style configuration configuration = EsmConfig()

# Initializing a model from the configuration model = ESMModel(configuration)

# Accessing the model configuration configuration = model.config
""",
        """CODE.from transformers import GPTNeoXJapaneseModel, GPTNeoXJapaneseConfig

# Initializing a GPTNeoXJapanese gpt-neox-japanese-2.7b style configuration
configuration = GPTNeoXJapaneseConfig()

# Initializing a model from the gpt-neox-japanese-2.7b style configuration
model = GPTNeoXJapaneseModel(configuration)

# Accessing the model configuration
configuration = model.config
""",
        """CODE.from transformers import GPTNeoXJapaneseTokenizer

tokenizer = GPTNeoXJapaneseTokenizer.from_pretrained("abeja/gpt-neox-japanese-2.7b")
# You can confirm both æ¶å¿ and æ¶æ are encoded to 17749
tokenizer("å¾è¼©ã¯ç«ã§ããð¯ãå®ã¯æ¶å¿(æ¶æ)å¤§å­¦åºèº«")["input_ids"]
[30014, 26883, 26638, 27228, 25, 26650, 31732, 31679, 27809, 26638, 17749, 31592, 17749, 31593, 321, 1281]

# Both æ¶å¿ and æ¶æ are decoded to æ¶å¿
tokenizer.decode(tokenizer("å¾è¼©ã¯ç«ã§ããð¯ãå®ã¯æ¶å¿(æ¶æ)å¤§å­¦åºèº«")["input_ids"])
'å¾è¼©ã¯ç«ã§ããð¯ãå®ã¯æ¶å¿(æ¶å¿)å¤§å­¦åºèº«'
""" .

<DEPENDENCY.transformers==4.25.1> <CONTAINS> """CODE.image_processor = CLIPImageProcessor.from_pretrained(
    "openai/clip-vit-base-patch32"
)  # Download image_processing_config from huggingface.co and cache.
image_processor = CLIPImageProcessor.from_pretrained(
    "./test/saved_model/"
)  # E.g. image processor (or model) was saved using *save_pretrained('./test/saved_model/')*
image_processor = CLIPImageProcessor.from_pretrained("./test/saved_model/preprocessor_config.json")
image_processor = CLIPImageProcessor.from_pretrained(
    "openai/clip-vit-base-patch32", do_normalize=False, foo=False
)
assert image_processor.do_normalize is False
image_processor, unused_kwargs = CLIPImageProcessor.from_pretrained(
    "openai/clip-vit-base-patch32", do_normalize=False, foo=False, return_unused_kwargs=True
)
assert image_processor.do_normalize is False
assert unused_kwargs == {"foo": False}
""" .

<DEPENDENCY.transformers==4.26.0> <CONTAINS> """CODE.
from transformers import GPTSw3Tokenizer
tokenizer = GPTSw3Tokenizer.from_pretrained("AI-Sweden/gpt-sw3-126m")
tokenizer("Svenska Ã¤r kul!")['input_ids']
[1814, 377, 3617, 63504]
""",
        """CODE.from PIL import Image
import requests
from transformers import AutoProcessor, AltCLIPModel

model = AltCLIPModel.from_pretrained("BAAI/AltCLIP")
processor = AutoProcessor.from_pretrained("BAAI/AltCLIP")
url = "http://images.cocodataset.org/val2017/000000039769.jpg"
image = Image.open(requests.get(url, stream=True).raw)
inputs = processor(images=image, return_tensors="pt")
image_features = model.get_image_features(**inputs)
""",
        """CODE.from PIL import Image
import requests
from transformers import AutoProcessor, BlipForConditionalGeneration

model = BlipForConditionalGeneration.from_pretrained("Salesforce/blip-image-captioning-base")
processor = AutoProcessor.from_pretrained("Salesforce/blip-image-captioning-base")

url = "http://images.cocodataset.org/val2017/000000039769.jpg"
image = Image.open(requests.get(url, stream=True).raw)

inputs = processor(images=image, return_tensors="pt")

outputs = model.generate(**inputs)
print(processor.decode(outputs[0], skip_special_tokens=True))
two cats are laying on a couch
""",
        """CODE.from PIL import Image
import requests
from transformers import AutoProcessor, BlipForQuestionAnswering

model = BlipForQuestionAnswering.from_pretrained("Salesforce/blip-vqa-base")
processor = AutoProcessor.from_pretrained("Salesforce/blip-vqa-base")

url = "http://images.cocodataset.org/val2017/000000039769.jpg"
image = Image.open(requests.get(url, stream=True).raw)
text = "How many cats are in the picture?"

inputs = processor(images=image, text=text, return_tensors="pt")

outputs = model.generate(**inputs)
print(processor.decode(outputs[0], skip_special_tokens=True))
2
""",
        """CODE.from PIL import Image
import requests
from transformers import AutoProcessor, BlipModel

model = BlipModel.from_pretrained("Salesforce/blip-image-captioning-base")
processor = AutoProcessor.from_pretrained("Salesforce/blip-image-captioning-base")

url = "http://images.cocodataset.org/val2017/000000039769.jpg"
image = Image.open(requests.get(url, stream=True).raw)

inputs = processor(images=image, return_tensors="pt")

image_features = model.get_image_features(**inputs)
""",
        """CODE.from transformers import AltCLIPConfig, AltCLIPModel
configuration = AltCLIPConfig()
model = AltCLIPModel(configuration)
configuration = model.config
config_text = AltCLIPTextConfig()
config_vision = AltCLIPVisionConfig()
config = AltCLIPConfig.from_text_vision_configs(config_text, config_vision)
""",
        """CODE.from transformers import AutoProcessor, AltCLIPModel
model = AltCLIPModel.from_pretrained("BAAI/AltCLIP")
processor = AutoProcessor.from_pretrained("BAAI/AltCLIP")
inputs = processor(text=["a photo of a cat", "a photo of a dog"], padding=True, return_tensors="pt")
text_features = model.get_text_features(**inputs)
""",
        """CODE.from transformers import AutoProcessor, BlipModel

model = BlipModel.from_pretrained("Salesforce/blip-image-captioning-base")
processor = AutoProcessor.from_pretrained("Salesforce/blip-image-captioning-base")

inputs = processor(text=["a photo of a cat", "a photo of a dog"], padding=True, return_tensors="pt")
text_features = model.get_text_features(**inputs)
""",
        """CODE.from transformers import BioGptModel, BioGptConfig

# Initializing a BioGPT microsoft/biogpt style configuration
configuration = BioGptConfig()

# Initializing a model from the microsoft/biogpt style configuration
model = BioGptModel(configuration)

# Accessing the model configuration
configuration = model.config
""",
        """CODE.from transformers import BitConfig, BitModel

# Initializing a BiT bit-50 style configuration
configuration = BitConfig()

# Initializing a model (with random weights) from the bit-50 style configuration
model = BitModel(configuration)

# Accessing the model configuration
configuration = model.config
""",
        """CODE.from transformers import BlipConfig, BlipModel

# Initializing a BlipConfig with Salesforce/blip-vqa-base style configuration
configuration = BlipConfig()

# Initializing a BlipPModel (with random weights) from the Salesforce/blip-vqa-base style configuration
model = BlipModel(configuration)

# Accessing the model configuration
configuration = model.config

# We can also initialize a BlipConfig from a BlipTextConfig and a BlipVisionConfig

# Initializing a BLIPText and BLIPVision configuration
config_text = BlipTextConfig()
config_vision = BlipVisionConfig()

config = BlipConfig.from_text_vision_configs(config_text, config_vision)
""",
        """CODE.from transformers import BlipTextConfig, BlipTextModel

# Initializing a BlipTextConfig with Salesforce/blip-vqa-base style configuration
configuration = BlipTextConfig()

# Initializing a BlipTextModel (with random weights) from the Salesforce/blip-vqa-base style configuration
model = BlipTextModel(configuration)

# Accessing the model configuration
configuration = model.config
""",
        """CODE.from transformers import EfficientFormerConfig, EfficientFormerModel

# Initializing a EfficientFormer efficientformer-l1 style configuration
configuration = EfficientFormerConfig()

# Initializing a EfficientFormerModel (with random weights) from the efficientformer-l3 style configuration
model = EfficientFormerModel(configuration)

# Accessing the model configuration
configuration = model.config
""",
        """CODE.from transformers import GitConfig, GitModel

# Initializing a GIT microsoft/git-base style configuration
configuration = GitConfig()

# Initializing a model (with random weights) from the microsoft/git-base style configuration
model = GitModel(configuration)

# Accessing the model configuration
configuration = model.config
""",
        """CODE.from transformers import GraphormerForGraphClassification, GraphormerConfig

# Initializing a Graphormer graphormer-base-pcqm4mv2 style configuration
configuration = GraphormerConfig()

# Initializing a model from the graphormer-base-pcqm4mv1 style configuration
model = GraphormerForGraphClassification(configuration)

# Accessing the model configuration
configuration = model.config
""",
        """CODE.from transformers import Mask2FormerConfig, Mask2FormerModel

# Initializing a Mask2Former facebook/mask2former-swin-small-coco-instance configuration
configuration = Mask2FormerConfig()

# Initializing a model (with random weights) from the facebook/mask2former-swin-small-coco-instance style configuration
model = Mask2FormerModel(configuration)

# Accessing the model configuration
configuration = model.config
""",
        """CODE.from transformers import OneFormerConfig, OneFormerModel

# Initializing a OneFormer shi-labs/oneformer_ade20k_swin_tiny configuration
configuration = OneFormerConfig()
# Initializing a model (with random weights) from the shi-labs/oneformer_ade20k_swin_tiny style configuration
model = OneFormerModel(configuration)
# Accessing the model configuration
configuration = model.config
""",
        """CODE.from transformers import Swin2SRConfig, Swin2SRModel

# Initializing a Swin2SR caidas/swin2sr-classicalsr-x2-64 style configuration
configuration = Swin2SRConfig()

# Initializing a model (with random weights) from the caidas/swin2sr-classicalsr-x2-64 style configuration
model = Swin2SRModel(configuration)

# Accessing the model configuration
configuration = model.config
""",
        """CODE.from transformers import TimesformerConfig, TimesformerModel

# Initializing a TimeSformer timesformer-base style configuration
configuration = TimesformerConfig()

# Randomly initializing a model from the configuration
model = TimesformerModel(configuration)

# Accessing the model configuration
configuration = model.config
""" .

<DEPENDENCY.transformers==4.27.0> <CONTAINS> """CODE.from PIL import Image
import requests
from transformers import AutoProcessor, AlignModel

model = AlignModel.from_pretrained("kakaobrain/align-base")
processor = AutoProcessor.from_pretrained("kakaobrain/align-base")

url = "http://images.cocodataset.org/val2017/000000039769.jpg"
image = Image.open(requests.get(url, stream=True).raw)

inputs = processor(images=image, return_tensors="pt")

image_features = model.get_image_features(**inputs)
""",
        """CODE.from transformers import (
...     Blip2VisionConfig,
...     Blip2QFormerConfig,
...     OPTConfig,
...     Blip2Config,
...     Blip2ForConditionalGeneration,
... )

# Initializing a Blip2Config with Salesforce/blip2-opt-2.7b style configuration
configuration = Blip2Config()

# Initializing a Blip2ForConditionalGeneration (with random weights) from the Salesforce/blip2-opt-2.7b style configuration
model = Blip2ForConditionalGeneration(configuration)

# Accessing the model configuration
configuration = model.config

# We can also initialize a Blip2Config from a Blip2VisionConfig, Blip2QFormerConfig and any PretrainedConfig

# Initializing BLIP-2 vision, BLIP-2 Q-Former and language model configurations
vision_config = Blip2VisionConfig()
qformer_config = Blip2QFormerConfig()
text_config = OPTConfig()

config = Blip2Config.from_text_vision_configs(vision_config, qformer_config, text_config)
""",
        """CODE.from transformers import AlignConfig, AlignModel

# Initializing a AlignConfig with kakaobrain/align-base style configuration
configuration = AlignConfig()

# Initializing a AlignModel (with random weights) from the kakaobrain/align-base style configuration
model = AlignModel(configuration)

# Accessing the model configuration
configuration = model.config

# We can also initialize a AlignConfig from a AlignTextConfig and a AlignVisionConfig
from transformers import AlignTextConfig, AlignVisionConfig

# Initializing ALIGN Text and Vision configurations
config_text = AlignTextConfig()
config_vision = AlignVisionConfig()

config = AlignConfig.from_text_vision_configs(config_text, config_vision)
""",
        """CODE.from transformers import AlignTextConfig, AlignTextModel

# Initializing a AlignTextConfig with kakaobrain/align-base style configuration
configuration = AlignTextConfig()

# Initializing a AlignTextModel (with random weights) from the kakaobrain/align-base style configuration
model = AlignTextModel(configuration)

# Accessing the model configuration
configuration = model.config
""",
        """CODE.from transformers import AlignVisionConfig, AlignVisionModel

# Initializing a AlignVisionConfig with kakaobrain/align-base style configuration
configuration = AlignVisionConfig()

# Initializing a AlignVisionModel (with random weights) from the kakaobrain/align-base style configuration
model = AlignVisionModel(configuration)

# Accessing the model configuration
configuration = model.config
""",
        """CODE.from transformers import AutoFeatureExtractor, ClapModel
import torch

model = ClapModel.from_pretrained("laion/clap-htsat-unfused")
feature_extractor = AutoFeatureExtractor.from_pretrained("laion/clap-htsat-unfused")
random_audio = torch.rand((16_000))
inputs = feature_extractor(random_audio, return_tensors="pt")
audio_features = model.get_audio_features(**inputs)
""",
        """CODE.from transformers import AutoTokenizer, AlignModel

model = AlignModel.from_pretrained("kakaobrain/align-base")
tokenizer = AutoTokenizer.from_pretrained("kakaobrain/align-base")

inputs = tokenizer(["a photo of a cat", "a photo of a dog"], padding=True, return_tensors="pt")
text_features = model.get_text_features(**inputs)
""",
        """CODE.from transformers import AutoTokenizer, ClapModel
model = ClapModel.from_pretrained("laion/clap-htsat-unfused")
tokenizer = AutoTokenizer.from_pretrained("laion/clap-htsat-unfused")
inputs = tokenizer(["the sound of a cat", "the sound of a dog"], padding=True, return_tensors="pt")
text_features = model.get_text_features(**inputs)""",
        """CODE.from transformers import Blip2QFormerConfig, Blip2QFormerModel

# Initializing a BLIP-2 Salesforce/blip2-opt-2.7b style configuration
configuration = Blip2QFormerConfig()

# Initializing a model (with random weights) from the Salesforce/blip2-opt-2.7b style configuration
model = Blip2QFormerModel(configuration)
# Accessing the model configuration
configuration = model.config
""",
        """CODE.from transformers import Blip2VisionConfig, Blip2VisionModel

# Initializing a Blip2VisionConfig with Salesforce/blip2-opt-2.7b style configuration
configuration = Blip2VisionConfig()

# Initializing a Blip2VisionModel (with random weights) from the Salesforce/blip2-opt-2.7b style configuration
model = Blip2VisionModel(configuration)

# Accessing the model configuration
configuration = model.config
""",
        """CODE.from transformers import BridgeTowerModel, BridgeTowerConfig

# Initializing a BridgeTower BridgeTower/bridgetower-base style configuration
configuration = BridgeTowerConfig()

# Initializing a model from the BridgeTower/bridgetower-base style configuration
model = BridgeTowerModel(configuration)

# Accessing the model configuration
configuration = model.config
""",
        """CODE.from transformers import BridgeTowerVisionConfig

# Initializing a BridgeTower BridgeTower/bridgetower-base style configuration for the vision model
configuration = BridgeTowerVisionConfig()

# Accessing the configuration
configuration
""",
        """CODE.from transformers import ClapAudioConfig, ClapAudioModel

# Initializing a ClapAudioConfig with laion/clap-htsat-fused style configuration
configuration = ClapAudioConfig()

# Initializing a ClapAudioModel (with random weights) from the laion/clap-htsat-fused style configuration
model = ClapAudioModel(configuration)

# Accessing the model configuration
configuration = model.config
""",
        """CODE.from transformers import ClapConfig, ClapModel
configuration = ClapConfig()
model = ClapModel(configuration)
configuration = model.config
from transformers import ClapTextConfig, ClapAudioConfig
config_text = ClapTextConfig()
config_audio = ClapAudioConfig()
config = ClapConfig.from_text_audio_configs(config_text, config_audio)
""",
        """CODE.from transformers import ClapTextConfig, ClapTextModel

# Initializing a CLAP text configuration
configuration = ClapTextConfig()

# Initializing a model (with random weights) from the configuration
model = ClapTextModel(configuration)

# Accessing the model configuration
configuration = model.config
""",
        """CODE.from transformers import DetaConfig, DetaModel

# Initializing a DETA SenseTime/deformable-detr style configuration
configuration = DetaConfig()

# Initializing a model (with random weights) from the SenseTime/deformable-detr style configuration
model = DetaModel(configuration)

# Accessing the model configuration
configuration = model.config
""",
        """CODE.import torch
from PIL import Image
import requests
from transformers import AutoProcessor, Blip2Model

device = "cuda" if torch.cuda.is_available() else "cpu"

model = Blip2Model.from_pretrained("Salesforce/blip2-opt-2.7b", torch_dtype=torch.float16)

model.to(device)  # doctest: +IGNORE_RESULT

processor = AutoProcessor.from_pretrained("Salesforce/blip2-opt-2.7b")
url = "http://images.cocodataset.org/val2017/000000039769.jpg"
image = Image.open(requests.get(url, stream=True).raw)
inputs = processor(images=image, return_tensors="pt").to(device, torch.float16)
image_outputs = model.get_image_features(**inputs)
""",
        """CODE.import torch
from PIL import Image
import requests
from transformers import Blip2Processor, Blip2Model

device = "cuda" if torch.cuda.is_available() else "cpu"

processor = Blip2Processor.from_pretrained("Salesforce/blip2-opt-2.7b")
model = Blip2Model.from_pretrained("Salesforce/blip2-opt-2.7b", torch_dtype=torch.float16)
model.to(device)  # doctest: +IGNORE_RESULT

url = "http://images.cocodataset.org/val2017/000000039769.jpg"
image = Image.open(requests.get(url, stream=True).raw)
inputs = processor(images=image, return_tensors="pt").to(device, torch.float16)
qformer_outputs = model.get_qformer_features(**inputs)
""",
        """CODE.import torch
from transformers import AutoTokenizer, Blip2Model

device = "cuda" if torch.cuda.is_available() else "cpu"

model = Blip2Model.from_pretrained("Salesforce/blip2-opt-2.7b", torch_dtype=torch.float16)

model.to(device)  # doctest: +IGNORE_RESULT

tokenizer = AutoTokenizer.from_pretrained("Salesforce/blip2-opt-2.7b")
inputs = tokenizer(["a photo of a cat", "a photo of a dog"], padding=True, return_tensors="pt").to(device)
text_features = model.get_text_features(**inputs)
""" .

<DEPENDENCY.transformers==4.28.0> <CONTAINS> """CODE.from transformers import AutoModelForCausalLM, AutoTokenizer, TextIteratorStreamer
from threading import Thread

tok = AutoTokenizer.from_pretrained("gpt2")
model = AutoModelForCausalLM.from_pretrained("gpt2")
inputs = tok(["An increasing sequence: one,"], return_tensors="pt")
streamer = TextIteratorStreamer(tok)

# Run the generation in a separate thread, so that we can fetch the generated text in a non-blocking way.
generation_kwargs = dict(inputs, streamer=streamer, max_new_tokens=20)
thread = Thread(target=model.generate, kwargs=generation_kwargs)
thread.start()
generated_text = ""
for new_text in streamer:
...     generated_text += new_text
generated_text
'An increasing sequence: one, two, three, four, five, six, seven, eight, nine, ten, eleven,'
""",
        """CODE.from transformers import AutoModelForCausalLM, AutoTokenizer, TextStreamer

tok = AutoTokenizer.from_pretrained("gpt2")
model = AutoModelForCausalLM.from_pretrained("gpt2")
inputs = tok(["An increasing sequence: one,"], return_tensors="pt")
streamer = TextStreamer(tok)

_ = model.generate(**inputs, streamer=streamer, max_new_tokens=20)
""" .

<DEPENDENCY.transformers==4.29.0> <CONTAINS> """CODE.from transformers import (
    SamVisionConfig,
    SamPromptEncoderConfig,
    SamMaskDecoderConfig,
    SamModel,
)

# Initializing a SamConfig with `"facebook/sam-vit-huge"` style configuration
configuration = SamConfig()

# Initializing a SamModel (with random weights) from the `"facebook/sam-vit-huge"` style configuration
model = SamModel(configuration)

# Accessing the model configuration
configuration = model.config

# We can also initialize a SamConfig from a SamVisionConfig, SamPromptEncoderConfig, and SamMaskDecoderConfig

# Initializing SAM vision, SAM Q-Former and language model configurations
vision_config = SamVisionConfig()
prompt_encoder_config = SamPromptEncoderConfig()
mask_decoder_config = SamMaskDecoderConfig()

config = SamConfig(vision_config, prompt_encoder_config, mask_decoder_config)
""",
        """CODE.from transformers import FocalNetConfig, FocalNetModel

# Initializing a FocalNet microsoft/focalnet-tiny style configuration
configuration = FocalNetConfig()

# Initializing a model (with random weights) from the microsoft/focalnet-tiny style configuration
model = FocalNetModel(configuration)

# Accessing the model configuration
configuration = model.config
""",
        """CODE.from transformers import HfAgent

agent = HfAgent("https://api-inference.huggingface.co/models/bigcode/starcoder")
agent.chat("Draw me a picture of rivers and lakes")

agent.chat("Transform the picture so that there is a rock in there")
""",
        """CODE.from transformers import HfAgent

agent = HfAgent("https://api-inference.huggingface.co/models/bigcode/starcoder")
agent.run("Draw me a picture of rivers and lakes")
""",
        """CODE.from transformers import HfAgent

agent = HfAgent("https://api-inference.huggingface.co/models/bigcode/starcoder")
agent.run("Is the following `text` (in Spanish) positive or negative?", text="Â¡Este es un API muy agradable!")
""",
        """CODE.from transformers import OpenAiAgent

agent = OpenAiAgent(model="text-davinci-003", api_key=xxx)
agent.run("Is the following `text` (in Spanish) positive or negative?", text="Â¡Este es un API muy agradable!")
""",
        """CODE.from transformers import OpenLlamaModel, OpenLlamaConfig

# Initializing a Open-Llama open_llama-7b style configuration
configuration = OpenLlamaConfig()

# Initializing a model from the open_llama-7b style configuration
model = OpenLlamaModel(configuration)

# Accessing the model configuration
configuration = model.config
""",
        """CODE.from transformers import RwkvConfig, RwkvModel

# Initializing a Rwkv configuration
configuration = RwkvConfig()

# Initializing a model (with random weights) from the configuration
model = RwkvModel(configuration)

# Accessing the model configuration
configuration = model.config
""",
        """CODE.from transformers.tools import TextClassificationTool
classifier = TextClassificationTool()
classifier("This is a super nice API!", labels=["positive", "negative"])""",
        """CODE.from transformers.tools import TextSummarizationTool
summarizer = TextSummarizationTool()
summarizer(long_text)""",
        """CODE.from transformers.tools import TranslationTool
translator = TranslationTool()
translator("This is a super nice API!", src_lang="English", tgt_lang="French")""" .

<DEPENDENCY.transformers==4.30.0> <CONTAINS> """CODE.AutoformerLayernorm(x) = nn.LayerNorm(x)
torch.mean(nn.LayerNorm(x))""",
        """CODE.from flytekit import current_context, task


@task
def train_hf_transformer():
    cp = current_context().checkpoint
    trainer = Trainer(..., callbacks=[FlyteCallback()])
    output = trainer.train(resume_from_checkpoint=cp.restore())
""",
        """CODE.from transformers import AutoformerConfig, AutoformerModel

# Initializing a default Autoformer configuration
configuration = AutoformerConfig()

# Randomly initializing a model (with random weights) from the configuration
model = AutoformerModel(configuration)

# Accessing the model configuration
configuration = model.config
""",
        """CODE.from transformers import AzureOpenAiAgent

agent = AzureAiAgent(deployment_id="Davinci-003", api_key=xxx, resource_name=yyy)
agent.run("Is the following `text` (in Spanish) positive or negative?", text="Â¡Este es un API muy agradable!")
""",
        """CODE.from transformers import MobileViTV2Config, MobileViTV2Model

# Initializing a mobilevitv2-small style configuration
configuration = MobileViTV2Config()

# Initializing a model from the mobilevitv2-small style configuration
model = MobileViTV2Model(configuration)

# Accessing the model configuration
configuration = model.config
""",
        """CODE.from transformers import SwiftFormerConfig, SwiftFormerModel

# Initializing a SwiftFormer swiftformer-base-patch16-224 style configuration
configuration = SwiftFormerConfig()

# Initializing a model (with random weights) from the swiftformer-base-patch16-224 style configuration
model = SwiftFormerModel(configuration)

# Accessing the model configuration
configuration = model.config
""",
        """CODE.from transformers import TimmBackboneConfig, TimmBackbone

# Initializing a timm backbone
configuration = TimmBackboneConfig("resnet50")

# Initializing a model from the configuration
model = TimmBackbone(configuration)

# Accessing the model configuration
configuration = model.config
""",
        """CODE.from transformers import Wav2Vec2ForCTC, AutoProcessor
ckpt = "facebook/mms-1b-all"
processor = AutoProcessor.from_pretrained(ckpt)
model = Wav2Vec2ForCTC.from_pretrained(ckpt, target_lang="eng")
processor.tokenizer.set_target_lang("spa")
model.load_adapter("spa")
""",
        """CODE.import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, LocalAgent

checkpoint = "bigcode/starcoder"
model = AutoModelForCausalLM.from_pretrained(checkpoint, device_map="auto", torch_dtype=torch.bfloat16)
tokenizer = AutoTokenizer.from_pretrained(checkpoint)

agent = LocalAgent(model, tokenizer)
agent.run("Draw me a picture of rivers and lakes.")""",
        """CODE.import torch
from transformers import LocalAgent

agent = LocalAgent.from_pretrained("bigcode/starcoder", device_map="auto", torch_dtype=torch.bfloat16)
agent.run("Draw me a picture of rivers and lakes.")
""" .

<DEPENDENCY.transformers==4.31.0> <CONTAINS> """CODE.from transformers import (
...     BarkSemanticConfig,
...     BarkCoarseConfig,
...     BarkFineConfig,
...     BarkModel,
...     BarkConfig,
...     AutoConfig,
... )

# Initializing Bark sub-modules configurations.
semantic_config = BarkSemanticConfig()
coarse_acoustics_config = BarkCoarseConfig()
fine_acoustics_config = BarkFineConfig()
codec_config = AutoConfig.from_pretrained("facebook/encodec_24khz")


# Initializing a Bark module style configuration
configuration = BarkConfig.from_sub_model_configs(
...     semantic_config, coarse_acoustics_config, fine_acoustics_config, codec_config
... )

# Initializing a model (with random weights)
model = BarkModel(configuration)

# Accessing the model configuration
configuration = model.config
""",
        """CODE.from transformers import AutoProcessor, BarkModel
processor = AutoProcessor.from_pretrained("ylacombe/bark-small")
model = BarkModel.from_pretrained("ylacombe/bark-small")
voice_preset = "v2/en_speaker_6"
inputs = processor("Hello, my dog is cute, I need him in my life", voice_preset=voice_preset)
audio_array = model.generate(**inputs, semantic_max_new_tokens=100)
audio_array = audio_array.cpu().numpy().squeeze()
""",
        """CODE.from transformers import EncodecModel, EncodecConfig

# Initializing a "facebook/encodec_24khz" style configuration
configuration = EncodecConfig()

# Initializing a model (with random weights) from the "facebook/encodec_24khz" style configuration
model = EncodecModel(configuration)

# Accessing the model configuration
configuration = model.config
""" .

<DEPENDENCY.transformers==4.32.0> <CONTAINS> """CODE.from transformers import Dinov2Config, Dinov2Model
configuration = Dinov2Config()
model = Dinov2Model(configuration)
configuration = model.config
""",
        """CODE.from transformers import IdeficsModel, IdeficsConfig

# Initializing a Idefics idefics-9b style configuration
configuration = IdeficsConfig()

# Initializing a model from the idefics-9b style configuration
model = IdeficsModel(configuration)

# Accessing the model configuration
configuration = model.config
""",
        """CODE.from transformers import MptConfig, MptModel

# Initializing a Mpt configuration
configuration = MptConfig()

# Initializing a model (with random weights) from the configuration
model = MptModel(configuration)

# Accessing the model configuration
configuration = model.config
""",
        """CODE.from transformers import PvtModel, PvtConfig

# Initializing a PVT Xrenya/pvt-tiny-224 style configuration
configuration = PvtConfig()

# Initializing a model from the Xrenya/pvt-tiny-224 style configuration
model = PvtModel(configuration)

# Accessing the model configuration
configuration = model.config
""" .

<DEPENDENCY.transformers==4.34.0> <CONTAINS> """CODE.correct_tables("\\begin{table} \\begin{tabular}{l l} & \\ \\end{tabular} \\end{table}")
"\\begin{table}
\\begin{tabular}{l l} & \\ \\end{tabular}
\\end{table}"
""",
        """CODE.from transformers import BrosConfig, BrosModel

# Initializing a BROS jinho8345/bros-base-uncased style configuration
configuration = BrosConfig()

# Initializing a model from the jinho8345/bros-base-uncased style configuration
model = BrosModel(configuration)

# Accessing the model configuration
configuration = model.config
""",
        """CODE.from transformers import MistralModel, MistralConfig

# Initializing a Mistral 7B style configuration
configuration = MistralConfig()

# Initializing a model from the Mistral 7B style configuration
model = MistralModel(configuration)

# Accessing the model configuration
configuration = model.config
""",
        """CODE.from transformers import PersimmonModel, PersimmonConfig

# Initializing a Persimmon persimmon-7b style configuration
configuration = PersimmonConfig()
""",
        """CODE.from transformers import VitMatteConfig, VitMatteForImageMatting

# Initializing a ViTMatte hustvl/vitmatte-small-composition-1k style configuration
configuration = VitMatteConfig()

# Initializing a model (with random weights) from the hustvl/vitmatte-small-composition-1k style configuration
model = VitMatteForImageMatting(configuration)

# Accessing the model configuration
configuration = model.config
""" .

<DEPENDENCY.transformers==4.35.0> <CONTAINS> """CODE.def clean_text_and_extract_entities_with_bboxes(text):
    clean_text = text.replace('<grounding>', '').replace('<phrase>', '').replace('</phrase>', '').replace('<object>', '').replace('</object>', '').replace('<patch_index_0044>', '').replace('<patch_index_0863>', '').replace('<patch_index_0005>', '').replace('<patch_index_0911>', '')
    entities = []
    entities.append(('a snowman', (12, 21), [(0.390625, 0.046875, 0.984375, 0.828125)]))
    entities.append(('a fire', (41, 47), [(0.171875, 0.015625, 0.484375, 0.890625)])
    return clean_text, entities
""",
        """CODE.from PIL import Image
import requests
from transformers import AutoProcessor, Owlv2Model

model = Owlv2Model.from_pretrained("google/owlv2-base-patch16-ensemble")
processor = AutoProcessor.from_pretrained("google/owlv2-base-patch16-ensemble")
url = "http://images.cocodataset.org/val2017/000000039769.jpg"
image = Image.open(requests.get(url, stream=True).raw)
inputs = processor(images=image, return_tensors="pt")
image_features = model.get_image_features(**inputs)
""",
        """CODE.from transformers import AutoProcessor, Owlv2Model

model = Owlv2Model.from_pretrained("google/owlv2-base-patch16-ensemble")
processor = AutoProcessor.from_pretrained("google/owlv2-base-patch16-ensemble")
inputs = processor(
    text=[["a photo of a cat", "a photo of a dog"], ["photo of a astranaut"]], return_tensors="pt"
)
text_features = model.get_text_features(**inputs)
""",
        """CODE.from transformers import FuyuConfig

# Initializing a Fuyu fuyu-7b style configuration
configuration = FuyuConfig()
""",
        """CODE.from transformers import Kosmos2Config, Kosmos2Model

# Initializing a Kosmos-2 kosmos-2-patch14-224 style configuration
configuration = Kosmos2Config()

# Initializing a model (with random weights) from the kosmos-2-patch14-224 style configuration
model = Kosmos2Model(configuration)

# Accessing the model configuration
configuration = model.config
""",
        """CODE.from transformers import Owlv2TextConfig, Owlv2TextModel

# Initializing a Owlv2TextModel with google/owlv2-base-patch32 style configuration
configuration = Owlv2TextConfig()

# Initializing a Owlv2TextConfig from the google/owlv2-base-patch32 style configuration
model = Owlv2TextModel(configuration)

# Accessing the model configuration
configuration = model.config
""",
        """CODE.from transformers import Owlv2VisionConfig, Owlv2VisionModel
configuration = Owlv2VisionConfig()
model = Owlv2VisionModel(configuration)
configuration = model.config
""",
        """CODE.from transformers import SeamlessM4TModel, SeamlessM4TConfig

# Initializing a SeamlessM4T "facebook/hf-seamless-m4t-medium" style configuration
configuration = SeamlessM4TConfig()

# Initializing a model from the "facebook/hf-seamless-m4t-medium" style configuration
model = SeamlessM4TModel(configuration)

# Accessing the model configuration
configuration = model.config
""",
        """CODE.from transformers import SeamlessM4TTokenizer

tokenizer = SeamlessM4TTokenizer.from_pretrained(
...     "facebook/hf-seamless-m4t-medium", src_lang="eng", tgt_lang="fra"
... )
example_english_phrase = " UN Chief Says There Is No Military Solution in Syria"
expected_translation_french = "Le chef de l'ONU affirme qu'il n'y a pas de solution militaire en Syrie."
inputs = tokenizer(example_english_phrase, text_target=expected_translation_french, return_tensors="pt")
""",
        """CODE.from transformers import SeamlessM4TTokenizerFast

tokenizer = SeamlessM4TTokenizerFast.from_pretrained(
...     "facebook/hf-seamless-m4t-medium", src_lang="eng", tgt_lang="fra"
... )
example_english_phrase = " UN Chief Says There Is No Military Solution in Syria"
expected_translation_french = "Le chef de l'ONU affirme qu'il n'y a pas de solution militaire en Syrie."
inputs = tokenizer(example_english_phrase, text_target=expected_translation_french, return_tensors="pt")
""",
        """CODE.import requests
from PIL import Image
import torch
from transformers import AutoProcessor, Owlv2ForObjectDetection

processor = AutoProcessor.from_pretrained("google/owlv2-base-patch16-ensemble")
model = Owlv2ForObjectDetection.from_pretrained("google/owlv2-base-patch16-ensemble")
url = "http://images.cocodataset.org/val2017/000000039769.jpg"
image = Image.open(requests.get(url, stream=True).raw)
query_url = "http://images.cocodataset.org/val2017/000000001675.jpg"
query_image = Image.open(requests.get(query_url, stream=True).raw)
inputs = processor(images=image, query_images=query_image, return_tensors="pt")
with torch.no_grad():
    outputs = model.image_guided_detection(**inputs)
# Target image sizes (height, width) to rescale box predictions [batch_size, 2]
target_sizes = torch.Tensor([image.size[::-1]])
# Convert outputs (bounding boxes and class logits) to COCO API
results = processor.post_process_image_guided_detection(
    outputs=outputs, threshold=0.9, nms_threshold=0.3, target_sizes=target_sizes
)
i = 0  # Retrieve predictions for the first image
boxes, scores = results[i]["boxes"], results[i]["scores"]
for box, score in zip(boxes, scores):
    box = [round(i, 2) for i in box.tolist()]
    print(f"Detected similar object with confidence {round(score.item(), 3)} at location {box}")""",
        """CODE.model = ...
model.embed_tokens.neftune_noise_alpha = 0.1
model.embed_tokens.register_forward_hook(neftune_post_forward_hook)
""",
        """CODE.text = "<grounding> An image of<phrase> a snowman</phrase><object><patch_index_0044><patch_index_0863></object> warming himself by<phrase> a fire</phrase><object><patch_index_0005><patch_index_0911></object>."
entities = extract_entities_with_patch_indices(text)
entities
[(' a snowman', (31, 41), [(44, 863)]), (' a fire', (130, 137), [(5, 911)])]
""" .

<DEPENDENCY.transformers==4.35.2> <CONTAINS> """CODE.from transformers import TFEncoderDecoderModel
model = TFEncoderDecoderModel.from_pretrained("ydshieh/bert2bert-cnn_dailymail-fp16")""",
        """CODE.from transformers import TFVisionEncoderDecoderModel, AutoImageProcessor, AutoTokenizer
from PIL import Image
import requests

image_processor = AutoImageProcessor.from_pretrained("ydshieh/vit-gpt2-coco-en")
decoder_tokenizer = AutoTokenizer.from_pretrained("ydshieh/vit-gpt2-coco-en")
model = TFVisionEncoderDecoderModel.from_pretrained("ydshieh/vit-gpt2-coco-en")

url = "http://images.cocodataset.org/val2017/000000039769.jpg"
img = Image.open(requests.get(url, stream=True).raw)
pixel_values = image_processor(images=img, return_tensors="tf").pixel_values  # Batch size 1

output_ids = model.generate(
    pixel_values, max_length=16, num_beams=4, return_dict_in_generate=True
).sequences

preds = decoder_tokenizer.batch_decode(output_ids, skip_special_tokens=True)
preds = [pred.strip() for pred in preds]

assert preds == ["a cat laying on top of a couch next to another cat"]""" .

<DEPENDENCY.transformers==4.36.0> <CONTAINS> """CODE.from transformers import ClvpConfig, ClvpModelForConditionalGeneration

# Initializing a ClvpConfig with susnato/clvp_dev style configuration
configuration = ClvpConfig()

# Initializing a ClvpModelForConditionalGeneration (with random weights) from the susnato/clvp_dev style configuration
model = ClvpModelForConditionalGeneration(configuration)

# Accessing the model configuration
configuration = model.config

# We can also initialize a CLVPConfig from a CLVPTextConfig, CLVPSpeechConfig and a CLVPAutoRegressiveConfig
from transformers import ClvpEncoderConfig, ClvpDecoderConfig

# Initializing a CLVP text, CLVP speech and CLVP decoder configuration
config_text = ClvpEncoderConfig()
config_speech = ClvpEncoderConfig()
decoder_config = ClvpDecoderConfig()

config = ClvpConfig.from_sub_model_configs(config_text, config_speech, decoder_config)
""",
        """CODE.from transformers import ClvpDecoderConfig, ClvpDecoder

# Initializing a ClvpDecoderConfig with susnato/clvp_dev style configuration
decoder_configuration = ClvpDecoderConfig()

# Initializing a ClvpDecoder (with random weights) from the susnato/clvp_dev style configuration
model = ClvpDecoder(decoder_configuration)

# Accessing the model configuration
configuration = model.config
""",
        """CODE.from transformers import ClvpEncoderConfig, ClvpEncoder

# Initializing a ClvpEncoderConfig with susnato/clvp_dev style configuration
encoder_configuration = ClvpEncoderConfig()

# Initializing a ClvpEncoder (with random weights) from the susnato/clvp_dev style configuration
model = ClvpEncoder(encoder_configuration)

# Accessing the model configuration
configuration = model.config
""",
        """CODE.from transformers import ClvpProcessor, ClvpModelForConditionalGeneration

# Define the Text
text = "This is an example text."

# Define processor and model
processor = ClvpProcessor.from_pretrained("susnato/clvp_dev")
model = ClvpModelForConditionalGeneration.from_pretrained("susnato/clvp_dev")

# Generate processor output and text embeds
processor_output = processor(text=text, return_tensors="pt")
text_embeds = model.get_text_features(input_ids=processor_output["input_ids"])
""",
        """CODE.from transformers import ClvpTokenizer

tokenizer = ClvpTokenizer.from_pretrained("susnato/clvp_dev")
tokenizer("Hello world")["input_ids"]
[62, 84, 28, 2, 179, 79]

tokenizer(" Hello world")["input_ids"]
[2, 62, 84, 28, 2, 179, 79]
""",
        """CODE.from transformers import LlavaForConditionalGeneration, LlavaConfig, CLIPVisionConfig, LlamaConfig
vision_config = CLIPVisionConfig()
text_config = LlamaConfig()
configuration = LlavaConfig(vision_config, text_config)
model = LlavaForConditionalGeneration(configuration)
configuration = model.config
""",
        """CODE.from transformers import MixtralModel, MixtralConfig

# Initializing a Mixtral 7B style configuration
configuration = MixtralConfig()

# Initializing a model from the Mixtral 7B style configuration
model = MixtralModel(configuration)

# Accessing the model configuration
configuration = model.config
""",
        """CODE.from transformers import PatchTSMixerConfig, PatchTSMixerModel

# Initializing a default PatchTSMixer configuration
configuration = PatchTSMixerConfig()

# Randomly initializing a model (with random weights) from the configuration
model = PatchTSMixerModel(configuration)

# Accessing the model configuration
configuration = model.config
""",
        """CODE.from transformers import PatchTSTConfig, PatchTSTModel

# Initializing an PatchTST configuration with 12 time steps for prediction
configuration = PatchTSTConfig(prediction_length=12)

# Randomly initializing a model (with random weights) from the configuration
model = PatchTSTModel(configuration)

# Accessing the model configuration
configuration = model.config
""",
        """CODE.from transformers import PhiModel, PhiConfig

# Initializing a Phi-1 style configuration
configuration = PhiConfig.from_pretrained("susnato/phi-1_dev")

# Initializing a model from the configuration
model = PhiModel(configuration)

# Accessing the model configuration
configuration = model.config
""",
        """CODE.from transformers import SeamlessM4Tv2Model, SeamlessM4Tv2Config

# Initializing a SeamlessM4Tv2 "" style configuration
configuration = SeamlessM4Tv2Config()

# Initializing a model from the "" style configuration
model = SeamlessM4Tv2Model(configuration)

# Accessing the model configuration
configuration = model.config
""",
        """CODE.from transformers import UnivNetModel, UnivNetConfig

# Initializing a Tortoise TTS style configuration
configuration = UnivNetConfig()

# Initializing a model (with random weights) from the Tortoise TTS style configuration
model = UnivNetModel(configuration)

# Accessing the model configuration
configuration = model.config
""",
        """CODE.import datasets
from transformers import ClvpProcessor, ClvpModelForConditionalGeneration

# Define the Text and Load the Audio (We are taking an audio example from HuggingFace Hub using `datasets` library)
text = "This is an example text."
ds = datasets.load_dataset("hf-internal-testing/librispeech_asr_dummy", "clean", split="validation")
ds = ds.cast_column("audio", datasets.Audio(sampling_rate=22050))
_, audio, sr = ds.sort("id").select(range(1))[:1]["audio"][0].values()

# Define processor and model
processor = ClvpProcessor.from_pretrained("susnato/clvp_dev")
model = ClvpModelForConditionalGeneration.from_pretrained("susnato/clvp_dev")

# Generate processor output and model output
processor_output = processor(raw_speech=audio, sampling_rate=sr, text=text, return_tensors="pt")
speech_embeds = model.get_speech_features(
...     input_ids=processor_output["input_ids"], input_features=processor_output["input_features"]
... )
""" .

<DEPENDENCY.transformers==4.37.0> <CONTAINS> """CODE.from transformers import (
    FastSpeech2ConformerConfig,
    FastSpeech2ConformerHifiGanConfig,
    FastSpeech2ConformerWithHifiGanConfig,
    FastSpeech2ConformerWithHifiGan,
)

# Initializing FastSpeech2ConformerWithHifiGan sub-modules configurations.
model_config = FastSpeech2ConformerConfig()
vocoder_config = FastSpeech2ConformerHifiGanConfig()

# Initializing a FastSpeech2ConformerWithHifiGan module style configuration
configuration = FastSpeech2ConformerWithHifiGanConfig(model_config.to_dict(), vocoder_config.to_dict())

# Initializing a model (with random weights)
model = FastSpeech2ConformerWithHifiGan(configuration)

# Accessing the model configuration
configuration = model.config
""",
        """CODE.from transformers import FastSpeech2ConformerHifiGan, FastSpeech2ConformerHifiGanConfig

# Initializing a FastSpeech2ConformerHifiGan configuration
configuration = FastSpeech2ConformerHifiGanConfig()

# Initializing a model (with random weights) from the configuration
model = FastSpeech2ConformerHifiGan(configuration)

# Accessing the model configuration
configuration = model.config
""",
        """CODE.from transformers import Qwen2Model, Qwen2Config

# Initializing a Qwen2 style configuration
configuration = Qwen2Config()

# Initializing a model from the Qwen2-7B style configuration
model = Qwen2Model(configuration)

# Accessing the model configuration
configuration = model.config
""",
        """CODE.from transformers import Qwen2Tokenizer

tokenizer = Qwen2Tokenizer.from_pretrained("Qwen/Qwen-tokenizer")
tokenizer("Hello world")["input_ids"]
[9707, 1879]

tokenizer(" Hello world")["input_ids"]
[21927, 1879]
""",
        """CODE.from transformers import Qwen2TokenizerFast

tokenizer = Qwen2TokenizerFast.from_pretrained("Qwen/Qwen-tokenizer")
tokenizer("Hello world")["input_ids"]
[9707, 1879]

tokenizer(" Hello world")["input_ids"]
[21927, 1879]
""",
        """CODE.from transformers import SiglipConfig, SiglipModel

# Initializing a SiglipConfig with google/siglip-base-patch16-224 style configuration
configuration = SiglipConfig()

# Initializing a SiglipModel (with random weights) from the google/siglip-base-patch16-224 style configuration
model = SiglipModel(configuration)

# Accessing the model configuration
configuration = model.config

# We can also initialize a SiglipConfig from a SiglipTextConfig and a SiglipVisionConfig
from transformers import SiglipTextConfig, SiglipVisionConfig

# Initializing a SiglipText and SiglipVision configuration
config_text = SiglipTextConfig()
config_vision = SiglipVisionConfig()

config = SiglipConfig.from_text_vision_configs(config_text, config_vision)
""",
        """CODE.from transformers import SiglipTextConfig, SiglipTextModel

# Initializing a SiglipTextConfig with google/siglip-base-patch16-224 style configuration
configuration = SiglipTextConfig()

# Initializing a SiglipTextModel (with random weights) from the google/siglip-base-patch16-224 style configuration
model = SiglipTextModel(configuration)

# Accessing the model configuration
configuration = model.config
""",
        """CODE.from transformers import SiglipVisionConfig, SiglipVisionModel

# Initializing a SiglipVisionConfig with google/siglip-base-patch16-224 style configuration
configuration = SiglipVisionConfig()

# Initializing a SiglipVisionModel (with random weights) from the google/siglip-base-patch16-224 style configuration
model = SiglipVisionModel(configuration)

# Accessing the model configuration
configuration = model.config
""" .

<DEPENDENCY.transformers==4.4.0> <CONTAINS> """CODE.self._memory_tracker = TrainerMemoryTracker(self.args.skip_memory_metrics)
self._memory_tracker.start()
code ...
metrics = {"train_runtime": 10.5}
self._memory_tracker.stop_and_update_metrics(metrics)
""" .

<DEPENDENCY.transformers==4.6.0> <CONTAINS> """CODE.debug_overflow = DebugUnderflowOverflow(model)
debug_overflow = DebugUnderflowOverflow(model, max_frames_to_save=100)
debug_overflow = DebugUnderflowOverflow(model, trace_batch_nums=[1,3])
debug_overflow = DebugUnderflowOverflow(model, trace_batch_nums=[1,3], abort_after_batch_num=3)
""" .

<DEPENDENCY.transformers==4.8.0> <CONTAINS> """CODE.from transformers import BartTokenizer, FlaxBartForConditionalGeneration
model = FlaxBartForConditionalGeneration.from_pretrained('facebook/bart-large-cnn')
tokenizer = BartTokenizer.from_pretrained('facebook/bart-large-cnn')
text = "My friends are cool but they eat too many carbs."
inputs = tokenizer(text, max_length=1024, return_tensors='jax')
encoder_outputs = model.encode(**inputs)""",
        """CODE.model = FlaxBartForConditionalGeneration.from_pretrained('facebook/bart-large-cnn')
tokenizer = BartTokenizer.from_pretrained('facebook/bart-large-cnn')
text = "My friends are cool but they eat too many carbs."
inputs = tokenizer(text, max_length=1024, return_tensors='jax')
encoder_outputs = model.encode(**inputs)
decoder_start_token_id = model.config.decoder_start_token_id
decoder_input_ids = jnp.ones((inputs.input_ids.shape[0], 1), dtype="i4") * decoder_start_token_id
outputs = model.decode(decoder_input_ids, encoder_outputs)
last_decoder_hidden_states = outputs.last_hidden_state""" .

<DEPENDENCY.transformers==4.9.0> <CONTAINS> """CODE.decoder_start_token_id = model.config.decoder_start_token_id
decoder_input_ids = jnp.ones((inputs.input_ids.shape[0], 1), dtype="i4") * decoder_start_token_id

outputs = model.decode(decoder_input_ids, encoder_outputs)
last_decoder_hidden_states = outputs.last_hidden_state""",
        """CODE.from transformers import BertConfig, CLIPConfig, HybridCLIPConfig, FlaxHybridCLIP

config_text = BertConfig()
config_vision = CLIPConfig()

config = HybridCLIPConfig.from_text_vision_configs(config_text, config_vision, projection_dim=512)

model = EncoderDecoderModel(config=config)

config_text = model.config.text_config
config_vision = model.config.vision_config

model.save_pretrained('my-model')

encoder_decoder_config = HybridCLIPConfig.from_pretrained('my-model')
model = FlaxHybridCLIP.from_pretrained('my-model', config=encoder_decoder_config)
""",
        """CODE.from transformers import MarianTokenizer, FlaxMarianMTModel
tokenizer = MarianTokenizer.from_pretrained('facebook/marian-large-cnn')
model = FlaxMarianMTModel.from_pretrained('Helsinki-NLP/opus-mt-en-de')
text = "My friends are cool but they eat too many carbs."
inputs = tokenizer(text, max_length=64, return_tensors='jax')
encoder_outputs = model.encode(**inputs)""" .

<DEPENDENCY.urllib3==0.3.1> <CONTAINS> """CODE.fields = {
    'foo': 'bar',
    'foofile': ('foofile.txt', 'contents of foofile'),
}

body, content_type = encode_multipart_formdata(fields)""" .

<DEPENDENCY.urllib3==1.10> <CONTAINS> """CODE.from urllib3.util import ssl_
context = ssl_.create_urllib3_context()
context.options &= ~ssl_.OP_NO_SSLv3
""" .

<DEPENDENCY.urllib3==1.25.2> <CONTAINS> """CODE... code-block:: python

    URIBuilder().add_scheme('HTTPS')""",
        """CODE.URIBuilder().add_credentials('root', 's3crete')
URIBuilder().add_credentials('root', None)""" .

<DEPENDENCY.urllib3==1.25.3> <CONTAINS> """CODE... code-block:: python

    URIBuilder().add_scheme('HTTPS')""",
        """CODE.URIBuilder().add_credentials('root', 's3crete')
URIBuilder().add_credentials('root', None)""" .

<DEPENDENCY.urllib3==1.7> <CONTAINS> """CODE.d = _parse_header("CD: fd; foo=\\"bar\\"; file*=utf-8''T%C3%A4st")[1]
d['file'] == 'T\\u00e4st'
d['foo']
'bar'""" .

<DEPENDENCY.urllib3==1.7.1> <CONTAINS> """CODE.timeout = urllib3.util.Timeout(connect=2.0, read=7.0)
pool = HTTPConnectionPool('www.google.com', 80, timeout=timeout)
pool.request(...)
""" .

<DEPENDENCY.urllib3==1.8> <CONTAINS> """CODE.headers = HTTPHeaderDict(foo='bar')
headers.add('Foo', 'baz')
headers['foo']
'bar, baz'""" .

<DEPENDENCY.urllib3==1.9> <CONTAINS> """CODE.retries = Retry(connect=5, read=2, redirect=5)
http = PoolManager(retries=retries)
response = http.request('GET', 'http://example.com/')

response = http.request('GET', 'http://example.com/', retries=Retry(10))

response = http.request('GET', 'http://example.com/', retries=False)
""" .

<DEPENDENCY.wandb==0.10.30> <CONTAINS> """CODE.# this line initializes the sweep
sweep_id = wandb.sweep({'name': 'my-awesome-sweep',
                        'metric': 'accuracy',
                        'method': 'grid',
                        'parameters': {'a': {'values': [1, 2, 3, 4]}}})

# this line actually runs it -- parameters are available to
# my_train_func via wandb.config
wandb.agent(sweep_id, function=my_train_func)
""" .

<DEPENDENCY.wandb==0.10.5> <CONTAINS> "CODE.wandb.log({'histogram-plot1': wandb.plot.histogram(table, \"height\")})" .

<DEPENDENCY.wandb==0.11.0> <CONTAINS> """CODE.AddressType = GraphQLObjectType('Address', {
    'street': GraphQLField(GraphQLString),
    'number': GraphQLField(GraphQLInt),
    'formatted': GraphQLField(GraphQLString,
        resolver=lambda obj, args, context, info: obj.number + ' ' + obj.street),
})

PersonType = GraphQLObjectType('Person', lambda: {
    'name': GraphQLField(GraphQLString),
    'bestFriend': GraphQLField(PersonType)
})""",
        """CODE.EntityType = GraphQLInterfaceType(
    name='Entity',
    fields={
        'name': GraphQLField(GraphQLString),
    })""",
        """CODE.RGBType = GraphQLEnumType(
    name='RGB',
    values=OrderedDict([
        ('RED', GraphQLEnumValue(0)),
        ('GREEN', GraphQLEnumValue(1)),
        ('BLUE', GraphQLEnumValue(2))
    ])
)""",
        """CODE.class GeoPoint(GraphQLInputObjectType):
    name = 'GeoPoint'
    fields = {
        'lat': GraphQLInputObjectField(NonNullFloat),
        'lon': GraphQLInputObjectField(NonNullFloat),
        'alt': GraphQLInputObjectField(GraphQLFloat(),
            default_value=0)""",
        """CODE.class PersonType(GraphQLObjectType):
    name = 'Person'

    def get_fields(self):
        return {
            'parents': GraphQLField(GraphQLList(PersonType())),
            'children': GraphQLField(GraphQLList(PersonType())),
        }""",
        """CODE.class PetType(GraphQLUnionType):
    name = 'Pet'
    types = [DogType, CatType]

    def resolve_type(self, value):
        if isinstance(value, Dog):
            return DogType()
        if isinstance(value, Cat):
            return CatType()""",
        """CODE.def coerce_odd(value):
    if value % 2 == 1:
        return value
    return None

OddType = GraphQLScalarType(name='Odd', serialize=coerce_odd)""" .

<DEPENDENCY.wandb==0.11.2> <CONTAINS> """CODE.AddressType = GraphQLObjectType('Address', {
    'street': GraphQLField(GraphQLString),
    'number': GraphQLField(GraphQLInt),
    'formatted': GraphQLField(GraphQLString,
        resolver=lambda obj, args, context, info: obj.number + ' ' + obj.street),
})

PersonType = GraphQLObjectType('Person', lambda: {
    'name': GraphQLField(GraphQLString),
    'bestFriend': GraphQLField(PersonType)
})""",
        """CODE.EntityType = GraphQLInterfaceType(
    name='Entity',
    fields={
        'name': GraphQLField(GraphQLString),
    })""",
        """CODE.def coerce_odd(value):
    if value % 2 == 1:
        return value
    return None

OddType = GraphQLScalarType(name='Odd', serialize=coerce_odd)""" .

<DEPENDENCY.wandb==0.12.11> <CONTAINS> """CODE.def add(a: float, b: float) -> float:
    \"\"\"Returns sum of two arguments\"\"\"
    return a + b

add_op = create_component_from_func(
    func=add,
    base_image='python:3.7', # Optional
    output_component_file='add.component.yaml', # Optional
    packages_to_install=['pandas==0.24'], # Optional
)

add_op.component_spec.save('add.component.yaml')

add_task = add_op(1, 3)

sum_output_ref = add_task.outputs['Output']

task2 = add_op(sum_output_ref, 5)

from typing import NamedTuple

def add_multiply_two_numbers(a: float, b: float) -> NamedTuple('Outputs', [('sum', float), ('product', float)]):
    \"\"\"Returns sum and product of two arguments\"\"\"
    return (a + b, a * b)

add_multiply_op = create_component_from_func(add_multiply_two_numbers)

add_multiply_task = add_multiply_op(1, 3)

sum_output_ref = add_multiply_task.outputs['sum']

task2 = add_multiply_op(sum_output_ref, 5)

def catboost_train_classifier(
    training_data_path: InputPath('CSV'),            # Path to input data file of type "CSV"
    trained_model_path: OutputPath('CatBoostModel'), # Path to output data file of type "CatBoostModel"
    number_of_trees: int = 100,                      # Small output of type "Integer"
) -> NamedTuple('Outputs', [
    ('Accuracy', float),  # Small output of type "Float"
    ('Precision', float), # Small output of type "Float"
    ('JobUri', 'URI'),    # Small output of type "URI"
]):
    \"\"\"Trains CatBoost classification model\"\"\"
    ...

    return (accuracy, precision, recall)
""" .

<DEPENDENCY.wandb==0.12.12> <CONTAINS> """CODE.# assuming you have previously logged a model with the name "my-simple-model"
sm = use_model("my-simple-model:latest")
model = sm.model_obj()
""",
        """CODE.def _resolve_aliases(aliases):
    if isinstance(aliases, str):
        aliases = [aliases]
    elif aliases is None:
        aliases = []
    aliases.append("latest")
    return aliases

aliases = _resolve_aliases(["best", "dev"])
assert aliases == ["best", "dev", "latest"]

aliases = _resolve_aliases("boom")
assert aliases == ["boom", "latest"]
""",
        """CODE.import torch.nn as nn
import torch.nn.functional as F

class Net(nn.Module):
    def __init__(self):
        super(Net, self).__init__()
        self.fc1 = nn.Linear(10, 10)

    def forward(self, x):
        x = self.fc1(x)
        x = F.relu(x)
        return x

model = Net()
sm = log_model(model, "my-simple-model", aliases=["best"])""",
        """CODE.import wandb
from wandb.integration.kfp.helpers import add_wandb_visualization

with wandb.init() as run:
    add_wandb_visualization(run, mlpipeline_ui_metadata_path)

    ... # the rest of your code here""",
        "CODE.link_model(sm, \"my-portfolio\")" .

<DEPENDENCY.wandb==0.12.3> <CONTAINS> """CODE.%wandb USERNAME/PROJECT/runs/RUN_ID
---
%%wandb -h 1024
with wandb.init() as run:
    run.log({"loss": 1})""",
        """CODE.parameter = HyperParameter('a', {'values': [1, 2, 3]})
assert parameter.value_to_int(2) == 1""",
        """CODE.run = SweepRun(
  name="my_run",
  state=RunState.running,
  config={"a": {"value": 1}},
)""",
        """CODE.run = SweepRun(history=[{'a': 1}, {'b': 3}, {'a': 2, 'b': 4}], summary_metrics={'a': 50})
assert run.metric_extremum('a', 'maximum') == 50""",
        """CODE.sweep_config = {'method': 'grid', 'parameters': {'a': {'values': [1, 2, 3]}}}
hps = HyperParameterSet.from_config(sweep_config['parameters'])""",
        """CODE.with torch.profiler.profile(
    schedule=torch.profiler.schedule(wait=1, warmup=1, active=3, repeat=1),
    on_trace_ready=wandb.profiler.torch_trace_handler(),
    record_shapes=True,
    with_stack=True,
) as prof:
    for i, batch in enumerate(dataloader):
        if step >= 5:
            break
        train(batch)
        prof.step()
""" .

<DEPENDENCY.wandb==0.15.0> <CONTAINS> """CODE.
from wandb.integration.langchain import WandbTracer
WandbTracer.init()
# ...
# end of notebook / script:
WandbTracer.finish()
""" .

<DEPENDENCY.wandb==0.16.3> <CONTAINS> """CODE.self.logger.experiment.some_wandb_function()
""" .

<DEPENDENCY.wandb==0.8.23> <CONTAINS> "CODE.wandb.sklearn.plot_clusterer(kmeans, X_train, cluster_labels, labels, 'KMeans')",
        "CODE.wandb.sklearn.plot_confusion_matrix(y_true, y_probas, labels)",
        "CODE.wandb.sklearn.plot_feature_importances(model, ['width', 'height, 'length'])",
        "CODE.wandb.sklearn.plot_precision_recall(y_true, y_probas, labels)",
        "CODE.wandb.sklearn.plot_regressor(reg, X_train, X_test, y_train, y_test, 'Ridge')" .

<DEPENDENCY.wcwidth==0.2.6> <CONTAINS> """CODE.UnicodeVersion.parse("14.0.0")
UnicodeVersion(major=14, minor=0, micro=0)""" .

<DEPENDENCY.wcwidth==0.2.8> <CONTAINS> "CODE.make_table([0,1,2,5,6,7,9])" .

"""DESCRIPTION.- Print the batch number at the beginning of every batch.
- Stream the epoch loss to a file in JSON format.
- Terminate some processes after having finished model training.""" <EXPLAINS> """CODE.# Print the batch number at the beginning of every batch.
batch_print_callback = LambdaCallback(
    on_batch_begin=lambda batch,logs: print(batch))

# Stream the epoch loss to a file in JSON format. The file content
# is not well-formed JSON but rather has a JSON object per line.
import json
json_log = open('loss_log.json', mode='wt', buffering=1)
json_logging_callback = LambdaCallback(
    on_epoch_end=lambda epoch, logs: json_log.write(
        json.dumps({'epoch': epoch, 'loss': logs['loss']}) + '\\n'),
    on_train_end=lambda logs: json_log.close()
)

# Terminate some processes after having finished model training.
processes = ...
cleanup_callback = LambdaCallback(
    on_train_end=lambda logs: [
        p.terminate() for p in processes if p.is_alive()])

model.fit(...,
          callbacks=[batch_print_callback,
                     json_logging_callback,
                     cleanup_callback])
""" .

"""DESCRIPTION.1. Customizes the BatchSizeFinder callback to dynamically adjust the batch size at specified epochs during model fine-tuning.

2. Utilizes the BatchSizeFinder callback to determine the optimal batch size for validation, testing, or prediction tasks.""" <EXPLAINS> """CODE.# 1. Customize the BatchSizeFinder callback to run at different epochs. This feature is
# useful while fine-tuning models since you can't always use the same batch size after
# unfreezing the backbone.
from pytorch_lightning.callbacks import BatchSizeFinder


class FineTuneBatchSizeFinder(BatchSizeFinder):
    def __init__(self, milestones, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.milestones = milestones

    def on_fit_start(self, *args, **kwargs):
        return

    def on_train_epoch_start(self, trainer, pl_module):
        if trainer.current_epoch in self.milestones or trainer.current_epoch == 0:
            self.scale_batch_size(trainer, pl_module)


trainer = Trainer(callbacks=[FineTuneBatchSizeFinder(milestones=(5, 10))])
trainer.fit(...)

# 2. Run batch size finder for validate/test/predict.
from pytorch_lightning.callbacks import BatchSizeFinder


class EvalBatchSizeFinder(BatchSizeFinder):
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)

    def on_fit_start(self, *args, **kwargs):
        return

    def on_test_start(self, trainer, pl_module):
        self.scale_batch_size(trainer, pl_module)


trainer = Trainer(callbacks=[EvalBatchSizeFinder()])
trainer.test(...)
""" .

"DESCRIPTION.Adds a layer of experimental synchronous batch normalization to the model." <EXPLAINS> """CODE.model.add(tf.keras.layers.experimental.SyncBatchNormalization())
""" .

"DESCRIPTION.Assign the minimum value between x and y to the specified indices in x." <EXPLAINS> """CODE.x[idx] = minimum(x[idx], y)
x = jax.numpy.ones((5, 6))
jax.ops.index_min(x, jnp.index_exp[2:4, 3:], 0.)""" .

"DESCRIPTION.Assigns the value of the variable \"trial_name\" to the variable \"name\"." <EXPLAINS> "CODE.name = self.trial_name" .

"DESCRIPTION.Calculate and update the root mean squared error metric based on the input data and sample weight provided." <EXPLAINS> """CODE.m = tf.keras.metrics.RootMeanSquaredError()
m.update_state([[0, 1], [0, 0]], [[1, 1], [0, 0]])
m.result().numpy()
m.reset_state()
m.update_state([[0, 1], [0, 0]], [[1, 1], [0, 0]],
               sample_weight=[1, 0])
m.result().numpy()""" .

"DESCRIPTION.Calculate the Area Under the Curve (AUC) metric using the predicted values (y_pred) and the true values (y_true)." <EXPLAINS> """CODE.y_pred = torch.tensor([0, 1, 2, 3])
y_true = torch.tensor([0, 1, 2, 2])
metric = AUC()
metric(y_pred, y_true)""" .

"DESCRIPTION.Calculate the Area Under the Receiver Operating Characteristic Curve (AUROC) for the predicted values compared to the target values." <EXPLAINS> """CODE.pred = torch.tensor([0, 1, 2, 3])
target = torch.tensor([0, 1, 2, 2])
metric = AUROC()
metric(pred, target)
tensor(0.3333)""" .

"DESCRIPTION.Calculate the Area Under the Receiver Operating Characteristic Curve (AUROC) metric between the predicted values and the target values." <EXPLAINS> """CODE.pred = torch.tensor([0, 1, 2, 3])
target = torch.tensor([0, 1, 2, 2])
metric = AUROC()
metric(pred, target)
tensor(0.3333)""" .

"DESCRIPTION.Calculate the BLEU score between the provided translation corpus and reference corpus." <EXPLAINS> """CODE.translate_corpus = ['the cat is on the mat'.split()]
reference_corpus = [['there is a cat on the mat'.split(), 'a cat is on the mat'.split()]]
metric = BLEUScore()
metric(translate_corpus, reference_corpus)""" .

"DESCRIPTION.Calculate the Bessel function of the first kind of order 1 for the input array [0.5, 1., 2., 4.]." <EXPLAINS> "CODE.tf.math.special.bessel_j1([0.5, 1., 2., 4.]).numpy()" .

"DESCRIPTION.Calculate the Dawson's integral for a set of input values." <EXPLAINS> "CODE.tf.math.special.dawsn([-1., -0.5, 0.5, 1.]).numpy()" .

"DESCRIPTION.Calculate the F-beta score with beta value of 0.25 between the predicted tensor and the target tensor." <EXPLAINS> """CODE.pred = torch.tensor([0, 1, 2, 3])
target = torch.tensor([0, 1, 2, 2])
metric = FBeta(0.25)
metric(pred, target)
tensor(0.7361)""" .

"DESCRIPTION.Calculate the F-beta score with beta=0.25 between the predicted tensor and the target tensor." <EXPLAINS> """CODE.pred = torch.tensor([0, 1, 2, 3])
target = torch.tensor([0, 1, 2, 2])
metric = FBeta(0.25)
metric(pred, target)
tensor(0.7361)""" .

"DESCRIPTION.Calculate the F-beta score with beta=0.25 between the predicted values and the target values." <EXPLAINS> """CODE.pred = torch.tensor([0, 1, 2, 3])
target = torch.tensor([0, 1, 2, 2])
metric = FBeta(0.25)
metric(pred, target)
tensor(0.7361)""" .

"DESCRIPTION.Calculate the F1 score between the predicted values (pred) and the target values (target)." <EXPLAINS> """CODE.pred = torch.tensor([0, 1, 2, 3])
target = torch.tensor([0, 1, 2, 2])
metric = F1()
metric(pred, target)
tensor(0.6667)""" .

"DESCRIPTION.Calculate the F1 score metric for the predicted values compared to the target values." <EXPLAINS> """CODE.pred = torch.tensor([0, 1, 2, 3])
target = torch.tensor([0, 1, 2, 2])
metric = F1()
metric(pred, target)
tensor(0.6667)""" .

"DESCRIPTION.Calculate the Hamming loss between the predicted tensor y_pred and the true tensor y_true." <EXPLAINS> """CODE.y_pred = torch.tensor([0, 1, 2, 3])
y_true = torch.tensor([1, 1, 2, 3])
metric = Hamming()
metric(y_pred, y_true)
tensor([0.2500])""" .

"DESCRIPTION.Calculate the Hinge loss between the predicted decisions and the true labels." <EXPLAINS> """CODE.pred_decision = torch.tensor([-2.17, -0.97, -0.19, -0.43])
y_true = torch.tensor([1, 1, 0, 0])
metric = Hinge()
metric(pred_decision, y_true)
tensor([1.6300])""" .

"DESCRIPTION.Calculate the Intersection over Union (IoU) metric between the predicted and target tensors representing binary segmentation masks." <EXPLAINS> """CODE.pred = torch.tensor([[0, 0, 0, 0, 0, 0, 0, 0],
                      [0, 0, 1, 1, 1, 0, 0, 0],
                      [0, 0, 0, 0, 0, 0, 0, 0]])
target = torch.tensor([[0, 0, 0, 0, 0, 0, 0, 0],
                        [0, 0, 0, 1, 1, 1, 0, 0],
                        [0, 0, 0, 0, 0, 0, 0, 0]])
metric = IoU()
metric(pred, target)""" .

"DESCRIPTION.Calculate the Jaccard similarity between the predicted tensor y_pred and the true tensor y_true." <EXPLAINS> """CODE.y_pred = torch.tensor([1, 1, 1])
y_true = torch.tensor([0, 1, 1])
metric = Jaccard()
metric(y_pred, y_true)
tensor([0.3333])""" .

"DESCRIPTION.Calculate the Jaccard similarity coefficient between the predicted tensor and true tensor." <EXPLAINS> """CODE.y_pred = torch.tensor([1, 1, 1])
y_true = torch.tensor([0, 1, 1])
metric = Jaccard()
metric(y_pred, y_true)
tensor([0.3333])""" .

"DESCRIPTION.Calculate the Kullback-Leibler divergence between two Cauchy distributions with different parameters." <EXPLAINS> """CODE.import paddle
from paddle.distribution import Cauchy

rv = Cauchy(loc=0.1, scale=1.2)
rv_other = Cauchy(loc=paddle.to_tensor(1.2), scale=paddle.to_tensor([2.3, 3.4]))
print(rv.kl_divergence(rv_other))""" .

"DESCRIPTION.Calculate the Root Mean Squared Logarithmic Error (RMSLE) between the predicted values and the target values." <EXPLAINS> """CODE.pred = torch.tensor([0., 1, 2, 3])
target = torch.tensor([0., 1, 2, 2])
metric = RMSLE()
metric(pred, target)
tensor(0.0207)""" .

"DESCRIPTION.Calculate the Sparse Categorical Crossentropy loss between the predicted probabilities and the true labels." <EXPLAINS> """CODE.cce = keras.losses.SparseCategoricalCrossentropy()
loss = cce(
    [0, 1, 2],
    [[.9, .05, .05], [.5, .89, .6], [.05, .01, .94]])


model = keras.Model(inputs, outputs)
model.compile('sgd', loss=keras.losses.SparseCategoricalCrossentropy())
""" .

"DESCRIPTION.Calculate the Sparse Categorical Crossentropy loss between the true labels [0, 1, 2] and the predicted probabilities [[.9, .05, .05], [.5, .89, .6], [.05, .01, .94]]. This loss function is commonly used in classification tasks to measure the difference between the true labels and the model predictions." <EXPLAINS> """CODE.cce = keras.losses.SparseCategoricalCrossentropy()
loss = cce(
    [0, 1, 2],
    [[.9, .05, .05], [.5, .89, .6], [.05, .01, .94]])


model = keras.Model(inputs, outputs)
model.compile('sgd', loss=keras.losses.SparseCategoricalCrossentropy())
""" .

"DESCRIPTION.Calculate the Structural Similarity Index (SSIM) between the predicted values (preds) and the target values (target) in the specified dimensions." <EXPLAINS> """CODE.from pytorch_lightning.metrics import SSIM
preds = torch.rand([16, 1, 16, 16])
target = preds * 0.75
ssim = SSIM()
ssim(preds, target)""" .

"DESCRIPTION.Calculate the Structural Similarity Index (SSIM) metric between the predicted tensor and the target tensor." <EXPLAINS> """CODE.pred = torch.rand([16, 1, 16, 16])
target = pred * 0.75
metric = SSIM()
metric(pred, target)""" .

"DESCRIPTION.Calculate the accuracy of the predictions compared to the target values." <EXPLAINS> """CODE.pred = torch.tensor([0, 1, 2, 3])
target = torch.tensor([0, 1, 2, 2])
metric = Accuracy()
metric(pred, target)
tensor(0.7500)""" .

"DESCRIPTION.Calculate the area under the curve (AUC) using the values in tensors x and y." <EXPLAINS> """CODE.x = torch.tensor([0, 1, 2, 3])
y = torch.tensor([0, 1, 2, 2])
auc(x, y)
""" .

"DESCRIPTION.Calculate the average precision score between the predicted tensor and the target tensor." <EXPLAINS> """CODE.pred = torch.tensor([0, 1, 2, 3])
target = torch.tensor([0, 1, 2, 2])
metric = AveragePrecision()
metric(pred, target)
tensor(0.3333)""" .

"DESCRIPTION.Calculate the average precision score between the predicted tensor values and the target tensor values." <EXPLAINS> """CODE.pred = torch.tensor([0, 1, 2, 3])
target = torch.tensor([0, 1, 2, 2])
metric = AveragePrecision()
metric(pred, target)
tensor(0.3333)""" .

"DESCRIPTION.Calculate the balanced accuracy of the predicted values y_pred compared to the true values y_true." <EXPLAINS> """CODE.y_pred = torch.tensor([0, 0, 0, 1])
y_true = torch.tensor([0, 0, 1, 1])
metric = BalancedAccuracy()
metric(y_pred, y_true)
tensor([0.7500])""" .

"DESCRIPTION.Calculate the balanced accuracy score for the predicted classification labels compared to the true classification labels." <EXPLAINS> """CODE.from sklearn.metrics import balanced_accuracy_score
y_true = [0, 1, 0, 0, 1, 0]
y_pred = [0, 1, 0, 0, 0, 1]
balanced_accuracy_score(y_true, y_pred)""" .

"DESCRIPTION.Calculate the batch dot product of x_batch and y_batch matrices along the specified axes." <EXPLAINS> """CODE.inner_products = []
for xi, yi in zip(x, y):
    inner_products.append(xi.dot(yi))
result = stack(inner_products)


x_batch = K.ones(shape=(32, 20, 1))
y_batch = K.ones(shape=(32, 30, 20))
xy_batch_dot = K.batch_dot(x_batch, y_batch, axes=(1, 2))
K.int_shape(xy_batch_dot)
(32, 1, 30)
""" .

"DESCRIPTION.Calculate the cosine similarity between two sets of vectors y_true and y_pred along axis 1 and return the calculated cosine similarity values." <EXPLAINS> """CODE.y_true = [[0., 1.], [1., 1.], [1., 1.]]
y_pred = [[1., 0.], [1., 1.], [-1., -1.]]
loss = tf.keras.losses.cosine_similarity(y_true, y_pred, axis=1)
loss.numpy()
array([-0., -0.999, 0.999], dtype=float32)""" .

"DESCRIPTION.Calculate the entropy of a geometric distribution with a success probability of 0.5." <EXPLAINS> """CODE.import paddle
from paddle.distribution import Geometric

geom = Geometric(0.5)
geom.entropy()""" .

"DESCRIPTION.Calculate the exponential integral function for the input array elements." <EXPLAINS> "CODE.tf.math.special.expint([1., 1.1, 2.1, 4.1]).numpy()" .

"DESCRIPTION.Calculate the exponentially scaled modified Bessel function of the first kind of order 1 for the values in the provided array." <EXPLAINS> "CODE.tf.math.special.bessel_i1e([-1., -0.5, 0.5, 1.]).numpy()" .

"DESCRIPTION.Calculate the gradient of the input image in the y-direction and then access the gradient values for the first channel of the first image in the y-direction." <EXPLAINS> """CODE.image = torch.arange(0, 1*1*5*5, dtype=torch.float32)
image = torch.reshape(image, (1, 1, 5, 5))
dy, dx = image_gradients(image)
dy[0, 0, :, :]""" .

"DESCRIPTION.Calculate the hash value of the file located at '/path/to/file.zip'." <EXPLAINS> """CODE.from keras.utils.data_utils import _hash_file
_hash_file('/path/to/file.zip')
""" .

"DESCRIPTION.Calculate the log probability of a given value according to a Laplace distribution with mean 0.0 and scale 1.0." <EXPLAINS> """CODE.import paddle

m = paddle.distribution.Laplace(paddle.to_tensor(0.0), paddle.to_tensor(1.0)
value = paddle.to_tensor(0.1)
m.log_prob(value)""" .

"DESCRIPTION.Calculate the loss using Sparse Categorical Crossentropy for the given true labels and predicted probabilities. Build a neural network model with the specified inputs, outputs, and optimizer, using Sparse Categorical Crossentropy loss function for training." <EXPLAINS> """CODE.cce = keras.losses.SparseCategoricalCrossentropy()
loss = cce(
    [0, 1, 2],
    [[.9, .05, .05], [.5, .89, .6], [.05, .01, .94]])


model = keras.Model(inputs, outputs)
model.compile('sgd', loss=keras.losses.SparseCategoricalCrossentropy())
""" .

"DESCRIPTION.Calculate the mean Tweedie deviance between the predicted values y_pred and the true values y_true." <EXPLAINS> """CODE.y_pred = torch.tensor([2, 0.5, 1, 4])
y_true = torch.tensor([0.5, 0.5, 2., 2.])
metric = MeanTweedieDeviance()
metric(y_pred, y_true)
tensor([1.8125])""" .

"DESCRIPTION.Calculate the mean absolute error (MAE) between the two input tensors x and y, where the value of the MAE is 0.2500." <EXPLAINS> """CODE.mae(x, y)
tensor(0.2500)""" .

"DESCRIPTION.Calculate the mean absolute error (MAE) between two tensors x and y." <EXPLAINS> """CODE.mae = nn.L1Loss(reduction='mean')
x = torch.tensor([0., 1, 2, 3])
y = torch.tensor([0., 1, 2, 2])
mae_value = mae(x, y)
print(mae_value)""",
        """CODE.x = torch.tensor([0., 1, 2, 3])
y = torch.tensor([0., 1, 2, 2])
mean_absolute_error(x, y)
tensor(0.2500)""" .

"DESCRIPTION.Calculate the mean absolute error between true and predicted values for two sets of data, one-dimensional and multi-dimensional." <EXPLAINS> """CODE.from sklearn.metrics import mean_absolute_error
y_true = [3, -0.5, 2, 7]
y_pred = [2.5, 0.0, 2, 8]
mean_absolute_error(y_true, y_pred)
y_true = [[0.5, 1], [-1, 1], [7, -6]]
y_pred = [[0, 2], [-1, 2], [8, -5]]
mean_absolute_error(y_true, y_pred)
""" .

"DESCRIPTION.Calculate the mean absolute percentage error between two arrays y_true and y_pred." <EXPLAINS> """CODE.y_true = np.random.random(size=(2, 3))
y_true = np.maximum(y_true, 1e-7)  # Prevent division by zero
y_pred = np.random.random(size=(2, 3))
loss = tf.keras.losses.mean_absolute_percentage_error(y_true, y_pred)
assert loss.shape == (2,)
assert np.array_equal(
...     loss.numpy(),
...     100. * np.mean(np.abs((y_true - y_pred) / y_true), axis=-1))""" .

"DESCRIPTION.Calculate the mean and variance of data points in each row of an array, normalize the data points, and then apply gamma and beta scaling to each data point." <EXPLAINS> """CODE.data = tf.constant(np.arange(10).reshape(5, 2) * 10, dtype=tf.float32)
print(data)


layer = tf.keras.layers.LayerNormalization(axis=1)
output = layer(data)
print(output)


mean_i = sum(x_i[j] for j in range(k)) / k
var_i = sum((x_i[j] - mean_i) ** 2 for j in range(k)) / k


x_i_normalized = (x_i - mean_i) / sqrt(var_i + epsilon)


output_i = x_i_normalized * gamma + beta
""" .

"DESCRIPTION.Calculate the mean relative error between two sets of torch tensors x and y." <EXPLAINS> """CODE.x = torch.tensor([0., 1, 2, 3])
y = torch.tensor([0., 1, 2, 2])
mean_relative_error(x, y)""" .

"DESCRIPTION.Calculate the mean squared error (MSE) between the predicted values 'pred' and the target values 'target'." <EXPLAINS> """CODE.pred = torch.tensor([0., 1, 2, 3])
target = torch.tensor([0., 1, 2, 2])
metric = MSE()
metric(pred, target)""" .

"DESCRIPTION.Calculate the mean squared error (MSE) between two tensors x and y." <EXPLAINS> """CODE.x = torch.tensor([0., 1, 2, 3])

y = torch.tensor([0., 1, 2, 2])

mean_squared_error(x, y)""",
        """CODE.x = torch.tensor([0., 1, 2, 3])

y = torch.tensor([0., 1, 2, 2])

mse(x, y)""" .

"DESCRIPTION.Calculate the mean squared error between two arrays x and y." <EXPLAINS> "CODE.mean_squared_error(x, y)" .

"DESCRIPTION.Calculate the mean squared log error between the predicted values x and actual values y." <EXPLAINS> """CODE.mean_squared_log_error(x, y)
tensor(0.0207)""" .

"DESCRIPTION.Calculate the median absolute error between the predicted values (y_pred) and the true values (y_true)." <EXPLAINS> """CODE.y_pred = torch.tensor([2.5, 0.0, 2, 8])
y_true = torch.tensor([3, -0.5, 2, 7])
metric = MedianAbsoluteError()
metric(y_pred, y_true)""" .

"DESCRIPTION.Calculate the median absolute error between two sets of values." <EXPLAINS> """CODE.from sklearn.metrics import median_absolute_error
y_true = [3, -0.5, 2, 7]
y_pred = [2.5, 0.0, 2, 8]
median_absolute_error(y_true, y_pred)""" .

"DESCRIPTION.Calculate the modified Bessel function of the second kind of order 0 for the input values [0.5, 1., 2., 4.]." <EXPLAINS> "CODE.tf.math.special.bessel_k0([0.5, 1., 2., 4.]).numpy()" .

"DESCRIPTION.Calculate the modified Bessel function of the second kind of order 1 for the given input values." <EXPLAINS> "CODE.tf.math.special.bessel_k1([0.5, 1., 2., 4.]).numpy()" .

"DESCRIPTION.Calculate the modified Bessel function of the second kind of order 1 for the input values 0.5, 1.0, 2.0, and 4.0." <EXPLAINS> "CODE.tf.math.special.bessel_k1e([0.5, 1., 2., 4.]).numpy()" .

"DESCRIPTION.Calculate the paired euclidean distances between the points in arrays X and Y." <EXPLAINS> """CODE.from sklearn.metrics.pairwise import paired_distances
X = [[0, 1], [1, 1]]
Y = [[0, 1], [2, 1]]
paired_distances(X, Y)
""" .

"DESCRIPTION.Calculate the precision metric for a classification task with 4 classes using the predicted values \"pred\" and the target values \"target\"." <EXPLAINS> """CODE.pred = torch.tensor([0, 1, 2, 3])
target = torch.tensor([0, 1, 2, 2])
metric = Precision(num_classes=4)
metric(pred, target)
tensor(0.7500)""" .

"DESCRIPTION.Calculate the product of all prime factors listed in the Counter dictionary and return the result." <EXPLAINS> """CODE.c = Counter('ABCABC')
sorted(c.elements())
['A', 'A', 'B', 'B', 'C', 'C']

prime_factors = Counter({2: 2, 3: 3, 17: 1})
product = 1
for factor in prime_factors.elements():
    product *= factor
product
1836""" .

"DESCRIPTION.Calculate the product of the shape dimensions of the input tensor x." <EXPLAINS> "CODE.np.prod(jax2tf.shape_as_value(x))" .

"DESCRIPTION.Calculate the recall metric for the predicted values compared to the target values." <EXPLAINS> """CODE.pred = torch.tensor([0, 1, 2, 3])
target = torch.tensor([0, 1, 2, 2])
metric = Recall()
metric(pred, target)
tensor(0.6250)""" .

"DESCRIPTION.Calculate the receiver operating characteristic area under the curve (ROC AUC) score of the predicted probabilities (y_scores) compared to the true binary labels (y_true)." <EXPLAINS> """CODE.import numpy as np
from sklearn.metrics import roc_auc_score
y_true = np.array([0, 0, 1, 1])
y_scores = np.array([0.1, 0.4, 0.35, 0.8])
roc_auc_score(y_true, y_scores)
0.75""" .

"DESCRIPTION.Calculate the root mean square energy of a given audio signal by computing the RMSE of its magnitude spectrogram." <EXPLAINS> """CODE.y, sr = librosa.load(librosa.util.example_audio_file())
librosa.feature.rmse(y=y)

S, phase = librosa.magphase(librosa.stft(y))
rms = librosa.feature.rmse(S=S)

S = librosa.magphase(librosa.stft(y, window=np.ones, center=False))[0]
librosa.feature.rmse(S=S)""" .

"DESCRIPTION.Calculate the root mean square energy of the audio signal y." <EXPLAINS> """CODE.y, sr = librosa.load(librosa.util.example_audio_file())
librosa.feature.rms(y=y)

S, phase = librosa.magphase(librosa.stft(y))
rms = librosa.feature.rms(S=S)

import matplotlib.pyplot as plt
plt.figure()
plt.subplot(2, 1, 1)
plt.semilogy(rms.T, label='RMS Energy')
plt.xticks([])
plt.xlim([0, rms.shape[-1]])
plt.legend(loc='best')
plt.subplot(2, 1, 2)
librosa.display.specshow(librosa.amplitude_to_db(S, ref=np.max),
...                          y_axis='log', x_axis='time')
plt.title('log Power spectrogram')
plt.tight_layout()

S = librosa.magphase(librosa.stft(y, window=np.ones, center=False))[0]
librosa.feature.rms(S=S)""",
        """CODE.y, sr = librosa.load(librosa.util.example_audio_file())
librosa.feature.rmse(y=y)

S, phase = librosa.magphase(librosa.stft(y))
rms = librosa.feature.rmse(S=S)

S = librosa.magphase(librosa.stft(y, window=np.ones, center=False))[0]
librosa.feature.rmse(S=S)""" .

"DESCRIPTION.Calculate the root mean squared error between two torch tensors x and y." <EXPLAINS> """CODE.x = torch.tensor([0., 1, 2, 3])
y = torch.tensor([0., 1, 2, 2])
rmse(x, y)
tensor(0.5000)""" .

"DESCRIPTION.Calculate the root mean squared logarithmic error (RMSLE) between two arrays x and y." <EXPLAINS> "CODE.rmsle(x, y)" .

"DESCRIPTION.Calculate the top k categorical accuracy between the true and predicted values for each sample in the input arrays y_true and y_pred." <EXPLAINS> """CODE.y_true = [[0, 0, 1], [0, 1, 0]]
y_pred = [[0.1, 0.9, 0.8], [0.05, 0.95, 0]]
m = tf.keras.metrics.top_k_categorical_accuracy(y_true, y_pred, k=3)
assert m.shape == (2,)
m.numpy()
array([1., 1.], dtype=float32)""" .

"DESCRIPTION.Calculate the trace of a complex matrix along specified dimensions with an offset of 1 and return a tensor with shape [3]." <EXPLAINS> """CODE.import paddle
import paddle.fluid.dygraph as dg
import numpy as np

case1 = np.random.randn(3, 10, 10).astype('float64') + 1j * np.random.randn(3, 10, 10).astype('float64')

with dg.guard():
    case1 = dg.to_variable(case1)
    data1 = paddle.complex.trace(case1, offset=1, dim1=1, dim2=2) # data1.shape = [3]""" .

"DESCRIPTION.Calculates the exponentially scaled modified Bessel function of the first kind, order 0, for a list of input values and returns the result as a numpy array." <EXPLAINS> """CODE.tf.math.special.bessel_i0e([-1., -0.5, 0.5, 1.]).numpy()
array([0.46575961, 0.64503527, 0.64503527, 0.46575961], dtype=float32)""" .

"DESCRIPTION.Check if the given file path is inside the specified special directory pattern." <EXPLAINS> """CODE._is_inside_unrequested_special_dir("__pycache__/b.txt", "**")
True
_is_inside_unrequested_special_dir("__pycache__/b.txt", "*/b.txt")
True
_is_inside_unrequested_special_dir("__pycache__/b.txt", "__pycache__/*")
False
_is_inside_unrequested_special_dir("__pycache__/b.txt", "__*/*")
False""" .

"DESCRIPTION.Checks if the input is a nested list-like object." <EXPLAINS> """CODE.is_nested_list_like([[1, 2, 3]])
is_nested_list_like([{1, 2, 3}, {1, 2, 3}])
is_nested_list_like(["foo"])
is_nested_list_like([])
is_nested_list_like([[1, 2, 3], 1])""" .

"DESCRIPTION.Combines the data from two datasets, with priority given to the data from the first dataset." <EXPLAINS> "CODE.a.combine_first(b)" .

"DESCRIPTION.Concatenate two arrays along the last axis." <EXPLAINS> """CODE.a = tf.constant([[1, 2, 3], [4, 5, 6], [7, 8, 9])
b = tf.constant([[10, 20, 30], [40, 50, 60], [70, 80, 90]])
tf.keras.backend.concatenate((a, b), axis=-1)""" .

"DESCRIPTION.Converts a given text sentence into a list of words." <EXPLAINS> """CODE.sample_text = 'This is a sample sentence.'
tf.keras.preprocessing.text.text_to_word_sequence(sample_text)""" .

"DESCRIPTION.Converts an array of time values to sample indices based on a given sample rate." <EXPLAINS> "CODE.librosa.time_to_samples(np.arange(0, 1, 0.1), sr=22050)" .

"DESCRIPTION.Converts the unsigned 32-bit integer value 0xFFFFFFFF to a signed 32-bit integer value." <EXPLAINS> """CODE.val = jnp.uint32(0xFFFFFFFF)
val.astype('int32')

_convert_and_clip_integer(val, 'int32')""" .

"DESCRIPTION.Display an explanation in a beta expander widget that includes a chart showing randomly generated numbers based on actual dice rolls." <EXPLAINS> """CODE.with st.beta_expander("See explanation"):
    st.write(\"\"\"
        The chart above shows some numbers I picked for you.
        I rolled actual dice for these, so they're *guaranteed* to
        be random.
    \"\"\")
    st.image("https://static.streamlit.io/examples/dice.jpg")""",
        """CODE.with st.expander("See explanation"):
    st.write(\"\"\"
        The chart above shows some numbers I picked for you.
        I rolled actual dice for these, so they're *guaranteed* to
        be random.
    \"\"\")
    st.image("https://static.streamlit.io/examples/dice.jpg")""" .

"DESCRIPTION.Display an explanation in a collapsible section that shows a chart of randomly generated numbers based on actual dice rolls. The explanation includes a statement that guarantees the randomness of the numbers." <EXPLAINS> """CODE.with st.beta_expander("See explanation"):
...     st.write(\"\"\"
...         The chart above shows some numbers I picked for you.
...         I rolled actual dice for these, so they're *guaranteed* to
...         be random.
...     \"\"\")
...     st.image("https://static.streamlit.io/examples/dice.jpg")""" .

"DESCRIPTION.Download a tokenizer configuration file from huggingface.co and cache it." <EXPLAINS> """CODE.# Download a tokenizer configuration from huggingface.co and cache.
tokenizer_config = get_file_from_repo("bert-base-uncased", "tokenizer_config.json")
# This model does not have a tokenizer config so the result will be None.
tokenizer_config = get_file_from_repo("xlm-roberta-base", "tokenizer_config.json")
""" .

"DESCRIPTION.Fill missing values in the array with the nearest non-missing value backwards with a limit of 1." <EXPLAINS> """CODE.arr = pd.array([np.nan, np.nan, 2, 3, np.nan, np.nan])
arr._pad_or_backfill(method="backfill", limit=1)""" .

"DESCRIPTION.Filter the array \"arr\" based on the \"mask\" array, keeping only the elements where the corresponding mask value is True. If the mask value is None, the behavior is controlled by the specified parameter \"null_selection_behavior\", which in this case is set to 'emit_null'." <EXPLAINS> """CODE.arr = pa.array(["a", "b", "c", None, "e"])
mask = pa.array([True, False, None, False, True])
arr.filter(mask)
arr.filter(mask, null_selection_behavior='emit_null')""" .

"DESCRIPTION.Generate 1000 random values from a logarithmically uniform distribution within the range of 0.001 to 10, compute the minimum and maximum values of these random values." <EXPLAINS> """CODE.from sklearn.utils.fixes import loguniform
rv = loguniform(1e-3, 1e1)
rvs = rv.rvs(random_state=42, size=1000)
rvs.min()  # doctest: +SKIP
rvs.max()  # doctest: +SKIP
""" .

"DESCRIPTION.Generate a 10x2x3 tensor of random values from a truncated normal distribution with specified mean, standard deviation, minimum values, and maximum values." <EXPLAINS> """CODE.means = 0.
stddevs = tf.math.exp(tf.random.uniform(shape=[2, 3]))
minvals = [-1., -2., -1000.]
maxvals = [[10000.], [1.]]
y = tf.random.stateless_parameterized_truncated_normal(
  shape=[10, 2, 3], seed=[7, 17],
  means=means, stddevs=stddevs, minvals=minvals, maxvals=maxvals)
y.shape""" .

"DESCRIPTION.Generate a sample from a Laplace distribution with mean 0 and scale 1 using the paddle library in Python." <EXPLAINS> """CODE.import paddle

m = paddle.distribution.Laplace(paddle.to_tensor(0.0), paddle.to_tensor(1.0))
m.sample()  # Laplace distributed with loc=0, scale=1
# Tensor(shape=[], dtype=float32, place=Place(cpu), stop_gradient=True,
#        3.68546247)""" .

"DESCRIPTION.Generate a sample from a Laplace distribution with mean 0 and scale 1." <EXPLAINS> """CODE.import paddle

m = paddle.distribution.Laplace(paddle.to_tensor([0.0]), paddle.to_tensor([1.0]))
m.rsample((1,))  # Laplace distributed with loc=0, scale=1
# Tensor(shape=[1, 1], dtype=float32, place=Place(cpu), stop_gradient=True,
# [[0.04337667]])""" .

"DESCRIPTION.Generate a sequence of text using the GPT-2 model starting from the given input sequence." <EXPLAINS> """CODE.from transformers import AutoModelForCausalLM, AutoTokenizer, TextStreamer

tok = AutoTokenizer.from_pretrained("gpt2")
model = AutoModelForCausalLM.from_pretrained("gpt2")
inputs = tok(["An increasing sequence: one,"], return_tensors="pt")
streamer = TextStreamer(tok)

_ = model.generate(**inputs, streamer=streamer, max_new_tokens=20)
""" .

"DESCRIPTION.Generate batches of size 3 from lists of numbers with lengths 7, 6, and 2, and return the batches as lists." <EXPLAINS> """CODE.from sklearn.utils import gen_batches
list(gen_batches(7, 3))
list(gen_batches(6, 3))
list(gen_batches(2, 3))""" .

"DESCRIPTION.Get the value at the specified row/column pair within a DataFrame." <EXPLAINS> """CODE.df = pd.DataFrame([[0, 2, 3], [0, 4, 1], [10, 20, 30]],
...                   columns=['A', 'B', 'C'])
df
    A   B   C
0   0   2   3
1   0   4   1
2  10  20  30

Get value at specified row/column pair

df.iat[1, 2]
1

Set value at specified row/column pair

df.iat[1, 2] = 10
df.iat[1, 2]
10

Get value within a series

df.loc[0].iat[1]""" .

"DESCRIPTION.I'm sorry, but I couldn't find any Python code provided." <EXPLAINS> "CODE.filters_to_expression([('foo', '==', 'bar')])" .

"DESCRIPTION.Merge two dataframes where values from dataframe 'a' are prioritized, using values from dataframe 'b' only to fill missing values in 'a'." <EXPLAINS> """CODE.a.combine_first(b)
    a's values prioritized, use values from b to fill holes""" .

"DESCRIPTION.Merge two dictionaries where the values of any keys that are present in both dictionaries are updated with the values from the second dictionary. If a key is present in one dictionary but not the other, it is simply added to the merged dictionary. The update is performed recursively for nested dictionaries." <EXPLAINS> """CODE.nested_update(
    {'a': 1, 'b': 2},
    {'b': 3, 'c': 4}
)

nested_update(
    {'x': {'a': 1, 'b': 2}, 'y': 5, 'z': 6},
    {'x': {'b': 3, 'c': 4}, 'z': 7, '0': 8},
)""" .

"DESCRIPTION.Remove non-ASCII characters from a given byte string." <EXPLAINS> """CODE.stripascii(b'string\\x00string\\n\\x01\\x00')
stripascii(b'\\x00')""" .

"DESCRIPTION.The \"outfeed_receiver\" must be started outside any jitted computation." <EXPLAINS> """CODE.with outfeed_receiver():
    jax.jit(func)(args)
    ...
    jax.pmap(another_func)(args)

The ``outfeed_receiver`` must be started outside any jitted computation.
""" .

"DESCRIPTION.The `Foo` class defines a function that calculates the gradient of the product of `x` and `y` with respect to `x` and `y`, using the `LearnScale` class as the model." <EXPLAINS> """CODE.class LearnScale(nn.Module):
    @nn.compact
    def __call__(self, x, y):
      p = self.param('scale', nn.initializers.zeros_init(), ())
      return p * x * y

class Foo(nn.Module):
    @nn.compact
    def __call__(self, x, y):
      x_grad, y_grad = nn.grad(
          lambda mdl, x, y: mdl(x, y), LearnScale(), x, y)
      return x_grad, y_grad
""" .

"DESCRIPTION.The `RandomContrast` class is a subclass of `BaseImageAugmentationLayer` and it is used to apply random contrast adjustment to an input image. The `factor` parameter specifies the range of contrast adjustment to be applied. Inside the `augment_image` method, a random factor is generated within the specified range, and the contrast of the input image is adjusted based on this random factor." <EXPLAINS> """CODE.class SubclassLayer(BaseImageAugmentationLayer):
  def __init__(self):
    super().__init__()
    self.auto_vectorize = False


class RandomContrast(BaseImageAugmentationLayer):

  def __init__(self, factor=(0.5, 1.5), **kwargs):
    super().__init__(**kwargs)
    self._factor = factor

  def augment_image(self, image, transformation):
    random_factor = tf.random.uniform([], self._factor[0], self._factor[1])
    mean = tf.math.reduced_mean(inputs, axis=-1, keep_dim=True)
    return (inputs - mean) * random_factor + mean
""" .

"DESCRIPTION.The `SpeechEncoderDecoderConfig` class is used to store the configuration of a Speech Encoder Decoder model. It is used to instantiate an Encoder-Decoder model by defining the configurations for both the encoder and the decoder." <EXPLAINS> """CODE.:class:`~transformers.SpeechEncoderDecoderConfig` is the configuration class to store the configuration of a
:class:`~transformers.SpeechEncoderDecoderModel`. It is used to instantiate an Encoder Decoder model according to
the specified arguments, defining the encoder and decoder configs.

Configuration objects inherit from :class:`~transformers.PretrainedConfig` and can be used to control the model
outputs. Read the documentation from :class:`~transformers.PretrainedConfig` for more information.

Args:
    kwargs (`optional`):
        Dictionary of keyword arguments. Notably:

            - **encoder** (:class:`~transformers.PretrainedConfig`, `optional`) -- An instance of a configuration
              object that defines the encoder config.
            - **decoder** (:class:`~transformers.PretrainedConfig`, `optional`) -- An instance of a configuration
              object that defines the decoder config.

Examples::

    from transformers import BertConfig, Wav2Vec2Config, SpeechEncoderDecoderConfig, SpeechEncoderDecoderModel

    # Initializing a Wav2Vec2 & BERT style configuration
    config_encoder = Wav2Vec2Config()
    config_decoder = BertConfig()

    config = SpeechEncoderDecoderConfig.from_encoder_decoder_configs(config_encoder, config_decoder)

    # Initializing a Wav2Vec2Bert model from a Wav2Vec2 & bert-base-uncased style configurations
    model = SpeechEncoderDecoderModel(config=config)

    # Accessing the model configuration
    config_encoder = model.config.encoder
    config_decoder  = model.config.decoder
    # set decoder config to causal lm
    config_decoder.is_decoder = True
    config_decoder.add_cross_attention = True

    # Saving the model, including its configuration
    model.save_pretrained('my-model')

    # loading model and config from pretrained folder
    encoder_decoder_config = SpeechEncoderDecoderConfig.from_pretrained('my-model')
    model = SpeechEncoderDecoderModel.from_pretrained('my-model', config=encoder_decoder_config)""" .

"DESCRIPTION.The code accelerates training of a model using Distributed Data Parallel (DDP) with uneven batch sizes." <EXPLAINS> """CODE.from accelerate import Accelerator

accelerator = Accelerator(even_batches=True)
ddp_model, optimizer, dataloader = accelerator.prepare(model, optimizer, dataloader)

with accelerator.join_uneven_inputs([ddp_model], even_batches=False):
...     for input, output in dataloader:
...         outputs = model(input)
...         loss = loss_func(outputs)
...         loss.backward()
...         optimizer.step()
...         optimizer.zero_grad()
""" .

"DESCRIPTION.The code accepts input tuples and returns specific values based on the structure of the tuple provided." <EXPLAINS> """CODE.thaw(s(1, 2))
thaw(v(1, m(a=3)))
thaw((1, v()))""" .

"DESCRIPTION.The code accesses datasets stored in an S3 bucket, both public and private, using different configurations and methods such as listing files, loading datasets, and saving datasets to the S3 bucket." <EXPLAINS> """CODE.import datasets
s3 = datasets.filesystems.S3FileSystem(anon=True)
s3.ls('public-datasets/imdb/train')

import datasets
s3 = datasets.filesystems.S3FileSystem(key=aws_access_key_id, secret=aws_secret_access_key)
s3.ls('my-private-datasets/imdb/train')

import botocore
from datasets.filesystems import S3Filesystem
s3_session = botocore.session.Session(profile_name='my_profile_name')
s3 = S3FileSystem(session=s3_session)

from datasets import load_from_disk
from datasets.filesystems import S3Filesystem
s3 = S3FileSystem(key=aws_access_key_id, secret=aws_secret_access_key)
dataset = load_from_disk('s3://my-private-datasets/imdb/train',fs=s3)
print(len(dataset))

from datasets import load_dataset
from datasets.filesystems import S3Filesystem
dataset = load_dataset("imdb")
s3 = S3FileSystem(key=aws_access_key_id, secret=aws_secret_access_key)
dataset.save_to_disk('s3://my-private-datasets/imdb/train',fs=s3)""" .

"DESCRIPTION.The code acquires a lock with a timeout of 20 seconds." <EXPLAINS> """CODE.with lock.acquire_(timeout = 20):
        pass""" .

"DESCRIPTION.The code adds a binding to the registry for the ControlI key. When the ControlI key is pressed, it will trigger the display_completions_like_readline function." <EXPLAINS> "CODE.registry.add_binding(Keys.ControlI)(display_completions_like_readline)" .

"DESCRIPTION.The code adds a dummy feature to each example in the input data array." <EXPLAINS> """CODE.from sklearn.preprocessing import add_dummy_feature
add_dummy_feature([[0, 1], [1, 0])
array([[ 1.,  0.,  1.],
       [ 1.,  1.,  0.]])""" .

"DESCRIPTION.The code adds a horizontal line to separate content in a Streamlit web app." <EXPLAINS> """CODE.import streamlit as st

st.divider()""" .

"DESCRIPTION.The code adds a layer of experimental SyncBatchNormalization to a neural network model." <EXPLAINS> """CODE.model.add(tf.keras.layers.experimental.SyncBatchNormalization())
""" .

"DESCRIPTION.The code adds a shared dense node to the model with the specified name and inputs from 'node_a' and 'node_b'. It also assigns the outputs of the dense node to 'dense_output_a' and 'dense_output_b'." <EXPLAINS> """CODE.model.add_shared_node(my_dense, name='shared_dense', inputs=['node_a', 'node_b'], ...)


model.add_shared_node(my_dense, name='shared_dense', inputs=['node_a', 'node_b'],
                      outputs=['dense_output_a', 'dense_outputs_b'])
""" .

"DESCRIPTION.The code adds custom scalars for the stock symbols 'twse/0050' and 'twse/2330' to a multiline chart on the writer." <EXPLAINS> "CODE.writer.add_custom_scalars_multilinechart(['twse/0050', 'twse/2330'])" .

"DESCRIPTION.The code adds initial observations and actions with rewards and next observations to a data collector for a given episode." <EXPLAINS> """CODE.collector.add_init_obs(my_episode, 0, "pol0", -1, obs)
collector.add_action_reward_next_obs(12345, 0, "pol0", False, {
    "action": action, "obs": obs, "reward": r, "done": done
})""" .

"DESCRIPTION.The code adds items to a collection on Hugging Face Hub related to climate data. The first item added is \"pierre-loic/climate-news-articles\" as a dataset. The second item added is \"datasets/climate_fever\" as a dataset with a note describing the dataset's methodology." <EXPLAINS> """CODE.from huggingface_hub import add_collection_item
collection = add_collection_item(
    collection_slug="davanstrien/climate-64f99dc2a5067f6b65531bab",
    item_id="pierre-loic/climate-news-articles",
    item_type="dataset"
)
collection.items[-1].item_id
"pierre-loic/climate-news-articles"

add_collection_item(
    collection_slug="davanstrien/climate-64f99dc2a5067f6b65531bab",
    item_id="datasets/climate_fever",
    item_type="dataset",
    note="This dataset adopts the FEVER methodology that consists of 1,535 real-world claims regarding climate-change collected on the internet."
)
""" .

"DESCRIPTION.The code adds new tokens to a BERT model's tokenizer and resizes the token embeddings of the model accordingly." <EXPLAINS> """CODE.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
    model = BertModel.from_pretrained('bert-base-uncased')

    num_added_toks = tokenizer.add_tokens(['new_tok1', 'my_new-tok2'])
    print('We have added', num_added_toks, 'tokens')
    model.resize_token_embeddings(len(tokenizer))""" .

"DESCRIPTION.The code adds new tokens to a BERT tokenizer and resizes the token embeddings of a BERT model to match the updated vocabulary size." <EXPLAINS> """CODE.tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')
model = BertModel.from_pretrained('bert-base-uncased')

num_added_toks = tokenizer.add_tokens(['new_tok1', 'my_new-tok2'])
print('We have added', num_added_toks, 'tokens')
model.resize_token_embeddings(len(tokenizer))  # Notice: resize_token_embeddings expect to receive the full size of the new vocabulary, i.e. the length of the tokenizer.""" .

"DESCRIPTION.The code adds scalar values for 'xsinx', 'xcosx', and 'arctanx' to the writer for the specified run, where the scalar values are calculated based on the current iteration 'i'." <EXPLAINS> """CODE.writer.add_scalars('run_14h',{'xsinx':i*np.sin(i/r),
                              'xcosx':i*np.cos(i/r),
                              'arctanx': numsteps*np.arctan(i/r)}, i)""" .

"DESCRIPTION.The code adds text descriptions to a writer object, with 'lstm' and 'rnn' as the keys and 'This is an lstm' and 'This is an rnn' as the corresponding text values. The texts are added at positions 0 and 10, respectively." <EXPLAINS> """CODE.writer.add_text('lstm', 'This is an lstm', 0)
writer.add_text('rnn', 'This is an rnn', 10)""" .

"DESCRIPTION.The code adds text labels to a writer object for 'lstm' and 'rnn' with corresponding descriptions and specific step values." <EXPLAINS> """CODE.writer.add_text('lstm', 'This is an lstm', 0)
writer.add_text('rnn', 'This is an rnn', 10)""" .

"DESCRIPTION.The code adds the integers 1 and 2 to the set 's'." <EXPLAINS> """CODE.s2 = s.add(1)
s3 = s.add(2)""" .

"DESCRIPTION.The code adjusts the brightness of a randomly generated image with shape (500, 500, 3) by applying a brightness transformation with a factor of 0.4." <EXPLAINS> """CODE.import numpy as np
from paddle.incubate.hapi.vision.transforms import BrightnessTransform
transform = BrightnessTransform(0.4)
fake_img = np.random.rand(500, 500, 3).astype('float32')
fake_img = transform(fake_img)
print(fake_img.shape)""" .

"DESCRIPTION.The code adjusts the learning rate of the model during training based on the validation loss, with a reduction factor of 0.2 after 5 epochs without improvement, ensuring a minimum learning rate of 0.001." <EXPLAINS> """CODE.reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2,
                              patience=5, min_lr=0.001)
model.fit(X_train, Y_train, callbacks=[reduce_lr])
""" .

"DESCRIPTION.The code adjusts the learning rate using natural exponential decay over time and uses gradient descent optimization to minimize a loss function with the updated learning rate." <EXPLAINS> """CODE.learning_rate = 0.1
decay_steps = 5
k = 0.5
learning_rate = tf.compat.v1.train.natural_exp_decay(learning_rate, global_step, decay_steps, k)

learning_step = (
    tf.compat.v1.train.GradientDescentOptimizer(learning_rate)
    .minimize(...my loss..., global_step=global_step)
)
""" .

"DESCRIPTION.The code aims to create a Torch DataLoader object from a given shard of data, which can be used for training a model using TorchTrainer." <EXPLAINS> """CODE.# create dummy data
spark.range(...).write.parquet(...)
# create MLDataset
data = ray.util.data.read_parquet(...)
# convert to TorchMLDataset
ds = data.to_torch(feature_columns=..., label_column=...)
# get the given shard data
shard = ds.get_shard(0)
# create the DataLoader from the shard data and this can be used for
# the TorchTrainer data creator as well
data = DataLoader(shard, batch_size=32)
""" .

"DESCRIPTION.The code aims to use the MT5 model to perform conditional generation by inputting an article in German and summarizing it into a shorter statement. The model generates the summary based on the input article using sequence-to-sequence preparation and outputs the loss function value as the evaluation metric." <EXPLAINS> """CODE.from transformers import MT5ForConditionalGeneration, T5Tokenizer
model = MT5ForConditionalGeneration.from_pretrained("google/mt5-small")
tokenizer = T5Tokenizer.from_pretrained("google/mt5-small")
article = "UN Offizier sagt, dass weiter verhandelt werden muss in Syrien."
summary = "Weiter Verhandlung in Syrien."
batch = tokenizer.prepare_seq2seq_batch(src_texts=[article], tgt_texts=[summary], return_tensors="pt")
outputs = model(**batch)
loss = outputs.loss""" .

"DESCRIPTION.The code allows for liking and unliking repositories on the Huggingface Hub." <EXPLAINS> """CODE.from huggingface_hub import like, list_liked_repos, unlike
like("gpt2")
"gpt2" in list_liked_repos().models
True
unlike("gpt2")
"gpt2" in list_liked_repos().models
False
""" .

"DESCRIPTION.The code allows for transforming the content of articles in a newspaper object to either a shorter or very short version by truncating the content." <EXPLAINS> """CODE.news_paper = freeze({'articles': [{'author': 'Sara', 'content': 'A short article'},
                                  {'author': 'Steve', 'content': 'A slightly longer article'}],
                     'weather': {'temperature': '11C', 'wind': '5m/s'}})
short_news = news_paper.transform(['articles', ny, 'content'], lambda c: c[:25] + '...' if len(c) > 25 else c)
very_short_news = news_paper.transform(['articles', ny, 'content'], lambda c: c[:15] + '...' if len(c) > 15 else c)
very_short_news.articles[0].content
very_short_news.articles[1].content""" .

"DESCRIPTION.The code allows the user to input a movie title using a text input field and displays the current movie title using the \"st.write\" function." <EXPLAINS> """CODE.title = st.text_input('Movie title', 'Life of Brian')
st.write('The current movie title is', title)""" .

"DESCRIPTION.The code allows the user to like and unlike a model on the Hugging Face model hub, and check if the model is listed as liked." <EXPLAINS> """CODE.from huggingface_hub import like, list_liked_repos, unlike
like("gpt2")
"gpt2" in list_liked_repos().models
True
unlike("gpt2")
"gpt2" in list_liked_repos().models
False
""" .

"DESCRIPTION.The code allows the user to navigate between different pages within an application using buttons for \"Home\", \"Page 1\", and \"Page 2\". Each button triggers a page switch to a specific python file associated with that page." <EXPLAINS> """CODE.if st.button("Home"):
    st.switch_page("your_app.py")
if st.button("Page 1"):
    st.switch_page("pages/page_1.py")
if st.button("Page 2"):
    st.switch_page("pages/page_2.py")""" .

"DESCRIPTION.The code allows the user to pick a color using a beta color picker tool, and then displays the selected color in a text format." <EXPLAINS> """CODE.st.beta_color_picker('Pick A Color', '#00f900')
st.write('The current color is', color)""" .

"DESCRIPTION.The code allows the user to pick a color using a beta color picker widget and displays the currently selected color." <EXPLAINS> """CODE.st.beta_color_picker('Pick A Color', '#00f900')
st.write('The current color is', color)""" .

"DESCRIPTION.The code allows the user to pick a color using a color picker dialog window and displays the selected color." <EXPLAINS> """CODE.st.color_picker('Pick A Color', '#00f900')
st.write('The current color is', color)""" .

"DESCRIPTION.The code allows the user to pick a color using a color picker interface and displays the selected color." <EXPLAINS> """CODE.st.beta_color_picker('Pick A Color', '#00f900')
st.write('The current color is', color)""" .

"DESCRIPTION.The code allows the user to pick a color using a color picker widget and displays the selected color." <EXPLAINS> """CODE.color = st.color_picker('Pick A Color', '#00f900')
st.write('The current color is', color)""",
        """CODE.st.beta_color_picker('Pick A Color', '#00f900')
st.write('The current color is', color)""" .

"DESCRIPTION.The code allows the user to select their favorite colors from a list of options and displays the selected colors." <EXPLAINS> """CODE.options = st.multiselect(
    'What are your favorite colors',
    ['Green', 'Yellow', 'Red', 'Blue'],
    ['Yellow', 'Red'])

st.write('You selected:', options)""" .

"DESCRIPTION.The code allows the user to use slider widgets to input values for age, a range of values, appointment time, and start time, and then displays the selected values or times in a message." <EXPLAINS> """CODE.age = st.slider('How old are you?', 0, 130, 25)
st.write("I'm ", age, 'years old')

values = st.slider(
    'Select a range of values',
    0.0, 100.0, (25.0, 75.0))
st.write('Values:', values)

from datetime import time
appointment = st.slider(
    "Schedule your appointment:",
    value=(time(11, 30), time(12, 45))
)
st.write("You're scheduled for:", appointment)

from datetime import datetime
start_time = st.slider(
    "When do you start?",
    value=datetime(2020, 1, 1, 9, 30),
    format="MM/DD/YY - hh:mm"
)
st.write("Start time:", start_time)""" .

"DESCRIPTION.The code allows the user to use sliders to select their age and a range of values, displaying the selected values in the application." <EXPLAINS> """CODE.age = st.slider('Age', 25, 0, 100, 1)
st.write("I'm ", age)

values = st.slider('A range of values', (25.0, 75.0), 0.0, 100.0, 1.0)
st.write("Values:", values)""" .

"DESCRIPTION.The code allows users to create a data table using pandas and streamlit, with a specific column configuration that turns URLs into clickable links, validates the URLs, and limits the displayed characters." <EXPLAINS> """CODE.import pandas as pd
import streamlit as st

data_df = pd.DataFrame(
    {
        "apps": [
            "https://roadmap.streamlit.app",
            "https://extras.streamlit.app",
            "https://issues.streamlit.app",
            "https://30days.streamlit.app",
        ],
    }
)

st.data_editor(
    data_df,
    column_config={
        "apps": st.column_config.LinkColumn(
            "Trending apps",
            help="The top trending Streamlit apps",
            validate="^https://[a-z]+\\.streamlit\\.app$",
            max_chars=100,
        )
    },
    hide_index=True,
)
""" .

"DESCRIPTION.The code allows users to input values using sliders for different scenarios such as age, range of values, appointment scheduling, and start time selection. The code then displays the selected values back to the user." <EXPLAINS> """CODE.age = st.slider('How old are you?', 0, 130, 25)
st.write("I'm ", age, 'years old')

values = st.slider(
    'Select a range of values',
    0.0, 100.0, (25.0, 75.0)
)
st.write('Values:', values)

from datetime import time
appointment = st.slider(
    "Schedule your appointment:",
    value=(time(11, 30), time(12, 45)
)
st.write("You're scheduled for:", appointment)

from datetime import datetime
start_time = st.slider(
    "When do you start?",
    value=datetime(2020, 1, 1, 9, 30),
    format="MM/DD/YY - hh:mm"
)
st.write("Start time:", start_time)""" .

"DESCRIPTION.The code allows users to pick a color using a beta color picker widget and displays the selected color." <EXPLAINS> """CODE.st.beta_color_picker('Pick A Color', '#00f900')
st.write('The current color is', color)""" .

"DESCRIPTION.The code allows users to upload a CSV file and then reads and displays the contents of the uploaded file in the web application." <EXPLAINS> """CODE.uploaded_file = st.file_uploader("Choose a CSV file", type="csv")
if uploaded_file is not None:
...     data = pd.read_csv(uploaded_file)
...     st.write(data)""" .

"DESCRIPTION.The code appends the value 3 to the left side of a deque containing the elements 1 and 2." <EXPLAINS> "CODE.pdeque([1, 2]).appendleft(3)" .

"DESCRIPTION.The code applies a function to each leaf node in the nested dictionary `params` based on whether the leaf node key contains the substring 'x' in its path. The function `f` increments the value by 5 if the key contains 'x' and negates the value otherwise." <EXPLAINS> """CODE.import jax.numpy as jnp
from flax import traverse_util

params = {'a': {'x': 10, 'y': 3}, 'b': {'x': 20}}
f = lambda path, x: x + 5 if 'x' in path else -x
traverse_util.path_aware_map(f, params)
""" .

"DESCRIPTION.The code applies a hue transformation with a value of 0.4 to a randomly generated image with dimensions 500x500x3, and then prints the shape of the transformed image." <EXPLAINS> """CODE.import numpy as np
from paddle.incubate.hapi.vision.transforms import HueTransform
transform = HueTransform(0.4)
fake_img = np.random.rand(500, 500, 3).astype('float32')
fake_img = transform(fake_img)
print(fake_img.shape)""" .

"DESCRIPTION.The code applies a saturation transformation with a factor of 0.4 to a randomly generated RGB image with dimensions 500x500." <EXPLAINS> """CODE.import numpy as np
from paddle.incubate.hapi.vision.transforms import SaturationTransform
transform = SaturationTransform(0.4)
fake_img = np.random.rand(500, 500, 3).astype('float32')

fake_img = transform(fake_img)
print(fake_img.shape)""" .

"DESCRIPTION.The code applies color jitter transformation to a randomly generated image with 3 channels, resulting in an image shape of (500, 500, 3)." <EXPLAINS> """CODE.import numpy as np
from paddle.incubate.hapi.vision.transforms import ColorJitter
transform = ColorJitter(0.4)
fake_img = np.random.rand(500, 500, 3).astype('float32')
fake_img = transform(fake_img)
print(fake_img.shape)""" .

"DESCRIPTION.The code applies different variants of the rectified linear activation function to the input array \"foo\" and returns the result as a numpy array." <EXPLAINS> """CODE.foo = tf.constant([-10, -5, 0.0, 5, 10], dtype = tf.float32)
tf.keras.activations.relu(foo).numpy()
tf.keras.activations.relu(foo, alpha=0.5).numpy()
tf.keras.activations.relu(foo, max_value=5).numpy()
tf.keras.activations.relu(foo, threshold=5).numpy()
""" .

"DESCRIPTION.The code applies random brightness adjustment to an input image by multiplying each channel by a randomly selected factor and rounding down the result." <EXPLAINS> """CODE.random_bright = tf.keras.layers.RandomBrightness(factor=0.2)

# An image with shape [2, 2, 3]
image = [[[1, 2, 3], [4 ,5 ,6]], [[7, 8, 9], [10, 11, 12]]]

# Assume we randomly select the factor to be 0.1, then it will apply
# 0.1 * 255 to all the channel
output = random_bright(image, training=True)

# output will be int64 with 25.5 added to each channel and round down.
tf.Tensor([[[26.5, 27.5, 28.5]
            [29.5, 30.5, 31.5]]
           [[32.5, 33.5, 34.5]
            [35.5, 36.5, 37.5]]],
          shape=(2, 2, 3), dtype=int64)
""" .

"DESCRIPTION.The code applies shearing transformation to a 3x3 identity matrix E along the specified axis using the provided shear factor. The function `librosa.util.shear` shears the matrix E based on the factor and axis input parameters." <EXPLAINS> """CODE.E = np.eye(3)
librosa.util.shear(E, factor=-1, axis=-1)
array([[1., 1., 1.],
       [0., 0., 0.],
       [0., 0., 0.]])
librosa.util.shear(E, factor=-1, axis=0)
array([[1., 0., 0.],
       [1., 0., 0.],
       [1., 0., 0.]])
librosa.util.shear(E, factor=1, axis=-1)
array([[1., 0., 0.],
       [0., 0., 1.],
       [0., 1., 0.]])""" .

"DESCRIPTION.The code applies the Gaussian Error Linear Unit (GELU) activation function to a given input array x, with and without the approximate parameter set to True. The output arrays y are then printed." <EXPLAINS> """CODE.x = tf.constant([-3.0, -1.0, 0.0, 1.0, 3.0], dtype=tf.float32)
y = tf.keras.activations.gelu(x)
y.numpy()
array([-0.00404951, -0.15865529,  0.        ,  0.8413447 ,  2.9959507 ],
    dtype=float32)
y = tf.keras.activations.gelu(x, approximate=True)
y.numpy()
array([-0.00363752, -0.15880796,  0.        ,  0.841192  ,  2.9963627 ],
    dtype=float32)
""" .

"DESCRIPTION.The code applies the Sigmoid Linear Unit (SiLU) activation function to a tensor x and returns the result in the tensor out." <EXPLAINS> """CODE.import paddle
import paddle.nn.functional as F

x = paddle.to_tensor([1.0, 2.0, 3.0, 4.0])
out = F.silu(x) # [ 0.731059, 1.761594, 2.857722, 3.928055 ]""" .

"DESCRIPTION.The code applies the softmax function to the input array and returns the result. Additionally, it applies a mask to the input array before applying the softmax function and returns the result." <EXPLAINS> """CODE.inp = np.asarray([1., 2., 1.])
layer = tf.keras.layers.Softmax()
layer(inp).numpy()
array([0.21194157, 0.5761169 , 0.21194157], dtype=float32)
mask = np.asarray([True, False, True], dtype=bool)
layer(inp, mask).numpy()
array([0.5, 0. , 0.5], dtype=float32)""" .

"DESCRIPTION.The code asserts that specific exceptions are raised with particular error messages for different scenarios involving the use of the `int` function, regular expressions, and dictionary operations." <EXPLAINS> """CODE.assertRaisesRegexp(ValueError, 'invalid literal for.*XYZ', int, 'XYZ')
import re
assertRaisesRegexp(ValueError, re.compile('literal'), int, 'XYZ')
assertRaisesRegexp(TypeError, 'literal', int, 'XYZ')
dct = {}
assertRaisesRegexp(KeyError, 'apple', dct.__getitem__, 'apple')
assertRaisesRegexp(Exception, 'operand type.*int.*dict', lambda : 2 + {})""" .

"DESCRIPTION.The code asserts that the value of the constant `CPU_BASIC` in the `SpaceHardware` class is equal to the string \"cpu-basic\"." <EXPLAINS> """CODE.assert SpaceHardware.CPU_BASIC == "cpu-basic"
""" .

"DESCRIPTION.The code assigns a dictionary containing predicted labels for images and corresponding ids to the variable \"predictions_to_write\", then writes this dictionary to a result object." <EXPLAINS> """CODE.predictions_to_write = {'preds': ['cat', 'dog'], 'ids': tensor([0, 1])}
result.write_dict(predictions_to_write)
""" .

"DESCRIPTION.The code assigns an array containing the values of x, y, and z to the variable tf.parallel_stack([])." <EXPLAINS> "CODE.tf.parallel_stack([x, y, z]) = np.asarray([x, y, z])" .

"DESCRIPTION.The code assigns specific values to certain columns in a DataFrame where missing values are then filled with these specified values." <EXPLAINS> """CODE.values = {'A': 0, 'B': 1, 'C': 2, 'D': 3}
df.fillna(value=values)
""" .

"DESCRIPTION.The code assigns the 2D coordinates 'x' and 'y' from the dataset to the variable '2d_coords'." <EXPLAINS> "CODE.2d_coords = dataset.fields(['x', 'y'])[:]" .

"DESCRIPTION.The code assigns the integer values for year, month, and day to the variables \"year\", \"month\", and \"day\" respectively." <EXPLAINS> """CODE.integer.setResultsName("year")
integer.setResultsName("month")
integer.setResultsName("day")
integer("year")
integer("month")
integer("day")""" .

"DESCRIPTION.The code assigns the path to a data.txt file within the TFDS (TensorFlow Datasets) directory." <EXPLAINS> """CODE.path = tfds.core.tfds_path() / 'path/to/data.txt'
path = tfds.core.tfds_path('path/to/data.txt')
path = tfds.core.tfds_path('path', 'to', 'data.txt')
""" .

"DESCRIPTION.The code assigns the value of the trial_id attribute from the self object to the variable trial_id." <EXPLAINS> "CODE.trial_id = self.trial_id" .

"DESCRIPTION.The code assigns the value of the variable 'trial_resources' to the variable 'self.trial_resources'." <EXPLAINS> "CODE.trial_resources = self.trial_resources" .

"DESCRIPTION.The code assigns values to the 'SI.hChannels.channelType' and 'SI.hChannels.channelsActive' keys in the dictionary 'd'. The value for 'SI.hChannels.channelType' is a list containing two 'stripe' strings. The value for 'SI.hChannels.channelsActive' is the integer 2." <EXPLAINS> """CODE.d = matlabstr2py("SI.hChannels.channelType = {'stripe' 'stripe'}\\n"
...                  "SI.hChannels.channelsActive = 2")
d['SI.hChannels.channelType']
['stripe', 'stripe']""" .

"DESCRIPTION.The code asynchronously reads log messages from different sockets by using Threads, allowing for concurrent execution of multiple tasks. Each socket listens for log messages related to a specific component (\"flow\" and \"work_1\" in this case), and the print_log_msg function is called to handle and print each incoming message. The Threads are started to run the sockets in the background, and the sockets are eventually closed after processing is complete." <EXPLAINS> """CODE.# Synchronous reading, run_forever() is blocking
def print_log_msg(ws_app, msg):
    print(msg)


flow_logs_socket = client.create_lightning_logs_socket("project_id", "app_id", "flow", print_log_msg)
flow_socket.run_forever()


# Asynchronous reading (with Threads)
def print_log_msg(ws_app, msg):
    print(msg)


flow_logs_socket = client.create_lightning_logs_socket("project_id", "app_id", "flow", print_log_msg)
work_logs_socket = client.create_lightning_logs_socket("project_id", "app_id", "work_1", print_log_msg)
flow_logs_thread = Thread(target=flow_logs_socket.run_forever)
work_logs_thread = Thread(target=work_logs_socket.run_forever)
flow_logs_thread.start()
work_logs_thread.start()
# .......
flow_logs_socket.close()
work_logs_thread.close()
""" .

"DESCRIPTION.The code asynchronously render a template with the parameter knights set to 'that say nih'." <EXPLAINS> "CODE.await template.render_async(knights='that say nih; asynchronously')" .

"DESCRIPTION.The code asynchronously renders a template with the variable 'knights' assigned the value 'that say nih'." <EXPLAINS> "CODE.await template.render_async(knights='that say nih; asynchronously')" .

"DESCRIPTION.The code asynchronously runs a remote function on multiple workers in parallel using Ray, with a timeout of 0.1 seconds. The function sleeps for 1 second on each worker. It then checks if the batches returned from the workers are None after the timeout, which is expected." <EXPLAINS> """CODE.asynchronous_parallel_sample(
    trainer,
    actors=trainer.workers.remote_workers(),
    ray_wait_timeout_s=0.1,
    remote_fn=lambda w: time.sleep(1)  # sleep 1sec
)
print(len(batches))
2
# Expect a timeout to have happened.
batches[0] is None and batches[1] is None
True""" .

"DESCRIPTION.The code attempts to execute a SQL query selecting all columns from a table named 'test' in a database connection, but it fails with a DatabaseError due to a syntax error near the table name 'test'." <EXPLAINS> """CODE.pd.read_sql('select * test', conn) # doctest: +SKIP
... # DatabaseError: Execution failed on sql 'test': near "test": syntax error""" .

"DESCRIPTION.The code attempts to infer the data type of the input argument, which is an object of type np.dtype(\"i8\")." <EXPLAINS> "CODE.maybe_infer_dtype_type(Foo(np.dtype(\"i8\")))" .

"DESCRIPTION.The code attempts to parse the string \"ABC\" as an integer using the Word class, sets the parsed result's name as \"integer\", and handles any parsing exceptions by printing the error message and the column where the parsing failed." <EXPLAINS> """CODE.try:
    Word(nums).setName("integer").parseString("ABC")
except ParseException as pe:
    print(pe)
    print("column: {}".format(pe.col))""" .

"DESCRIPTION.The code attempts to run a training loop for a certain number of steps, updating a metric `loss` within a try-except block. If an OutOfRangeError is raised during the training loop, the error is cleared and the final value of `loss` is logged." <EXPLAINS> """CODE.try:
  with tf.experimental.async_scope():
    for _ in range(num_steps):
      # Step function updates the metric `loss` internally
      train_step_fn()
except tf.errors.OutOfRangeError:
  tf.experimental.async_clear_error()
logging.info('loss =', loss.numpy())
""" .

"DESCRIPTION.The code builds a DebugRegressor model, defines input builders for training and evaluation data, fits the model using the training data, evaluates the squared-loss between the test and train targets, and predicts target scores based on new samples using the trained model." <EXPLAINS> """CODE.# Build DebugRegressor
regressor = DebugRegressor()

# Input builders
def input_fn_train(): # returns x, y (where y represents label's class index).
  pass

def input_fn_eval(): # returns x, y (where y represents label's class index).
  pass

# Fit model.
regressor.fit(input_fn=input_fn_train)

# Evaluate squared-loss between the test and train targets.
loss = regressor.evaluate(input_fn=input_fn_eval)["loss"]

# predict_scores outputs mean value seen during training.
predicted_targets = regressor.predict_scores(new_samples)
""" .

"DESCRIPTION.The code builds a URI by adding a scheme 'HTTPS' to the URI." <EXPLAINS> """CODE... code-block:: python

    URIBuilder().add_scheme('HTTPS')""" .

"DESCRIPTION.The code builds a baseline multi-label classifier using TensorFlow, defines input functions for training and evaluation, trains the model, evaluates the cross-entropy loss between test and train labels, and then makes predictions on new samples." <EXPLAINS> """CODE.# Build baseline multi-label classifier.
estimator = BaselineEstimator(
    head=tf.contrib.estimator.multi_label_head(n_classes=3))

# Input builders
def input_fn_train: # returns x, y (where y represents label's class index).
  pass

def input_fn_eval: # returns x, y (where y represents label's class index).
  pass

# Fit model.
estimator.train(input_fn=input_fn_train)

# Evaluates cross entropy between the test and train labels.
loss = classifier.evaluate(input_fn=input_fn_eval)["loss"]

# For each class, predicts the ratio of training examples that contain the
# class.
predictions = classifier.predict(new_samples)
""" .

"DESCRIPTION.The code builds a baseline multi-label classifier using TensorFlow. It includes input functions for training and evaluation, trains the model, evaluates the cross entropy loss, and predicts the ratio of training examples that contain each class for new samples." <EXPLAINS> """CODE.# Build baseline multi-label classifier.
estimator = BaselineEstimator(
    head=tf.contrib.estimator.multi_label_head(n_classes=3))

# Input builders
def input_fn_train: # returns x, y (where y represents label's class index).
  pass

def input_fn_eval: # returns x, y (where y represents label's class index).
  pass

# Fit model.
estimator.train(input_fn=input_fn_train)

# Evaluates cross entropy between the test and train labels.
loss = classifier.evaluate(input_fn=input_fn_eval)["loss"]

# For each class, predicts the ratio of training examples that contain the
# class.
predictions = classifier.predict(new_samples)
""" .

"DESCRIPTION.The code builds a neural network model using Bidirectional LSTM layers with specified configurations, followed by a dense layer and softmax activation function. The model is compiled with a categorical crossentropy loss function and rmsprop optimizer. Additionally, the code demonstrates the use of a custom backward layer in the Bidirectional setup with LSTM layers." <EXPLAINS> """CODE.model = Sequential()
model.add(Bidirectional(LSTM(10, return_sequences=True), input_shape=(5, 10)))
model.add(Bidirectional(LSTM(10)))
model.add(Dense(5))
model.add(Activation('softmax'))
model.compile(loss='categorical_crossentropy', optimizer='rmsprop')

# With custom backward layer
model = Sequential()
forward_layer = LSTM(10, return_sequences=True)
backward_layer = LSTM(10, activation='relu', return_sequences=True,
                      go_backwards=True)
model.add(Bidirectional(forward_layer, backward_layer=backward_layer,
                        input_shape=(5, 10)))
model.add(Dense(5))
model.add(Activation('softmax'))
model.compile(loss='categorical_crossentropy', optimizer='rmsprop')
""" .

"DESCRIPTION.The code builds an autoencoder that performs compression and decompression of input data. The autoencoder consists of a forward and backward transformation. The compression is achieved using entropy bottleneck, which calculates likelihoods and information content of each batch element in bits. The loss function is a weighted sum of mean squared error and entropy, with the weight controlling the trade-off between approximation error and entropy. The model is optimized using Adam optimizer with different learning rates for the main and auxiliary losses." <EXPLAINS> """CODE.
# Build autoencoder.
x = tf.placeholder(tf.float32, shape=[None, 16, 16, 1])
y = forward_transform(x)
entropy_bottleneck = EntropyBottleneck()
y_, likelihoods = entropy_bottleneck(y, training=True)
x_ = backward_transform(y_)

# Information content (= predicted codelength) in bits of each batch element
# (note that taking the natural logarithm and dividing by `log(2)` is
# equivalent to taking base-2 logarithms):
bits = tf.reduce_sum(tf.log(likelihoods), axis=(1, 2, 3)) / -np.log(2)

# Squared difference of each batch element:
squared_error = tf.reduce_sum(tf.squared_difference(x, x_), axis=(1, 2, 3))

# The loss is a weighted sum of mean squared error and entropy (average
# information content), where the weight controls the trade-off between
# approximation error and entropy.
main_loss = 0.5 * tf.reduce_mean(squared_error) + tf.reduce_mean(bits)

# Minimize loss and auxiliary loss, and execute update op.
main_optimizer = tf.train.AdamOptimizer(learning_rate=1e-4)
main_step = optimizer.minimize(main_loss)
# 1e-2 is a good starting point for the learning rate of the auxiliary loss,
# assuming Adam is used.
aux_optimizer = tf.train.AdamOptimizer(learning_rate=1e-2)
aux_step = optimizer.minimize(entropy_bottleneck.losses[0])
step = tf.group(main_step, aux_step, entropy_bottleneck.updates[0])



# Build autoencoder.
x = tf.placeholder(tf.float32, shape=[None, 16, 16, 1])
y = forward_transform(x)
strings = EntropyBottleneck().compress(y)
shape = tf.shape(y)[1:]



strings = tf.placeholder(tf.string, shape=[None])
shape = tf.placeholder(tf.int32, shape=[3])
entropy_bottleneck = EntropyBottleneck(dtype=tf.float32)
y_ = entropy_bottleneck.decompress(strings, shape, channels=5)
x_ = backward_transform(y_)
""" .

"DESCRIPTION.The code calculates a decayed learning rate using a combination of linear decay and cosine decay based on the global step and decay steps. The decayed learning rate is then calculated by multiplying the original learning rate with the decayed value." <EXPLAINS> """CODE.global_step = min(global_step, decay_steps)
linear_decay = (decay_steps - global_step) / decay_steps)
cosine_decay = 0.5 * (
    1 + cos(pi * 2 * num_periods * global_step / decay_steps))
decayed = (alpha + linear_decay + eps_t) * cosine_decay + beta
decayed_learning_rate = learning_rate * decayed

decay_steps = 1000
lr_decayed = noisy_linear_cosine_decay(
  learning_rate, global_step, decay_steps)
""" .

"DESCRIPTION.The code calculates a value based on the input arguments using a function called epsilon1. If two arguments are provided, it returns the product of the two arguments. If only one argument is provided, it returns the product of that argument and 100 plus 40." <EXPLAINS> """CODE.epsilon1(10, 20)
40
epsilon1(30)
1040""" .

"DESCRIPTION.The code calculates and displays harmonic-related information using Librosa library in Python." <EXPLAINS> """CODE.h_range = [1, 2, 3, 4, 5]
f_tempo = librosa.tempo_frequencies(len(tempi), sr=sr)
t_harmonics = librosa.interp_harmonics(tempi, f_tempo, h_range)
print(t_harmonics.shape)

import matplotlib.pyplot as plt
plt.figure()
librosa.display.specshow(t_harmonics, x_axis='tempo', sr=sr)
plt.yticks(0.5 + np.arange(len(h_range)),
...            ['{:.3g}'.format(_) for _ in h_range])
plt.ylabel('Harmonic')
plt.xlabel('Tempo (BPM)')
plt.tight_layout()

h_range = [1./3, 1./2, 1, 2, 3, 4]
S = np.abs(librosa.stft(y))
fft_freqs = librosa.fft_frequencies(sr=sr)
S_harm = librosa.interp_harmonics(S, fft_freqs, h_range, axis=0)
print(S_harm.shape)

plt.figure()
for i, _sh in enumerate(S_harm, 1):
...     plt.subplot(3, 2, i)
...     librosa.display.specshow(librosa.amplitude_to_db(_sh,
...                                                      ref=S.max()),
...                              sr=sr, y_axis='log')
...     plt.title('h={:.3g}'.format(h_range[i-1]))
...     plt.yticks([])
plt.tight_layout()""" .

"DESCRIPTION.The code calculates and plots the spectral bandwidth of an audio signal using different methods and then displays the results in a subplot format." <EXPLAINS> """CODE.spec_bw = librosa.feature.spectral_bandwidth(y=y, sr=sr)
spec_bw
array([[ 3379.878,  1429.486, ...,  3235.214,  3080.148]])

S, phase = librosa.magphase(librosa.stft(y=y))
librosa.feature.spectral_bandwidth(S=S)
array([[ 3379.878,  1429.486, ...,  3235.214,  3080.148]])

if_gram, D = librosa.ifgram(y)
librosa.feature.spectral_bandwidth(S=np.abs(D), freq=if_gram)
array([[ 3380.011,  1429.11 , ...,  3235.22 ,  3080.148]])

import matplotlib.pyplot as plt
plt.figure()
plt.subplot(2, 1, 1)
plt.semilogy(spec_bw.T, label='Spectral bandwidth')
plt.ylabel('Hz')
plt.xticks([])
plt.xlim([0, spec_bw.shape[-1]])
plt.legend()
plt.subplot(2, 1, 2)
librosa.display.specshow(librosa.logamplitude(S**2, ref_power=np.max),
...                          y_axis='log', x_axis='time')
plt.title('log Power spectrogram')
plt.tight_layout()""" .

"DESCRIPTION.The code calculates and visualizes the spectral centroid of an audio signal using librosa library." <EXPLAINS> """CODE.y, sr = librosa.load(librosa.util.example_audio_file())
cent = librosa.feature.spectral_centroid(y=y, sr=sr)
cent
array([[ 4382.894,   626.588, ...,  5037.07 ,  5413.398]])

S, phase = librosa.magphase(librosa.stft(y=y))
librosa.feature.spectral_centroid(S=S)
array([[ 4382.894,   626.588, ...,  5037.07 ,  5413.398]])

y, sr = librosa.load(librosa.util.example_audio_file())
if_gram, D = librosa.ifgram(y)
librosa.feature.spectral_centroid(S=np.abs(D), freq=if_gram)

import matplotlib.pyplot as plt
plt.figure()
plt.subplot(2, 1, 1)
plt.semilogy(cent.T, label='Spectral centroid')
plt.ylabel('Hz')
plt.xticks([])
plt.xlim([0, cent.shape[-1]])
plt.legend()
plt.subplot(2, 1, 2)
librosa.display.specshow(librosa.logamplitude(S**2, ref_power=np.max),
...                          y_axis='log', x_axis='time')
plt.title('log Power spectrogram')
plt.tight_layout()""" .

"DESCRIPTION.The code calculates binary accuracy metrics for a model's predictions. The first part updates the binary accuracy metric with given predictions and labels, then resets the state and updates the metric with sample weights included. Finally, it retrieves the computed binary accuracy as a numpy array. The second part compiles a model using stochastic gradient descent optimizer, mean squared error loss function, and binary accuracy as a metric." <EXPLAINS> """CODE.m = tf.keras.metrics.BinaryAccuracy()
m.update_state([[1], [1], [0], [0]], [[0.98], [1], [0], [0.6]])
m.result().numpy()

m.reset_state()
m.update_state([[1], [1], [0], [0]], [[0.98], [1], [0], [0.6]],
               sample_weight=[1, 0, 0, 1])
m.result().numpy()

model.compile(optimizer='sgd',
              loss='mse',
              metrics=[tf.keras.metrics.BinaryAccuracy()])
""" .

"DESCRIPTION.The code calculates line and quadratic coefficients using poly_features function from librosa, then visualizes the coefficients using matplotlib." <EXPLAINS> """CODE.line = librosa.feature.poly_features(S=S, sr=sr)
line
array([[ -2.406e-08,  -5.051e-06, ...,  -1.103e-08,  -5.651e-09],
       [  3.445e-04,   3.834e-02, ...,   2.661e-04,   2.239e-04]])

quad = librosa.feature.poly_features(S=S, order=2)
quad
array([[  6.276e-12,   2.010e-09, ...,   1.493e-12,   1.000e-13],
       [ -9.325e-08,  -2.721e-05, ...,  -2.749e-08,  -6.754e-09],
       [  4.715e-04,   7.902e-02, ...,   2.963e-04,   2.259e-04]])

import matplotlib.pyplot as plt
plt.figure()
plt.subplot(3, 1, 1)
librosa.display.specshow(line)
plt.colorbar()
plt.title('Line coefficients')
plt.subplot(3, 1, 2)
librosa.display.specshow(quad)
plt.colorbar()
plt.title('Quadratic coefficients')
plt.subplot(3, 1, 3)
librosa.display.specshow(librosa.logamplitude(S**2, ref_power=np.max),
...                          y_axis='log', x_axis='time')
plt.title('log Power spectrogram')
plt.colorbar(format='%+2.0f dB')
plt.tight_layout()""" .

"DESCRIPTION.The code calculates pairwise distances between vectors using a specified threshold value 'r' to determine neighbor relationships. The function 'pairwise_distances_chunked' chunks the distance calculations into smaller parts, and a 'reduce_func' function is used to find neighbors within the threshold for each chunk. The code can handle different threshold values for different pairs of vectors." <EXPLAINS> """CODE.X = np.random.RandomState(0).rand(5, 3)
D_chunk = next(pairwise_distances_chunked(X))
D_chunk  # doctest: +ELLIPSIS
array([[0.  ..., 0.29..., 0.41..., 0.19..., 0.57...],
       [0.29..., 0.  ..., 0.57..., 0.41..., 0.76...],
       [0.41..., 0.57..., 0.  ..., 0.44..., 0.90...],
       [0.19..., 0.41..., 0.44..., 0.  ..., 0.51...],
       [0.57..., 0.76..., 0.90..., 0.51..., 0.  ...]])

r = .2
def reduce_func(D_chunk, start):
...     neigh = [np.flatnonzero(d < r) for d in D_chunk]
...     avg_dist = (D_chunk * (D_chunk < r)).mean(axis=1)
...     return neigh, avg_dist
gen = pairwise_distances_chunked(X, reduce_func=reduce_func)
neigh, avg_dist = next(gen)
neigh
[array([0, 3]), array([1]), array([2]), array([0, 3]), array([4])]
avg_dist  # doctest: +ELLIPSIS
array([0.039..., 0.        , 0.        , 0.039..., 0.        ])

r = [.2, .4, .4, .3, .1]
def reduce_func(D_chunk, start):
...     neigh = [np.flatnonzero(d < r[i])
...              for i, d in enumerate(D_chunk, start)]
...     return neigh
neigh = next(pairwise_distances_chunked(X, reduce_func=reduce_func))
neigh
[array([0, 3]), array([0, 1]), array([2]), array([0, 3]), array([4])]

gen = pairwise_distances_chunked(X, reduce_func=reduce_func,
...                                  working_memory=0)
next(gen)
[array([0, 3])]
next(gen)
[array([0, 1])]
""" .

"DESCRIPTION.The code calculates precision and recall metrics for a multiclass classification model based on the predicted values (pred) and the true target values (target)." <EXPLAINS> """CODE.pred = torch.tensor([[0.85, 0.05, 0.05, 0.05],
                        [0.05, 0.85, 0.05, 0.05],
                        [0.05, 0.05, 0.85, 0.05],
                        [0.05, 0.05, 0.05, 0.85]])
target = torch.tensor([0, 1, 3, 2])
metric = MulticlassPrecisionRecall()
metric(pred, target)   # doctest: +NORMALIZE_WHITESPACE""" .

"DESCRIPTION.The code calculates precision and recall metrics for a multiclass classification task based on the predicted probabilities (pred) and the actual target labels (target)." <EXPLAINS> """CODE.pred = torch.tensor([[0.85, 0.05, 0.05, 0.05],
                        [0.05, 0.85, 0.05, 0.05],
                        [0.05, 0.05, 0.85, 0.05],
                        [0.05, 0.05, 0.05, 0.85]])
target = torch.tensor([0, 1, 3, 2])
metric = MulticlassPrecisionRecall()
metric(pred, target)   # doctest: +NORMALIZE_WHITESPACE""" .

"DESCRIPTION.The code calculates precision at a specific recall value for a binary classification task using the PrecisionAtRecall metric in TensorFlow-Keras. It first computes the precision at recall=0.5 based on the predicted and actual values, and then resets the state to calculate precision at recall=0.8 with sample weights. Finally, it compiles a model with SGD optimizer, mean squared error loss, and precision at recall=0.8 as a metric." <EXPLAINS> """CODE.m = tf.keras.metrics.PrecisionAtRecall(0.5)
m.update_state([0, 0, 0, 1, 1], [0, 0.3, 0.8, 0.3, 0.8])
m.result().numpy()
0.5

m.reset_state()
m.update_state([0, 0, 0, 1, 1], [0, 0.3, 0.8, 0.3, 0.8],
...                sample_weight=[2, 2, 2, 1, 1])
m.result().numpy()
0.33333333

model.compile(
    optimizer='sgd',
    loss='mse',
    metrics=[tf.keras.metrics.PrecisionAtRecall(recall=0.8)])
""" .

"DESCRIPTION.The code calculates precision, recall, and thresholds based on the predicted and target values." <EXPLAINS> """CODE.pred = torch.tensor([0, 1, 2, 3])
target = torch.tensor([0, 1, 2, 2])
metric = PrecisionRecall()
prec, recall, thr = metric(pred, target)
prec
tensor([0.3333, 0.0000, 0.0000, 1.0000])
recall
tensor([1., 0., 0., 0.])
thr
tensor([1., 2., 3.])""" .

"DESCRIPTION.The code calculates the 50th percentile (median) of a numpy array 'a'." <EXPLAINS> """CODE.from scipy import stats
import numpy as np

a = np.arange(100)
stats.scoreatpercentile(a, 50)
""" .

"DESCRIPTION.The code calculates the Area Under the Receiver Operating Characteristic (AUROC) metric using the predicted values (pred) and target values (target). The AUROC value obtained is 0.3333." <EXPLAINS> """CODE.pred = torch.tensor([0, 1, 2, 3])
target = torch.tensor([0, 1, 2, 2])
metric = AUROC()
metric(pred, target)
tensor(0.3333)""" .

"DESCRIPTION.The code calculates the Average Precision (AP) metric for both binary and multi-class classification cases based on the predicted values and target labels provided. The binary case calculates the AP for a specified positive label, while the multi-class case calculates the AP for multiple classes." <EXPLAINS> """CODE.Example (binary case):

    pred = torch.tensor([0, 1, 2, 3])
    target = torch.tensor([0, 1, 1, 1])
    average_precision = AveragePrecision(pos_label=1)
    average_precision(pred, target)
    tensor(1.)

Example (multiclass case):

    pred = torch.tensor([[0.75, 0.05, 0.05, 0.05, 0.05],
    ...                      [0.05, 0.75, 0.05, 0.05, 0.05],
    ...                      [0.05, 0.05, 0.75, 0.05, 0.05],
    ...                      [0.05, 0.05, 0.05, 0.75, 0.05]])
    target = torch.tensor([0, 1, 3, 2])
    average_precision = AveragePrecision(num_classes=5)
    average_precision(pred, target)
    [tensor(1.), tensor(1.), tensor(0.2500), tensor(0.2500), tensor(nan)]
""" .

"DESCRIPTION.The code calculates the BLEU score (bilingual evaluation understudy) between the translation corpus and the reference corpus, which measures the similarity between the translation and reference texts." <EXPLAINS> """CODE.translate_corpus = ['the cat is on the mat'.split()]
reference_corpus = [['there is a cat on the mat'.split(), 'a cat is on the mat'.split()]]
metric = BLEUScore()
metric(translate_corpus, reference_corpus)
tensor(0.7598)""" .

"DESCRIPTION.The code calculates the Binary Cross-Entropy (BCE) loss between the input data and the label data using the paddle fluid framework in Python. It then runs the calculation on a CPU platform, prints the output data, and converts the code to dygraph mode to perform the same calculation and print the output." <EXPLAINS> """CODE.import paddle.fluid as fluid
import numpy as np
input = fluid.data(name="input", shape=[3, 1], dtype='float32')
label = fluid.data(name="label", shape=[3, 1], dtype='float32')
bce_loss = fluid.dygraph.BCELoss()
output = bce_loss(input, label)
place = fluid.CPUPlace()
exe = fluid.Executor(place)
exe.run(fluid.default_startup_program())

input_data = np.array([0.5, 0.6, 0.7]).astype("float32")
label_data = np.array([1.0, 0.0, 1.0]).astype("float32")
output_data = exe.run(fluid.default_main_program(),
        feed={"input":input_data, "label":label_data},
        fetch_list=[output],
        return_numpy=True)

print(output_data)  # [array([0.65537095], dtype=float32)]

import paddle.fluid.dygraph as dg
with dg.guard(place) as g:
    input = dg.to_variable(input_data)
    label = dg.to_variable(label_data)
    output = bce_loss(input, label)
    print(output.numpy())  # [0.65537095]""" .

"DESCRIPTION.The code calculates the Brier score loss for different input scenarios, including binary and categorical targets, and different probability predictions. It evaluates the accuracy of the predicted probabilities compared to the actual outcomes." <EXPLAINS> """CODE.import numpy as np
from sklearn.metrics import brier_score_loss
y_true = np.array([0, 1, 1, 0])
y_true_categorical = np.array(["spam", "ham", "ham", "spam"])
y_prob = np.array([0.1, 0.9, 0.8, 0.3])
brier_score_loss(y_true, y_prob)  # doctest: +ELLIPSIS
0.037...
brier_score_loss(y_true, 1-y_prob, pos_label=0)  # doctest: +ELLIPSIS
0.037...
brier_score_loss(y_true_categorical, y_prob, pos_label="ham")  # doctest: +ELLIPSIS
0.037...
brier_score_loss(y_true, np.array(y_prob) > 0.5)
0.0""" .

"DESCRIPTION.The code calculates the Categorical Focal Crossentropy loss between the true distribution (`y_true`) and the predicted distribution (`y_pred`) using different reduction types such as 'auto', 'sum', and 'none'. It demonstrates the use of the CategoricalFocalCrossentropy loss function with TensorFlow/Keras for evaluating the model's performance during training." <EXPLAINS> """CODE.y_true = [[0., 1., 0.], [0., 0., 1.]]
y_pred = [[0.05, 0.95, 0], [0.1, 0.8, 0.1]]
# Using 'auto'/'sum_over_batch_size' reduction type.
cce = tf.keras.losses.CategoricalFocalCrossentropy()
cce(y_true, y_pred).numpy()
0.23315276

# Calling with 'sample_weight'.
cce(y_true, y_pred, sample_weight=tf.constant([0.3, 0.7])).numpy()
0.1632

# Using 'sum' reduction type.
cce = tf.keras.losses.CategoricalFocalCrossentropy(
...     reduction=tf.keras.losses.Reduction.SUM)
cce(y_true, y_pred).numpy()
0.46631

# Using 'none' reduction type.
cce = tf.keras.losses.CategoricalFocalCrossentropy(
...     reduction=tf.keras.losses.Reduction.NONE)
cce(y_true, y_pred).numpy()
array([3.2058331e-05, 4.6627346e-01], dtype=float32)

model.compile(optimizer='adam',
              loss=tf.keras.losses.CategoricalFocalCrossentropy())
""" .

"DESCRIPTION.The code calculates the Categorical Hinge loss between the true labels (y_true) and the predicted labels (y_pred) in a neural network model. It uses different parameters for the loss calculation such as sample weights, reduction method, and optimization method (sgd)." <EXPLAINS> """CODE.h = tf.keras.losses.CategoricalHinge()
h(y_true, y_pred).numpy()

h(y_true, y_pred, sample_weight=[1, 0]).numpy()

h = tf.keras.losses.CategoricalHinge(
    reduction=tf.keras.losses.Reduction.SUM)
h(y_true, y_pred).numpy()

h = tf.keras.losses.CategoricalHinge(
    reduction=tf.keras.losses.Reduction.NONE)
h(y_true, y_pred).numpy()

model.compile(optimizer='sgd', loss=tf.keras.losses.CategoricalHinge())
""" .

"DESCRIPTION.The code calculates the Cohen's Kappa score as a performance metric by comparing the predicted values (y_pred) with the true values (y_true)." <EXPLAINS> """CODE.y_pred = torch.tensor([1, 2, 0, 2])
y_true = torch.tensor([2, 2, 2, 1])
metric = CohenKappaScore()
metric(y_pred, y_true)
tensor([-0.3333])""" .

"DESCRIPTION.The code calculates the Cohen's Kappa score between the predicted values (y_pred) and the true values (y_true)." <EXPLAINS> """CODE.y_pred = torch.tensor([1, 2, 0, 2])
y_true = torch.tensor([2, 2, 2, 1])
metric = CohenKappaScore()
metric(y_pred, y_true)
tensor([-0.3333])""" .

"DESCRIPTION.The code calculates the Constant-Q Transform (CQT) of an audio signal 'y' with a given sampling rate 'sr' using the specified basis filters." <EXPLAINS> """CODE.y, sr = librosa.load('file.wav')
C = librosa.cqt(y, sr)

C = librosa.cqt(y, sr, fmin=librosa.midi_to_hz(36), fmax=librosa.midi_to_hz(96))

basis = librosa.filters.constant_q(sr, ...)
C = librosa.cqt(y, sr, basis=basis)""" .

"DESCRIPTION.The code calculates the Discounted Cumulative Gain (DCG) metric for a set of predicted scores (y_score) and true labels (y_true)." <EXPLAINS> """CODE.metric = DCG()
metric(y_score, y_true)""" .

"DESCRIPTION.The code calculates the Discounted Cumulative Gain (DCG) metric for the predicted scores `y_score` and the true labels `y_true`." <EXPLAINS> """CODE.y_score = torch.tensor([[.1, .2, .3, 4, 70]])
y_true = torch.tensor([[10, 0, 0, 1, 5]])
metric = DCG()
metric(y_score, y_true)
tensor([9.4995])""" .

"DESCRIPTION.The code calculates the F-beta score for a given beta value, precision, and recall values." <EXPLAINS> """CODE.b2 = beta ** 2
f_beta_score = (1 + b2) * (precision * recall) / (precision * b2 + recall)

metric = tf.keras.metrics.FBetaScore(beta=2.0, threshold=0.5)
y_true = np.array([[1, 1, 1],
                   [1, 0, 0],
                   [1, 1, 0]], np.int32)
y_pred = np.array([[0.2, 0.6, 0.7],
                   [0.2, 0.6, 0.6],
                   [0.6, 0.8, 0.0]], np.float32)
metric.update_state(y_true, y_pred)
result = metric.result()
result.numpy()
""" .

"DESCRIPTION.The code calculates the F-beta score with beta=0.25 between the predicted tensor and target tensor." <EXPLAINS> """CODE.pred = torch.tensor([0, 1, 2, 3])
target = torch.tensor([0, 1, 2, 2])
metric = FBeta(0.25)
metric(pred, target)
tensor(0.7361)""" .

"DESCRIPTION.The code calculates the F1 score between the predicted tensor and the target tensor." <EXPLAINS> """CODE.pred = torch.tensor([0, 1, 2, 3])
target = torch.tensor([0, 1, 2, 2])
metric = F1()
metric(pred, target)
tensor(0.6667)""" .

"DESCRIPTION.The code calculates the F1 score between the predicted values and the target values. The F1 score is 0.6667 for the given predictions and targets." <EXPLAINS> """CODE.pred = torch.tensor([0, 1, 2, 3])
target = torch.tensor([0, 1, 2, 2])
metric = F1()
metric(pred, target)
tensor(0.6667)""" .

"DESCRIPTION.The code calculates the False Positive Rate (fps), True Positive Rate (tps), and thresholds using the Receiver Operating Characteristic (ROC) metric for the predicted values (pred) compared to the target values (target)." <EXPLAINS> """CODE.pred = torch.tensor([0, 1, 2, 3])
target = torch.tensor([0, 1, 2, 2])
metric = ROC()
fps, tps, thresholds = metric(pred, target)
fps
tensor([0.0000, 0.3333, 0.6667, 0.6667, 1.0000])
tps
tensor([0., 0., 0., 1., 1.])
thresholds
tensor([4., 3., 2., 1., 0.])""" .

"DESCRIPTION.The code calculates the Fourier frequencies corresponding to tempo values for music signals with a frame size of 384." <EXPLAINS> "CODE.librosa.fourier_tempo_frequencies(384)" .

"DESCRIPTION.The code calculates the Fowlkes-Mallows score to measure the similarity between two clustering assignments." <EXPLAINS> """CODE.from sklearn.metrics.cluster import fowlkes_mallows_score
fowlkes_mallows_score([0, 0, 1, 1], [0, 0, 1, 1])
fowlkes_mallows_score([0, 0, 1, 1], [1, 1, 0, 0])
fowlkes_mallows_score([0, 0, 0, 0], [0, 1, 2, 3])""" .

"DESCRIPTION.The code calculates the Fresnel sine integral for a list of input values." <EXPLAINS> "CODE.tf.math.special.fresnel_sin([-1., -0.1, 0.1, 1.]).numpy()" .

"DESCRIPTION.The code calculates the Hamming loss metric between the predicted tensor y_pred and the true tensor y_true, resulting in a tensor value of 0.2500." <EXPLAINS> """CODE.y_pred = torch.tensor([0, 1, 2, 3])
y_true = torch.tensor([1, 1, 2, 3])
metric = Hamming()
metric(y_pred, y_true)
tensor([0.2500])""" .

"DESCRIPTION.The code calculates the Hessian matrix for a given function, in this case, the function is defined as the sum of the square of the input vector elements. The Hessian matrix represents the second partial derivatives of the function with respect to its inputs." <EXPLAINS> """CODE.import paddle


def reducer(x):
    return paddle.sum(x * x)


x = paddle.rand([2, 2])
h = paddle.incubate.autograd.Hessian(reducer, x)
print(h[:])
# Tensor(shape=[4, 4], dtype=float32, place=Place(gpu:0), stop_gradient=False,
#        [[2., 0., 0., 0.],
#         [0., 2., 0., 0.],
#         [0., 0., 2., 0.],
#         [0., 0., 0., 2.]])
""" .

"DESCRIPTION.The code calculates the Hinge loss between true labels (y_true) and predicted labels (y_pred) using different settings such as default reduction (average), sample weights, sum reduction, and none reduction." <EXPLAINS> """CODE.y_true = [[0., 1.], [0., 0.]]
y_pred = [[0.6, 0.4], [0.4, 0.6]]
h = tf.keras.losses.Hinge()
h(y_true, y_pred).numpy()
1.3

h(y_true, y_pred, sample_weight=[1, 0]).numpy()
0.55

h = tf.keras.losses.Hinge(
    reduction=tf.keras.losses.Reduction.SUM)
h(y_true, y_pred).numpy()
2.6

h = tf.keras.losses.Hinge(
    reduction=tf.keras.losses.Reduction.NONE)
h(y_true, y_pred).numpy()
array([1.1, 1.5], dtype=float32)
""" .

"DESCRIPTION.The code calculates the Intersection over Union (IoU) metric between two sets of binary masks represented as tensors." <EXPLAINS> """CODE.pred = torch.tensor([[0, 0, 0, 0, 0, 0, 0, 0],
...                      [0, 0, 1, 1, 1, 0, 0, 0],
...                      [0, 0, 0, 0, 0, 0, 0, 0]])
target = torch.tensor([[0, 0, 0, 0, 0, 0, 0, 0],
...                        [0, 0, 0, 1, 1, 1, 0, 0],
...                        [0, 0, 0, 0, 0, 0, 0, 0]])
metric = IoU()
metric(pred, target)
tensor(0.7045)""" .

"DESCRIPTION.The code calculates the Jaccard similarity score between two sets of labels." <EXPLAINS> """CODE.import numpy as np
from sklearn.metrics import jaccard_similarity_score
y_pred = [0, 2, 1, 3]
y_true = [0, 1, 2, 3]
jaccard_similarity_score(y_true, y_pred)
0.5
jaccard_similarity_score(y_true, y_pred, normalize=False)
2
jaccard_similarity_score(np.array([[0.0, 1.0], [1.0, 1.0]]), np.ones((2, 2)))
0.75
jaccard_similarity_score([(1, ), (3, )], [(1, 2), tuple()])
0.25""" .

"DESCRIPTION.The code calculates the Jacobian matrix of the function \"func\" with respect to the input tensor x using the PaddlePaddle library. The Jacobian matrix represents all first-order partial derivatives of the function with respect to the input tensor x. The calculated Jacobian matrix is then printed out with specific rows or columns selected for display." <EXPLAINS> """CODE.import paddle


def func(x, y):
    return paddle.matmul(x, y)


x = paddle.to_tensor([[1., 2.], [3., 4.]])
J = paddle.incubate.autograd.Jacobian(func, [x, x])
print(J[:, :])
# Tensor(shape=[4, 8], dtype=float32, place=Place(gpu:0), stop_gradient=False,
#        [[1., 3., 0., 0., 1., 0., 2., 0.],
#         [2., 4., 0., 0., 0., 1., 0., 2.],
#         [0., 0., 1., 3., 3., 0., 4., 0.],
#         [0., 0., 2., 4., 0., 3., 0., 4.]])

print(J[0, :])
# Tensor(shape=[8], dtype=float32, place=Place(gpu:0), stop_gradient=False,
#        [1., 3., 0., 0., 1., 0., 2., 0.])
print(J[:, 0])
# Tensor(shape=[4], dtype=float32, place=Place(gpu:0), stop_gradient=False,
#        [1., 2., 0., 0.]""" .

"DESCRIPTION.The code calculates the Julian datetime corresponding to the provided Julian date and time in days and seconds." <EXPLAINS> """CODE.julian_datetime(2451576, 54362783)
datetime.datetime(2000, 2, 2, 15, 6, 2, 783)""" .

"DESCRIPTION.The code calculates the Kronecker product of two complex arrays 'a' and 'b', converts them to variables using PaddlePaddle library, computes the Kronecker product using paddle.complex.kron function, and then prints the output array." <EXPLAINS> """CODE.import paddle
from paddle import fluid
import paddle.fluid.dygraph as dg

a = np.array([[1.0+1.0j, 2.0+1.0j], [3.0+1.0j, 4.0+1.0j]])
b = np.array([[5.0+2.0j, 6.0+2.0j], [7.0+2.0j, 8.0+2.0j]])

place = fluid.CPUPlace()
with dg.guard(place):
    x = dg.to_variable(a)
    y = dg.to_variable(b)
    out = paddle.complex.kron(x, y)
    print(out.numpy())""" .

"DESCRIPTION.The code calculates the Kronecker product of two complex matrices 'a' and 'b' and prints the result." <EXPLAINS> """CODE.import paddle
from paddle import fluid
import paddle.fluid.dygraph as dg

a = np.array([[1.0+1.0j, 2.0+1.0j], [3.0+1.0j, 4.0+1.0j]])
b = np.array([[5.0+2.0j, 6.0+2.0j], [7.0+2.0j, 8.0+2.0j]])

place = fluid.CPUPlace()
with dg.guard(place):
    x = dg.to_variable(a)
    y = dg.to_variable(b)
    out = paddle.complex.kron(x, y)
    print(out.numpy())""" .

"DESCRIPTION.The code calculates the Kullback-Leibler Divergence (KL Divergence) between two probability distributions represented by y_true and y_pred. It demonstrates how to calculate KL Divergence with different settings such as sample weights, reduction method, and returning the divergence values." <EXPLAINS> """CODE.kl = tf.keras.losses.KLDivergence()
kl(y_true, y_pred).numpy()
0.458

kl(y_true, y_pred, sample_weight=[0.8, 0.2]).numpy()
0.366

kl = tf.keras.losses.KLDivergence(
    reduction=tf.keras.losses.Reduction.SUM)
kl(y_true, y_pred).numpy()
0.916

kl = tf.keras.losses.KLDivergence(
    reduction=tf.keras.losses.Reduction.NONE)
kl(y_true, y_pred).numpy()
array([0.916, -3.08e-06], dtype=float32)
""" .

"DESCRIPTION.The code calculates the Kullback-Leibler Divergence (KL Divergence) between two probability distributions using TensorFlow. The `KLDivergence` object is initialized with different reduction methods (default, SUM, and NONE), and the KL divergence is computed between the true distribution `y_true` and the predicted distribution `y_pred`. The resulting KL divergence values are returned as a float or an array of floats." <EXPLAINS> """CODE.kl = tf.keras.losses.KLDivergence()
kl(y_true, y_pred).numpy()
0.458

kl(y_true, y_pred, sample_weight=[0.8, 0.2]).numpy()
0.366

kl = tf.keras.losses.KLDivergence(
    reduction=tf.keras.losses.Reduction.SUM)
kl(y_true, y_pred).numpy()
0.916

kl = tf.keras.losses.KLDivergence(
    reduction=tf.keras.losses.Reduction.NONE)
kl(y_true, y_pred).numpy()
array([0.916, -3.08e-06], dtype=float32)
""" .

"DESCRIPTION.The code calculates the Linear Predictive Coding (LPC) coefficients of an audio signal and uses them to make a forward prediction of the signal. It then plots the original signal (y) along with the predicted signal (y_hat) to visually compare their waveforms." <EXPLAINS> """CODE.y, sr = librosa.load(librosa.util.example_audio_file(), offset=30,
...                      duration=10)
librosa.lpc(y, 16)

import matplotlib.pyplot as plt
import scipy
y, sr = librosa.load(librosa.util.example_audio_file(), offset=30,
...                      duration=0.020)
a = librosa.lpc(y, 2)
y_hat = scipy.signal.lfilter([0] + -1*a[1:], [1], y)
plt.figure()
plt.plot(y)
plt.plot(y_hat, linestyle='--')
plt.legend(['y', 'y_hat'])
plt.title('LP Model Forward Prediction')
plt.show()""" .

"DESCRIPTION.The code calculates the LogCosh loss function for a given set of true and predicted values. The LogCosh loss function is used to calculate the logarithm of the hyperbolic cosine of the difference between the predicted and true values. It can be used in training machine learning models to measure the difference between predicted and true values." <EXPLAINS> """CODE.logcosh = log((exp(x) + exp(-x))/2)

y_true = [[0., 1.], [0., 0.]]
y_pred = [[1., 1.], [0., 0.]]
l = tf.keras.losses.LogCosh()
l(y_true, y_pred).numpy()

l(y_true, y_pred, sample_weight=[0.8, 0.2]).numpy()

l = tf.keras.losses.LogCosh(
    reduction=tf.keras.losses.Reduction.SUM)
l(y_true, y_pred).numpy()

l = tf.keras.losses.LogCosh(
    reduction=tf.keras.losses.Reduction.NONE)
l(y_true, y_pred).numpy()

model.compile(optimizer='sgd', loss=tf.keras.losses.LogCosh())
""" .

"DESCRIPTION.The code calculates the MFCC (Mel-frequency cepstral coefficients) features, the first order delta MFCCs, and the second order delta MFCCs from an audio signal." <EXPLAINS> """CODE.mfccs       = librosa.feature.mfcc(y=y, sr=sr)
delta_mfcc  = librosa.feature.delta(mfccs)
delta2_mfcc = librosa.feature.delta(mfccs, order=2)""" .

"DESCRIPTION.The code calculates the Mean Absolute Percentage Error (MAPE) between the predicted values (y_pred) and true values (y_true) using different configurations such as default, with sample weights, with reduction to sum, and with no reduction. The output values are the MAPE calculated in percentage for each configuration." <EXPLAINS> """CODE.mape = tf.keras.losses.MeanAbsolutePercentageError()
mape(y_true, y_pred).numpy()
50.

mape(y_true, y_pred, sample_weight=[0.7, 0.3]).numpy()
20.

mape = tf.keras.losses.MeanAbsolutePercentageError(
    reduction=tf.keras.losses.Reduction.SUM)
mape(y_true, y_pred).numpy()
100.

mape = tf.keras.losses.MeanAbsolutePercentageError(
    reduction=tf.keras.losses.Reduction.NONE)
mape(y_true, y_pred).numpy()
array([25., 75.], dtype=float32)
""" .

"DESCRIPTION.The code calculates the Mean Absolute Percentage Error (MAPE) between the true values (y_true) and the predicted values (y_pred) using different reduction methods such as SUM and NONE. It also demonstrates how to compile a model using MAPE as the loss function with the optimizer set to Stochastic Gradient Descent (SGD)." <EXPLAINS> """CODE.mape = tf.keras.losses.MeanAbsolutePercentageError()
mape(y_true, y_pred).numpy()

mape(y_true, y_pred, sample_weight=[0.7, 0.3]).numpy()

mape = tf.keras.losses.MeanAbsolutePercentageError(
    reduction=tf.keras.losses.Reduction.SUM)
mape(y_true, y_pred).numpy()

mape = tf.keras.losses.MeanAbsolutePercentageError(
    reduction=tf.keras.losses.Reduction.NONE)
mape(y_true, y_pred).numpy()

model.compile(optimizer='sgd',
              loss=tf.keras.losses.MeanAbsolutePercentageError())
""" .

"DESCRIPTION.The code calculates the Mean Absolute Percentage Error loss between two arrays and compiles a model using stochastic gradient descent optimizer and the Mean Absolute Percentage Error loss function." <EXPLAINS> """CODE.mape = keras.losses.MeanAbsolutePercentageError()
loss = mape([0., 0., 1., 1.], [1., 1., 1., 0.])
model = keras.Model(inputs, outputs)
model.compile('sgd', loss=keras.losses.MeanAbsolutePercentageError())
""" .

"DESCRIPTION.The code calculates the Mean Absolute Percentage Error loss for a set of predicted and actual values, and then compiles a model using stochastic gradient descent optimizer with Mean Absolute Percentage Error as the loss function." <EXPLAINS> """CODE.mape = keras.losses.MeanAbsolutePercentageError()
loss = mape([0., 0., 1., 1.], [1., 1., 1., 0.])
model = keras.Model(inputs, outputs)
model.compile('sgd', loss=keras.losses.MeanAbsolutePercentageError())
""" .

"DESCRIPTION.The code calculates the Mean Gamma Deviance metric between the predicted values (y_pred) and true values (y_true) using torch tensors." <EXPLAINS> """CODE.y_pred = torch.tensor([0.5, 0.5, 2., 2.])
y_true = torch.tensor([2, 0.5, 1, 4])
metric = MeanGammaDeviance()
metric(y_pred, y_true)
tensor([1.0569])""" .

"DESCRIPTION.The code calculates the Mean Squared Error loss between two arrays and compiles a neural network model using Stochastic Gradient Descent optimizer with Mean Squared Error loss." <EXPLAINS> """CODE.mse = keras.losses.MeanSquaredError()
loss = mse([0., 0., 1., 1.], [1., 1., 1., 0.])
model = keras.Model(inputs, outputs)
model.compile('sgd', loss=keras.losses.MeanSquaredError())""" .

"DESCRIPTION.The code calculates the Mean Squared Logarithmic Error (MSLE) between the true values (y_true) and the predicted values (y_pred). It also demonstrates different ways to customize the MSLE calculation by specifying parameters such as sample weights, reduction method, and optimizer during model compilation." <EXPLAINS> """CODE.loss = square(log(y_true + 1.) - log(y_pred + 1.))

y_true = [[0., 1.], [0., 0.]]
y_pred = [[1., 1.], [1., 0.]]
msle = tf.keras.losses.MeanSquaredLogarithmicError()
msle(y_true, y_pred).numpy()

msle(y_true, y_pred, sample_weight=[0.7, 0.3]).numpy()

msle = tf.keras.losses.MeanSquaredLogarithmicError(
    reduction=tf.keras.losses.Reduction.SUM)
msle(y_true, y_pred).numpy()

msle = tf.keras.losses.MeanSquaredLogarithmicError(
    reduction=tf.keras.losses.Reduction.NONE)
msle(y_true, y_pred).numpy()
array([0.240, 0.240], dtype=float32)

model.compile(optimizer='sgd',
              loss=tf.keras.losses.MeanSquaredLogarithmicError())
""" .

"DESCRIPTION.The code calculates the Mean Squared Logarithmic Error (MSLE) loss between two sets of true and predicted values, with optional sample weights, using TensorFlow's built-in loss function. The code also demonstrates different reduction options for the loss calculation and compiles a model with MSLE as the loss function and Stochastic Gradient Descent (SGD) as the optimizer." <EXPLAINS> """CODE.y_true = [[0., 1.], [0., 0.]]
y_pred = [[1., 1.], [1., 0.]]
msle = tf.keras.losses.MeanSquaredLogarithmicError()
msle(y_true, y_pred).numpy()
msle(y_true, y_pred, sample_weight=[0.7, 0.3]).numpy()
msle = tf.keras.losses.MeanSquaredLogarithmicError(
    reduction=tf.keras.losses.Reduction.SUM)
msle(y_true, y_pred).numpy()
msle = tf.keras.losses.MeanSquaredLogarithmicError(
    reduction=tf.keras.losses.Reduction.NONE)
msle(y_true, y_pred).numpy()
model.compile(optimizer='sgd',
              loss=tf.keras.losses.MeanSquaredLogarithmicError())
""" .

"DESCRIPTION.The code calculates the Mean Squared Logarithmic Error loss between two sets of values and compiles a model using Stochastic Gradient Descent optimizer with the same loss function." <EXPLAINS> """CODE.msle = keras.losses.MeanSquaredLogarithmicError()
loss = msle([0., 0., 1., 1.], [1., 1., 1., 0.])
model = keras.Model(inputs, outputs)
model.compile('sgd', loss=keras.losses.MeanSquaredLogarithmicError())
""" .

"DESCRIPTION.The code calculates the Mel frequencies for 20 bands and applies A-weighting to the frequencies." <EXPLAINS> """CODE.freqs   = librosa.mel_frequencies(20)
librosa.A_weighting(freqs)""" .

"DESCRIPTION.The code calculates the Multiclass ROC (Receiver Operating Characteristic) curve for the predictions `pred` and corresponding true labels `target` using the MulticlassROC class. It returns the true positive rate, false positive rate, and thresholds for each class." <EXPLAINS> """CODE.pred = torch.tensor([[0.85, 0.05, 0.05, 0.05],
...                     [0.05, 0.85, 0.05, 0.05],
...                     [0.05, 0.05, 0.85, 0.05],
...                     [0.05, 0.05, 0.05, 0.85]])
target = torch.tensor([0, 1, 3, 2])
metric = MulticlassROC()
classes_roc = metric(pred, target)
metric(pred, target)   # doctest: +NORMALIZE_WHITESPACE
((tensor([0., 0., 1.]), tensor([0., 1., 1.]), tensor([1.8500, 0.8500, 0.0500])),
 (tensor([0., 0., 1.]), tensor([0., 1., 1.]), tensor([1.8500, 0.8500, 0.0500])),
 (tensor([0.0000, 0.3333, 1.0000]), tensor([0., 0., 1.]), tensor([1.8500, 0.8500, 0.0500])),
 (tensor([0.0000, 0.3333, 1.0000]), tensor([0., 0., 1.]), tensor([1.8500, 0.8500, 0.0500])))""" .

"DESCRIPTION.The code calculates the Normalized Discounted Cumulative Gain (NDCG) score between the true ranking of items (y_true) and the predicted ranking of items (y_score) at a given cut-off point (k) value." <EXPLAINS> """CODE.y_true = [1, 0, 2]
y_score = [[0.15, 0.55, 0.2], [0.7, 0.2, 0.1], [0.06, 0.04, 0.9]]
ndcg_score(y_true, y_score, k=2)
1.0
y_score = [[0.9, 0.5, 0.8], [0.7, 0.2, 0.1], [0.06, 0.04, 0.9]]
ndcg_score(y_true, y_score, k=2)
0.66666666666666663""" .

"DESCRIPTION.The code calculates the Normalized Discounted Cumulative Gain (NDCG) score for a given set of true labels and predicted scores at a given cut-off point k. It first computes the NDCG score for the input y_true and y_score lists when k=2, then updates the y_score list and computes the NDCG score again with the updated scores." <EXPLAINS> """CODE.y_true = [1, 0, 2]
y_score = [[0.15, 0.55, 0.2], [0.7, 0.2, 0.1], [0.06, 0.04, 0.9]]
ndcg_score(y_true, y_score, k=2)
1.0
y_score = [[0.9, 0.5, 0.8], [0.7, 0.2, 0.1], [0.06, 0.04, 0.9]]
ndcg_score(y_true, y_score, k=2)
0.66666666666666663""" .

"DESCRIPTION.The code calculates the Poisson loss between the true values (y_true) and the predicted values (y_pred) using TensorFlow's keras library. It demonstrates different scenarios such as calculating Poisson loss with or without sample weights, with reduction methods like SUM or NONE, and returns the resulting loss values in various formats." <EXPLAINS> """CODE.y_true = [[0., 1.], [0., 0.]]
y_pred = [[1., 1.], [0., 0.]]
p = tf.keras.losses.Poisson()
p(y_true, y_pred).numpy()
0.5

p(y_true, y_pred, sample_weight=[0.8, 0.2]).numpy()
0.4

p = tf.keras.losses.Poisson(
    reduction=tf.keras.losses.Reduction.SUM)
p(y_true, y_pred).numpy()
0.999

p = tf.keras.losses.Poisson(
    reduction=tf.keras.losses.Reduction.NONE)
p(y_true, y_pred).numpy()
array([0.999, 0.], dtype=float32)
""" .

"DESCRIPTION.The code calculates the Poisson loss between the true values y_true and predicted values y_pred." <EXPLAINS> """CODE.y_true = np.random.randint(0, 2, size=(2, 3))
y_pred = np.random.random(size=(2, 3))
loss = tf.keras.losses.poisson(y_true, y_pred)
assert loss.shape == (2,)
y_pred = y_pred + 1e-7
assert np.allclose(
...     loss.numpy(), np.mean(y_pred - y_true * np.log(y_pred), axis=-1),
...     atol=1e-5)""" .

"DESCRIPTION.The code calculates the Poisson loss for a given ground truth array `y_true` and a prediction array `y_pred` using TensorFlow's Poisson loss function. It demonstrates different ways to compute the Poisson loss with varying reductions (NONE, SUM) and sample weights. Finally, it compiles a model using stochastic gradient descent optimizer and Poisson loss function." <EXPLAINS> """CODE.y_true = [[0., 1.], [0., 0.]]
y_pred = [[1., 1.], [0., 0.]]
p = tf.keras.losses.Poisson()
p(y_true, y_pred).numpy()
p(y_true, y_pred, sample_weight=[0.8, 0.2]).numpy()
p = tf.keras.losses.Poisson(reduction=tf.keras.losses.Reduction.SUM)
p(y_true, y_pred).numpy()
p = tf.keras.losses.Poisson(reduction=tf.keras.losses.Reduction.NONE)
p(y_true, y_pred).numpy()
model.compile(optimizer='sgd', loss=tf.keras.losses.Poisson())
""" .

"DESCRIPTION.The code calculates the R-squared score (coefficient of determination) between the target and predicted values using the PyTorch Lightning library." <EXPLAINS> """CODE.from pytorch_lightning.metrics.functional import r2score
target = torch.tensor([3, -0.5, 2, 7])
preds = torch.tensor([2.5, 0.0, 2, 8])
r2score(preds, target)

target = torch.tensor([[0.5, 1], [-1, 1], [7, -6]])
preds = torch.tensor([[0, 2], [-1, 2], [8, -5]])
r2score(preds, target, multioutput='raw_values')""" .

"DESCRIPTION.The code calculates the R-squared score (coefficient of determination) of the predicted values (y_pred) compared to the true values (y_true)." <EXPLAINS> """CODE.y_pred = torch.tensor([2.5, 0.0, 2, 8])
y_true = torch.tensor([3, -0.5, 2, 7])
metric = R2Score()
metric(y_pred, y_true)
tensor([0.9486])""" .

"DESCRIPTION.The code calculates the RNNT loss between the input \"acts\" and the target labels." <EXPLAINS> """CODE.import numpy as np
import paddle
from paddle.nn import RNNTLoss

fn = RNNTLoss(reduction='sum', fastemit_lambda=0.0)

acts = np.array([[[[0.1, 0.6, 0.1, 0.1, 0.1],
                [0.1, 0.1, 0.6, 0.1, 0.1],
                [0.1, 0.1, 0.2, 0.8, 0.1]],
                [[0.1, 0.6, 0.1, 0.1, 0.1],
                [0.1, 0.1, 0.2, 0.1, 0.1],
                [0.7, 0.1, 0.2, 0.1, 0.1]]]])
labels = [[1, 2]]

acts = paddle.to_tensor(acts, stop_gradient=False)

lengths = [acts.shape[1]] * acts.shape[0]
label_lengths = [len(l) for l in labels]
labels = paddle.to_tensor(labels, paddle.int32)
lengths = paddle.to_tensor(lengths, paddle.int32)
label_lengths = paddle.to_tensor(label_lengths, paddle.int32)

costs = fn(acts, labels, lengths, label_lengths)
print(costs)""" .

"DESCRIPTION.The code calculates the Receiver Operating Characteristic (ROC) curve using tensors x and y. It returns the false positive rate (fpr), true positive rate (tpr), and corresponding thresholds." <EXPLAINS> """CODE.x = torch.tensor([0, 1, 2, 3])
y = torch.tensor([0, 1, 1, 1])
fpr, tpr, thresholds = __roc(x, y)
fpr
tensor([0., 0., 0., 0., 1.])
tpr
tensor([0.0000, 0.3333, 0.6667, 1.0000, 1.0000])
thresholds
tensor([4, 3, 2, 1, 0])""" .

"DESCRIPTION.The code calculates the Receiver Operating Characteristic (ROC) curve using the input tensors x and y, and returns the false positive rate (fpr), true positive rate (tpr), and thresholds." <EXPLAINS> """CODE._fpr, _tpr, _thresholds = _roc(x, y)
_fpr
_tpr
_thresholds""",
        """CODE.x = torch.tensor([0, 1, 2, 3])
y = torch.tensor([0, 1, 1, 1])
fpr, tpr, thresholds = __roc(x, y)
fpr
tensor([0., 0., 0., 0., 1.])
tpr
tensor([0.0000, 0.3333, 0.6667, 1.0000, 1.0000])
thresholds
tensor([4, 3, 2, 1, 0])
""" .

"DESCRIPTION.The code calculates the Root Mean Square (RMS) energy of an audio signal and displays it along with the log power spectrogram of the signal using librosa and matplotlib libraries." <EXPLAINS> """CODE.y, sr = librosa.load(librosa.util.example_audio_file())
librosa.feature.rms(y=y)

S, phase = librosa.magphase(librosa.stft(y))
rms = librosa.feature.rms(S=S)

import matplotlib.pyplot as plt
plt.figure()
plt.subplot(2, 1, 1)
plt.semilogy(rms.T, label='RMS Energy')
plt.xticks([])
plt.xlim([0, rms.shape[-1]])
plt.legend(loc='best')
plt.subplot(2, 1, 2)
librosa.display.specshow(librosa.amplitude_to_db(S, ref=np.max),
...                          y_axis='log', x_axis='time')
plt.title('log Power spectrogram')
plt.tight_layout()

S = librosa.magphase(librosa.stft(y, window=np.ones, center=False))[0]
librosa.feature.rms(S=S)""" .

"DESCRIPTION.The code calculates the Root Mean Square Energy (RMSE) of an audio file and creates a plot displaying the RMSE Energy and log Power spectrogram of the audio file." <EXPLAINS> """CODE.y, sr = librosa.load(librosa.util.example_audio_file())
librosa.feature.rmse(y=y)

S, phase = librosa.magphase(librosa.stft(y))
rms = librosa.feature.rmse(S=S)

import matplotlib.pyplot as plt
plt.figure()
plt.subplot(2, 1, 1)
plt.semilogy(rms.T, label='RMS Energy')
plt.xticks([])
plt.xlim([0, rms.shape[-1]])
plt.legend(loc='best')
plt.subplot(2, 1, 2)
librosa.display.specshow(librosa.logamplitude(S**2, ref_power=np.max),
...                          y_axis='log', x_axis='time')
plt.title('log Power spectrogram')
plt.tight_layout()""" .

"DESCRIPTION.The code calculates the Root Mean Square Error (RMSE) between the predicted values (pred) and the target values (target)." <EXPLAINS> """CODE.pred = torch.tensor([0., 1, 2, 3])
target = torch.tensor([0., 1, 2, 2])
metric = RMSE()
metric(pred, target)
tensor(0.5000)""" .

"DESCRIPTION.The code calculates the Root Mean Squared Logarithmic Error (RMSLE) between two tensors x and y." <EXPLAINS> """CODE.x = torch.tensor([0., 1, 2, 3])

y = torch.tensor([0., 1, 2, 2])

mean_squared_log_error(x, y)""",
        """CODE.x = torch.tensor([0., 1, 2, 3])

y = torch.tensor([0., 1, 2, 2])

rmsle(x, y)""" .

"DESCRIPTION.The code calculates the Short-Time Fourier Transform (STFT) of the input signal y using the librosa library, then separates the magnitude and phase components of the STFT. Finally, it checks if the original STFT can be reconstructed by multiplying the magnitude and phase components together." <EXPLAINS> """CODE.D = librosa.stft(y)
S, P = librosa.magphase(D)
D == S * P""" .

"DESCRIPTION.The code calculates the Spence function values for the input values [0.5, 1., 2., 3.]." <EXPLAINS> """CODE.tf.math.special.spence([0.5, 1., 2., 3.]).numpy()
array([ 0.58224034,  0.        , -0.82246685, -1.4367464], dtype=float32)""" .

"DESCRIPTION.The code calculates the accuracy between two sets of data x and y, and returns the value of 0.75." <EXPLAINS> """CODE.accuracy(x, y)
tensor(0.7500)""" .

"DESCRIPTION.The code calculates the accuracy of a model by comparing the predicted values with the actual values." <EXPLAINS> """CODE.m1 = tf.keras.metrics.Accuracy()
_ = m1.update_state([[1], [2]], [[0], [2]])

m2 = tf.keras.metrics.Accuracy()
_ = m2.update_state([[3], [4]], [[3], [4]])

m2.merge_state([m1])
m2.result().numpy()
0.75""" .

"DESCRIPTION.The code calculates the accuracy of a model's predictions on a set of input data. It first initializes a accuracy metric and updates its state with the predicted and actual values. Then it resets the state, updates it again with sample weights, and returns the accuracy as a numpy array. It also compiles a model with stochastic gradient descent optimizer, mean squared error loss function, and accuracy metric for evaluation." <EXPLAINS> """CODE.m = tf.keras.metrics.Accuracy()
m.update_state([[1], [2], [3], [4]], [[0], [2], [3], [4]])
m.result().numpy()
0.75

m.reset_state()
m.update_state([[1], [2], [3], [4]], [[0], [2], [3], [4]],
               sample_weight=[1, 1, 0, 0])
m.result().numpy()
0.5

model.compile(optimizer='sgd',
              loss='mse',
              metrics=[tf.keras.metrics.Accuracy()])
""" .

"DESCRIPTION.The code calculates the accuracy of the predicted values compared to the target values." <EXPLAINS> """CODE.pred = torch.tensor([0, 1, 2, 3])
target = torch.tensor([0, 1, 2, 2])
metric = Accuracy()
metric(pred, target)""" .

"DESCRIPTION.The code calculates the adjoint (conjugate transpose) of matrix x and then performs matrix multiplication between matrix and b with the adjoint of b." <EXPLAINS> """CODE.tf.matrix_adjoint(x) ==> [[1 4]
                          [2 5]
                          [-3j 6j]]
tf.matmul(matrix, b, adjoint_b=True)
""" .

"DESCRIPTION.The code calculates the average pooling operation on a 3D input tensor x with a pool size of 2 and strides of 2." <EXPLAINS> """CODE. y = tf.compat.v1.layers.average_pooling3d(x, pool_size=2, strides=2)
""" .

"DESCRIPTION.The code calculates the average precision between two tensors x and y." <EXPLAINS> """CODE.x = torch.tensor([0, 1, 2, 3])
y = torch.tensor([0, 1, 2, 2])
average_precision(x, y)""" .

"DESCRIPTION.The code calculates the balanced accuracy score for the predicted values 'y_pred' and true values 'y_true'." <EXPLAINS> """CODE.y_pred = torch.tensor([0, 0, 0, 1])
y_true = torch.tensor([0, 0, 1, 1])
metric = BalancedAccuracy()
metric(y_pred, y_true)
tensor([0.7500])""" .

"DESCRIPTION.The code calculates the batch matrix-matrix product of two input tensors x and y." <EXPLAINS> """CODE.import paddle
import paddle.fluid as fluid
x = fluid.layers.data(name='x', shape=[10, 3, 4], dtype='float32')
y = fluid.layers.data(name='y', shape=[10, 4, 5], dtype='float32')
out = fluid.layers.bmm(x, y)

input1 = np.array([[[1.0, 1.0, 1.0],[2.0, 2.0, 2.0]],[[3.0, 3.0, 3.0],[4.0, 4.0, 4.0]]])
input2 = np.array([[[1.0, 1.0],[2.0, 2.0],[3.0, 3.0]],[[4.0, 4.0],[5.0, 5.0],[6.0, 6.0]]])
with fluid.dygraph.guard():
    x = fluid.dygraph.to_variable(input1)
    y = fluid.dygraph.to_variable(input2)
    out = fluid.layers.bmm(x, y)
    out_np = out.numpy()""" .

"DESCRIPTION.The code calculates the binary accuracy between the true labels (y_true) and the predicted labels (y_pred) for a binary classification task." <EXPLAINS> """CODE.y_true = [[1], [1], [0], [0]]
y_pred = [[1], [1], [0], [0]]
m = tf.keras.metrics.binary_accuracy(y_true, y_pred)
assert m.shape == (4,)
m.numpy()
array([1., 1., 1., 1.], dtype=float32)""" .

"DESCRIPTION.The code calculates the binary cross-entropy loss between the predicted values and the actual values for a binary classification model and compiles the model using stochastic gradient descent optimizer and binary cross-entropy loss function." <EXPLAINS> """CODE.bce = keras.losses.BinaryCrossentropy()
loss = bce([0., 0., 1., 1.], [1., 1., 1., 0.])
model = keras.Model(inputs, outputs)
model.compile('sgd', loss=keras.losses.BinaryCrossentropy())
""" .

"DESCRIPTION.The code calculates the binary cross-entropy loss between the predicted values and the actual values, and compiles a neural network model using stochastic gradient descent optimizer with binary cross-entropy loss function." <EXPLAINS> """CODE.bce = keras.losses.BinaryCrossentropy()
loss = bce([0., 0., 1., 1.], [1., 1., 1., 0.])
model = keras.Model(inputs, outputs)
model.compile('sgd', loss=keras.losses.BinaryCrossentropy())
""" .

"DESCRIPTION.The code calculates the binary cross-entropy loss between the true labels and predicted logits using TensorFlow's BinaryCrossentropy loss function with different configurations such as weights, reductions, and multiple label formats." <EXPLAINS> """CODE.model.compile(
  loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),
  ....
)

y_true = [0, 1, 0, 0]
y_pred = [-18.6, 0.51, 2.94, -12.8]
bce = tf.keras.losses.BinaryCrossentropy(from_logits=True)
bce(y_true, y_pred).numpy()

y_true = [[0, 1], [0, 0]]
y_pred = [[-18.6, 0.51], [2.94, -12.8]]
bce = tf.keras.losses.BinaryCrossentropy(from_logits=True)
bce(y_true, y_pred).numpy()
bce(y_true, y_pred, sample_weight=[0.8, 0.2]).numpy()
bce = tf.keras.losses.BinaryCrossentropy(from_logits=True,
    reduction=tf.keras.losses.Reduction.SUM)
bce(y_true, y_pred).numpy()
bce = tf.keras.losses.BinaryCrossentropy(from_logits=True,
    reduction=tf.keras.losses.Reduction.NONE)
bce(y_true, y_pred).numpy()
""" .

"DESCRIPTION.The code calculates the binary crossentropy loss between the true labels and predicted values using the BinaryCrossentropy class from TensorFlow. It includes different variations of calculating the loss such as with or without sample weights, and with different reduction methods. It also demonstrates how to use the BinaryCrossentropy class without explicitly passing in arguments." <EXPLAINS> """CODE.model.compile(
  loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),
  ....
)

y_true = [0, 1, 0, 0]
y_pred = [-18.6, 0.51, 2.94, -12.8]
bce = tf.keras.losses.BinaryCrossentropy(from_logits=True)
bce(y_true, y_pred).numpy()

y_true = [[0, 1], [0, 0]]
y_pred = [[-18.6, 0.51], [2.94, -12.8]]
bce = tf.keras.losses.BinaryCrossentropy(from_logits=True)
bce(y_true, y_pred).numpy()
bce(y_true, y_pred, sample_weight=[0.8, 0.2]).numpy()
bce = tf.keras.losses.BinaryCrossentropy(from_logits=True,
    reduction=tf.keras.losses.Reduction.SUM)
bce(y_true, y_pred).numpy()
bce = tf.keras.losses.BinaryCrossentropy(from_logits=True,
    reduction=tf.keras.losses.Reduction.NONE)
bce(y_true, y_pred).numpy()

tf.keras.losses.BinaryCrossentropy()
y_pred = [0.6, 0.3, 0.2, 0.8]
""" .

"DESCRIPTION.The code calculates the categorical accuracy of a model's predictions compared to the true labels. It uses the CategoricalAccuracy metric to update and reset its state for different sets of predictions and true labels, including sample weights. Finally, it compiles a model with stochastic gradient descent optimizer, mean squared error loss, and categorical accuracy as one of the metrics." <EXPLAINS> """CODE.m = tf.keras.metrics.CategoricalAccuracy()
m.update_state([[0, 0, 1], [0, 1, 0]], [[0.1, 0.9, 0.8],
...                 [0.05, 0.95, 0]])
m.result().numpy()
0.5

m.reset_state()
m.update_state([[0, 0, 1], [0, 1, 0]], [[0.1, 0.9, 0.8],
...                 [0.05, 0.95, 0]],
...                sample_weight=[0.7, 0.3])
m.result().numpy()
0.3

model.compile(
  optimizer='sgd',
  loss='mse',
  metrics=[tf.keras.metrics.CategoricalAccuracy()])
""" .

"DESCRIPTION.The code calculates the categorical cross-entropy loss between two sets of probability distributions." <EXPLAINS> """CODE.cce = keras.losses.CategoricalCrossentropy()
loss = cce(
    [[1., 0., 0.], [0., 1., 0.], [0., 0., 1.]],
    [[.9, .05, .05], [.5, .89, .6], [.05, .01, .94]])

model = keras.Model(inputs, outputs)
model.compile('sgd', loss=keras.losses.CategoricalCrossentropy())
""" .

"DESCRIPTION.The code calculates the categorical crossentropy loss between the given true labels and predicted probabilities." <EXPLAINS> """CODE.cce = keras.losses.CategoricalCrossentropy()
loss = cce(
    [[1., 0., 0.], [0., 1., 0.], [0., 0., 1.]],
    [[.9, .05, .05], [.5, .89, .6], [.05, .01, .94]])

model = keras.Model(inputs, outputs)
model.compile('sgd', loss=keras.losses.CategoricalCrossentropy())
""" .

"DESCRIPTION.The code calculates the categorical crossentropy loss between the predicted values and the actual values in a classification task." <EXPLAINS> """CODE.cce = keras.losses.CategoricalCrossentropy()
loss = cce(
    [[1., 0., 0.], [0., 1., 0.], [0., 0., 1.]],
    [[.9, .05, .05], [.5, .89, .6], [.05, .01, .94]])

model = keras.Model(inputs, outputs)
model.compile('sgd', loss=keras.losses.CategoricalCrossentropy())
""" .

"DESCRIPTION.The code calculates the categorical crossentropy loss between the true labels (y_true) and predicted labels (y_pred) using different reduction types (auto, sum_over_batch_size, sum, none) in TensorFlow. It returns the loss values in the form of a single loss value for 'auto' and 'sum' reduction types, and an array of loss values for 'none' reduction type." <EXPLAINS> """CODE.y_true = [[0, 1, 0], [0, 0, 1]]
y_pred = [[0.05, 0.95, 0], [0.1, 0.8, 0.1]]
# Using 'auto'/'sum_over_batch_size' reduction type.
cce = tf.keras.losses.CategoricalCrossentropy()
cce(y_true, y_pred).numpy()
1.177

# Calling with 'sample_weight'.
cce(y_true, y_pred, sample_weight=tf.constant([0.3, 0.7])).numpy()
0.814

# Using 'sum' reduction type.
cce = tf.keras.losses.CategoricalCrossentropy(
...     reduction=tf.keras.losses.Reduction.SUM)
cce(y_true, y_pred).numpy()
2.354

# Using 'none' reduction type.
cce = tf.keras.losses.CategoricalCrossentropy(
...     reduction=tf.keras.losses.Reduction.NONE)
cce(y_true, y_pred).numpy()
array([0.0513, 2.303], dtype=float32)""" .

"DESCRIPTION.The code calculates the categorical crossentropy loss between two sets of categorical data represented as tensors 'a' and 'b'. It then prints out the rounded values of the calculated loss." <EXPLAINS> """CODE.a = tf.keras.utils.to_categorical([0, 1, 2, 3], num_classes=4)
a = tf.constant(a, shape=[4, 4])
print(a)
tf.Tensor(
  [[1. 0. 0. 0.]
   [0. 1. 0. 0.]
   [0. 0. 1. 0.]
   [0. 0. 0. 1.]], shape=(4, 4), dtype=float32)

b = tf.constant([.9, .04, .03, .03,
...                  .3, .45, .15, .13,
...                  .04, .01, .94, .05,
...                  .12, .21, .5, .17],
...                 shape=[4, 4])
loss = tf.keras.backend.categorical_crossentropy(a, b)
print(np.around(loss, 5))
[0.10536 0.82807 0.1011  1.77196]

loss = tf.keras.backend.categorical_crossentropy(a, a)
print(np.around(loss, 5))
[0. 0. 0. 0.]""" .

"DESCRIPTION.The code calculates the categorical crossentropy loss between two sets of probability distributions." <EXPLAINS> """CODE.a = tf.constant([1., 0., 0., 0., 1., 0., 0., 0., 1.], shape=[3,3])
b = tf.constant([.9, .05, .05, .05, .89, .06, .05, .01, .94], shape=[3,3])
loss = tf.keras.backend.categorical_crossentropy(a, b)
loss = tf.keras.backend.categorical_crossentropy(a, a)
print(np.around(loss, 5))""" .

"DESCRIPTION.The code calculates the chroma spectrogram of an audio file \"test.wav\" by using the input parameters such as number of chroma bins, tuning, central octave, octave width, normalization factor, and base C. It first loads the audio file and then applies spectrogram and chroma scale transformations to obtain the chroma spectrogram." <EXPLAINS> """CODE.n_chroma = 12
tuning = 0.0
ctroct = 5.0
octwidth = 2.0
norm = 2
base_c = True

waveform, sample_rate = torchaudio.load("test.wav", normalize=True)
spectrogram_transform = transforms.Spectrogram(n_fft=1024)
spectrogram = spectrogram_transform(waveform)
chroma_transform = transforms.ChromaScale(sample_rate=sample_rate, n_freqs=1024 // 2 + 1)
chroma_spectrogram = chroma_transform(spectrogram)
""" .

"DESCRIPTION.The code calculates the chromagram of an audio signal 'y' using the librosa library." <EXPLAINS> """CODE.C = librosa.chromagram(y, sr)

S = np.abs(librosa.stft(y, n_fft=4096))
C = librosa.chromagram(S=S)
""" .

"DESCRIPTION.The code calculates the connected components of a graph represented by the input matrix D." <EXPLAINS> """CODE.from scipy.sparse import cs_graph_components
import numpy as np
D = np.eye(4)
D[0,1] = D[1,0] = 1
cs_graph_components(D)

from scipy.sparse import dok_matrix
cs_graph_components(dok_matrix(D))""" .

"DESCRIPTION.The code calculates the constant-Q transform (CQT) of an audio signal y with a sampling rate sr using a specified basis filter with a resolution of 2." <EXPLAINS> """CODE.basis   = librosa.filters.constant_q(22050)
CQT     = librosa.cqt(y, sr, basis=basis)

basis   = librosa.filters.constant_q(22050, window=np.hanning)

basis   = librosa.filters.constant_q(22050, resolution=2)""" .

"DESCRIPTION.The code calculates the cosine integral of the array [-1., -0.1, 0.1, 1.]." <EXPLAINS> "CODE.tf.math.special.fresnel_cos([-1., -0.1, 0.1, 1.]).numpy()" .

"DESCRIPTION.The code calculates the cosine similarity loss between the true values (y_true) and the predicted values (y_pred). It can be customized by adding sample weights, changing the reduction method, or specifying the axis for computation." <EXPLAINS> """CODE.loss = -sum(l2_norm(y_true) * l2_norm(y_pred))


cosine_loss = tf.keras.losses.CosineSimilarity(axis=1)


cosine_loss(y_true, y_pred).numpy()


cosine_loss(y_true, y_pred, sample_weight=[0.8, 0.2]).numpy()


cosine_loss = tf.keras.losses.CosineSimilarity(axis=1,
    reduction=tf.keras.losses.Reduction.SUM)


cosine_loss(y_true, y_pred).numpy()


cosine_loss = tf.keras.losses.CosineSimilarity(axis=1,
    reduction=tf.keras.losses.Reduction.NONE)


cosine_loss(y_true, y_pred).numpy()


model.compile(optimizer='sgd', loss=tf.keras.losses.CosineSimilarity(axis=1))
""" .

"DESCRIPTION.The code calculates the cumulative count of each group in column 'A' in the DataFrame 'df'." <EXPLAINS> """CODE.df.groupby('A').cumcount()
df.groupby('A').cumcount(ascending=False)""" .

"DESCRIPTION.The code calculates the decayed learning rate based on the global step, initial learning rate, end learning rate, decay steps, and a power value. The learning rate decays over time according to a polynomial decay formula." <EXPLAINS> """CODE.global_step = min(global_step, decay_steps)
decayed_learning_rate = (learning_rate - end_learning_rate) * (1 - global_step / decay_steps) ** power + end_learning_rate


decay_steps = decay_steps * ceil(global_step / decay_steps)
decayed_learning_rate = (learning_rate - end_learning_rate) * (1 - global_step / decay_steps) ** power + end_learning_rate


global_step = tf.Variable(0, trainable=False)
starter_learning_rate = 0.1
end_learning_rate = 0.01
decay_steps = 10000
learning_rate = tf.compat.v1.train.polynomial_decay(starter_learning_rate, global_step, decay_steps, end_learning_rate, power=0.5)
learning_step = (tf.compat.v1.train.GradientDescentOptimizer(learning_rate).minimize(...my loss..., global_step=global_step))
""" .

"DESCRIPTION.The code calculates the derivative of the sum of the output of the mod module with respect to its input variables for debugging purposes." <EXPLAINS> """CODE.mod = ...
comm_mode = CommDebugMode()
with comm_mode:
    mod.sum().backward()
""" .

"DESCRIPTION.The code calculates the detection error tradeoff (DET) curve using the given true labels and predicted scores. The DET curve shows the false positive rate (fpr) and false negative rate (fnr) at different decision thresholds. The thresholds at which the fpr and fnr are calculated are also returned." <EXPLAINS> """CODE.from sklearn.metrics import det_curve
y_true = np.array([0, 0, 1, 1])
y_scores = np.array([0.1, 0.4, 0.35, 0.8])
fpr, fnr, thresholds = det_curve(y_true, y_scores)
fpr
array([0.5, 0.5, 0. ])
fnr
array([0. , 0.5, 0.5])
thresholds
array([0.35, 0.4 , 0.8 ])""" .

"DESCRIPTION.The code calculates the difference between two dictionaries `old` and `new` by comparing their key-value pairs, and returns the delta." <EXPLAINS> """CODE.old = {"a": 1, "b": 2}
new = {"a": 3, "d": 4}
compute_dict_delta(old, new)
""" .

"DESCRIPTION.The code calculates the difference between two index objects." <EXPLAINS> """CODE.index - index2
index.diff(index2)""",
        "CODE.index.difference(index2)" .

"DESCRIPTION.The code calculates the dot product between two sets of input data using TensorFlow's Dot layer with specific axes, resulting in a tensor of shape (1, 2, 2) for the first set of data. It then applies a Dense layer to two sets of input data, calculates the dot product between them, and returns a tensor of shape (5, 1)." <EXPLAINS> """CODE.x = np.arange(10).reshape(1, 5, 2)
print(x)
[[[0 1]
  [2 3]
  [4 5]
  [6 7]
  [8 9]]]
y = np.arange(10, 20).reshape(1, 2, 5)
print(y)
[[[10 11 12 13 14]
  [15 16 17 18 19]]]
tf.keras.layers.Dot(axes=(1, 2))([x, y])
<tf.Tensor: shape=(1, 2, 2), dtype=int64, numpy=
array([[[260, 360],
        [320, 445]]])>

x1 = tf.keras.layers.Dense(8)(np.arange(10).reshape(5, 2))
x2 = tf.keras.layers.Dense(8)(np.arange(10, 20).reshape(5, 2))
dotted = tf.keras.layers.Dot(axes=1)([x1, x2])
dotted.shape
TensorShape([5, 1])""" .

"DESCRIPTION.The code calculates the dot product between two tensors and returns the resulting tensor shape and data type." <EXPLAINS> """CODE.    # dot product between tensors
    x = K.placeholder(shape=(2, 3))
    y = K.placeholder(shape=(3, 4))
    xy = K.dot(x, y)
    xy
    <tf.Tensor 'MatMul_9:0' shape=(2, 4) dtype=float32>


    # dot product between tensors
    x = K.placeholder(shape=(32, 28, 3))
    y = K.placeholder(shape=(3, 4))
    xy = K.dot(x, y)
    xy
    <tf.Tensor 'MatMul_9:0' shape=(32, 28, 4) dtype=float32>


    # Theano-like behavior example
    x = K.random_uniform_variable(shape=(2, 3), low=0, high=1)
    y = K.ones((4, 3, 5))
    xy = K.dot(x, y)
    K.int_shape(xy)
    (2, 4, 5)
""" .

"DESCRIPTION.The code calculates the dot product between two tensors." <EXPLAINS> """CODE.    # dot product between tensors
    x = K.placeholder(shape=(2, 3))
    y = K.placeholder(shape=(3, 4))
    xy = K.dot(x, y)
    xy


    # dot product between tensors
    x = K.placeholder(shape=(32, 28, 3))
    y = K.placeholder(shape=(3, 4))
    xy = K.dot(x, y)
    xy


    # Theano-like behavior example
    x = K.random_uniform_variable(shape=(2, 3), low=0, high=1)
    y = K.ones((4, 3, 5))
    xy = K.dot(x, y)
    K.int_shape(xy)
""",
        """CODE.    # dot product between tensors
    x = K.placeholder(shape=(2, 3))
    y = K.placeholder(shape=(3, 4))
    xy = K.dot(x, y)
    xy
    <tf.Tensor 'MatMul_9:0' shape=(2, 4) dtype=float32>


    # dot product between tensors
    x = K.placeholder(shape=(32, 28, 3))
    y = K.placeholder(shape=(3, 4))
    xy = K.dot(x, y)
    xy
    <tf.Tensor 'MatMul_9:0' shape=(32, 28, 4) dtype=float32>


    # Theano-like behavior example
    x = K.random_uniform_variable(shape=(2, 3), low=0, high=1)
    y = K.ones((4, 3, 5))
    xy = K.dot(x, y)
    K.int_shape(xy)
    (2, 4, 5)
""" .

"DESCRIPTION.The code calculates the dot product of two dictionaries of arrays." <EXPLAINS> """CODE.optax.tree_utils.tree_vdot(
  {a: jnp.array([1, 2]), b: jnp.array([1, 2])},
  {a: jnp.array([-1, -1]), b: jnp.array([1, 1])},
)""" .

"DESCRIPTION.The code calculates the dot product of two tensors x and y using TensorFlow's backend operations." <EXPLAINS> """CODE.x = tf.keras.backend.placeholder(shape=(2, 3))
y = tf.keras.backend.placeholder(shape=(3, 4))
xy = tf.keras.backend.dot(x, y)

x = tf.keras.backend.placeholder(shape=(32, 28, 3))
y = tf.keras.backend.placeholder(shape=(3, 4))
xy = tf.keras.backend.dot(x, y)

x = tf.keras.backend.random_uniform_variable(shape=(2, 3), low=0, high=1)
y = tf.keras.backend.ones((4, 3, 5))
xy = tf.keras.backend.dot(x, y)
tf.keras.backend.int_shape(xy)""" .

"DESCRIPTION.The code calculates the duration of audio signal loaded using Librosa library, with and without centering the short-time Fourier transform (STFT)." <EXPLAINS> """CODE.y, sr = librosa.load(librosa.util.example_audio())
d = librosa.get_duration(y=y, sr=sr)
d


S = librosa.stft(y)
d = librosa.get_duration(S=S, sr=sr)


S_left = librosa.stft(y, center=False)
d = librosa.get_duration(S=S_left, sr=sr)
""" .

"DESCRIPTION.The code calculates the element-wise maximum between two sets of values obtained through Dense layers and reshaping of arrays." <EXPLAINS> """CODE.x1 = tf.keras.layers.Dense(8)(np.arange(10).reshape(5, 2)
x2 = tf.keras.layers.Dense(8)(np.arange(10, 20).reshape(5, 2)
maxed = tf.keras.layers.Maximum()([x1, x2])
maxed.shape""" .

"DESCRIPTION.The code calculates the entropy of a Bernoulli distribution with a probability of success of 0.3." <EXPLAINS> """CODE.import paddle
from paddle.distribution import Bernoulli

rv = Bernoulli(0.3)
print(rv.entropy())""" .

"DESCRIPTION.The code calculates the explained variance between the predicted tensor and the true tensor values." <EXPLAINS> """CODE.y_pred = torch.tensor([2.5, 0.0, 2, 8])
y_true = torch.tensor([3, -0.5, 2, 7])
metric = ExplainedVariance()
metric(y_pred, y_true)
tensor([0.9572])""" .

"DESCRIPTION.The code calculates the explained variance between the predicted values (y_pred) and the true values (y_true) using the ExplainedVariance metric, and returns the result as a tensor." <EXPLAINS> """CODE.y_pred = torch.tensor([2.5, 0.0, 2, 8])
y_true = torch.tensor([3, -0.5, 2, 7])
metric = ExplainedVariance()
metric(y_pred, y_true)
tensor([0.9572])""" .

"DESCRIPTION.The code calculates the explained variance between the target and predicted tensors." <EXPLAINS> """CODE.from pytorch_lightning.metrics.functional import explained_variance
target = torch.tensor([3, -0.5, 2, 7])
preds = torch.tensor([2.5, 0.0, 2, 8])
explained_variance(preds, target)

target = torch.tensor([[0.5, 1], [-1, 1], [7, -6]])
preds = torch.tensor([[0, 2], [-1, 2], [8, -5]])
explained_variance(preds, target, multioutput='raw_values')""" .

"DESCRIPTION.The code calculates the false positive rate (fps), true positive rate (tps), and thresholds for a Receiver Operating Characteristic (ROC) curve based on the predicted values (pred) and the target values (target)." <EXPLAINS> """CODE.pred = torch.tensor([0, 1, 2, 3])
target = torch.tensor([0, 1, 2, 2])
metric = ROC()
fps, tps, thresholds = metric(pred, target)
fps
tensor([0.0000, 0.3333, 0.6667, 0.6667, 1.0000])
tps
tensor([0., 0., 0., 1., 1.])
thresholds
tensor([4., 3., 2., 1., 0.])""" .

"DESCRIPTION.The code calculates the false positive rate (fps), true positive rate (tps), and thresholds for receiver operating characteristic (ROC) analysis using the predicted values (pred) and target values (target) tensors." <EXPLAINS> """CODE.pred = torch.tensor([0, 1, 2, 3])
target = torch.tensor([0, 1, 2, 2])
metric = ROC()
fps, tps, thresholds = metric(pred, target)
fps
tensor([0.0000, 0.3333, 0.6667, 0.6667, 1.0000])
tps
tensor([0., 0., 0., 1., 1.])
thresholds
tensor([4., 3., 2., 1., 0.])""" .

"DESCRIPTION.The code calculates the false positive rates (fps), true positive rates (tps), and thresholds for a ROC (Receiver Operating Characteristic) curve based on the predicted values (pred) and the target values (target)." <EXPLAINS> """CODE.pred = torch.tensor([0, 1, 2, 3])
target = torch.tensor([0, 1, 2, 2])
metric = ROC()
fps, tps, thresholds = metric(pred, target)""" .

"DESCRIPTION.The code calculates the forward floating point operations (flops) of a model using a given input data, and then calculates the combined forward and backward flops of the model when a loss function is added." <EXPLAINS> """CODE.model_fwd = lambda: model(x)
fwd_flops = measure_flops(model, model_fwd)

model_loss = lambda y: y.sum()
fwd_and_bwd_flops = measure_flops(model, model_fwd, model_loss)
""" .

"DESCRIPTION.The code calculates the forward pass of a linear neural network on a randomly generated input data, computes the mean loss of the output, and performs backward propagation to update the network parameters using the SGD optimizer with gradient clipping by value." <EXPLAINS> """CODE.import paddle

x = paddle.uniform([10, 10], min=-1.0, max=1.0, dtype='float32')
linear = paddle.nn.Linear(in_features=10, out_features=10,
                          weight_attr=paddle.ParamAttr(need_clip=True),
                          bias_attr=paddle.ParamAttr(need_clip=False))
out = linear(x)
loss = paddle.mean(out)
loss.backward()

clip = paddle.nn.ClipGradByValue(min=-1, max=1)
sdg = paddle.optimizer.SGD(learning_rate=0.1, parameters=linear.parameters(), grad_clip=clip)
sdg.step()""" .

"DESCRIPTION.The code calculates the frequencies of intervals within a specified range, using different tuning systems such as Pythagorean and Just Intonation with a base frequency of 55 Hz and dividing the octave into 12 bins." <EXPLAINS> """CODE.librosa.interval_frequencies(24, fmin=55, intervals="pythagorean", bins_per_octave=12)
librosa.interval_frequencies(24, fmin=55, intervals="ji5", bins_per_octave=12)
intervals = [1, 4/3, 3/2]
librosa.interval_frequencies(9, fmin=55, intervals=intervals)""" .

"DESCRIPTION.The code calculates the frequency of unique elements in the input list and returns a dictionary where the keys are the elements and the values are the corresponding frequencies." <EXPLAINS> """CODE.pbag([1, 2, 3, 2])
pbag([1, 2, 2, 3])""" .

"DESCRIPTION.The code calculates the fused embedding sequence pooling for the input data with specified dictionary size and embedding dimensions, using the specified padding index." <EXPLAINS> """CODE.import numpy as np
import paddle.fluid as fluid

dict_size = 20
data_t = fluid.layers.data(name='word', shape=[1], dtype='int64', lod_level=1)
padding_idx = np.random.randint(1, 10)
out = fluid.contrib.fused_embedding_seq_pool(
    input=data_t,
    size=[dict_size, 32],
    param_attr='w',
    padding_idx=padding_idx,
    is_sparse=False)""" .

"DESCRIPTION.The code calculates the global average pooling along the Time axis for a 3D tensor x with shape (2, 3, 4), resulting in a new tensor y with shape (2, 4)." <EXPLAINS> """CODE.input_shape = (2, 3, 4)
x = tf.random.normal(input_shape)
y = tf.keras.layers.GlobalAveragePooling1D()(x)
print(y.shape)
(2, 4)""" .

"DESCRIPTION.The code calculates the gradients of the given samples using a certain method implemented in the Ev2 object." <EXPLAINS> """CODE.batch = ev.sample()
grads, info = ev2.compute_gradients(samples)""" .

"DESCRIPTION.The code calculates the hard sigmoid activation function on a given input array using TensorFlow." <EXPLAINS> """CODE.a = tf.constant([-3.0,-1.0, 0.0,1.0,3.0], dtype = tf.float32)
b = tf.keras.activations.hard_sigmoid(a)
b.numpy()""" .

"DESCRIPTION.The code calculates the hinge loss between the predicted decisions and the true labels." <EXPLAINS> """CODE.pred_decision = torch.tensor([-2.17, -0.97, -0.19, -0.43])
y_true = torch.tensor([1, 1, 0, 0])
metric = Hinge()
metric(pred_decision, y_true)
tensor([1.6300])""" .

"DESCRIPTION.The code calculates the inverse of a lower triangular matrix." <EXPLAINS> """CODE.tfd.bijectors.MatrixInverseTriL().forward(x=[[1., 0], [2, 1]])
# Result: [[1., 0], [-2, 1]], i.e., inv(x)

tfd.bijectors.MatrixInverseTriL().inverse(y=[[1., 0], [-2, 1]])
# Result: [[1., 0], [2, 1]], i.e., inv(y).
""" .

"DESCRIPTION.The code calculates the learning rate decay based on the step number in the training process using a formula that divides the initial learning rate by (1 + decay_rate * step / decay_step)." <EXPLAINS> """CODE.def decayed_learning_rate(step):
  return initial_learning_rate / (1 + decay_rate * step / decay_step)


def decayed_learning_rate(step):
  return initial_learning_rate / (1 + decay_rate * floor(step / decay_step))


...
initial_learning_rate = 0.1
decay_steps = 1.0
decay_rate = 0.5
learning_rate_fn = keras.optimizers.schedules.InverseTimeDecay(
  initial_learning_rate, decay_steps, decay_rate)

model.compile(optimizer=tf.keras.optimizers.SGD(
                  learning_rate=learning_rate_fn),
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])

model.fit(data, labels, epochs=5)
""" .

"DESCRIPTION.The code calculates the log probability mass function value for a Geometric distribution with a probability of success of 0.5 when the input value is 2." <EXPLAINS> """CODE.import paddle
from paddle.distribution import Geometric

geom = Geometric(0.5)
geom.log_pmf(2)""" .

"DESCRIPTION.The code calculates the log softmax function for the input data along the last dimension using the PaddlePaddle framework." <EXPLAINS> """CODE.import paddle.fluid as fluid
import numpy as np

data = np.array([[[-2.0, 3.0, -4.0, 5.0],
                [3.0, -4.0, 5.0, -6.0],
                [-7.0, -8.0, 8.0, 9.0]],
               [[1.0, -2.0, -3.0, 4.0],
                [-5.0, 6.0, 7.0, -8.0],
                [6.0, 7.0, 8.0, 9.0]]]).astype('float32')
with fluid.dygraph.guard():
    data = fluid.dygraph.to_variable(data)
    res = fluid.layers.log_softmax(data, -1)""" .

"DESCRIPTION.The code calculates the log-sum-exponential operation on the input tensor x along specified dimensions and prints the result." <EXPLAINS> """CODE.import paddle
import paddle.fluid as fluid
import numpy as np

with fluid.dygraph.guard():
  np_x = np.random.uniform(0.1, 1, [10]).astype(np.float32)
  x = fluid.dygraph.to_variable(np_x)
  print(fluid.layers.logsumexp(x).numpy())

import paddle
import paddle.fluid as fluid
import numpy as np

with fluid.dygraph.guard():
    np_x = np.random.uniform(0.1, 1, [2, 3, 4]).astype(np.float32)
    x = fluid.dygraph.to_variable(np_x)
    print(fluid.layers.logsumexp(x, dim=1).numpy())
    print(fluid.layers.logsumexp(x, dim=[0, 2]).numpy())""" .

"DESCRIPTION.The code calculates the logarithm of each element in the input array x, using natural logarithm with base 1. The input array x is a 2x1 array with dtype float32. The result is stored in the variable res. The code then creates an executor using CPU, executes the operation using the input array x_i, and prints the result which is an array of natural logarithms of the input elements." <EXPLAINS> """CODE.import paddle
import paddle.fluid as fluid
import numpy as np
# Graph Organizing
x = fluid.data(name="x", shape=[2,1], dtype="float32")
res = fluid.layers.log1p(x)
# Create an executor using CPU as an example
exe = fluid.Executor(fluid.CPUPlace())
# Execute
x_i = np.array([[0], [1]]).astype(np.float32)
res_val, = exe.run(fluid.default_main_program(), feed={'x':x_i}, fetch_list=[res])
print(res_val) # [[0.], [0.6931472]]""" .

"DESCRIPTION.The code calculates the loss function for a Poisson regression model using the predicted values `y_pred` and the true values `y_true`, then compiles the model using stochastic gradient descent optimizer." <EXPLAINS> """CODE.loss = y_pred - y_true * log(y_pred)
model = keras.Model(inputs, outputs)
model.compile('sgd', loss=keras.losses.Poisson())
""" .

"DESCRIPTION.The code calculates the loss function for a regression problem using the Huber loss function. If the absolute value of the input x is less than or equal to a threshold d, the loss is calculated as 0.5 * x^2. If the absolute value of x is greater than d, the loss is calculated as 0.5 * d^2 + d * (abs(x) - d)." <EXPLAINS> """CODE.loss = 0.5 * x**2 if abs(x) <= d
loss = 0.5 * d**2 + d * (abs(x) - d) if abs(x) > d


model = keras.Model(inputs, outputs)
model.compile('sgd', loss=keras.losses.Huber())
""" .

"DESCRIPTION.The code calculates the loss using the Categorical Hinge loss function for a given true and predicted value." <EXPLAINS> """CODE.loss = maximum(neg - pos + 1, 0)

neg=maximum((1-y_true)*y_pred) and pos=sum(y_true*y_pred)

y_true = [[0, 1], [0, 0]]
y_pred = [[0.6, 0.4], [0.4, 0.6]]
h = tf.keras.losses.CategoricalHinge()
h(y_true, y_pred).numpy()

h(y_true, y_pred, sample_weight=[1, 0]).numpy()

h = tf.keras.losses.CategoricalHinge(
    reduction=tf.keras.losses.Reduction.SUM)
h(y_true, y_pred).numpy()

h = tf.keras.losses.CategoricalHinge(
    reduction=tf.keras.losses.Reduction.NONE)
h(y_true, y_pred).numpy()

model.compile(optimizer='sgd', loss=tf.keras.losses.CategoricalHinge())
""" .

"DESCRIPTION.The code calculates the loss value based on the input value x and a threshold value d." <EXPLAINS> """CODE.loss = 0.5 * x**2 if abs(x) <= d else 0.5 * d**2 + d * (abs(x) - d)


model = keras.Model(inputs, outputs)
model.compile('sgd', loss=keras.losses.Huber())
""" .

"DESCRIPTION.The code calculates the lower triangular portion of the input matrix x with different settings for the diagonal elements." <EXPLAINS> """CODE.import paddle.fluid as fluid

data = np.arange(1, 13, dtype="int64").reshape(3,-1)
x = fluid.data(shape=(-1, 4), dtype='int64', name='x')
exe = fluid.Executor(fluid.CPUPlace())

tril = fluid.layers.tril(x)
tril_out, = exe.run(fluid.default_main_program(), feed={"x": data},
    fetch_list=[tril], return_numpy=True)

tril = fluid.layers.tril(x, diagonal=2)
tril_out, = exe.run(fluid.default_main_program(), feed={"x": data},
    fetch_list=[tril], return_numpy=True)

tril = fluid.layers.tril(x, diagonal=-1)
tril_out, = exe.run(fluid.default_main_program(), feed={"x": data},
    fetch_list=[tril], return_numpy=True)""" .

"DESCRIPTION.The code calculates the matrix multiplication of two randomly generated tensors of shapes (2, 3, 4) and (4, 5) using broadcasting, resulting in a tensor of shape (2, 3, 5). It then extracts and computes the matrix multiplication for each batch element along the first axis." <EXPLAINS> """CODE.a = tf.random_normal(shape=(2, 3, 4))
b = tf.random_normal(shape=(4, 5))
result = matmul_with_broadcast(a, b)
result.shape
==> (2, 3, 5)
result[0,...]
==> tf.matmul(a[0,...], b)
result[1,...]
==> tf.matmul(a[1,...], b)
""" .

"DESCRIPTION.The code calculates the maximum value of columns \"B\" and \"C\" after grouping by column \"A\" for a dataset created with values from 0 to 99 with columns \"A\", \"B\", and \"C\"." <EXPLAINS> """CODE.import ray
ray.data.le(100).groupby("value").max()
ray.data.from_items([
    {"A": i % 3, "B": i, "C": i**2}
    for i in range(100)]) \\
    .groupby("A") \\
    .max(["B", "C"])
""" .

"DESCRIPTION.The code calculates the mean Intersection over Union (IoU) metric for a binary classification problem. It first calculates the IoU without sample weights, and then calculates the IoU with sample weights included." <EXPLAINS> """CODE.m = tf.keras.metrics.MeanIoU(num_classes=2)
m.update_state([0, 0, 1, 1], [0, 1, 0, 1])
m.result().numpy()
0.33333334

m.reset_state()
m.update_state([0, 0, 1, 1], [0, 1, 0, 1],
...                sample_weight=[0.3, 0.3, 0.3, 0.1])
m.result().numpy()
0.23809525
""" .

"DESCRIPTION.The code calculates the mean Poisson deviance between the predicted tensor values (y_pred) and the true tensor values (y_true)." <EXPLAINS> """CODE.y_pred = torch.tensor([2, 0.5, 1, 4])
y_true = torch.tensor([0.5, 0.5, 2., 2.])
metric = MeanPoissonDeviance()
metric(y_pred, y_true)
tensor([0.9034])""" .

"DESCRIPTION.The code calculates the mean Poisson deviance between the predicted values (y_pred) and the true values (y_true) using the MeanPoissonDeviance metric." <EXPLAINS> """CODE.y_pred = torch.tensor([2, 0.5, 1, 4])
y_true = torch.tensor([0.5, 0.5, 2., 2.])
metric = MeanPoissonDeviance()
metric(y_pred, y_true)
tensor([0.9034])""" .

"DESCRIPTION.The code calculates the mean Poisson deviance between the true values (y_true) and the predicted values (y_pred)." <EXPLAINS> """CODE.from sklearn.metrics import mean_poisson_deviance
y_true = [2, 0, 1, 4]
y_pred = [0.5, 0.5, 2., 2.]
mean_poisson_deviance(y_true, y_pred)
""" .

"DESCRIPTION.The code calculates the mean Tweedie deviance between the true values and predicted values using a power parameter of 1." <EXPLAINS> """CODE.from sklearn.metrics import mean_tweedie_deviance
y_true = [2, 0, 1, 4]
y_pred = [0.5, 0.5, 2., 2.]
mean_tweedie_deviance(y_true, y_pred, power=1)
""" .

"DESCRIPTION.The code calculates the mean absolute error (L1 loss) between the input and label data using the PaddlePaddle library in Python. It then prints out the output which is the L1 loss value." <EXPLAINS> """CODE.import paddle.fluid as fluid
import numpy as np
input = fluid.data(name="input", shape=[1])
label = fluid.data(name="label", shape=[1])
l1_loss = fluid.dygraph.L1Loss(reduction='mean')
output = l1_loss(input,label)

import paddle.fluid.dygraph as dg
with dg.guard(place) as g:
    input = dg.to_variable(input_data)
    label = dg.to_variable(label_data)
    l1_loss = fluid.dygraph.L1Loss(reduction='mean')
    output = l1_loss(input,label)
    print(output.numpy())  # [0.2]""" .

"DESCRIPTION.The code calculates the mean absolute error between two arrays y_true and y_pred and asserts that the shape of the calculated loss is (2,) and it is equal to the mean absolute error calculated using numpy." <EXPLAINS> """CODE.y_true = np.random.randint(0, 2, size=(2, 3))
y_pred = np.random.random(size=(2, 3))
loss = tf.keras.losses.mean_absolute_error(y_true, y_pred)
assert loss.shape == (2,)
assert np.array_equal(
    loss.numpy(), np.mean(np.abs(y_true - y_pred), axis=-1))
""" .

"DESCRIPTION.The code calculates the mean absolute error between two sets of data x and y." <EXPLAINS> """CODE.mean_absolute_error(x, y)
tensor(0.2500)""" .

"DESCRIPTION.The code calculates the mean absolute error loss between two arrays, creates a model using a stochastic gradient descent optimizer, and compiles the model with the mean absolute error loss function." <EXPLAINS> """CODE.mae = keras.losses.MeanAbsoluteError()
loss = mae([0., 0., 1., 1.], [1., 1., 1., 0.])
model = keras.Model(inputs, outputs)
model.compile('sgd', loss=keras.losses.MeanAbsoluteError())""" .

"DESCRIPTION.The code calculates the mean absolute percentage error between the true and predicted values of two arrays, y_true and y_pred, respectively. It prevents division by zero by ensuring that no values in y_true are below 1e-7. The result is a 1D array of shape (2,) containing the calculated error percentages." <EXPLAINS> """CODE.loss = 100 * mean(abs(y_true - y_pred) / y_true, axis=-1)
y_true = np.random.random(size=(2, 3))
y_true = np.maximum(y_true, 1e-7)  # Prevent division by zero
y_pred = np.random.random(size=(2, 3))
loss = tf.keras.losses.mean_absolute_percentage_error(y_true, y_pred)
assert loss.shape == (2,)
assert np.array_equal(
    loss.numpy(),
    100. * np.mean(np.abs((y_true - y_pred) / y_true), axis=-1)""" .

"DESCRIPTION.The code calculates the mean absolute percentage error between true and predicted values for regression tasks. The function can handle single or multiple output regression tasks." <EXPLAINS> """CODE.from sklearn.metrics import mean_absolute_percentage_error
y_true = [3, -0.5, 2, 7]
y_pred = [2.5, 0.0, 2, 8]
mean_absolute_percentage_error(y_true, y_pred)
y_true = [[0.5, 1], [-1, 1], [7, -6]]
y_pred = [[0, 2], [-1, 2], [8, -5]]
mean_absolute_percentage_error(y_true, y_pred)
mean_absolute_percentage_error(y_true, y_pred, multioutput=[0.3, 0.7])
""" .

"DESCRIPTION.The code calculates the mean gamma deviance between the predicted tensor values and the true tensor values." <EXPLAINS> """CODE.y_pred = torch.tensor([0.5, 0.5, 2., 2.])
y_true = torch.tensor([2, 0.5, 1, 4])
metric = MeanGammaDeviance()
metric(y_pred, y_true)
tensor([1.0569])""" .

"DESCRIPTION.The code calculates the mean of values grouped by the modulo 3 result of each value, using different data structures and syntax variations." <EXPLAINS> """CODE.ray.data.range(100).groupby(lambda x: x % 3).mean()
ray.data.from_items([
    (i % 3, i, i**2)
    for i in range(100)])
    .groupby(lambda x: x[0] % 3)
    .mean(lambda x: x[2])
ray.data.range_arrow(100).groupby("value").mean()
ray.data.from_items([
    {"A": i % 3, "B": i, "C": i**2}
    for i in range(100)])
    .groupby("A")
    .mean(["B", "C"])
""" .

"DESCRIPTION.The code calculates the mean relative error between two sets of values using the specified normalizer." <EXPLAINS> """CODE.m = tf.keras.metrics.MeanRelativeError(normalizer=[1, 3, 2, 3])
m.update_state([1, 3, 2, 3], [2, 4, 6, 8])

model.compile(
  optimizer='sgd',
  loss='mse',
  metrics=[tf.keras.metrics.MeanRelativeError(normalizer=[1, 3])])
""" .

"DESCRIPTION.The code calculates the mean squared error (MSE) between the true values (y_true) and predicted values (y_pred), with different settings for sample weights and reduction methods. The first calculation without sample weights and default reduction method returns 0.5, the second calculation with sample weights of [0.7, 0.3] returns 0.25, the third calculation with SUM reduction method returns 1.0, and the fourth calculation with NONE reduction method returns an array of [0.5, 0.5]." <EXPLAINS> """CODE.mse = tf.keras.losses.MeanSquaredError()
mse(y_true, y_pred).numpy()
0.5

mse(y_true, y_pred, sample_weight=[0.7, 0.3]).numpy()
0.25

mse = tf.keras.losses.MeanSquaredError(reduction=tf.keras.losses.Reduction.SUM)
mse(y_true, y_pred).numpy()
1.0

mse = tf.keras.losses.MeanSquaredError(reduction=tf.keras.losses.Reduction.NONE)
mse(y_true, y_pred).numpy()
array([0.5, 0.5], dtype=float32)
""" .

"DESCRIPTION.The code calculates the mean squared error (MSE) loss between the input data and label data using both declarative and imperative modes in PaddlePaddle framework." <EXPLAINS> """CODE.import numpy as np
from paddle import fluid
import paddle.fluid.dygraph as dg

mse_loss = fluid.dygraph.MSELoss()
input = fluid.data(name="input", shape=[1])
label = fluid.data(name="label", shape=[1])
place = fluid.CPUPlace()
input_data = np.array([1.5]).astype("float32")
label_data = np.array([1.7]).astype("float32")

# declarative mode
output = mse_loss(input,label)
exe = fluid.Executor(place)
exe.run(fluid.default_startup_program())
output_data = exe.run(
    fluid.default_main_program(),
    feed={"input":input_data, "label":label_data},
    fetch_list=[output],
    return_numpy=True)
print(output_data)

# imperative mode
with dg.guard(place) as g:
    input = dg.to_variable(input_data)
    label = dg.to_variable(label_data)
    output = mse_loss(input, label)
    print(output.numpy())
    # [0.04000002]
""" .

"DESCRIPTION.The code calculates the mean squared error loss between two sets of values and compiles a model using stochastic gradient descent optimizer with the mean squared error loss function." <EXPLAINS> """CODE.mse = keras.losses.MeanSquaredError()
loss = mse([0., 0., 1., 1.], [1., 1., 1., 0.])
model = keras.Model(inputs, outputs)
model.compile('sgd', loss=keras.losses.MeanSquaredError())
""" .

"DESCRIPTION.The code calculates the mean squared error loss between two sets of values, creates a keras model, and compiles it using stochastic gradient descent optimizer with mean squared error loss." <EXPLAINS> """CODE.mse = keras.losses.MeanSquaredError()
loss = mse([0., 0., 1., 1.], [1., 1., 1., 0.])
model = keras.Model(inputs, outputs)
model.compile('sgd', loss=keras.losses.MeanSquaredError())
""" .

"DESCRIPTION.The code calculates the mean squared error loss between two sets of values, creates a neural network model with specified inputs and outputs, and compiles the model using stochastic gradient descent optimizer with mean squared error loss." <EXPLAINS> """CODE.mse = keras.losses.MeanSquaredError()
loss = mse([0., 0., 1., 1.], [1., 1., 1., 0.])
model = keras.Model(inputs, outputs)
model.compile('sgd', loss=keras.losses.MeanSquaredError())
""" .

"DESCRIPTION.The code calculates the mean squared log error between the predicted values (y_pred) and the true values (y_true) using the MeanSquaredLogError metric." <EXPLAINS> """CODE.y_pred = torch.tensor([2.5, 5, 4, 8])
y_true = torch.tensor([3, 5, 2.5, 7])
metric = MeanSquaredLogError()
metric(y_pred, y_true)
tensor([0.0397])""" .

"DESCRIPTION.The code calculates the mean squared logarithmic error between predicted and target values, and compiles a model using stochastic gradient descent optimizer with the mean squared logarithmic error as the loss function." <EXPLAINS> """CODE.msle = keras.losses.MeanSquaredLogarithmicError()
loss = msle([0., 0., 1., 1.], [1., 1., 1., 0.])
model = keras.Model(inputs, outputs)
model.compile('sgd', loss=keras.losses.MeanSquaredLogarithmicError())
""" .

"DESCRIPTION.The code calculates the mean squared logarithmic error between the true values (y_true) and predicted values (y_pred) using the formula \\( \\frac{1}{n} \\sum_{i=1}^{n} ( \\log(y_{true}+1) - \\log(y_{pred}+1))^2 \\), where n is the number of elements in y_true and y_pred." <EXPLAINS> """CODE.y_true = np.random.randint(0, 2, size=(2, 3))
y_pred = np.random.random(size=(2, 3))
loss = tf.keras.losses.mean_squared_logarithmic_error(y_true, y_pred)
assert loss.shape == (2,)
y_true = np.maximum(y_true, 1e-7)
y_pred = np.maximum(y_pred, 1e-7)
assert np.array_equal(
...     loss.numpy(),
...     np.mean(
...         np.square(np.log(y_true + 1.) - np.log(y_pred + 1.)), axis=-1))""" .

"DESCRIPTION.The code calculates the mean squared logarithmic error between two arrays, y_true and y_pred. It first generates random arrays for y_true and y_pred, then calculates the mean squared logarithmic error using TensorFlow's keras library. It asserts that the shape of the loss array is (2,) and then performs element-wise maximum operation on y_true and y_pred arrays. Finally, it asserts that the loss array is approximately equal to the mean of the squared difference of the logarithm of the modified y_true and y_pred arrays." <EXPLAINS> """CODE.y_true = np.random.randint(0, 2, size=(2, 3))
y_pred = np.random.random(size=(2, 3))
loss = tf.keras.losses.mean_squared_logarithmic_error(y_true, y_pred)
assert loss.shape == (2,)
y_true = np.maximum(y_true, 1e-7)
y_pred = np.maximum(y_pred, 1e-7)
assert np.allclose(
...     loss.numpy(),
...     np.mean(
...         np.square(np.log(y_true + 1.) - np.log(y_pred + 1.)), axis=-1))""" .

"DESCRIPTION.The code calculates the mean squared logarithmic error loss between the predicted values and the actual values for a neural network model, and compiles the model using stochastic gradient descent as the optimizer." <EXPLAINS> """CODE.msle = keras.losses.MeanSquaredLogarithmicError()
loss = msle([0., 0., 1., 1.], [1., 1., 1., 0.])
model = keras.Model(inputs, outputs)
model.compile('sgd', loss=keras.losses.MeanSquaredLogarithmicError())
""" .

"DESCRIPTION.The code calculates the mean value of a set of numbers using tf.keras.metrics.Mean() object. It updates the mean value state with the provided numbers, calculates the mean value, resets the state, then updates the mean value state again with the provided numbers and sample weights, and calculates the new mean value. Finally, it adds the mean value as a metric to a model and compiles the model for training with optimizer 'sgd' and loss 'mse'." <EXPLAINS> """CODE.m = tf.keras.metrics.Mean()
m.update_state([1, 3, 5, 7])
m.result().numpy()
4.0
m.reset_state()
m.update_state([1, 3, 5, 7], sample_weight=[1, 1, 0, 0])
m.result().numpy()
2.0
model.add_metric(tf.keras.metrics.Mean(name='mean_1')(outputs))
model.compile(optimizer='sgd', loss='mse')
""" .

"DESCRIPTION.The code calculates the mean value of the \"value\" column after grouping the data by the \"value\" column of a Ray dataframe, and then calculates the mean values of columns \"B\" and \"C\" after grouping the data by the \"A\" column of another Ray dataframe." <EXPLAINS> """CODE.ray.data.le(100).groupby("value").mean()
ray.data.from_items([
    {"A": i % 3, "B": i, "C": i**2}
    for i in range(100)]) \\
    .groupby("A") \\
    .mean(["B", "C"])
""" .

"DESCRIPTION.The code calculates the mean value of the array 'x' along the 'i' axis using JAX's parallel computation capabilities, and then divides each element of the array 'x' by the mean value along the 'i' axis." <EXPLAINS> """CODE.x = np.arange(4)
y = jax.pmap(lambda x: jax.lax.pmean(x, 'i'), axis_name='i')(x)
print(y)
[ 1.5         1.5         1.5         1.5       ]
y = jax.pmap(lambda x: x / jax.lax.pmean(x, 'i'), axis_name='i')(x)
print(y)""" .

"DESCRIPTION.The code calculates the mean value of the provided list of numbers." <EXPLAINS> """CODE.m = keras.metrics.Mean()
m.update_state([1, 3, 5, 7])
m.result()
""" .

"DESCRIPTION.The code calculates the median absolute error between two sets of predicted and true values." <EXPLAINS> """CODE.y_pred = torch.tensor([2.5, 0.0, 2, 8])
y_true = torch.tensor([3, -0.5, 2, 7])
metric = MedianAbsoluteError()
metric(y_pred, y_true)
tensor([0.5000])""" .

"DESCRIPTION.The code calculates the minimum values for each group in a dataset based on a specific column." <EXPLAINS> """CODE.import ray
ray.data.le(100).groupby("value").min()
ray.data.from_items([
    {"A": i % 3, "B": i, "C": i**2}
    for i in range(100)]) \\
    .groupby("A") \\
    .min(["B", "C"])
""" .

"DESCRIPTION.The code calculates the mode (most frequently occurring value) of the array 'arr'." <EXPLAINS> """CODE.import pyarrow as pa
import pyarrow.compute as pc
arr = pa.array([1, 1, 2, 2, 3, 2, 2, 2])
pc.mode(arr)""" .

"DESCRIPTION.The code calculates the multiclass ROC (Receiver Operating Characteristic) curve using the predicted probabilities and target labels." <EXPLAINS> """CODE.pred = torch.tensor([[0.85, 0.05, 0.05, 0.05],
    ...                      [0.05, 0.85, 0.05, 0.05],
    ...                      [0.05, 0.05, 0.85, 0.05],
    ...                      [0.05, 0.05, 0.05, 0.85]])

target = torch.tensor([0, 1, 3, 2])

__multiclass_roc(pred, target)""",
        """CODE.pred = torch.tensor([[0.85, 0.05, 0.05, 0.05],
    ...                      [0.05, 0.85, 0.05, 0.05],
    ...                      [0.05, 0.05, 0.85, 0.05],
    ...                      [0.05, 0.05, 0.05, 0.85]])

target = torch.tensor([0, 1, 3, 2])

multiclass_roc(pred, target)""" .

"DESCRIPTION.The code calculates the multiclass ROC (Receiver Operating Characteristic) metric for a given set of predictions and targets. It evaluates the performance of a classification model using the ROC curve for multiple classes." <EXPLAINS> """CODE.pred = torch.tensor([[0.85, 0.05, 0.05, 0.05],
...                     [0.05, 0.85, 0.05, 0.05],
...                     [0.05, 0.05, 0.85, 0.05],
...                     [0.05, 0.05, 0.05, 0.85]])
target = torch.tensor([0, 1, 3, 2])
metric = MulticlassROC()
classes_roc = metric(pred, target)
metric(pred, target)   # doctest: +NORMALIZE_WHITESPACE
((tensor([0., 0., 1.]), tensor([0., 1., 1.]), tensor([1.8500, 0.8500, 0.0500])),
 (tensor([0., 0., 1.]), tensor([0., 1., 1.]), tensor([1.8500, 0.8500, 0.0500])),
 (tensor([0.0000, 0.3333, 1.0000]), tensor([0., 0., 1.]), tensor([1.8500, 0.8500, 0.0500])),
 (tensor([0.0000, 0.3333, 1.0000]), tensor([0., 0., 1.]), tensor([1.8500, 0.8500, 0.0500]))))""" .

"DESCRIPTION.The code calculates the multiclass area under the receiver operating characteristic curve (AUROC) for a given prediction tensor and target tensor with 4 classes. The result is a tensor with the value 0.6667." <EXPLAINS> """CODE.pred = torch.tensor([[0.85, 0.05, 0.05, 0.05],
...                      [0.05, 0.85, 0.05, 0.05],
...                      [0.05, 0.05, 0.85, 0.05],
...                      [0.05, 0.05, 0.05, 0.85]])
target = torch.tensor([0, 1, 3, 2])
multiclass_auroc(pred, target, num_classes=4)
tensor(0.6667)""" .

"DESCRIPTION.The code calculates the number of false positives in a binary classification model by comparing the predicted values with the actual values. The `FalsePositives` metric object is initialized and updated with different sets of predictions and actual values. The `result()` method returns the final count of false positives as a numpy array." <EXPLAINS> """CODE.m = tf.keras.metrics.FalsePositives()
m.update_state([0, 1, 0, 0], [0, 0, 1, 1])
m.result().numpy()
2.0

m.reset_state()
m.update_state([0, 1, 0, 0], [0, 0, 1, 1], sample_weight=[0, 0, 1, 0])
m.result().numpy()
1.0

model.compile(optimizer='sgd',
              loss='mse',
              metrics=[tf.keras.metrics.FalsePositives()])
""" .

"DESCRIPTION.The code calculates the number of true positives in a binary classification task. It first initializes a TruePositives metric object, updates its state with ground truth labels and predictions, retrieves the result as a numpy array, resets the state, updates the state with sample weights included, retrieves the updated result, and then compiles a model with the calculated true positives metric." <EXPLAINS> """CODE.m = tf.keras.metrics.TruePositives()
m.update_state([0, 1, 1, 1], [1, 0, 1, 1])
m.result().numpy()
2.0

m.reset_state()
m.update_state([0, 1, 1, 1], [1, 0, 1, 1], sample_weight=[0, 0, 1, 0])
m.result().numpy()
1.0

model.compile(optimizer='sgd',
              loss='mse',
              metrics=[tf.keras.metrics.TruePositives()])
""" .

"DESCRIPTION.The code calculates the numerical group labels for the values in column 'A' of the DataFrame based on different grouping criteria." <EXPLAINS> """CODE.df = pd.DataFrame({"A": list("aaabba")})
df.groupby('A').ngroup()
0    0
1    0
2    0
3    1
4    1
5    0
dtype: int64
df.groupby('A').ngroup(ascending=False)
0    1
1    1
2    1
3    0
4    0
5    1
dtype: int64
df.groupby(["A", [1,1,2,3,2,1]]).ngroup()
0    0
1    0
2    1
3    3
4    2
5    0
dtype: int64""" .

"DESCRIPTION.The code calculates the onset strength envelope of an audio signal and then computes both the Fourier tempogram and the autocorrelation tempogram using the onset envelope, sample rate, and hop length provided." <EXPLAINS> """CODE.y, sr = librosa.load(librosa.util.example_audio_file())
hop_length = 512
oenv = librosa.onset.onset_strength(y=y, sr=sr, hop_length=hop_length)
tempogram = librosa.feature.fourier_tempogram(onset_envelope=oenv, sr=sr,
...                                               hop_length=hop_length)
ac_tempogram = librosa.feature.tempogram(onset_envelope=oenv, sr=sr,
...                                          hop_length=hop_length, norm=None)""" .

"DESCRIPTION.The code calculates the onset strength of multiple subbands from an audio signal 'y' with a sampling rate of 'sr' using a specified frequency channels." <EXPLAINS> "CODE.onset_subbands = librosa.onset.onset_strength_multi(y=y, sr=sr, channels=[0, 32, 64, 96, 128])" .

"DESCRIPTION.The code calculates the pair confusion matrix based on the true labels and predicted labels provided as input." <EXPLAINS> """CODE.from sklearn.metrics.cluster import pair_confusion_matrix
pair_confusion_matrix([0, 0, 1, 0], [1, 1, 0, 1])
""" .

"DESCRIPTION.The code calculates the partial sum of the input arrays x and y from index 0 to index 1 (length 2) and returns the result." <EXPLAINS> """CODE.import paddle.fluid.layers as layers
import paddle.fluid as fluid
import numpy as np
x = fluid.data(name="x", shape=[None, 3], dtype="float32")
y = fluid.data(name="y", shape=[None, 3], dtype="float32")
sum = layers.partial_sum([x,y], start_index=0, length=2)
place = fluid.CPUPlace()
exe = fluid.Executor(place)
xx = np.array([1,2,3,4,5,6]).reshape((2,3)).astype("float32")
yy = np.array([6,5,4,4,5,6]).reshape((2,3)).astype("float32")
out = exe.run(feed={"x":xx, "y":yy}, fetch_list=[sum])""" .

"DESCRIPTION.The code calculates the percentage of CPU times spent on different tasks such as user, system, idle, iowait, irq, softirq, steal, guest, and guest_nice." <EXPLAINS> """CODE.cpu_times_percent()
cpupercent(user=4.8, nice=0.0, system=4.8, idle=90.5, iowait=0.0,
             irq=0.0, softirq=0.0, steal=0.0, guest=0.0, guest_nice=0.0)""" .

"DESCRIPTION.The code calculates the pitch frequencies for a constant-Q transform with 24 bins per octave and a range starting from 55 Hz, then applies pitch tuning with a specified value of 0.25. Subsequently, it extracts pitches and magnitudes using the ifptrack algorithm and filters out pitches based on magnitudes above the median, before applying pitch tuning again." <EXPLAINS> """CODE.freqs = librosa.cqt_frequencies(24, 55, tuning=0.25)
librosa.feature.pitch_tuning(freqs)
0.25

pitches, magnitudes, stft = librosa.feature.ifptrack(y, sr)
pitches = pitches[magnitudes > np.median(magnitudes)]
librosa.feature.pitch_tuning(pitches)
""" .

"DESCRIPTION.The code calculates the precision metric for a classification task with 4 classes using the predicted values stored in the 'pred' tensor and the target ground truth values stored in the 'target' tensor." <EXPLAINS> """CODE.pred = torch.tensor([0, 1, 2, 3])
target = torch.tensor([0, 1, 2, 2])
metric = Precision(num_classes=4)
metric(pred, target)""" .

"DESCRIPTION.The code calculates the precision metric for a classification task with 4 classes, comparing the predicted values to the target values." <EXPLAINS> """CODE.pred = torch.tensor([0, 1, 2, 3])
target = torch.tensor([0, 1, 2, 2])
metric = Precision(num_classes=4)
metric(pred, target)""" .

"DESCRIPTION.The code calculates the precision, recall, and thresholds for a given set of predictions and targets." <EXPLAINS> """CODE.precision, recall, thresholds = precision_recall_curve(pred, target)
precision
tensor([0.3333, 0.0000, 0.0000, 1.0000])
recall
tensor([1., 0., 0., 0.])
thresholds
tensor([1, 2, 3])""" .

"DESCRIPTION.The code calculates the probability of a given value under a deterministic distribution defined by the input constant or loc." <EXPLAINS> """CODE.constant = tf.contrib.distributions.Deterministic(0.)
constant.prob(0.)
==> 1.
constant.prob(2.)
==> 0.

loc = [[0., 1.], [2., 3.]]
x = [[0., 1.1], [1.99, 3.]]
constant = tf.contrib.distributions.Deterministic(loc)
constant.prob(x)
==> [[1., 0.], [0., 1.]]
""" .

"DESCRIPTION.The code calculates the pseudo-inverse of a matrix \"a\" and then checks if the product of \"a\" with the product of its pseudo-inverse and itself is approximately equal to \"a\". It also checks if the product of the pseudo-inverse with the product of \"a\" with itself and the pseudo-inverse is approximately equal to the pseudo-inverse." <EXPLAINS> """CODE.import numpy as np
a = np.random.randn(9, 6)
a = np.dot(a, a.T)
B = pinvh(a)
np.allclose(a, np.dot(a, np.dot(B, a)))
np.allclose(B, np.dot(B, np.dot(a, B)))
""" .

"DESCRIPTION.The code calculates the pseudo-inverse of a matrix 'a' and verifies if the pseudo-inverse fulfills the properties of a Moore-Penrose pseudo-inverse." <EXPLAINS> """CODE.from numpy import *
a = random.randn(9, 6)
a = np.dot(a, a.T)
B = pinvh(a)
allclose(a, dot(a, dot(B, a)))
allclose(B, dot(B, dot(a, B)))
""" .

"DESCRIPTION.The code calculates the rank attention mechanism using the provided input data and rank offsets. It utilizes the parameters specified to determine the attention weights for ranking, with a maximum rank of 3." <EXPLAINS> """CODE.import paddle.fluid as fluid
import numpy as np

input = fluid.data(name="input", shape=[None, 2], dtype="float32")
rank_offset = fluid.data(name="rank_offset", shape=[None, 7], dtype="int32")
out = fluid.contrib.layers.rank_attention(input=input,
                                          rank_offset=rank_offset,
                                          rank_param_shape=[18,3],
                                          rank_param_attr=
                                            fluid.ParamAttr(learning_rate=1.0,
                                                          name="ubm_rank_param.w_0",
                                                          initializer=
                                                          fluid.initializer.Xavier(uniform=False)),
                                          max_rank=3)""" .

"DESCRIPTION.The code calculates the recall metric for a prediction tensor and a target tensor." <EXPLAINS> """CODE.pred = torch.tensor([0, 1, 2, 3])
target = torch.tensor([0, 1, 2, 2])
metric = Recall()
metric(pred, target)
tensor(0.6250)""" .

"DESCRIPTION.The code calculates the recall metric for approximate nearest neighbors search algorithm by comparing the result neighbors with the ground truth neighbors. It calculates the recall for all ground truth neighbors and for the top 10 ground truth neighbors, separately." <EXPLAINS> """CODE.ann_recall(result_neighbors, ground_truth_neighbors)
ann_recall(result_neighbors, ground_truth_neighbors[:,0:10])""" .

"DESCRIPTION.The code calculates the recall metric for the predicted values 'pred' compared to the target values 'target'." <EXPLAINS> """CODE.pred = torch.tensor([0, 1, 2, 3])
target = torch.tensor([0, 1, 2, 2])
metric = Recall()
metric(pred, target)
tensor(0.6250)""" .

"DESCRIPTION.The code calculates the recall metric for the predictions \"pred\" compared to the target values \"target\". The recall value is 0.6250." <EXPLAINS> """CODE.pred = torch.tensor([0, 1, 2, 3])
target = torch.tensor([0, 1, 2, 2])
metric = Recall()
metric(pred, target)
tensor(0.6250)""" .

"DESCRIPTION.The code calculates the recall score of the result_neighbors compared to the ground_truth_neighbors." <EXPLAINS> """CODE.ann_recall(result_neighbors, ground_truth_neighbors)
ann_recall(result_neighbors, ground_truth_neighbors[:,0:10])""" .

"DESCRIPTION.The code calculates the recurrence matrix based on the Mel-frequency cepstral coefficients (MFCC) extracted from the audio signal. The recurrence matrix can be computed with different parameters such as the number of nearest neighbors (k), width, distance metric (cosine), and symmetric setting." <EXPLAINS> """CODE.mfcc    = librosa.feature.mfcc(y=y, sr=sr)
R       = librosa.segment.recurrence_matrix(mfcc)

R       = librosa.segment.recurrence_matrix(mfcc, k=5)

R       = librosa.segment.recurrence_matrix(mfcc, width=7)

R       = librosa.segment.recurrence_matrix(mfcc, metric='cosine')

R       = librosa.segment.recurrence_matrix(mfcc, sym=True)
""" .

"DESCRIPTION.The code calculates the recurrence matrix of a chroma feature extracted from an audio signal, and then finds the optimal alignment path using a recurrence quantification analysis algorithm. It displays the recurrence matrix and alignment score matrix in two subplots, with the optimal path overlaid on the alignment score matrix plot." <EXPLAINS> """CODE.import numpy as np
import matplotlib.pyplot as plt
import librosa

y, sr = librosa.load(librosa.util.example_audio_file(), offset=10, duration=30)
chroma = librosa.feature.chroma_cqt(y=y, sr=sr)
chroma_stack = librosa.feature.stack_memory(chroma, n_steps=3)
rec = librosa.segment.recurrence_matrix(chroma_stack, width=43, mode='affinity', metric='cosine')
L_score, L_path = librosa.sequence.rqa(rec, np.inf, np.inf, knight_moves=False)
plt.figure(figsize=(10, 4))
plt.subplot(1,2,1)
librosa.display.specshow(rec, x_axis='frames', y_axis='frames')
plt.title('Recurrence matrix')
plt.colorbar()
plt.subplot(1,2,2)
librosa.display.specshow(L_score, x_axis='frames', y_axis='frames')
plt.title('Alignment score matrix')
plt.colorbar()
plt.plot(L_path[:, 1], L_path[:, 0], label='Optimal path', color='c')
plt.legend()
plt.show()

score, path = librosa.sequence.rqa(rec, 5, 10)
plt.figure(figsize=(10, 4))
plt.subplot(1,2,1)
librosa.display.specshow(rec, x_axis='frames', y_axis='frames')
plt.title('Recurrence matrix')
plt.colorbar()
plt.subplot(1,2,2)
librosa.display.specshow(score, x_axis='frames', y_axis='frames')
plt.title('Alignment score matrix')
plt.plot(path[:, 1], path[:, 0], label='Optimal path', color='c')
plt.colorbar()
plt.legend()
plt.show()""" .

"DESCRIPTION.The code calculates the recurrence matrix, structure feature, and inverse structure feature of the input MFCCs using the librosa library." <EXPLAINS> """CODE.R = librosa.feature.recurrence_matrix(mfccs)
S = librosa.feature.structure_feature(R)
R_hat = librosa.feature.structure_feature(S, inverse=True)""" .

"DESCRIPTION.The code calculates the salience spectrogram of an audio signal using librosa library in Python. It first computes the short-time Fourier transform (STFT) of the audio signal, then calculates the salience spectrogram using specified harmonic frequencies and weights. Finally, it displays the salience spectrogram as a plot using matplotlib." <EXPLAINS> """CODE.import numpy as np
import librosa

S = np.abs(librosa.stft(y))
freqs = librosa.core.fft_frequencies(sr)
harms = [1, 2, 3, 4]
weights = [1.0, 0.5, 0.33, 0.25]
S_sal = librosa.salience(S, freqs, harms, weights, fill_value=0)
print(S_sal.shape)
import matplotlib.pyplot as plt
plt.figure()
librosa.display.specshow(librosa.amplitude_to_db(S_sal, ref=np.max), sr=sr, y_axis='log', x_axis='time')
plt.colorbar()
plt.title('Salience spectrogram')
plt.tight_layout()""" .

"DESCRIPTION.The code calculates the short-time Fourier transform (STFT) of the input signal 'y' using a window size of 2048 samples and a hop length of 512 samples. It then applies a phase vocoder with a speedup factor of 2.0 to the STFT result to obtain a faster version of the signal 'y' (y_fast). Similarly, another phase vocoder with a slowdown factor of 1/3 is applied to the STFT result to obtain a slower version of the signal 'y' (y_slow)." <EXPLAINS> """CODE.librosa.stft(y, n_fft=2048, hop_length=512)
D_fast  = librosa.phase_vocoder(D, 2.0, hop_length=512)
y_fast  = librosa.istft(D_fast, hop_length=512)
D_slow  = librosa.phase_vocoder(D, 1./3, hop_length=512)
y_slow  = librosa.istft(D_slow, hop_length=512)""" .

"DESCRIPTION.The code calculates the sigmoid activation function of the input array using TensorFlow in Python." <EXPLAINS> """CODE.a = tf.constant([-20, -1.0, 0.0, 1.0, 20], dtype = tf.float32)
b = tf.keras.activations.sigmoid(a)
b.numpy()
array([2.0611537e-09, 2.6894143e-01, 5.0000000e-01, 7.3105860e-01,
         1.0000000e+00], dtype=float32)""" .

"DESCRIPTION.The code calculates the sigmoid function of the input array 'a' using the TensorFlow library." <EXPLAINS> """CODE.a = tf.constant([-20, -1.0, 0.0, 1.0, 20], dtype = tf.float32)
b = tf.keras.activations.sigmoid(a)
b.numpy()
array([2.0611537e-09, 2.6894143e-01, 5.0000000e-01, 7.3105860e-01,
         1.0000000e+00], dtype=float32)""" .

"DESCRIPTION.The code calculates the similarity between a given sentence \"Machine learning is so easy.\" and a list of other sentences using a pre-trained model. It returns a list of similarity scores between the given sentence and each of the sentences in the list." <EXPLAINS> """CODE.from huggingface_hub import InferenceClient
client = InferenceClient()
client.sentence_similarity(
...     "Machine learning is so easy.",
...     other_sentences=[
...         "Deep learning is so straightforward.",
...         "This is so difficult, like rocket science.",
...         "I can't believe how much I struggled with this.",
...     ],
... )
[0.7785726189613342, 0.45876261591911316, 0.2906220555305481]
""" .

"DESCRIPTION.The code calculates the sine values of an array from 0 to 4*2*pi with 20 elements, calculates zero crossings of the sine values using the librosa library, stacks the sine values and zero crossings horizontally, and then finds the indices where zero crossings occur." <EXPLAINS> """CODE.y = np.sin(np.linspace(0, 4 * 2 * np.pi, 20))
z = librosa.zero_crossings(y)
np.vstack([y, z]).T
np.nonzero(z)""" .

"DESCRIPTION.The code calculates the softsign activation function for an input array of values." <EXPLAINS> """CODE.a = tf.constant([-1.0, 0.0, 1.0], dtype = tf.float32)
b = tf.keras.activations.softsign(a)
b.numpy()
array([-0.5,  0. ,  0.5], dtype=float32)""" .

"DESCRIPTION.The code calculates the sparse categorical accuracy between the true labels y_true and the predicted labels y_pred. It returns an array of accuracy values corresponding to each sample in y_true." <EXPLAINS> """CODE.y_true = [2, 1]
y_pred = [[0.1, 0.9, 0.8], [0.05, 0.95, 0]]
m = tf.keras.metrics.sparse_categorical_accuracy(y_true, y_pred)
assert m.shape == (2,)
m.numpy()
array([0., 1.], dtype=float32)""" .

"DESCRIPTION.The code calculates the sparse categorical crossentropy loss between the predicted values and the actual labels for a classification task." <EXPLAINS> """CODE.cce = keras.losses.SparseCategoricalCrossentropy()
loss = cce(
    [0, 1, 2],
    [[.9, .05, .05], [.5, .89, .6], [.05, .01, .94]])


model = keras.Model(inputs, outputs)
model.compile('sgd', loss=keras.losses.SparseCategoricalCrossentropy())
""" .

"DESCRIPTION.The code calculates the sparse categorical crossentropy loss between the true values (y_true) and predicted values (y_pred) using different reduction methods, such as \"SUM\" or \"NONE\", and with or without sample weights. The output is the loss value in each case." <EXPLAINS> """CODE.scce = tf.keras.losses.SparseCategoricalCrossentropy()
scce(y_true, y_pred).numpy()
1.177
scce(y_true, y_pred, sample_weight=tf.constant([0.3, 0.7])).numpy()
0.814
scce = tf.keras.losses.SparseCategoricalCrossentropy(
...     reduction=tf.keras.losses.Reduction.SUM)
scce(y_true, y_pred).numpy()
2.354
scce = tf.keras.losses.SparseCategoricalCrossentropy(
...     reduction=tf.keras.losses.Reduction.NONE)
scce(y_true, y_pred).numpy()
array([0.0513, 2.303], dtype=float32)""" .

"DESCRIPTION.The code calculates the sparse categorical crossentropy loss between true labels and predicted probabilities using different reduction types: 'auto'/'sum_over_batch_size', 'sum', and 'none'. It returns the loss value in each case." <EXPLAINS> """CODE.y_true = [1, 2]
y_pred = [[0.05, 0.95, 0], [0.1, 0.8, 0.1]]
# Using 'auto'/'sum_over_batch_size' reduction type.
scce = tf.keras.losses.SparseCategoricalCrossentropy()
scce(y_true, y_pred).numpy()
1.177

# Calling with 'sample_weight'.
scce(y_true, y_pred, sample_weight=tf.constant([0.3, 0.7])).numpy()
0.814

# Using 'sum' reduction type.
scce = tf.keras.losses.SparseCategoricalCrossentropy(
...     reduction=tf.keras.losses.Reduction.SUM)
scce(y_true, y_pred).numpy()
2.354

# Using 'none' reduction type.
scce = tf.keras.losses.SparseCategoricalCrossentropy(
...     reduction=tf.keras.losses.Reduction.NONE)
scce(y_true, y_pred).numpy()
array([0.0513, 2.303], dtype=float32)""" .

"DESCRIPTION.The code calculates the sparse top-k categorical accuracy for the true and predicted labels, with k=3, and returns an array of accuracy values for each instance." <EXPLAINS> """CODE.y_true = [2, 1]
y_pred = [[0.1, 0.9, 0.8], [0.05, 0.95, 0]]
m = tf.keras.metrics.sparse_top_k_categorical_accuracy(
...     y_true, y_pred, k=3)
assert m.shape == (2,)
m.numpy()
array([1., 1.], dtype=float32)""" .

"DESCRIPTION.The code calculates the specified percentile value for the input array, with different interpolation methods and axis selections for multidimensional arrays." <EXPLAINS> """CODE.# Get 30th percentile with default ('nearest') interpolation.
x = [1., 2., 3., 4.]
percentile(x, q=30.)

# Get 30th percentile with 'lower' interpolation
x = [1., 2., 3., 4.]
percentile(x, q=30., interpolation='lower')

# Get 100th percentile (maximum).  By default, this is computed over every dim
x = [[1., 2.]
     [3., 4.]]
percentile(x, q=100.)

# Treat the leading dim as indexing samples, and find the 100th quantile (max)
# over all such samples.
x = [[1., 2.]
     [3., 4.]]
percentile(x, q=100., axis=[0])
""" .

"DESCRIPTION.The code calculates the squared absolute values of complex numbers and arrays of complex numbers." <EXPLAINS> """CODE.librosa.util.abs2(3 + 4j)
25.0
librosa.util.abs2((0.5j)**np.arange(8))
array([1.000e+00, 2.500e-01, 6.250e-02, 1.562e-02, 3.906e-03, 9.766e-04,
   2.441e-04, 6.104e-05])""" .

"DESCRIPTION.The code calculates the squared hinge loss between the true labels (y_true) and the predicted labels (y_pred) for a binary classification task." <EXPLAINS> """CODE.y_true = np.random.choice([-1, 1], size=(2, 3))
y_pred = np.random.random(size=(2, 3))
loss = tf.keras.losses.squared_hinge(y_true, y_pred)
assert loss.shape == (2,)
assert np.array_equal(
...     loss.numpy(),
...     np.mean(np.square(np.maximum(1. - y_true * y_pred, 0.)), axis=-1))""" .

"DESCRIPTION.The code calculates the standard deviation of the values grouped by a specific key using different methods in Ray data processing framework." <EXPLAINS> """CODE.ray.data.range(100).groupby(lambda x: x % 3).std()
ray.data.from_items([
    (i % 3, i, i**2)
    for i in range(100)]).groupby(lambda x: x[0] % 3).std(lambda x: x[2)
ray.data.range_arrow(100).groupby("value").std(ddof=0)
ray.data.from_items([
    {"A": i % 3, "B": i, "C": i**2}
    for i in range(100)]).groupby("A").std(["B", "C"])
""" .

"DESCRIPTION.The code calculates the sum of 1.0 added 5 times and returns the final result." <EXPLAINS> """CODE.with Scope() as s:
    s.data = 0.
    for i in s.range(5):
      s.data += 1.
    return s.data""" .

"DESCRIPTION.The code calculates the sum of the input values [1, 3, 5, 7] using the tf.keras.metrics.Sum() metric." <EXPLAINS> """CODE.m = tf.keras.metrics.Sum()
m.update_state([1, 3, 5, 7])
m.result().numpy()
16.0

model.add_metric(tf.keras.metrics.Sum(name='sum_1')(outputs))
model.compile(optimizer='sgd', loss='mse')
""" .

"DESCRIPTION.The code calculates the sum of values in each column of the DataFrame \"df\"." <EXPLAINS> """CODE.df
    c1  c2
a   1   0
b   0   2
c   3   0
d   0   4

df.sum(axis=0)
c1    4
c2    6""" .

"DESCRIPTION.The code calculates the tempo and beats of an audio signal using librosa library and saves the beat times in a CSV file." <EXPLAINS> """CODE.tempo, beats = librosa.beat.beat_track(y, sr=sr, hop_length=64)
librosa.output.frames_csv('beat_times.csv', frames, sr=sr, hop_length=64)""" .

"DESCRIPTION.The code calculates the throughput of a process by measuring the time taken to process a certain number of samples and then computing the throughput every 10 samples." <EXPLAINS> """CODE.throughput = Throughput()
t0 = time()
for i in range(1000):
    do_work()
    if torch.cuda.is_available(): torch.cuda.synchronize()  # required or else time() won't be correct
    throughput.update(time=time() - t0, samples=i)
    if i % 10 == 0:
        print(throughput.compute())""" .

"DESCRIPTION.The code calculates the top k accuracy score of a model's predictions compared to the true labels, where k is specified. It can be calculated with or without normalization." <EXPLAINS> """CODE.import numpy as np
from sklearn.metrics import top_k_accuracy_score
y_true = np.array([0, 1, 2, 2])
y_score = np.array([[0.5, 0.2, 0.2],
                    [0.3, 0.4, 0.2],
                    [0.2, 0.4, 0.3],
                    [0.7, 0.2, 0.1]])
top_k_accuracy_score(y_true, y_score, k=2)
top_k_accuracy_score(y_true, y_score, k=2, normalize=False)""" .

"DESCRIPTION.The code calculates the top-k categorical accuracy metric for a model's predictions. It updates the metric's state with the true labels and model predictions, and then retrieves the metric result in numpy format. The metric value is calculated based on the proportion of correct predictions within the top k values." <EXPLAINS> """CODE.m = tf.keras.metrics.TopKCategoricalAccuracy(k=1)
m.update_state([[0, 0, 1], [0, 1, 0]],
               [[0.1, 0.9, 0.8], [0.05, 0.95, 0]])
m.result().numpy()
0.5

m.reset_state()
m.update_state([[0, 0, 1], [0, 1, 0]],
               [[0.1, 0.9, 0.8], [0.05, 0.95, 0]],
               sample_weight=[0.7, 0.3])
m.result().numpy()
0.3
""" .

"DESCRIPTION.The code calculates the trace of a complex input tensor along specified dimensions and with a specified offset." <EXPLAINS> """CODE.import paddle
import paddle.fluid.dygraph as dg
import numpy as np

case1 = np.random.randn(3, 10, 10).astype('float64') + 1j * np.random.randn(3, 10, 10).astype('float64')

with dg.guard():
    case1 = dg.to_variable(case1)
    data1 = paddle.complex.trace(case1, offset=1, dim1=1, dim2=2) # data1.shape = [3]""" .

"DESCRIPTION.The code calculates the value of 'out' based on a conditional statement comparing 'a' and 'b', where if 'a' is less than 'b', 'out' is assigned the value of 'a + (a * b)', otherwise 'out' is assigned the value of 'b * b'." <EXPLAINS> """CODE.import paddle.fluid as fluid
a = fluid.data(name='a', shape=[-1, 1], dtype='float32')
b = fluid.data(name='b', shape=[-1, 1], dtype='float32')
c = a * b
out = fluid.layers.cond(a < b, lambda: a + c, lambda: b * b)
""" .

"DESCRIPTION.The code calculates the value of replica_id_in_sync_group divided by num_replicas_in_sync, and then distributes this value using a MirroredStrategy in TensorFlow. The final result is the local result obtained from the distributed values." <EXPLAINS> """CODE.def value_fn(context):
    return context.replica_id_in_sync_group/context.num_replicas_in_sync

context = tf.distribute.experimental.ValueContext(
    replica_id_in_sync_group=2, num_replicas_in_sync=4)
per_replica_value = value_fn(context)
per_replica_value

strategy = tf.distribute.MirroredStrategy()
def value_fn(value_context):
    return value_context.num_replicas_in_sync

distributed_values = (
     strategy.experimental_distribute_values_from_function(
       value_fn))
local_result = strategy.experimental_local_results(distributed_values)
local_result""" .

"DESCRIPTION.The code calculates the value of x * log(1 + y) for different values of x and y." <EXPLAINS> """CODE.tf.math.xlog1py(0., 1.)
tf.math.xlog1py(1., 1.)
tf.math.xlog1py(2., 2.)
tf.math.xlog1py(0., -1.)""" .

"DESCRIPTION.The code calculates the zero-one loss between the predicted and true values, both with and without normalization." <EXPLAINS> """CODE.from sklearn.metrics import zero_one
y_pred = [1, 2, 3, 4]
y_true = [2, 2, 3, 4]
zero_one(y_true, y_pred)
zero_one(y_true, y_pred, normalize=True)
""" .

"DESCRIPTION.The code calculates the zero-one loss between the predicted values (y_pred) and the true values (y_true), where the zero-one loss is the fraction of misclassifications. The function can optionally return the unnormalized zero-one loss value." <EXPLAINS> """CODE.from sklearn.metrics import zero_one_loss
y_pred = [1, 2, 3, 4]
y_true = [2, 2, 3, 4]
zero_one_loss(y_true, y_pred)
zero_one_loss(y_true, y_pred, normalize=False)
""" .

"DESCRIPTION.The code calculates the zeroth-order Bessel function of the first kind for the given input values and returns the result as a NumPy array." <EXPLAINS> "CODE.tf.math.special.bessel_j0([0.5, 1., 2., 4.]).numpy()" .

"DESCRIPTION.The code calculates various metrics such as accuracy, precision, and recall for a multi-class classification task using PyTorch Lightning." <EXPLAINS> """CODE.from pytorch_lightning.metrics import MetricCollection, Accuracy, Precision, Recall
target = torch.tensor([0, 2, 0, 2, 0, 1, 0, 2])
preds = torch.tensor([2, 1, 2, 0, 1, 2, 2, 2])
metrics = MetricCollection([Accuracy(),
                            Precision(num_classes=3, average='macro'),
                            Recall(num_classes=3, average='macro')])
metrics(preds, target)

metrics = MetricCollection({'micro_recall': Recall(num_classes=3, average='micro'),
                            'macro_recall': Recall(num_classes=3, average='macro')})
metrics(preds, target)""" .

"DESCRIPTION.The code calls the CastMagic function from the mymock module with the arguments 3, an instance of IgnoreArg class, and the string 'disappear'." <EXPLAINS> "CODE.mymock.CastMagic(3, IgnoreArg(), 'disappear')" .

"DESCRIPTION.The code calls the function \"balloons()\" from the module \"st\"." <EXPLAINS> "CODE.st.balloons()" .

"DESCRIPTION.The code calls the function operation2." <EXPLAINS> "CODE.operation2()" .

"DESCRIPTION.The code captures and saves the current global state and then restores it." <EXPLAINS> """CODE.snapshot = _GlobalStateSnapshot.capture()
snapshot.restore()""" .

"DESCRIPTION.The code changes the default data type in Keras backend from 'float32' to 'float16' using the functions provided by Keras backend." <EXPLAINS> """CODE.from keras import backend as K
K.floatx()
'float32'
K.set_floatx('float16')
K.floatx()
'float16'
""" .

"DESCRIPTION.The code changes the row and column labels of a pandas Series and DataFrame respectively. It can change the row labels of a Series and the row or column labels of a DataFrame. The labels can be changed using a list of new labels, and the changes can be made inplace or not." <EXPLAINS> """CODE.s = pd.Series([1, 2, 3])
s
0    1
1    2
2    3
dtype: int64

s.set_axis(['a', 'b', 'c'], axis=0, inplace=False)
a    1
b    2
c    3
dtype: int64

df = pd.DataFrame({"A": [1, 2, 3], "B": [4, 5, 6]})

Change the row labels.

df.set_axis(['a', 'b', 'c'], axis='index', inplace=False)
   A  B
a  1  4
b  2  5
c  3  6

Change the column labels.

df.set_axis(['I', 'II'], axis='columns', inplace=False)
   I  II
0  1   4
1  2   5
2  3   6

Now, update the labels inplace.

df.set_axis(['i', 'ii'], axis='columns', inplace=True)
df
   i  ii
0  1   4
1  2   5
2  3   6""" .

"DESCRIPTION.The code checks compatibility between different data types using the functions is_compatible_with and as_ref in the DType class." <EXPLAINS> """CODE.DType(T)       .is_compatible_with(DType(T))        == True
DType(T)       .is_compatible_with(DType(T).as_ref) == True
DType(T).as_ref.is_compatible_with(DType(T))        == False
DType(T).as_ref.is_compatible_with(DType(T).as_ref) == True
""" .

"DESCRIPTION.The code checks for NaN (Not a Number) values in a tensor containing numerical values and returns a new tensor with 0s where the values are not NaN and 1s where the values are NaN." <EXPLAINS> """CODE.torch.isnan(torch.tensor([1, float('nan'), 2]))
tensor([ 0,  1,  0], dtype=torch.uint8)""" .

"DESCRIPTION.The code checks for duplicated values within the pandas Series 'animals' and returns a boolean Series indicating whether each element is a duplicate. The 'keep' parameter allows for customization on how to treat duplicate values." <EXPLAINS> """CODE.animals = pd.Series(['lama', 'cow', 'lama', 'beetle', 'lama'])
animals.duplicated()
animals.duplicated(keep='first')
animals.duplicated(keep='last')
animals.duplicated(keep=False)""" .

"DESCRIPTION.The code checks for expected warnings to be raised and asserts if unexpected warnings are raised or if expected warnings are not raised." <EXPLAINS> """CODE.import warnings
with assert_produces_warning():
...     warnings.warn(UserWarning())
...
with assert_produces_warning(False):
...     warnings.warn(RuntimeWarning())
...
Traceback (most recent call last):
    ...
AssertionError: Caused unexpected warning(s): ['RuntimeWarning'].
with assert_produces_warning(UserWarning):
...     warnings.warn(RuntimeWarning())
Traceback (most recent call last):
    ...
AssertionError: Did not see expected warning of class 'UserWarning'.""" .

"DESCRIPTION.The code checks if GPU is supported, and if it is, it assigns certain operations to CUDAPlace(0). It creates two constant tensors filled with a value of 0.5, and then calculates the shape of one of the tensors. It slices the shape tensor on CPUPlace, and crops the data1 tensor based on the sliced shape tensor on either CPUPlace or CUDAPlace(0) depending on GPU support. Finally, it runs the program on the selected place and fetches the result." <EXPLAINS> """CODE.import paddle.fluid as fluid

support_gpu = fluid.is_compiled_with_cuda()
place = fluid.CPUPlace()
if support_gpu:
    place = fluid.CUDAPlace(0)

# if GPU is supported, the three OPs below will be automatically assigned to CUDAPlace(0)
data1 = fluid.layers.fill_constant(shape=[1, 3, 8, 8], value=0.5, dtype='float32')
data2 = fluid.layers.fill_constant(shape=[1, 3, 5, 5], value=0.5, dtype='float32')
shape = fluid.layers.shape(data2)

with fluid.device_guard("cpu"):
    # Ops created here will be placed on CPUPlace
    shape = fluid.layers.slice(shape, axes=[0], starts=[0], ends=[4])
with fluid.device_guard('gpu'):
    # if GPU is supported, OPs created here will be placed on CUDAPlace(0), otherwise on CPUPlace
    out = fluid.layers.crop_tensor(data1, shape=shape)

exe = fluid.Executor(place)
exe.run(fluid.default_startup_program())
result = exe.run(fetch_list=[out])""" .

"DESCRIPTION.The code checks if a breakpoint should be triggered based on a loss value, and then sets the trigger with an Accelerator object. It then checks if the breakpoint is triggered and breaks the program if it is." <EXPLAINS> """CODE.from accelerate import Accelerator

accelerator = Accelerator()
if should_do_breakpoint(loss):
...     accelerator.set_trigger()
if accelerator.check_breakpoint():
...     break
""" .

"DESCRIPTION.The code checks if a breakpoint should be triggered based on the loss value. If a breakpoint should be triggered, the accelerator is set to trigger. Then it checks if the accelerator trigger is activated, and if so, a break is executed." <EXPLAINS> """CODE.if should_do_breakpoint(loss):
    accelerator.set_trigger()
if accelerator.check_trigger():
    break
""" .

"DESCRIPTION.The code checks if a given object is of type datetime with nanosecond resolution." <EXPLAINS> """CODE.is_datetime64_ns_dtype(DatetimeTZDtype("ns", "US/Eastern"))
is_datetime64_ns_dtype(pd.DatetimeIndex([1, 2, 3], dtype=np.datetime64))  # has 'ns' unit""" .

"DESCRIPTION.The code checks if a given path is a file or a symbolic link pointing to a file." <EXPLAINS> "CODE.path.check(file=1, link=1)  # a link pointing to a file" .

"DESCRIPTION.The code checks if a placeholder tensor 'a' is sparse or not using keras backend, and then checks if another placeholder tensor 'b' is sparse or not." <EXPLAINS> """CODE.from keras import backend as K
a = K.placeholder((2, 2), sparse=False)
print(K.is_sparse(a))
b = K.placeholder((2, 2), sparse=True)
print(K.is_sparse(b))
""" .

"DESCRIPTION.The code checks if a specific version of the torch library is available and returns a boolean value indicating whether the requirement is met or not." <EXPLAINS> """CODE._RequirementAvailable("torch>=0.1")
bool(_RequirementAvailable("torch>=0.1"))
bool(_RequirementAvailable("torch>100.0"))""" .

"DESCRIPTION.The code checks if a specified module is available in the Python environment." <EXPLAINS> """CODE._module_available('os')
_module_available('bla.bla')""" .

"DESCRIPTION.The code checks if an object is a string. It returns True if the object is a string, otherwise it returns False." <EXPLAINS> """CODE.is_re_compilable = lambda obj: isinstance(obj, str)

print(is_re_compilable(".*"))
print(is_re_compilable(1))""" .

"DESCRIPTION.The code checks if the 'os' package is available and returns True if it is available, otherwise it returns False." <EXPLAINS> """CODE._package_available('os')
_package_available('bla')""" .

"DESCRIPTION.The code checks if the Jedi package is installed, if not, it falls back to using the stdlib readline completer for tab completion functionality in Python interactive console." <EXPLAINS> """CODE.try:
    from jedi.utils import setup_readline
    setup_readline()
except ImportError:
    # Fallback to the stdlib readline completer if it is installed.
    # Taken from http://docs.python.org/2/library/rlcompleter.html
    print("Jedi is not installed, falling back to readline")
    try:
        import readline
        import rlcompleter
        readline.parse_and_bind("tab: complete")
    except ImportError:
        print("Readline is not installed either. No tab completion is enabled.")

ran<TAB> # doctest: +SKIP

range(10).cou<TAB> # doctest: +SKIP
""" .

"DESCRIPTION.The code checks if the PaddlePaddle framework is compiled with ROCm support and assigns the result to the variable \"support_gpu\"." <EXPLAINS> """CODE.import paddle
support_gpu = paddle.is_compiled_with_rocm()""" .

"DESCRIPTION.The code checks if the PaddlePaddle framework is compiled with support for neural processing units (NPU) and stores the result in the variable \"support_npu\"." <EXPLAINS> """CODE.import paddle
support_npu = paddle.is_compiled_with_npu()""" .

"DESCRIPTION.The code checks if the Windows version is less than or equal to Windows Vista before executing the following code block." <EXPLAINS> """CODE.if get_winver() <= WIN_VISTA:
    ...""" .

"DESCRIPTION.The code checks if the current Windows version is less than or equal to Windows Vista." <EXPLAINS> """CODE.if get_winver() <= WIN_VISTA:
    ...""" .

"DESCRIPTION.The code checks if the current execution is within a cross-replica context in TensorFlow. If it is, it retrieves the distribution strategy being used." <EXPLAINS> """CODE.if tf.distribute.in_cross_replica_context():
  strategy = tf.distribute.get_strategy()""" .

"DESCRIPTION.The code checks if the current process has a global rank of 0, and if so, it downloads the dataset. After that, all processes wait at a barrier before reading the dataset and starting the training process." <EXPLAINS> """CODE.if self.global_rank == 0:
    # let process 0 download the dataset
    dataset.download_files()

# let all processes wait before reading the dataset
self.barrier()

# now all processes can read the files and start training""" .

"DESCRIPTION.The code checks if the current process is the main process. If it is, it sleeps for 2 seconds. If it is not the main process, it waits for the main process to finish sleeping. After that, it waits for all processes to reach a synchronization barrier and prints \"Everyone is here\"." <EXPLAINS> """CODE.import time
from accelerate.state import PartialState

state = PartialState()
if state.is_main_process:
    time.sleep(2)
else:
    print("I'm waiting for the main process to finish its sleep...")
state.wait_for_everyone()
print("Everyone is here")
""" .

"DESCRIPTION.The code checks if the data type of a randomly generated key is a pseudo-random number generator (PRNG) key." <EXPLAINS> """CODE.from jax import random
from jax import dtypes
key = random.key(0)
jnp.issubdtype(key.dtype, dtypes.prng_key)""" .

"DESCRIPTION.The code checks if the data type of the random number generated using JAX is a subtype of the extended data types." <EXPLAINS> """CODE.from jax import random
from jax._src import dtypes
key = random.key(0)
jnp.issubdtype(key.dtype, dtypes.extended)
True""" .

"DESCRIPTION.The code checks if the given list is strictly monotonic increasing." <EXPLAINS> """CODE.Index([1, 2, 3])._is_strictly_monotonic_increasing
Index([1, 2, 2])._is_strictly_monotonic_increasing
Index([1, 3, 2])._is_strictly_monotonic_increasing""" .

"DESCRIPTION.The code checks if the given version is a pre-release version." <EXPLAINS> """CODE.Version("1.2.3").is_prerelease
Version("1.2.3a1").is_prerelease
Version("1.2.3b1").is_prerelease
Version("1.2.3rc1").is_prerelease
Version("1.2.3dev1").is_prerelease""" .

"DESCRIPTION.The code checks if the given version number is a post-release version." <EXPLAINS> """CODE.Version("1.2.3").is_postrelease
Version("1.2.3.post1").is_postrelease""" .

"DESCRIPTION.The code checks if the input arrays represent a multilabel setting." <EXPLAINS> """CODE.import numpy as np
from sklearn.utils.multiclass import is_multilabel
is_multilabel([0, 1, 0, 1])
is_multilabel([[1], [0, 2], []])
is_multilabel(np.array([[1, 0], [0, 0]]))
is_multilabel(np.array([[1], [0], [0]]))
is_multilabel(np.array([[1, 0, 0]]))
""" .

"DESCRIPTION.The code checks if the input audio signal `y` is valid for audio processing, allowing for either mono or stereo input." <EXPLAINS> """CODE.librosa.util.valid_audio(y)
librosa.util.valid_audio(y, mono=False)""" .

"DESCRIPTION.The code checks if the input is a boolean type, whether it is a boolean value, a NumPy boolean type, or a NumPy array containing boolean values." <EXPLAINS> """CODE.is_bool_dtype(bool)
True
is_bool_dtype(np.bool)
True
is_bool_dtype(np.array([True, False]))
True""" .

"DESCRIPTION.The code checks if the input is a label indicator matrix." <EXPLAINS> """CODE.import numpy as np
from sklearn.utils.multiclass import is_label_indicator_matrix
is_label_indicator_matrix([0, 1, 0, 1])
is_label_indicator_matrix([[1], [0, 2], []])
is_label_indicator_matrix(np.array([[1, 0], [0, 0]]))
is_label_indicator_matrix(np.array([[1], [0], [0]]))
is_label_indicator_matrix(np.array([[1, 0, 0]]))""" .

"DESCRIPTION.The code checks if the input is a sequence of sequences." <EXPLAINS> """CODE.import numpy as np
from sklearn.utils.multiclass import is_multilabel
is_sequence_of_sequences([0, 1, 0, 1])
is_sequence_of_sequences([[1], [0, 2], []])
is_sequence_of_sequences(np.array([[1], [0, 2], []], dtype=object))
is_sequence_of_sequences([(1,), (0, 2), ()])
is_sequence_of_sequences(np.array([[1, 0], [0, 0]]))
is_sequence_of_sequences(np.array([[1], [0], [0]]))
is_sequence_of_sequences(np.array([[1, 0, 0]]))""" .

"DESCRIPTION.The code checks if the input is of integer data type." <EXPLAINS> """CODE.is_integer_dtype(int)
is_integer_dtype(np.uint64)
is_integer_dtype(pd.Series([1, 2]))
is_integer_dtype(np.array([], dtype=np.timedelta64))""" .

"DESCRIPTION.The code checks if the input object is a sparse matrix implemented using the Scipy library or a sparse data structure implemented using the Pandas library." <EXPLAINS> """CODE.from scipy.sparse import bsr_matrix
is_scipy_sparse(bsr_matrix([1, 2, 3]))
is_scipy_sparse(pd.SparseArray([1, 2, 3]))
is_scipy_sparse(pd.SparseSeries([1, 2, 3]))
""" .

"DESCRIPTION.The code checks if the input object is of a categorical data type." <EXPLAINS> """CODE.is_categorical_dtype(object)
is_categorical_dtype(CategoricalDtype())
is_categorical_dtype([1, 2, 3])
is_categorical_dtype(pd.Categorical([1, 2, 3]))
is_categorical_dtype(pd.CategoricalIndex([1, 2, 3]) )""" .

"DESCRIPTION.The code checks if the input parameter is a sequence." <EXPLAINS> """CODE.is_sequence(l)
is_sequence(iter(l))""" .

"DESCRIPTION.The code checks if the input string is exactly equal to the keyword \"start\". If it is, it returns a list containing the keyword \"start\". If the input string is not exactly \"start\", it raises an exception." <EXPLAINS> """CODE.Keyword("start").parseString("start")  # -> ['start']
Keyword("start").parseString("starting")  # -> Exception
""" .

"DESCRIPTION.The code checks if the input string is printable, meaning all characters in the string can be printed as opposed to containing non-printable characters." <EXPLAINS> """CODE.isprintable('abc')
isprintable(b'\\x01')""" .

"DESCRIPTION.The code checks if the input variable \"t\" is a tensor, and if not, converts it to a tensor. It then returns the data type of the tensor \"t\"." <EXPLAINS> """CODE.if not tf.is_tensor(t):
  t = tf.convert_to_tensor(t)
return t.dtype
""" .

"DESCRIPTION.The code checks if the representation of the DataFrame 'df' is equal to the string representation of 'df' using the parameters obtained from 'get_dataframe_repr_params()'." <EXPLAINS> """CODE.import pandas as pd

df = pd.DataFrame([[1, 2], [3, 4]])
repr_params = pd.io.formats.format.get_dataframe_repr_params()
repr(df) == df.to_string(**repr_params)""" .

"DESCRIPTION.The code checks if the script is being run as the main program, then calls a function \"harness\" from the module \"testlib\" and exits the program with the return value of the function." <EXPLAINS> """CODE.#!/usr/bin/env python
if __name__ == "__main__":
    retval = testlib.harness()
    sys.exit(retval)""" .

"DESCRIPTION.The code checks if the specified version number is a pre-release version." <EXPLAINS> """CODE.Version("1.2.3").is_prerelease
Version("1.2.3a1").is_prerelease
Version("1.2.3b1").is_prerelease
Version("1.2.3rc1").is_prerelease
Version("1.2.3dev1").is_prerelease""" .

"DESCRIPTION.The code checks if the support vector machine classifier in scikit-learn has a fit parameter called \"sample_weight\"." <EXPLAINS> """CODE.from sklearn.svm import SVC
has_fit_parameter(SVC(), "sample_weight")
True""" .

"DESCRIPTION.The code checks if the values in a DataFrame match the specified values or are present in another DataFrame." <EXPLAINS> """CODE.df = DataFrame({'A': [1, 2, 3], 'B': ['a', 'b', 'f']})
df.isin([1, 3, 12, 'a'])

df = DataFrame({'A': [1, 2, 3], 'B': [1, 4, 7]})
df.isin({'A': [1, 3], 'B': [4, 7, 12]})

df = DataFrame({'A': [1, 2, 3], 'B': ['a', 'b', 'f']})
other = DataFrame({'A': [1, 3, 3, 2], 'B': ['e', 'f', 'f', 'e']})
df.isin(other)
""" .

"DESCRIPTION.The code checks if the variable \"name\" is blank or not. If it is blank, it displays a warning message asking for input and then stops. If the variable is not blank, it displays a success message thanking the user for inputting a name." <EXPLAINS> """CODE.if not name:
  st.warning('Please input a name.')
  st.stop()
st.success('Thank you for inputting a name.')""" .

"DESCRIPTION.The code checks if the version number of 1.2.3 is a postrelease version and also checks if the version number of 1.2.3.post1 is a postrelease version." <EXPLAINS> """CODE.Version("1.2.3").is_postrelease
Version("1.2.3.post1").is_postrelease""" .

"DESCRIPTION.The code checks if the version number provided is a development release or not." <EXPLAINS> """CODE.Version("1.2.3").is_devrelease
Version("1.2.3.dev1").is_devrelease""" .

"DESCRIPTION.The code checks if the version numbers provided are development releases or not." <EXPLAINS> """CODE.Version("1.2.3").is_devrelease
Version("1.2.3.dev1").is_devrelease""" .

"DESCRIPTION.The code checks if there are any available GPU devices and gets details about the first GPU device if present, including its name. If no GPU device is available, it returns 'Unknown GPU'." <EXPLAINS> """CODE.gpu_devices = tf.config.list_physical_devices('GPU')
if gpu_devices:
...   details = tf.config.experimental.get_device_details(gpu_devices[0])
...   details.get('device_name', 'Unknown GPU')""" .

"DESCRIPTION.The code checks if there is a Cache object 'c' available from the st module. If 'c' exists, data is fetched from a URL, cleaned up, and assigned to the 'data' attribute of 'c'." <EXPLAINS> """CODE.c = st.Cache()
if c:
    # Fetch data from URL here, and then clean it up. Finally assign to c.
    c.data = ...

if c := st.Cache():
    # Fetch data from URL here, and then clean it up. Finally assign to c.
    c.data = ...""" .

"DESCRIPTION.The code checks if there is a list of all jobs available, ensures that a job named \"failed_job\" is in the list with the status \"FAILED\", and then resumes all jobs including the failed job." <EXPLAINS> """CODE.jobs = workflow.list_all()
assert jobs == [("failed_job", workflow.FAILED)]
assert workflow.resume_all(include_failed=True).get("failed_job") is not None""" .

"DESCRIPTION.The code checks if torch script is not currently being used, then it converts custom_fwd into a torch script. Otherwise, it leaves custom_fwd unchanged." <EXPLAINS> "CODE.custom_fwd = torch.jit.script(custom_fwd) if not torch.jit.is_scripting() else custom_fwd" .

"DESCRIPTION.The code checks the pre-release version of different version numbers." <EXPLAINS> """CODE.Version("1.2.3").pre
Version("1.2.3a1").pre
Version("1.2.3b1").pre
Version("1.2.3rc1").pre""" .

"DESCRIPTION.The code checks whether the input data type is a signed integer." <EXPLAINS> """CODE.is_signed_integer_dtype(int)
is_signed_integer_dtype(float)
is_signed_integer_dtype(np.uint64)  # unsigned
is_signed_integer_dtype(np.datetime64)
is_signed_integer_dtype(np.timedelta64)
is_signed_integer_dtype(np.array(['a', 'b']))
is_signed_integer_dtype(pd.Series([1, 2]))
is_signed_integer_dtype(np.array([], dtype=np.timedelta64))
is_signed_integer_dtype(pd.Index([1, 2.]))  # float
is_signed_integer_dtype(np.array([1, 2], dtype=np.uint32))  # unsigned""" .

"DESCRIPTION.The code checks whether the input data type is any form of integer datatype." <EXPLAINS> """CODE.is_any_int_dtype(int)
is_any_int_dtype(np.uint64)
is_any_int_dtype(np.timedelta64)
is_any_int_dtype(np.array([], dtype=np.timedelta64))""" .

"DESCRIPTION.The code checks whether the input is a label indicator matrix or not." <EXPLAINS> """CODE.import numpy as np
from sklearn.utils.multiclass import is_label_indicator_matrix
is_label_indicator_matrix([0, 1, 0, 1])
is_label_indicator_matrix([[1], [0, 2], []])
is_label_indicator_matrix(np.array([[1, 0], [0, 0]]))
is_label_indicator_matrix(np.array([[1], [0], [0]]))
is_label_indicator_matrix(np.array([[1, 0, 0]]))""" .

"DESCRIPTION.The code checks whether the input is similar to a string or not. If the input is a string, it returns True; otherwise, it returns False." <EXPLAINS> """CODE.is_string_like("foo")
True
is_string_like(1)
False""" .

"DESCRIPTION.The code clears a specific table within a fleet." <EXPLAINS> "CODE.fleet.clear_one_table(0)" .

"DESCRIPTION.The code combines evaluation metrics \"accuracy\", \"f1\", \"precision\", and \"recall\", and then computes the performance metrics \"predictions\"=[0,1] and \"references\"=[1,1] using a classifier." <EXPLAINS> """CODE.combine(["accuracy", "f1", "precision","recall"])
clf_metrics.compute(predictions=[0,1], references=[1,1])""" .

"DESCRIPTION.The code compares corresponding elements in two tensors x and y and returns a new tensor m where each element is the maximum of the corresponding elements in x and y." <EXPLAINS> """CODE.x = tf.Variable([[1, 2], [3, 4]])
y = tf.Variable([[2, 1], [0, -1]])
m = tf.keras.backend.maximum(x, y)
m
<tf.Tensor: shape=(2, 2), dtype=int32, numpy=
array([[2, 2],
       [3, 4]], dtype=int32)>""" .

"DESCRIPTION.The code compares each element of two matrices x and y, and returns a matrix m where each element is the maximum value from the corresponding elements of x and y." <EXPLAINS> """CODE.x = tf.Variable([[1, 2], [3, 4]])
y = tf.Variable([[2, 1], [0, -1]])
m = tf.keras.backend.maximum(x, y)
m
<tf.Tensor: shape=(2, 2), dtype=int32, numpy=
array([[2, 2],
       [3, 4]], dtype=int32)>""" .

"DESCRIPTION.The code compares the constant 'TRAINING' from the RunningStage enum with the string 'train' to check if they are equal." <EXPLAINS> "CODE.RunningStage.TRAINING == 'train'" .

"DESCRIPTION.The code compares the intervals generated by 3-limit tuning to Pythagorean tuning and 12-TET. It also creates interval sets with specific limits using different tuning methods." <EXPLAINS> """CODE.# Compare 3-limit tuning to Pythagorean tuning and 12-TET
librosa.plimit_intervals(primes=[3], bins_per_octave=12)
# Pythagorean intervals:
librosa.pythagorean_intervals(bins_per_octave=12)
# 12-TET intervals:
2**(np.arange(12)/12)

# Create a 7-bin, 5-limit interval set
librosa.plimit_intervals(primes=[3, 5], bins_per_octave=7)

# The same example, but now in factored form
librosa.plimit_intervals(primes=[3, 5], bins_per_octave=7, return_factors=True)
""" .

"DESCRIPTION.The code compares the performance of models with different hyperparameter values by evaluating them. If the model with hyperparameter C=1 performs better than C=10, then it further evaluates the model with C=0.1." <EXPLAINS> """CODE.def _run_search(self, evaluate_candidates):
    'Try C=0.1 only if C=1 is better than C=10'
    all_results = evaluate_candidates([{'C': 1}, {'C': 10}])
    score = all_results['mean_test_score']
    if score[0] < score[1]:
        evaluate_candidates([{'C': 0.1}])""" .

"DESCRIPTION.The code compares the representation of a Pandas Series using the `repr` function with the string representation of the Series using the `to_string` method with specified parameters." <EXPLAINS> """CODE.import pandas as pd

ser = pd.Series([1, 2, 3, 4])
repr_params = pd.io.formats.format.get_series_repr_params()
repr(ser) == ser.to_string(**repr_params)""" .

"DESCRIPTION.The code compares the value of the PrecisionType.HALF constant to the integer 16, and then checks if the PrecisionType.HALF constant is included in a tuple containing the integer 16 and the string \"16\"." <EXPLAINS> """CODE.PrecisionType.HALF == 16
PrecisionType.HALF in (16, "16")""" .

"DESCRIPTION.The code compares the value of the attribute LoggerStages.TRAIN with the string 'train'." <EXPLAINS> "CODE.LoggerStages.TRAIN == 'train'" .

"DESCRIPTION.The code compares the version number of the package \"torch\" with the version number \"0.1\" using the greater than or equal to operator." <EXPLAINS> "CODE._compare_version(\"torch\", operator.ge, \"0.1\")" .

"DESCRIPTION.The code compares two strings ('ddp' and 'ddp2') with the StrategyType enum values in a case-insensitive manner and checks if they match." <EXPLAINS> """CODE.# you can match the type with string
_StrategyType.DDP == 'ddp'
# which is case invariant
_StrategyType.DDP2 in ('ddp2', )""" .

"DESCRIPTION.The code compiles a model using stochastic gradient descent (sgd) optimizer, mean squared error (mse) loss function, and adds a metric to track the recall at precision of 0.8 during training." <EXPLAINS> """CODE.model.compile(
    optimizer='sgd',
    loss='mse',
    metrics=[tf.keras.metrics.RecallAtPrecision(precision=0.8)])
""" .

"DESCRIPTION.The code compiles a model using the stochastic gradient descent optimizer and the categorical hinge loss function." <EXPLAINS> "CODE.model.compile('sgd', loss=keras.losses.CategoricalHinge())" .

"DESCRIPTION.The code compiles a model with Adam optimizer, BinaryCrossentropy loss function, BinaryAccuracy metric, and FalseNegatives metric for binary classification tasks." <EXPLAINS> """CODE.model.compile(optimizer=tf.keras.optimizer.Adam(learning_rate=1e-3),
              loss=tf.keras.losses.BinaryCrossentropy(),
              metrics=[tf.keras.metrics.BinaryAccuracy(),
                       tf.keras.metrics.FalseNegatives()])
""" .

"DESCRIPTION.The code compiles a neural network model using Stochastic Gradient Descent (SGD) optimizer and LogCoshError metric for evaluation." <EXPLAINS> """CODE.model.compile('sgd', metrics=[keras.metrics.LogCoshError()])
""" .

"DESCRIPTION.The code compresses an array of values using the mu-law compression algorithm with different settings such as quantization and mu value." <EXPLAINS> """CODE.x = np.linspace(-1, 1, num=16)
x
array([-1.        , -0.86666667, -0.73333333, -0.6       , -0.46666667,
       -0.33333333, -0.2       , -0.06666667,  0.06666667,  0.2       ,
        0.33333333,  0.46666667,  0.6       ,  0.73333333,  0.86666667,
        1.        ])
y = librosa.mu_compress(x, quantize=False)
y
array([-1.        , -0.97430198, -0.94432361, -0.90834832, -0.86336132,
       -0.80328309, -0.71255496, -0.52124063,  0.52124063,  0.71255496,
        0.80328309,  0.86336132,  0.90834832,  0.94432361,  0.97430198,
        1.        ])
y = librosa.mu_compress(x, quantize=True)
y
array([-128, -124, -120, -116, -110, -102,  -91,  -66,   66,   91,  102,
       110,  116,  120,  124,  127])
y = librosa.mu_compress(x, mu=15, quantize=True)
y
array([-8, -7, -7, -6, -6, -5, -4, -2,  2,  4,  5,  6,  6,  7,  7,  7])""" .

"DESCRIPTION.The code computes and returns the next gradient operation asynchronously using the specified number of workers." <EXPLAINS> """CODE.AsyncGradients(workers)
grads_op = AsyncGradients(workers)
print(next(grads_op))""" .

"DESCRIPTION.The code computes the Mean Absolute Percentage Error as a loss function, compiles a model using stochastic gradient descent optimizer, and sets the loss function to Mean Absolute Percentage Error for the model training." <EXPLAINS> """CODE.mape = keras.losses.MeanAbsolutePercentageError()
loss = mape([0., 0., 1., 1.], [1., 1., 1., 0.])
model = keras.Model(inputs, outputs)
model.compile('sgd', loss=keras.losses.MeanAbsolutePercentageError())""" .

"DESCRIPTION.The code computes the ROUGE metric evaluation for a text-to-text generation task using the BART model on a subset of the CNN/Daily Mail dataset validation set." <EXPLAINS> """CODE.from evaluate import evaluator
from datasets import load_dataset
task_evaluator = evaluator("text2text-generation")
data = load_dataset("cnn_dailymail", "3.0.0", split="validation[:40]")
results = task_evaluator.compute(
    model_or_pipeline="facebook/bart-large-cnn",
    data=data,
    input_column="article",
    label_column="highlights",
    metric="rouge",
)""" .

"DESCRIPTION.The code computes the dot product between two tensors." <EXPLAINS> """CODE.    # dot product between tensors
    x = K.placeholder(shape=(2, 3))
    y = K.placeholder(shape=(3, 4))
    xy = K.dot(x, y)
    xy


    # dot product between tensors
    x = K.placeholder(shape=(32, 28, 3))
    y = K.placeholder(shape=(3, 4))
    xy = K.dot(x, y)
    xy


    # Theano-like behavior example
    x = K.random_uniform_variable(shape=(2, 3), low=0, high=1)
    y = K.ones((4, 3, 5))
    xy = K.dot(x, y)
    K.int_shape(xy)
""" .

"DESCRIPTION.The code computes the gradients for each worker and then prints the gradients for the next worker." <EXPLAINS> """CODE.grads_op = rollouts.for_each(ComputeGradients(workers))
print(next(grads_op))""" .

"DESCRIPTION.The code computes the normalized mutual information score between two sets of clusters." <EXPLAINS> """CODE.from sklearn.metrics.cluster import normalized_mutual_info_score
normalized_mutual_info_score([0, 0, 1, 1], [0, 0, 1, 1])
normalized_mutual_info_score([0, 0, 1, 1], [1, 1, 0, 0])
normalized_mutual_info_score([0, 0, 0, 0], [0, 1, 2, 3])""" .

"DESCRIPTION.The code concatenates two constant matrices along the last axis." <EXPLAINS> """CODE.a = tf.constant([[1, 2, 3], [4, 5, 6], [7, 8, 9])
b = tf.constant([[10, 20, 30], [40, 50, 60], [70, 80, 90]])
tf.keras.backend.concatenate((a, b), axis=-1)""" .

"DESCRIPTION.The code concatenates two datasets, ds1 and ds2, and stores the result in a new dataset ds3." <EXPLAINS> """CODE.ds3 = _concatenate_iterable_datasets([ds1, ds2])
""" .

"DESCRIPTION.The code concatenates two input data tensors along the specified axis." <EXPLAINS> """CODE.import paddle.fluid as fluid
x = fluid.data(name="x", shape=[None,3], dtype="float32")
y = fluid.data(name="y", shape=[None,3], dtype="float32")
concat = fluid.contrib.layers.partial_concat(
    [x, y], start_index=0, length=2)""" .

"DESCRIPTION.The code concatenates two input tensors along the second axis to create a merged tensor." <EXPLAINS> """CODE.tensor_a = Input(shape=(32,))
tensor_b = Input(shape=(32,))
merged_tensor = merge([tensor_a, tensor_b], mode='concat', concat_axis=1)
""" .

"DESCRIPTION.The code concatenates two map-style datasets (ds1 and ds2) into a single dataset (ds3)." <EXPLAINS> """CODE.ds3 = _concatenate_map_style_datasets([ds1, ds2])
""" .

"DESCRIPTION.The code concatenates two sets `a` and `b` and returns a new set containing all elements from both sets without duplicates." <EXPLAINS> """CODE.a = { 1, 2, 3 }
b = { 4, 5, 6, 7 }

a.concatenate(b) == { 1, 2, 3, 4, 5, 6, 7 }
""" .

"DESCRIPTION.The code concatenates two sets of arrays along a specific axis using TensorFlow's Concatenate function. The first set of arrays, x and y, are reshaped arrays created using NumPy's arange function. The second set of arrays, x1 and x2, are created by applying dense layers to reshaped arrays generated using NumPy's arange function. Finally, the concatenated array concatted is created by concatenating x1 and x2 along the default axis." <EXPLAINS> """CODE.x = np.arange(20).reshape(2, 2, 5)
print(x)
y = np.arange(20, 30).reshape(2, 1, 5)
print(y)
tf.keras.layers.Concatenate(axis=1)([x, y])

x1 = tf.keras.layers.Dense(8)(np.arange(10).reshape(5, 2))
x2 = tf.keras.layers.Dense(8)(np.arange(10, 20).reshape(5, 2))
concatted = tf.keras.layers.Concatenate()([x1, x2])
concatted.shape""" .

"DESCRIPTION.The code concatenates two tensors along a specified axis to create a new tensor." <EXPLAINS> """CODE.t1 = [[1, 2, 3], [4, 5, 6]]
t2 = [[7, 8, 9], [10, 11, 12]]
tf.concat_v2([t1, t2], 0) ==> [[1, 2, 3], [4, 5, 6], [7, 8, 9], [10, 11, 12]]
tf.concat_v2([t1, t2], 1) ==> [[1, 2, 3, 7, 8, 9], [4, 5, 6, 10, 11, 12]]

# tensor t3 with shape [2, 3]
# tensor t4 with shape [2, 3]
tf.shape(tf.concat_v2([t3, t4], 0)) ==> [4, 3]
tf.shape(tf.concat_v2([t3, t4], 1)) ==> [2, 6]
""" .

"DESCRIPTION.The code conducts parallel rollouts to generate training data for training a policy on one batch." <EXPLAINS> """CODE.rollouts = ParallelRollouts(...)
train_op = rollouts.for_each(TrainOneStep(workers))
print(next(train_op))  # This trains the policy on one batch.
{"learner_stats": {"policy_loss": ...}}""" .

"DESCRIPTION.The code configures the style of a StyledDataFrame object and displays the data-frame using the Styler from pandas.io.formats.style." <EXPLAINS> """CODE.df = StyledDataFrame(...)
df.current_style.apply(...)  # Configure the style
df  # The data-frame is displayed using ` pandas.io.formats.style.Styler`""" .

"DESCRIPTION.The code configures the style of a dataframe and displays the styled dataframe using the pandas Styler." <EXPLAINS> """CODE.
df = StyledDataFrame(...)
df.current_style.apply(...)  # Configure the style
df  # The data-frame is displayed using ` pandas.io.formats.style.Styler`
""" .

"DESCRIPTION.The code connects to a Snowpark database using Streamlit, retrieves data from a table named \"mytable\", limits the data to 10 rows, converts it to a pandas dataframe, and displays the dataframe in Streamlit." <EXPLAINS> """CODE.import streamlit as st

conn = st.experimental_connection("snowpark")
with conn.safe_session() as session:
    df = session.table("mytable").limit(10).to_pandas()

st.dataframe(df)""" .

"DESCRIPTION.The code constructs a neural network model using TensorFlow/Keras with one input layer and one output layer. The input layer has a shape of (32,) and the output layer applies a softmax activation function with 16 units." <EXPLAINS> """CODE.x = Input(shape=(32,))
y = Dense(16, activation='softmax')(x)
model = Model(x, y)


x = Input(shape=(32,))
y = tf.square(x)
model = Model(x, y)


x = Input(type_spec=tf.RaggedTensorSpec(shape=[None, None], dtype=tf.float32, ragged_rank=1))
y = x.values
model = Model(x, y)
""" .

"DESCRIPTION.The code constructs a neural network model using keras, compiles the model with stochastic gradient descent optimizer, mean squared error loss function, and sparse categorical accuracy metric." <EXPLAINS> """CODE.model = keras.Model(inputs, outputs)
model.compile(
    'sgd',
    loss='mse',
    metrics=[keras.metrics.SparseCategoricalAccuracy()])
""" .

"DESCRIPTION.The code constructs a neural network model using the given inputs and outputs. It compiles the model using stochastic gradient descent as optimizer, mean squared error as the loss function, and false negatives as the evaluation metric." <EXPLAINS> """CODE.model = keras.Model(inputs, outputs)
model.compile('sgd', loss='mse', metrics=[keras.metrics.FalseNegatives()])
""" .

"DESCRIPTION.The code constructs a one-dimensional NumPy array containing floating point numbers and a None value, with the data type set to string." <EXPLAINS> """CODE.np.array([1.0, 2.0, None], dtype='str')
construct_1d_ndarray_preserving_na([1.0, 2.0, None], dtype='str')""" .

"DESCRIPTION.The code constructs a sequence input layer for a recurrent neural network (RNN) model in TensorFlow. It creates a sequence categorical column with identity for watches data, creates an embedding column for watches data with a dimension of 10, and constructs a list of columns including the watches embedding. It then parses the input data using a specified parse example specification with the columns, creates a sequence input layer using the parsed features and columns, sets up a basic RNN cell with a specified hidden size, and runs the dynamic RNN operation with the RNN cell, input layer, and sequence lengths to generate outputs and states." <EXPLAINS> """CODE.watches = sequence_categorical_column_with_identity(
    'watches', num_buckets=1000)
watches_embedding = embedding_column(watches, dimension=10)
columns = [watches_embedding]

features = tf.parse_example(..., features=make_parse_example_spec(columns))
input_layer, sequence_length = sequence_input_layer(features, columns)

rnn_cell = tf.nn.rnn_cell.BasicRNNCell(hidden_size)
outputs, state = tf.nn.dynamic_rnn(
    rnn_cell, inputs=input_layer, sequence_length=sequence_length)""" .

"DESCRIPTION.The code contributes to retrieving a set of names for checkpoint variables from the latest checkpoint in a specified directory." <EXPLAINS> """CODE.object_graph = tf.contrib.checkpoint.object_metadata(
    tf.train.latest_checkpoint(checkpoint_directory))
ckpt_variable_names = set()
for node in object_graph.nodes:
  for attribute in node.attributes:
    ckpt_variable_names.add(attribute.full_name)""" .

"DESCRIPTION.The code converts Mela number to Svara name or vice versa." <EXPLAINS> """CODE.librosa.mela_to_svara(1)
librosa.mela_to_svara(19)
librosa.mela_to_svara(31)
librosa.mela_to_svara(34)
librosa.mela_to_svara(36)
librosa.mela_to_svara('chalanatta')""" .

"DESCRIPTION.The code converts a DLpack capsule (dlcapsule) into a TensorFlow tensor (a) and allows TensorFlow to use the shared memory from the DLpack capsule." <EXPLAINS> """CODE.a = tf.experimental.dlpack.from_dlpack(dlcapsule)
# `a` uses the memory shared by dlpack
""" .

"DESCRIPTION.The code converts a Mela scale degree ('kanakangi') to its corresponding pitch degree." <EXPLAINS> """CODE.librosa.mela_to_degrees(1)
librosa.mela_to_degrees('kanakangi')""" .

"DESCRIPTION.The code converts a PyTorch model (prepared_model) to an equivalent model in the ONNX format." <EXPLAINS> "CODE.quantized_model = convert_pt2e(prepared_model)" .

"DESCRIPTION.The code converts a Stata elapsed date value of 52 to a corresponding datetime object representing January 1, 1961." <EXPLAINS> "CODE._stata_elapsed_date_to_datetime(52, \"%tw\")                                datetime.datetime(1961, 1, 1, 0, 0)" .

"DESCRIPTION.The code converts a TensorFlow ragged tensor into a string representation. The function `ragged_tensor_to_string` converts the ragged tensor into a string containing the nested lists of values within the tensor, with an option to summarize the output by specifying a limit on the number of elements displayed in each nested list." <EXPLAINS> """CODE.rt1 = tf.ragged.constant([[1, 2, 3], [4, 5]])
ragged_tensor_to_string(rt1).numpy()
b'[[1, 2, 3], [4, 5]]'

rt2 = tf.ragged.constant([[['a'], ['b', 'c']], [['d', 'e', 'f'], []]])
ragged_tensor_to_string(rt2).numpy()
b"[[['a'], ['b', 'c']], [['d', 'e', 'f'], []]]"

rt3 = tf.ragged.constant([[1], [2, 3, 4, 5, 6], [], [], [7], [8, 9]])
ragged_tensor_to_string(rt3, summarize=2).numpy()
b'[[1], [2, 3, ..., 5, 6], ..., [7], [8, 9]]'""" .

"DESCRIPTION.The code converts a dataset into an array of strings." <EXPLAINS> "CODE.str_array = dataset.asstr()[:]" .

"DESCRIPTION.The code converts a decimal number to a time stride based on the unit specified ('T' for minutes, 'H' for hours, 'D' for days)." <EXPLAINS> """CODE.Resolution.get_stride_from_decimal(1.5, 'T')
Resolution.get_stride_from_decimal(1.04, 'H')
Resolution.get_stride_from_decimal(1, 'D')""" .

"DESCRIPTION.The code converts a dense tensor batch into a sparse representation by providing the indices of non-zero elements, the values at those indices, and the shape of the dense tensor." <EXPLAINS> """CODE.a = { ['a', 'b', 'c'], ['a', 'b'], ['a', 'b', 'c', 'd'] }

a.dense_to_sparse_batch(batch_size=2, row_shape=[6]) == {
    ([[0, 0], [0, 1], [0, 2], [1, 0], [1, 1]],  # indices
     ['a', 'b', 'c', 'a', 'b'],                 # values
     [2, 6]),                                   # dense_shape
    ([[2, 0], [2, 1], [2, 2], [2, 3]],
     ['a', 'b', 'c', 'd'],
     [1, 6])
}
""" .

"DESCRIPTION.The code converts a file size represented as a string (e.g., \"1MB\") into an integer representing the size in bytes." <EXPLAINS> """CODE.convert_file_size_to_int("1MB")
1048576
""" .

"DESCRIPTION.The code converts a given string to either a boolean value or an integer based on the string content." <EXPLAINS> """CODE.str_to_bool_or_int("FALSE")
str_to_bool_or_int("1")
str_to_bool_or_int("2")
str_to_bool_or_int("abc")""" .

"DESCRIPTION.The code converts a given text description into an image of \"An astronaut riding a horse on the moon\" and saves it as \"astronaut.png\". It then converts the same text description into a higher quality image by using a different model with the negative prompt \"low resolution, blurry\" and saves it as \"better_astronaut.png\"." <EXPLAINS> """CODE.from huggingface_hub import InferenceClient
client = InferenceClient()

image = client.text_to_image("An astronaut riding a horse on the moon.")
image.save("astronaut.png")

image = client.text_to_image(
...     "An astronaut riding a horse on the moon.",
...     negative_prompt="low resolution, blurry",
...     model="stabilityai/stable-diffusion-2-1",
... )
image.save("better_astronaut.png")
""" .

"DESCRIPTION.The code converts a hexadecimal string into its corresponding binary data and then performs a hexadecimal dump of the binary data." <EXPLAINS> "CODE.hexdump(binascii.unhexlify('49492a00080000000e00fe0004000100'))" .

"DESCRIPTION.The code converts a list of numpy arrays containing integers into a list of numpy arrays containing objects." <EXPLAINS> """CODE._to_object_array([np.array([0]), np.array([1])])
_to_object_array([np.array([0]), np.array([1, 2])])
""" .

"DESCRIPTION.The code converts a list of sentences to lowercase." <EXPLAINS> """CODE.import jiwer

sentences = ["You're PRETTY"]

print(jiwer.ToLowerCase()(sentences))""" .

"DESCRIPTION.The code converts a numpy array to a new array with the same data type as the Keras backend float type." <EXPLAINS> """CODE.from keras import backend as K
K.floatx()
arr = numpy.array([1.0, 2.0], dtype='float64')
arr.dtype
new_arr = K.cast_to_floatx(arr)
new_arr
new_arr.dtype
""" .

"DESCRIPTION.The code converts a numpy array to keras float data type." <EXPLAINS> """CODE.    from keras import backend as K
    K.floatx()
    arr = numpy.array([1.0, 2.0], dtype='float64')
    arr.dtype
    new_arr = K.cast_to_floatx(arr)
    new_arr
    new_arr.dtype
""" .

"DESCRIPTION.The code converts a pandas Series object with some NaN values into a sparse pandas Series, then further converts it into a COO (Coordinate Format) representation with specified row and column levels and sorted labels." <EXPLAINS> """CODE.from numpy import nan
s = Series([3.0, nan, 1.0, 3.0, nan, nan])
s.index = MultiIndex.from_tuples([(1, 2, 'a', 0),
                                  (1, 2, 'a', 1),
                                  (1, 1, 'b', 0),
                                  (1, 1, 'b', 1),
                                  (2, 1, 'b', 0),
                                  (2, 1, 'b', 1)],
                                  names=['A', 'B', 'C', 'D'])
ss = s.to_sparse()
A, rows, columns = ss.to_coo(row_levels=['A', 'B'],
                             column_levels=['C', 'D'],
                             sort_labels=True)""" .

"DESCRIPTION.The code converts a randomly generated 100x100x3 array into an image, and then converts the image back into an array." <EXPLAINS> """CODE.from PIL import Image
img_data = np.random.random(size=(100, 100, 3))
img = tf.keras.preprocessing.image.array_to_img(img_data)
array = tf.keras.preprocessing.image.img_to_array(img)
""" .

"DESCRIPTION.The code converts a range of samples to time values using a specified sampling rate." <EXPLAINS> "CODE.librosa.samples_to_time(np.arange(0, 22050, 512))" .

"DESCRIPTION.The code converts a sample sentence into a list of individual words." <EXPLAINS> """CODE.sample_text = 'This is a sample sentence.'
tf.keras.preprocessing.text.text_to_word_sequence(sample_text)""" .

"DESCRIPTION.The code converts a series of strings representing dates and times to datetime objects and then localizes the timezone to 'Europe/Warsaw'. In the first case, when 'nonexistent' is set to 'shift_forward', it adjusts the datetime object to '2015-03-29 03:00:00+02:00' and '2015-03-29 03:30:00+02:00'. In the second case, when 'nonexistent' is set to 'shift_backward', it adjusts the datetime object to '2015-03-29 01:59:59.999999999+01:00' and '2015-03-29 03:30:00+02:00'. In the third case, when 'nonexistent' is set to pd.Timedelta('1H'), it adjusts the datetime object to '2015-03-29 03:30:00+02:00' for both entries." <EXPLAINS> """CODE.s = pd.to_datetime(pd.Series([
... '2015-03-29 02:30:00',
... '2015-03-29 03:30:00']))
s.dt.tz_localize('Europe/Warsaw', nonexistent='shift_forward')
0   2015-03-29 03:00:00+02:00
1   2015-03-29 03:30:00+02:00
dtype: datetime64[ns, 'Europe/Warsaw']
s.dt.tz_localize('Europe/Warsaw', nonexistent='shift_backward')
0   2015-03-29 01:59:59.999999999+01:00
1   2015-03-29 03:30:00+02:00
dtype: datetime64[ns, 'Europe/Warsaw']
s.dt.tz_localize('Europe/Warsaw', nonexistent=pd.Timedelta('1H'))
0   2015-03-29 03:30:00+02:00
1   2015-03-29 03:30:00+02:00
dtype: datetime64[ns, 'Europe/Warsaw']""" .

"DESCRIPTION.The code converts a set of tensors divided into two parts based on two different strategies provided in strategy_1 and strategy_2. The strategies define the shape, group, and mapping of the tensors to be processed. The conversion is performed by the Converter class using the tensors dictionary and the specified strategies to generate a result." <EXPLAINS> """CODE.import numpy as np
complete_tensors = np.arange(4).reshape([2, 2])
partitial_tensors = np.split(complete_tensors, 2, axis=0)
name = "tmp_0"
tensors_dict = {name: partitial_tensors}
strategy_1 = {
    name: {
        "process_shape": [2],
        "process_group": [0, 1],
        "dims_mapping": [0, -1]
    }
}
strategy_2 = {
    name: {
        "process_shape": [2],
        "process_group": [0, 1],
        "dims_mapping": [-1, -1]
    }
}
converter = Converter(tensors_dict, strategy_1, strategy_2)
result = converter.convert()""" .

"DESCRIPTION.The code converts a single discrete action into a multi-discrete action using a lookup table or mapping." <EXPLAINS> """CODE.discrete_to_multi_discrete = DiscreteToMultiDiscrete(multi_discrete)
discrete_action = discrete_to_multi_discrete.sample()
multi_discrete_action = discrete_to_multi_discrete(discrete_action)""" .

"DESCRIPTION.The code converts a sparse matrix represented by a dataframe into a COO (Coordinate) format sparse matrix and returns the COO matrix, row indices, and column indices. The row_levels and column_levels are defined and the labels are sorted." <EXPLAINS> """CODE.ss = s.to_sparse()
A, rows, columns = ss.to_coo(row_levels=['A', 'B'],
                                 column_levels=['C', 'D'],
                                 sort_labels=True)
A
<3x4 sparse matrix of type '<class 'numpy.float64'>'
        with 3 stored elements in COOrdinate format>
A.todense()
matrix([[ 0.,  0.,  1.,  3.],
[ 3.,  0.,  0.,  0.],
[ 0.,  0.,  0.,  0.]])
rows
[(1, 1), (1, 2), (2, 1)]
columns
[('a', 0), ('a', 1), ('b', 0), ('b', 1)]""" .

"DESCRIPTION.The code converts a tuple of mixed data types into a tuple of strings." <EXPLAINS> """CODE.name = 'foo'
_column_name_to_strings(name)
'foo'
name = ('foo', 'bar')
_column_name_to_strings(name)
('foo', 'bar')
import pandas as pd
name = (1, pd.Timestamp('2017-02-01 00:00:00'))
_column_name_to_strings(name)
('1', '2017-02-01 00:00:00')""" .

"DESCRIPTION.The code converts an image of a cat into an image of a tiger using a pretrained model loaded from the Hugging Face Hub." <EXPLAINS> """CODE.from huggingface_hub import InferenceClient
client = InferenceClient()
image = client.image_to_image("cat.jpg", prompt="turn the cat into a tiger")
image.save("tiger.jpg")
""" .

"DESCRIPTION.The code converts boolean values within a Pandas array to a NumPy array and specifies the data type as boolean. It also demonstrates converting a Pandas array of integers to a NumPy array with a specified data type." <EXPLAINS> """CODE.a = pd.array([True, False, pd.NA], dtype="boolean")
a.to_numpy()
array([True, False, NA], dtype=object)

pd.array([True, False], dtype="boolean").to_numpy(dtype="bool")
array([ True, False])
pd.array([1, 2], dtype="Int64").to_numpy("int64")
array([1, 2])
""" .

"DESCRIPTION.The code converts continuous action space represented as a Box into a discrete action space represented as a MultiDiscrete and samples an action from the MultiDiscrete action space." <EXPLAINS> """CODE.box_to_multi_discrete = BoxToMultiDiscrete(multi_discrete)
box_action = box_to_multi_discrete.sample()
multi_discrete_action = box_to_multi_discrete(box_action)
""" .

"DESCRIPTION.The code converts frequency codes (e.g. 'D', 'W', 'M', 'H', 'S') into timestamps based on a specified base." <EXPLAINS> """CODE.get_to_timestamp_base(get_freq_code('D')[0])
get_to_timestamp_base(get_freq_code('W')[0])
get_to_timestamp_base(get_freq_code('M')[0])
get_to_timestamp_base(get_freq_code('H')[0])
get_to_timestamp_base(get_freq_code('S')[0] )""" .

"DESCRIPTION.The code converts frequency values to svara symbols based on the provided Sa (tonic) and mela (scale) in Carnatic music tradition." <EXPLAINS> """CODE.librosa.hz_to_svara_c([261/2, 261, 261*2], Sa=261, mela='kanakangi')
['SÌ£', 'S', 'SÌ']

freqs = librosa.cqt_frequencies(12, fmin=261)
librosa.hz_to_svara_c(freqs, Sa=freqs[0], mela=36)
['S', 'Râ', 'Râ', 'Râ', 'Gâ', 'Mâ', 'Mâ', 'P', 'Dâ', 'Dâ', 'Dâ', 'Nâ']
""" .

"DESCRIPTION.The code converts hexadecimal integers to decimal integers, capitalizes all alphabetic characters, and title-cases all words in a string." <EXPLAINS> """CODE.hex_ints = OneOrMore(Word(hexnums)).setParseAction(tokenMap(int, 16))
upperword = Word(alphas).setParseAction(tokenMap(str.upper))
wd = Word(alphas).setParseAction(tokenMap(str.title)).setParseAction(' '.join)""" .

"DESCRIPTION.The code converts images into text descriptions of the objects in the image." <EXPLAINS> """CODE.from huggingface_hub import InferenceClient
client = InferenceClient()
client.image_to_text("cat.jpg")
'a cat standing in a grassy field '
client.image_to_text("https://upload.wikimedia.org/wikipedia/commons/thumb/4/43/Cute_dog.jpg/320px-Cute_dog.jpg")
'a dog laying on the grass next to a flower pot '
""" .

"DESCRIPTION.The code converts musical intervals to Just Intonation frequencies." <EXPLAINS> """CODE.librosa.interval_to_fjs(3/2, unison='C')
librosa.interval_to_fjs(4/3, unison='F')
librosa.interval_to_fjs(5/4, unison='A')
librosa.interval_to_fjs(6/5, unison='A')
librosa.interval_to_fjs(25/14, unison='F#')
librosa.interval_to_fjs(25/14, unison='F#', unicode=False)""" .

"DESCRIPTION.The code converts musical key names to a list of notes, taking into account sharps, double-sharps, flats, and double-flats as needed." <EXPLAINS> """CODE.# `C:maj` will use all sharps
librosa.key_to_notes('C:maj')
['C', 'Câ¯', 'D', 'Dâ¯', 'E', 'F', 'Fâ¯', 'G', 'Gâ¯', 'A', 'Aâ¯', 'B']

# `A:min` has the same notes
librosa.key_to_notes('A:min')
['C', 'Câ¯', 'D', 'Dâ¯', 'E', 'F', 'Fâ¯', 'G', 'Gâ¯', 'A', 'Aâ¯', 'B']

# `Aâ¯:min` will use sharps, but spell note 0 (`C`) as `Bâ¯`
librosa.key_to_notes('A#:min')
['Bâ¯', 'Câ¯', 'D', 'Dâ¯', 'E', 'Eâ¯', 'Fâ¯', 'G', 'Gâ¯', 'A', 'Aâ¯', 'B']

# `Gâ¯:maj` will use a double-sharp to spell note 7 (`G`) as `Fðª`:
librosa.key_to_notes('G#:maj')
['Bâ¯', 'Câ¯', 'D', 'Dâ¯', 'E', 'Eâ¯', 'Fâ¯', 'Fðª', 'Gâ¯', 'A', 'Aâ¯', 'B']

# `Fâ­:min` will use double-flats
librosa.key_to_notes('Fb:min')
['Dð«', 'Dâ­', 'Eð«', 'Eâ­', 'Fâ­', 'F', 'Gâ­', 'Að«', 'Aâ­', 'Bð«', 'Bâ­', 'Câ­']
""" .

"DESCRIPTION.The code converts musical notes in string format to their corresponding MIDI note numbers." <EXPLAINS> """CODE.librosa.note_to_midi('C')
librosa.note_to_midi('C#3')
librosa.note_to_midi('f4')
librosa.note_to_midi('Bb-1')
librosa.note_to_midi('A!8')""" .

"DESCRIPTION.The code converts octave values to corresponding frequencies in hertz." <EXPLAINS> """CODE.librosa.octs_to_hz(1)
librosa.octs_to_hz([-2, -1, 0, 1, 2])""" .

"DESCRIPTION.The code converts the first word of each sentence to title case." <EXPLAINS> """CODE.wd = Word(alphas)
wd.setParseAction(lambda toks: toks[0].title())

print(wd.transformString("now is the winter of our discontent made glorious summer by this sun of york."))""" .

"DESCRIPTION.The code converts the input array to a floating point array and returns a new array with data type float32." <EXPLAINS> """CODE.cast_to_floatx(arr)
new_arr
array([1.,  2.], dtype=float32)
new_arr.dtype
dtype('float32')""" .

"DESCRIPTION.The code converts the input specification into a canonical format, which represents the input specification in a standardized way using SpecDict and TypeSpec objects." <EXPLAINS> """CODE.spec = ["foo", ("bar", "baz")]
output = _convert_to_canonical_format(spec)
# output = SpecDict({"foo": None, ("bar", "baz"): None})

spec = {"foo": int, "bar": {"baz": None}}
output = _convert_to_canonical_format(spec)
# output = SpecDict(
#   {"foo": TypeSpec(int), "bar": SpecDict({"baz": None})}
# )

spec = {"foo": int, "bar": {"baz": str}}
output = _convert_to_canonical_format(spec)
# output = SpecDict(
#   {"foo": TypeSpec(int), "bar": SpecDict({"baz": TypeSpec(str)})}
# )

spec = {"foo": int, "bar": {"baz": TorchTensorSpec("b,h")}}
output = _convert_to_canonical_format(spec)
# output = SpecDict(
#   {"foo": TypeSpec(int), "bar": SpecDict({"baz": TorchTensorSpec("b,h")})}
# )

spec = int
output = _convert_to_canonical_format(spec)
# output = TypeSpec(int)

spec = None
output = _convert_to_canonical_format(spec)
# output = None

spec = TorchTensorSpec("b,h")
output = _convert_to_canonical_format(spec)
# output = TorchTensorSpec("b,h")
""",
        """CODE.spec = ["foo", ("bar", "baz")]
output = convert_to_canonical_format(spec)
# output = SpecDict({"foo": None, ("bar", "baz"): None})

spec = {"foo": int, "bar": {"baz": None}}
output = convert_to_canonical_format(spec)
# output = SpecDict(
#   {"foo": TypeSpec(int), "bar": SpecDict({"baz": None})}
# )

spec = {"foo": int, "bar": {"baz": str}}
output = convert_to_canonical_format(spec)
# output = SpecDict(
#   {"foo": TypeSpec(int), "bar": SpecDict({"baz": TypeSpec(str)})}
# )

spec = {"foo": int, "bar": {"baz": TorchTensorSpec("b,h")}}
output = convert_to_canonical_format(spec)
# output = SpecDict(
#   {"foo": TypeSpec(int), "bar": SpecDict({"baz": TorchTensorSpec("b,h")})}
# )


# Example of canoncial format #2:

spec = int
output = convert_to_canonical_format(spec)
# output = TypeSpec(int)

spec = None
output = convert_to_canonical_format(spec)
# output = None

spec = TorchTensorSpec("b,h")
output = convert_to_canonical_format(spec)
# output = TorchTensorSpec("b,h")
""" .

"DESCRIPTION.The code converts the input tensor into a 2-dimensional tensor by adding a new dimension if the input tensor is not already 2-dimensional." <EXPLAINS> """CODE.x = torch.tensor(1.)
torch.atleast_2d(x)
x = torch.randn(2,2)
torch.atleast_2d(x)
x = torch.tensor(0.5)
y = torch.tensor(1.)
torch.atleast_2d((x,y))""" .

"DESCRIPTION.The code converts the model parameters to bfloat16 format, flattens the parameters into a dictionary, creates a mask based on the parameter paths with specific conditions, unflattens the mask into a dictionary, and converts the model parameters to bfloat16 format using the mask." <EXPLAINS> """CODE.model.params = model.to_bf16(model.params)
flat_params = traverse_util.flatten_dict(model.params)
mask = {path: (path[-2] != ("LayerNorm", "bias") and path[-2:] != ("LayerNorm", "scale")) for path in flat_params}
mask = traverse_util.unflatten_dict(mask)
model.params = model.to_bf16(model.params, mask)
""" .

"DESCRIPTION.The code converts the named thaat (musical mode in Hindustani music) to degrees in the corresponding scale." <EXPLAINS> """CODE.librosa.thaat_to_degrees('bilaval')
librosa.thaat_to_degrees('todi')""" .

"DESCRIPTION.The code converts the parameters of a model to half-precision floating point format (16-bit) and then converts them back to single-precision floating point format (32-bit)." <EXPLAINS> """CODE.model.params = model.to_f16(model.params)
model.params = model.to_fp32(model.params)
""" .

"DESCRIPTION.The code converts the sentences in the list to uppercase." <EXPLAINS> """CODE.import jiwer

sentences = ["You're amazing"]

print(jiwer.ToUpperCase()(sentences))""" .

"DESCRIPTION.The code converts the text \"Hello world\" into speech using an InferenceClient from the huggingface_hub library, and then saves the audio as a .wav file named \"hello_world\"." <EXPLAINS> """CODE.from pathlib import Path
from huggingface_hub import InferenceClient
client = InferenceClient()

audio = client.text_to_speech("Hello world")
Path("hello_world.wav").write_bytes(audio)
""" .

"DESCRIPTION.The code converts time in seconds to frames using a sample rate of 22050 and a hop length of 512." <EXPLAINS> "CODE.librosa.time_to_frames(np.arange(0, 1, 0.1), sr=22050, hop_length=512)" .

"DESCRIPTION.The code copies a node from one graph to another graph while transforming the arguments from the original node's graph to the new graph." <EXPLAINS> """CODE.copy a node from one graph into another. arg_transform needs to transform arguments from the graph of node
to the graph of self. Example:

g : torch._fx.Graph = ...
new_graph = torch._fx.graph()
value_remap = {}
for node in g.nodes:
    value_remap[node] = new_graph.node_copy(node, lambda n : value_remap[n])""" .

"DESCRIPTION.The code counts the frequency of each character in the string 'abcdeabcdabcaba' and returns the three most common characters along with their counts." <EXPLAINS> "CODE.Counter('abcdeabcdabcaba').most_common(3)" .

"DESCRIPTION.The code counts the frequency of each unique character in the string 'abracadabra' and returns the three most common characters along with their corresponding counts." <EXPLAINS> "CODE.Counter('abracadabra').most_common(3)" .

"DESCRIPTION.The code counts the unique elements in a list and returns the count of each unique element." <EXPLAINS> """CODE.count_unique(['a','b','c','a','c','c','a','c','c'])
count_unique(['a','b','c','a','c','c','a','c','c'])[0][1]""" .

"DESCRIPTION.The code creates 2 PeepholeLSTMCells with sizes 128 and 256 respectively, then creates a layer composed sequentially of these LSTM cells, and finally applies the layer to the input data." <EXPLAINS> """CODE.# Create 2 PeepholeLSTMCells
peephole_lstm_cells = [PeepholeLSTMCell(size) for size in [128, 256]]
# Create a layer composed sequentially of the peephole LSTM cells.
layer = RNN(peephole_lstm_cells)
input = keras.Input((timesteps, input_dim))
output = layer(input)
""" .

"DESCRIPTION.The code creates 4 ReplayActor instances and sets up ParallelRollouts object. It then stores the SampleBatch data generated from rollouts into the ReplayActors using StoreToReplayActors function." <EXPLAINS> """CODE.actors = [ReplayActor.remote() for _ in range(4)]
rollouts = ParallelRollouts(...)
store_op = rollouts.for_each(StoreToReplayActors(actors))
next(store_op)
SampleBatch(...)""" .

"DESCRIPTION.The code creates TensorFlow constant tensors with values 5, 10, and 10, and adds their references to a set. It then checks if the reference of tensor x is in the set, which returns True. Finally, it creates a dictionary mapping tensor references to string values, and retrieves the string value 'ten' associated with tensor y." <EXPLAINS> """CODE.x = tf.constant(5)
y = tf.constant(10)
z = tf.constant(10)
tensor_set = {x.ref(), y.ref(), z.ref()}
x.ref() in tensor_set
True
tensor_dict = {x.ref(): 'five', y.ref(): 'ten', z.ref(): 'ten'}
tensor_dict[y.ref()]
'ten'
x = tf.constant(5)
x.ref().deref()
<tf.Tensor: shape=(), dtype=int32, numpy=5>""" .

"DESCRIPTION.The code creates a 3D max pooling layer with a pool size of 3x3x3 and applies it to the input data, resulting in an output tensor with shape (batch_size, 10, 10, 10, 3)." <EXPLAINS> """CODE.inputs = tf.keras.Input(shape=(depth, height, width, input_channels))
layer = tf.keras.layers.MaxPooling3D(pool_size=3)
outputs = layer(inputs)  # Shape: (batch_size, 10, 10, 10, 3)
""" .

"DESCRIPTION.The code creates a 3D numpy array 'x' with shape (2, 3, 2) filled with consecutive numbers starting from 0. It then uses TensorFlow's Cropping1D layer to crop 1 element from the start and end of each row in 'x', resulting in a new 3D tensor 'y' with shape (2, 1, 2)." <EXPLAINS> """CODE.input_shape = (2, 3, 2)
x = np.arange(np.prod(input_shape)).reshape(input_shape)
print(x)
[[[ 0  1]
  [ 2  3]
  [ 4  5]]
 [[ 6  7]
  [ 8  9]
  [10 11]]]
y = tf.keras.layers.Cropping1D(cropping=1)(x)
print(y)
tf.Tensor(
  [[[2 3]]
   [[8 9]]], shape=(2, 1, 2), dtype=int64)""" .

"DESCRIPTION.The code creates a 5D array of random data with dimensions 2, 5, 3, 301, 219 and saves each slice of the array as a compressed image file in TIFF format with a compression level of 6." <EXPLAINS> """CODE.data = numpy.random.rand(2, 5, 3, 301, 219)
with TiffWriter('temp.tif', bigtiff=True) as tif:
...     for i in range(data.shape[0]):
...         tif.save(data[i], compress=6)""" .

"DESCRIPTION.The code creates a 5x6 matrix filled with ones using JAX library, then updates a specific slice of the matrix to be the maximum value between the slice and 6." <EXPLAINS> """CODE.x = jax.numpy.ones((5, 6))
jax.ops.index_max(x, jnp.index_exp[2:4, 3:], 6.)
""" .

"DESCRIPTION.The code creates a CSV logger to record training progress, then fits a machine learning model on the training data while logging the training progress to a CSV file." <EXPLAINS> """CODE.csv_logger = CSVLogger('training.log')
model.fit(X_train, Y_train, callbacks=[csv_logger])
""" .

"DESCRIPTION.The code creates a CartPole environment and initializes it with a function called OrderEnforcing. It then resets the environment, renders it, and takes a step with an action index of 0." <EXPLAINS> """CODE.from gym.envs.classic_control import CartPoleEnv
env = CartPoleEnv()
env = OrderEnforcing(env)
env.reset()
env.render()
env.step(0)
""" .

"DESCRIPTION.The code creates a ChainableUndefined object named 'foo' and accesses a key 'baz' in its 'bar' attribute as a string, then performs addition of 42 to the value." <EXPLAINS> """CODE.foo = ChainableUndefined(name='foo')
str(foo.bar['baz'])
foo.bar['baz'] + 42""" .

"DESCRIPTION.The code creates a DLpack data structure representation of a tensor 'a' using TensorFlow." <EXPLAINS> """CODE.a = tf.tensor([1, 10])
dlcapsule = tf.experimental.dlpack.to_dlpack(a)
# dlcapsule represents the dlpack data structure
""" .

"DESCRIPTION.The code creates a DNNClassifier using TensorFlow and trains it using the provided training input function. It also uses an InMemoryEvaluatorHook to evaluate the model using the evaluation input function during training." <EXPLAINS> """CODE.def train_input_fn():
  ...
  return train_dataset

def eval_input_fn():
  ...
  return eval_dataset

estimator = tf.estimator.DNNClassifier(...)

evaluator = tf.contrib.estimator.InMemoryEvaluatorHook(
    estimator, eval_input_fn)
estimator.train(train_input_fn, hooks=[evaluator])
""" .

"DESCRIPTION.The code creates a DataFrame using Spark, with columns 'id' and 'name', and then creates a Dataset from the DataFrame." <EXPLAINS> """CODE.df = spark.createDataFrame(
    data=[[1, "Elia"], [2, "Teo"], [3, "Fang"]],
    columns=["id", "name"],
)
ds = Dataset.from_spark(df)
""" .

"DESCRIPTION.The code creates a DataFrame using pandas with values and column names specified. It then creates an index object and retrieves its values. Next, it creates a MultiIndex object with specified arrays and names. Finally, it retrieves the values of the MultiIndex object and checks the number of dimensions." <EXPLAINS> """CODE.df = pd.DataFrame([[1, 2, 3], [4, 5, 6], [7, 8, 9]],
                   index=['a', 'b', 'c'], columns=['A', 'B', 'C'])
df.index._internal_get_values()
idx = pd.Index(['1', '2', '3'])
idx._internal_get_values()
midx = pd.MultiIndex.from_arrays([[1, 2, 3], ['a', 'b', 'c']],
                                  names=('number', 'letter'))
midx._internal_get_values()
midx._internal_get_values().ndim""",
        """CODE.df = pd.DataFrame([[1, 2, 3], [4, 5, 6], [7, 8, 9]],
                   index=['a', 'b', 'c'], columns=['A', 'B', 'C'])
df.index.get_values()
idx = pd.Index(['1', '2', '3'])
idx.get_values()
midx = pd.MultiIndex.from_arrays([[1, 2, 3], ['a', 'b', 'c']],
                                 names=('number', 'letter'))
midx.get_values()
midx.get_values().ndim""" .

"DESCRIPTION.The code creates a DataFrame using pandas with values and column names specified. It then creates an index object and retrieves its values. Next, it creates a MultiIndex object with two levels of indexes and retrieves its values. Finally, it checks the number of dimensions of the values in the MultiIndex object." <EXPLAINS> """CODE.df = pd.DataFrame([[1, 2, 3], [4, 5, 6], [7, 8, 9]],
                   index=['a', 'b', 'c'], columns=['A', 'B', 'C'])
df.index._internal_get_values()
idx = pd.Index(['1', '2', '3'])
idx._internal_get_values()
midx = pd.MultiIndex.from_arrays([[1, 2, 3], ['a', 'b', 'c']],
                                  names=('number', 'letter'))
midx._internal_get_values()
midx._internal_get_values().ndim""",
        """CODE.df = pd.DataFrame([[1, 2, 3], [4, 5, 6], [7, 8, 9]],
                   index=['a', 'b', 'c'], columns=['A', 'B', 'C'])
df.index.get_values()
idx = pd.Index(['1', '2', '3'])
idx.get_values()
midx = pd.MultiIndex.from_arrays([[1, 2, 3], ['a', 'b', 'c']],
                                 names=('number', 'letter'))
midx.get_values()
midx.get_values().ndim""" .

"DESCRIPTION.The code creates a DataFrame with 1000 rows and 2 columns containing random latitude and longitude values. It then generates a Pydeck map chart using the data from the DataFrame. The map is centered at latitude 37.76 and longitude -122.4, with a zoom level of 11 and a pitch of 50. The chart includes a HexagonLayer displaying hexagonal bins based on the latitude and longitude data, with specified radius, elevation scale, and range. It also includes a ScatterplotLayer displaying scatterplot points based on the latitude and longitude data, with specified color and radius values." <EXPLAINS> """CODE.df = pd.DataFrame(
...    np.random.randn(1000, 2) / [50, 50] + [37.76, -122.4],
...    columns=['lat', 'lon'])

st.pydeck_chart(pdk.Deck(
...     map_style='mapbox://styles/mapbox/light-v9',
...     initial_view_state=pdk.ViewState(
...         latitude=37.76,
...         longitude=-122.4,
...         zoom=11,
...         pitch=50,
...     ),
...     layers=[
...         pdk.Layer(
...            'HexagonLayer',
...            data=df,
...            get_position='[lon, lat]',
...            radius=200,
...            elevation_scale=4,
...            elevation_range=[0, 1000],
...            pickable=True,
...            extruded=True,
...         ),
...         pdk.Layer(
...             'ScatterplotLayer',
...             data=df,
...             get_position='[lon, lat]',
...             get_color='[200, 30, 0, 160]',
...             get_radius=200,
...         ),
...     ],
... ))""" .

"DESCRIPTION.The code creates a DataFrame with 1000 rows and 4 columns filled with random numbers and then plots a scatter matrix showing the relationships between the columns in the DataFrame." <EXPLAINS> """CODE.df = DataFrame(np.random.randn(1000, 4), columns=['A','B','C','D'])
scatter_matrix(df, alpha=0.2)""" .

"DESCRIPTION.The code creates a DataFrame with a column \"A\" containing mixed data types (string and integers), then removes the first row. It displays the resulting DataFrame and the data types of the column \"A\". Lastly, it infers the data types of the column \"A\" and displays the updated data types." <EXPLAINS> """CODE.df = pd.DataFrame({"A": ["a", 1, 2, 3]})
df = df.iloc[1:]
df
   A
1  1
2  2
3  3

df.dtypes
A    object
dtype: object

df.infer_objects().dtypes
A    int64
dtype: object
""" .

"DESCRIPTION.The code creates a DataFrame with a column \"y\" and a datetime index. It then converts the datetime index to a PeriodIndex with a monthly frequency. Finally, it generates a date range starting from \"2017-01-01\" with 2 periods and converts it to a PeriodIndex with a daily frequency." <EXPLAINS> """CODE.df = pd.DataFrame({"y": [1,2,3]},
...                   index=pd.to_datetime(["2000-03-31 00:00:00",
...                                         "2000-05-31 00:00:00",
...                                         "2000-08-31 00:00:00"]))
df.index.to_period("M")
PeriodIndex(['2000-03', '2000-05', '2000-08'],
            dtype='period[M]', freq='M')

Infer the daily frequency

idx = pd.date_range("2017-01-01", periods=2)
idx.to_period()
PeriodIndex(['2017-01-01', '2017-01-02'],
            dtype='period[D]', freq='D')""" .

"DESCRIPTION.The code creates a DataFrame with columns \"a\" and \"b\", formats the DataFrame into a table with a specified caption, label, and column format, and then prints the formatted table." <EXPLAINS> """CODE.from pandas import DataFrame
from pandas.io.formats import format as fmt
df = DataFrame({"a": [1, 2], "b": ["b1", "b2"]})
formatter = fmt.DataFrameFormatter(df)
builder = RegularTableBuilder(formatter, caption='caption', label='lab', column_format='lrc')
table = builder.get_result()
print(table)""" .

"DESCRIPTION.The code creates a DataFrame with columns 'A' and 'B' and groups the DataFrame by column 'A', then selects the last row for each group. It also selects the first row for each group without resetting the index." <EXPLAINS> """CODE.df = DataFrame([[1, 2], [1, 4], [5, 6]], columns=['A', 'B'])
df.groupby('A', as_index=False).tail(1)
df.groupby('A').head(1)""" .

"DESCRIPTION.The code creates a DataFrame with columns 'A' and 'B', and then adds a new column 'ln_A' which is the natural logarithm of the values in column 'A'." <EXPLAINS> """CODE.df = DataFrame({'A': range(1, 11), 'B': np.random.randn(10)})
df.assign(ln_A = lambda x: np.log(x.A))
newcol = np.log(df['A'])
df.assign(ln_A=newcol)""" .

"DESCRIPTION.The code creates a DataFrame with columns 'id', 'value1', and 'value2', then groups the DataFrame by 'id' and returns the number of unique values for each group. It then filters the groups based on the condition that any group with more than one unique value will be kept in the DataFrame." <EXPLAINS> """CODE.df = pd.DataFrame({'id': ['spam', 'egg', 'egg', 'spam',
                          'ham', 'ham'],
                   'value1': [1, 5, 5, 2, 5, 5],
                   'value2': list('abbaxy')})

df.groupby('id').nunique()

df.groupby('id').filter(lambda g: (g.nunique() > 1).any())""" .

"DESCRIPTION.The code creates a DataFrame with columns A, B, and C, where column A contains values [1, 1, 3], column B contains values [5, None, 6], and column C contains values [1, 2, 3]. It then groups the DataFrame by column A and returns the last row for each group in columns B and C." <EXPLAINS> """CODE.df = pd.DataFrame(dict(A=[1, 1, 3], B=[5, None, 6], C=[1, 2, 3]))
df.groupby("A").last()
     B  C
A
1  5.0  2
3  6.0  3""" .

"DESCRIPTION.The code creates a DataFrame with columns for length, width, and species. It then plots two scatter plots using the DataFrame - one with 'length' on the x-axis and 'width' on the y-axis in DarkBlue color and another with 'length' on the x-axis, 'width' on the y-axis, and different species colored using the 'viridis' colormap." <EXPLAINS> """CODE.df = pd.DataFrame([[5.1, 3.5, 0], [4.9, 3.0, 0], [7.0, 3.2, 1],
                   [6.4, 3.2, 1], [5.9, 3.0, 2]],
                  columns=['length', 'width', 'species'])
ax1 = df.plot.scatter(x='length',
                      y='width',
                      c='DarkBlue')

ax2 = df.plot.scatter(x='length',
                      y='width',
                      c='species',
                      colormap='viridis')
""" .

"DESCRIPTION.The code creates a DataFrame with specified values, row and column labels. It demonstrates how to get and set values at specific row/column pairs in the DataFrame using the `at` method. Additionally, it shows how to access a value within a Series using the `loc` and `at` methods." <EXPLAINS> """CODE.df = pd.DataFrame([[0, 2, 3], [0, 4, 1], [10, 20, 30],
...                   index=[4, 5, 6], columns=['A', 'B', 'C'])
df
    A   B   C
4   0   2   3
5   0   4   1
6  10  20  30

Get value at specified row/column pair

df.at[4, 'B']
2

Set value at specified row/column pair

df.at[4, 'B'] = 10
df.at[4, 'B']
10

Get value within a Series

df.loc[5].at['B']
4""" .

"DESCRIPTION.The code creates a DataFrame with two columns \"a\" and \"b\", then formats the DataFrame as a long table with a caption, label, and column format, and finally prints the table." <EXPLAINS> """CODE.from pandas import DataFrame
from pandas.io.formats import format as fmt
df = DataFrame({"a": [1, 2], "b": ["b1", "b2"]})
formatter = fmt.DataFrameFormatter(df)
builder = LongTableBuilder(formatter, caption='a long table', label='tab:long', column_format='lrl')
table = builder.get_result()
print(table)""" .

"DESCRIPTION.The code creates a DataFrame with values and column names, then retrieves the index values. It also creates an Index object with values and retrieves the values. Finally, it creates a MultiIndex object with arrays and names, retrieves the values, and checks the number of dimensions of the values." <EXPLAINS> """CODE.df = pd.DataFrame([[1, 2, 3], [4, 5, 6], [7, 8, 9]],
                   index=['a', 'b', 'c'], columns=['A', 'B', 'C'])
df.index._internal_get_values()
idx = pd.Index(['1', '2', '3'])
idx._internal_get_values()
midx = pd.MultiIndex.from_arrays([[1, 2, 3], ['a', 'b', 'c']],
                                  names=('number', 'letter'))
midx._internal_get_values()
midx._internal_get_values().ndim""",
        """CODE.df = pd.DataFrame([[1, 2, 3], [4, 5, 6], [7, 8, 9]],
                   index=['a', 'b', 'c'], columns=['A', 'B', 'C'])
df.index.get_values()
idx = pd.Index(['1', '2', '3'])
idx.get_values()
midx = pd.MultiIndex.from_arrays([[1, 2, 3], ['a', 'b', 'c']],
                                 names=('number', 'letter'))
midx.get_values()
midx.get_values().ndim""" .

"DESCRIPTION.The code creates a DataFrame with values and indexes, then creates an Index and a MultiIndex. It retrieves the values of the indexes and multi-index, and checks the number of dimensions of the values of the multi-index." <EXPLAINS> """CODE.df = pd.DataFrame([[1, 2, 3], [4, 5, 6], [7, 8, 9]],
                   index=['a', 'b', 'c'], columns=['A', 'B', 'C'])
df.index._internal_get_values()
idx = pd.Index(['1', '2', '3'])
idx._internal_get_values()
midx = pd.MultiIndex.from_arrays([[1, 2, 3], ['a', 'b', 'c']],
                                  names=('number', 'letter'))
midx._internal_get_values()
midx._internal_get_values().ndim""",
        """CODE.df = pd.DataFrame([[1, 2, 3], [4, 5, 6], [7, 8, 9]],
                   index=['a', 'b', 'c'], columns=['A', 'B', 'C'])
df.index.get_values()
idx = pd.Index(['1', '2', '3'])
idx.get_values()
midx = pd.MultiIndex.from_arrays([[1, 2, 3], ['a', 'b', 'c']],
                                 names=('number', 'letter'))
midx.get_values()
midx.get_values().ndim""" .

"DESCRIPTION.The code creates a DatetimeIndex object with 3 datetime entries starting from '2014-08-01 09:00' in the time zone 'Europe/Berlin'. It then converts the time zone of the DatetimeIndex entries to None, resulting in the times being shifted by 2 hours earlier." <EXPLAINS> """CODE.dti = pd.date_range(start='2014-08-01 09:00',freq='H',
...                     periods=3, tz='Europe/Berlin')

dti
DatetimeIndex(['2014-08-01 09:00:00+02:00',
               '2014-08-01 10:00:00+02:00',
               '2014-08-01 11:00:00+02:00'],
                dtype='datetime64[ns, Europe/Berlin]', freq='H')

dti.tz_convert(None)
DatetimeIndex(['2014-08-01 07:00:00',
               '2014-08-01 08:00:00',
               '2014-08-01 09:00:00'],
                dtype='datetime64[ns]', freq='H')""" .

"DESCRIPTION.The code creates a DebugClassifier object and defines input functions for training and evaluation. It fits the model using the training data, evaluates the cross entropy loss using the evaluation data, predicts the most commonly seen class in training data, and outputs the class distribution from the training data for new samples." <EXPLAINS> """CODE.# Build DebugClassifier
classifier = DebugClassifier()

# Input builders
def input_fn_train(): # returns x, y (where y represents label's class index).
  pass

def input_fn_eval(): # returns x, y (where y represents label's class index).
  pass

# Fit model.
classifier.fit(input_fn=input_fn_train)

# Evaluate cross entropy between the test and train labels.
loss = classifier.evaluate(input_fn=input_fn_eval)["loss"]

# predict_class outputs the most commonly seen class in training.
predicted_label = classifier.predict_class(new_samples)

# predict_proba outputs the class distribution from training.
label_distribution = classifier.predict_proba(new_samples)
""" .

"DESCRIPTION.The code creates a DummyOutputDatasource object, writes a range of numbers from 0 to 9 to the datasource using Ray, and then asserts that the number of successful writes to the datasource is expected to be 1." <EXPLAINS> """CODE.output = DummyOutputDatasource()
ray.data.range(10).write_datasource(output)
assert output.num_ok == 1""" .

"DESCRIPTION.The code creates a FunctionalPreprocessingStage that takes two inputs, x2 and x1, normalizes x2 and duplicates x1, and returns the original x1 and the duplicated x1 and normalized x2 as outputs." <EXPLAINS> """CODE.inputs = {'x2': tf.keras.Input(shape=(5,)),
...           'x1': tf.keras.Input(shape=(1,))}
norm_layer = tf.keras.layers.experimental.preprocessing.Normalization()
y = norm_layer(inputs['x2'])
y, z = tf.keras.layers.Lambda(lambda x: (x, x))(inputs['x1'])
outputs = [inputs['x1'], [y, z]]
stage = FunctionalPreprocessingStage(inputs, outputs)""" .

"DESCRIPTION.The code creates a FusedDropout layer with a dropout probability of 0.5 and applies it to the input tensor 'x' to generate an output tensor 'y_train'. It then switches the FusedDropout layer to test phase and applies it again to 'x' to generate another output tensor 'y_test'." <EXPLAINS> """CODE.import paddle

x = paddle.to_tensor([[1, 2, 3], [4, 5, 6]], dtype="float32")
m = paddle.incubate.nn.FusedDropout(p=0.5)

y_train = m(x)
print(y_train)

m.eval()  # switch the model to test phase
y_test = m(x)
print(y_test)""" .

"DESCRIPTION.The code creates a GcsErrorSubscriber object, subscribes to it, polls for error data while a flag \"running\" is true, and then closes the subscriber." <EXPLAINS> """CODE.subscriber = GcsErrorSubscriber()
subscriber.subscribe()
while running:
    error_id, error_data = subscriber.poll()
subscriber.close()""" .

"DESCRIPTION.The code creates a GcsFunctionKeySubscriber object, subscribes to it, continuously polls for keys while a specified condition 'running' is true, and then closes the subscriber after the loop is finished." <EXPLAINS> """CODE.subscriber = GcsFunctionKeySubscriber()
subscriber.subscribe()
while running:
    key = subscriber.poll()
subscriber.close()""" .

"DESCRIPTION.The code creates a GcsLogSubscriber object, subscribes to logs, and then continuously polls for logs until the program stops running. Finally, it closes the subscriber." <EXPLAINS> """CODE.subscriber = GcsLogSubscriber()
subscriber.subscribe()
while running:
    log = subscriber.poll()
subscriber.close()""" .

"DESCRIPTION.The code creates a IntervalIndex object from arrays with specified start and end values, including NaN values. It then converts the IntervalIndex object to a list of tuples, excluding NaN values." <EXPLAINS> """CODE.idx = pd.IntervalIndex.from_arrays([0, np.nan, 2], [1, np.nan, 3])
idx.to_tuples()
idx.to_tuples(na_tuple=False)""" .

"DESCRIPTION.The code creates a JAX array from a specified input data shape using a named sharding strategy based on the 'x' and 'y' dimensions. The input data is split across multiple devices defined by the global mesh configuration. The code defines a callback function that retrieves data from the global input data array based on the index provided. Finally, it accesses the shape of the addressable data at index 0 in the created JAX array." <EXPLAINS> """CODE.from jax.experimental.maps import Mesh
from jax.experimental import PartitionSpec as P
import numpy as np

input_shape = (8, 8)
global_input_data = np.arange(prod(input_shape)).reshape(input_shape)
global_mesh = Mesh(np.array(jax.devices()).reshape(2, 4), ('x', 'y'))
inp_sharding = jax.sharding.NamedSharding(global_mesh, P('x', 'y'))

def cb(index):
 return global_input_data[index]

arr = jax.make_array_from_callback(input_shape, inp_sharding, cb)
arr.addressable_data(0).shape
""" .

"DESCRIPTION.The code creates a JSON object with keys 'foo', 'baz', and 'stuff', where 'foo' is assigned the value 'bar', 'baz' is assigned the value 'boz', and 'stuff' is assigned a list of strings." <EXPLAINS> """CODE.st.json({
    'foo': 'bar',
    'baz': 'boz',
    'stuff': [
        'stuff 1',
        'stuff 2',
        'stuff 3',
        'stuff 5',
    ],
})""" .

"DESCRIPTION.The code creates a Keras variable initialized with random values of shape (2, 3), and then tiles the identity matrix of shape (2, 2) to match the shape of the variable. Finally, it evaluates and returns the tiled identity matrix." <EXPLAINS> """CODE.from keras import backend as K
import numpy as np

kvar = K.variable(np.random.random((2, 3)))
kvar_tile = K.tile(K.eye(2), (2, 3))
K.eval(kvar_tile)
""" .

"DESCRIPTION.The code creates a Lightning app that includes a Panel frontend with a basic layout configuration." <EXPLAINS> """CODE.import panel as pn

pn.panel("Hello **Panel â¡** World").servable()

import lightning as L
from lightning_app.frontend.panel import PanelFrontend


class LitPanel(L.LightningFlow):
    def configure_layout(self):
        return PanelFrontend("panel_app_basic.py")


class LitApp(L.LightningFlow):
    def __init__(self):
        super().__init__()
        self.lit_panel = LitPanel()

    def configure_layout(self):
        return {"name": "home", "content": self.lit_panel}


app = L.LightningApp(LitApp())""" .

"DESCRIPTION.The code creates a LocalFS client object and uses it to create a directory named \"test_localFS_mkdirs\" and then deletes the directory." <EXPLAINS> """CODE.from paddle.distributed.fleet.utils import LocalFS
client = LocalFS()
client.mkdirs("test_localFS_mkdirs")
client.delete("test_localFS_mkdirs")""" .

"DESCRIPTION.The code creates a LocalFS client object from the paddle.distributed.fleet.utils package, which is used to interact with the local file system. The code then creates a file named \"test_touch\" using the touch method, and deletes the file using the delete method." <EXPLAINS> """CODE.from paddle.distributed.fleet.utils import LocalFS
client = LocalFS()
client.touch("test_touch")
client.delete("test_touch")""" .

"DESCRIPTION.The code creates a LocalFS client object, creates a directory named \"test_is_dir\" using the client object, checks if \"test_is_file\" is a directory (returns True), and then deletes the \"test_is_dir\" directory." <EXPLAINS> """CODE.from paddle.distributed.fleet.utils import LocalFS
client = LocalFS()
client.mkdirs("test_is_dir")
print(client.is_dir("test_is_file")) # True
client.delete("test_is_dir")""" .

"DESCRIPTION.The code creates a LocalFS client, creates a file named \"test_mv_src\", moves the file \"test_mv_src\" to \"test_mv_dst\", and then deletes the file \"test_mv_dst\"." <EXPLAINS> """CODE.from paddle.distributed.fleet.utils import LocalFS
client = LocalFS()
client.touch("test_mv_src")
client.mv("test_mv_src", "test_mv_dst")
client.delete("test_mv_dst")""" .

"DESCRIPTION.The code creates a MemoryChecker object to track memory usage. It then creates a list of tensors containing the constant value 1 and records the memory usage after each tensor creation. Finally, it reports the memory usage and checks for memory leaks if all but one tensor is removed." <EXPLAINS> """CODE.with MemoryChecker() as memory_checker:
  tensors = []
  for _ in range(10):
    tensors.append(tf.constant(1))
    memory_checker.record_snapshot()

memory_checker.report()
memory_checker.assert_no_leak_if_all_possibly_except_one()""" .

"DESCRIPTION.The code creates a MultiDict object with key \"foo\" and a list of values [1, 2, 3]. It then checks if zipping the keys of the MultiDict with its list values is equal to its lists of values." <EXPLAINS> """CODE.d = MultiDict({"foo": [1, 2, 3]})
zip(d.keys(), d.listvalues()) == d.lists()
True""" .

"DESCRIPTION.The code creates a MultiDict object with key \"foo\" and values [1, 2, 3], then removes the key \"foo\" from the MultiDict object and checks if the key \"foo\" is still present in the MultiDict object." <EXPLAINS> """CODE.d = MultiDict({"foo": [1, 2, 3]})
d.pop("foo")
"foo" in d""" .

"DESCRIPTION.The code creates a MultiDict object, sets a list of values for the key 'foo', retrieves the first value associated with key 'foo', and then retrieves the list of all values associated with key 'foo'." <EXPLAINS> """CODE.d = MultiDict()
d.setlist('foo', ['1', '2'])
d['foo']
'1'
d.getlist('foo')
['1', '2']""" .

"DESCRIPTION.The code creates a Nearest Neighbors Classifier model with 2 neighbors and fits the model using the data X and corresponding labels y." <EXPLAINS> """CODE.from sklearn.neighbors import NeighborsClassifier
neigh = NeighborsClassifier(n_neighbors=2)
neigh.fit(X, y)""" .

"DESCRIPTION.The code creates a Pandas DataFrame with a sparse array in column \"A\" and converts it to a dense representation." <EXPLAINS> """CODE.df = pd.DataFrame({"A": pd.SparseArray([0, 1, 0])})
df.sparse.to_dense()""" .

"DESCRIPTION.The code creates a Pandas Series with values 'a', 'b', and 'c', and then checks if the Series contains the element 'a'." <EXPLAINS> """CODE.from pandas import Series
s = Series(list('abc'))
s.isin(['a'])
""" .

"DESCRIPTION.The code creates a Pandas series with integer values, converts it to an unsigned integer type, and then changes the value at index 0 to 128." <EXPLAINS> """CODE.s = pd.Series([-2, -1, 0, 1, 2], dtype='int8')
us = s.view('uint8')
us[0] = 128
""" .

"DESCRIPTION.The code creates a ParallelRollouts object, combines batches using ConcatBatches with a minimum batch size of 10000, and then prints the count of the next batch, which is 10000." <EXPLAINS> """CODE.rollouts = ParallelRollouts(...)
rollouts = rollouts.combine(ConcatBatches(min_batch_size=10000))
print(next(rollouts).count)
10000""" .

"DESCRIPTION.The code creates a Parquet table containing data on animals with their corresponding years and number of legs. It then writes this table to a dataset in Parquet format, partitioned by the 'year' column. Subsequently, it reads the 'n_legs' column from the dataset." <EXPLAINS> """CODE.import pyarrow as pa
table = pa.table({'year': [2020, 2022, 2021, 2022, 2019, 2021],
...                   'n_legs': [2, 2, 4, 4, 5, 100],
...                   'animal': ["Flamingo", "Parrot", "Dog", "Horse",
...                              "Brittle stars", "Centipede"]})
import pyarrow.parquet as pq
pq.write_to_dataset(table, root_path='dataset_v2_read',
...                     partition_cols=['year'],
...                     use_legacy_dataset=False)
dataset = pq.ParquetDataset('dataset_v2_read/',
...                             use_legacy_dataset=False)
dataset.read(columns=["n_legs"])""" .

"DESCRIPTION.The code creates a Parquet table with columns 'year', 'n_legs', and 'animal' containing data on different animals. It then writes this table to a Parquet dataset with partitioning by the 'year' column in the 'dataset_v2_files' directory. Finally, it retrieves the files in the created Parquet dataset." <EXPLAINS> """CODE.import pyarrow as pa
table = pa.table({'year': [2020, 2022, 2021, 2022, 2019, 2021],
                  'n_legs': [2, 2, 4, 4, 5, 100],
                  'animal': ["Flamingo", "Parrot", "Dog", "Horse",
                             "Brittle stars", "Centipede"]})
import pyarrow.parquet as pq
pq.write_to_dataset(table, root_path='dataset_v2_files',
                    partition_cols=['year'],
                    use_legacy_dataset=False)
dataset = pq._ParquetDatasetV2('dataset_v2_files/')
dataset.files""" .

"DESCRIPTION.The code creates a Parquet table with columns for year, number of legs, and animal. It then writes the table to a dataset with the year as a partition column. Finally, it reads the dataset and retrieves the schema." <EXPLAINS> """CODE.import pyarrow as pa
table = pa.table({'year': [2020, 2022, 2021, 2022, 2019, 2021],
                  'n_legs': [2, 2, 4, 4, 5, 100],
                  'animal': ["Flamingo", "Parrot", "Dog", "Horse",
                             "Brittle stars", "Centipede"]})
import pyarrow.parquet as pq
pq.write_to_dataset(table, root_path='dataset_v2_schema',
                    partition_cols=['year'],
                    use_legacy_dataset=False)
dataset = pq.ParquetDataset('dataset_v2_schema/',
                            use_legacy_dataset=False)
dataset.schema""" .

"DESCRIPTION.The code creates a PoolManager object for making HTTP requests with specified retry settings, where the number of connect, read, and redirect retries can be configured. It then uses this PoolManager object to make multiple GET requests to 'http://example.com/' with different retry settings or disabling retries entirely." <EXPLAINS> """CODE.retries = Retry(connect=5, read=2, redirect=5)
http = PoolManager(retries=retries)
response = http.request('GET', 'http://example.com/')
response = http.request('GET', 'http://example.com/', retries=Retry(10))
response = http.request('GET', 'http://example.com/', retries=False)
""" .

"DESCRIPTION.The code creates a PoolManager object to manage HTTP requests with specified retry settings, and then sends three GET requests to 'http://example.com/' with different retry configurations." <EXPLAINS> """CODE.retries = Retry(connect=5, read=2, redirect=5)
http = PoolManager(retries=retries)
response = http.request('GET', 'http://example.com/')

response = http.request('GET', 'http://example.com/', retries=Retry(10))

response = http.request('GET', 'http://example.com/', retries=False)
""" .

"DESCRIPTION.The code creates a PoolManager object with 2 pools, then sends GET requests to 'http://google.com/', 'http://google.com/mail', and 'http://yahoo.com/'. Finally, it returns the number of pools in the PoolManager object." <EXPLAINS> """CODE.manager = PoolManager(num_pools=2)
r = manager.request('GET', 'http://google.com/')
r = manager.request('GET', 'http://google.com/mail')
r = manager.request('GET', 'http://yahoo.com/')
len(manager.pools)
""" .

"DESCRIPTION.The code creates a PredefinedSplit object for splitting data into training and testing sets based on the specified test fold indices. It then splits the input arrays X and y into training and testing sets according to the predefined test fold indices, and prints the indices of training and testing sets for each iteration." <EXPLAINS> """CODE.from sklearn.cross_validation import PredefinedSplit
X = np.array([[1, 2], [3, 4], [1, 2], [3, 4]])
y = np.array([0, 0, 1, 1])
ps = PredefinedSplit(test_fold=[0, 1, -1, 1])
len(ps)
2
print(ps)       # doctest: +NORMALIZE_WHITESPACE +ELLIPSIS
sklearn.cross_validation.PredefinedSplit(test_fold=[ 0  1 -1  1])
for train_index, test_index in ps:
...    print("TRAIN:", train_index, "TEST:", test_index)
...    X_train, X_test = X[train_index], X[test_index]
...    y_train, y_test = y[train_index], y[test_index]
TRAIN: [1 2 3] TEST: [0]
TRAIN: [0 2] TEST: [1 3]""" .

"DESCRIPTION.The code creates a Proximal Policy Optimization (PPO) Lightning model for solving the CartPole-v0 environment and trains the model using the Trainer.fit method." <EXPLAINS> """CODE.model = PPOLightning("CartPole-v0")
trainer = Trainer()
trainer.fit(model)""" .

"DESCRIPTION.The code creates a Proximal Policy Optimization (PPO) Lightning model for the CartPole-v0 environment and trains the model using a Trainer." <EXPLAINS> """CODE.model = PPOLightning("CartPole-v0")
trainer = Trainer()
trainer.fit(model)""" .

"DESCRIPTION.The code creates a Proximal Policy Optimization (PPO) model for solving the CartPole-v0 environment and trains the model using a Trainer." <EXPLAINS> """CODE.model = PPOLightning("CartPole-v0")
trainer = Trainer()
trainer.fit(model)""" .

"DESCRIPTION.The code creates a PyArrow table with columns 'year', 'n_legs', and 'animal', stores it as a Parquet file in a dataset with partitions based on the 'year' column, and then reads the dataset fragments." <EXPLAINS> """CODE.import pyarrow as pa
table = pa.table({'year': [2020, 2022, 2021, 2022, 2019, 2021],
                  'n_legs': [2, 2, 4, 4, 5, 100],
                  'animal': ["Flamingo", "Parrot", "Dog", "Horse",
                             "Brittle stars", "Centipede"]})
import pyarrow.parquet as pq
pq.write_to_dataset(table, root_path='dataset_v2_fragments',
                    partition_cols=['year'],
                    use_legacy_dataset=False)
dataset = pq.ParquetDataset('dataset_v2_fragments/',
                            use_legacy_dataset=False)
dataset.fragments
""" .

"DESCRIPTION.The code creates a PyArrow table with columns 'year', 'n_legs', and 'animal'. It then writes the table to a Parquet dataset at 'dataset_v2_files' location, partitioning it by the 'year' column. Finally, it reads the dataset back and returns the list of Parquet files within the dataset." <EXPLAINS> """CODE.import pyarrow as pa
table = pa.table({'year': [2020, 2022, 2021, 2022, 2019, 2021],
                  'n_legs': [2, 2, 4, 4, 5, 100],
                  'animal': ["Flamingo", "Parrot", "Dog", "Horse",
                             "Brittle stars", "Centipede"]})
import pyarrow.parquet as pq
pq.write_to_dataset(table, root_path='dataset_v2_files',
                    partition_cols=['year'],
                    use_legacy_dataset=False)
dataset = pq.ParquetDataset('dataset_v2_files/',
                            use_legacy_dataset=False)
dataset.files""" .

"DESCRIPTION.The code creates a PyArrow table with columns 'year', 'n_legs', and 'animal'. It then writes the table to a Parquet dataset with a root path of 'dataset_v2_schema' and partitions it by the 'year' column. Finally, it reads the schema of the Parquet dataset named 'dataset_v2_schema/'." <EXPLAINS> """CODE.import pyarrow as pa
table = pa.table({'year': [2020, 2022, 2021, 2022, 2019, 2021],
                  'n_legs': [2, 2, 4, 4, 5, 100],
                  'animal': ["Flamingo", "Parrot", "Dog", "Horse",
                             "Brittle stars", "Centipede"]})
import pyarrow.parquet as pq
pq.write_to_dataset(table, root_path='dataset_v2_schema',
                    partition_cols=['year'])
dataset = pq.ParquetDataset('dataset_v2_schema/')
dataset.schema""" .

"DESCRIPTION.The code creates a PyArrow table with columns 'year', 'n_legs', and 'animal'. It then writes the table to a Parquet dataset with partitioning by the 'year' column in the 'dataset_v2_fragments' directory. Finally, it creates a Parquet dataset from the fragments in the 'dataset_v2_fragments' directory." <EXPLAINS> """CODE.import pyarrow as pa
table = pa.table({'year': [2020, 2022, 2021, 2022, 2019, 2021],
                  'n_legs': [2, 2, 4, 4, 5, 100],
                  'animal': ["Flamingo", "Parrot", "Dog", "Horse",
                             "Brittle stars", "Centipede"]})
import pyarrow.parquet as pq
pq.write_to_dataset(table, root_path='dataset_v2_fragments',
                    partition_cols=['year'],
                    use_legacy_dataset=False)
dataset = pq._ParquetDatasetV2('dataset_v2_fragments/')
dataset.fragments
""" .

"DESCRIPTION.The code creates a Python file named \"foobar\" using the makepyfile method from the pytester module." <EXPLAINS> """CODE.pytester.makepyfile("foobar")
pytester.makepyfile(custom="foobar")""" .

"DESCRIPTION.The code creates a Replay Buffer object with a size of 5 and passes it as an argument to an RLDataset object." <EXPLAINS> "CODE.RLDataset(ReplayBuffer(5))" .

"DESCRIPTION.The code creates a RetinaNet object for object detection, using a MobileNetV2 backbone with predefined weights. It specifies anchor sizes and aspect ratios for the object detection, sets the number of classes to 2, and generates predictions for two input images of different sizes." <EXPLAINS> """CODE.import torch
import torchvision
from torchvision.models.detection import RetinaNet
from torchvision.models.detection.anchor_utils import AnchorGenerator

backbone = torchvision.models.mobilenet_v2(pretrained=True).features
backbone.out_channels = 1280

anchor_generator = AnchorGenerator(
    sizes=((32, 64, 128, 256, 512),),
    aspect_ratios=((0.5, 1.0, 2.0),)
)

model = RetinaNet(backbone,
                  num_classes=2,
                  anchor_generator=anchor_generator)
model.eval()
x = [torch.rand(3, 300, 400), torch.rand(3, 500, 400)]
predictions = model(x)""" .

"DESCRIPTION.The code creates a RootFlow class that contains a list of CounterWork instances. The CounterWork instances have a counter attribute that gets incremented by 1 every time the run method is called. The RootFlow class contains a run method that iterates through the list of CounterWork instances and calls their run methods. Finally, the code creates an instance of RootFlow, runs it, and then asserts that the counter attribute of the first CounterWork instance in the list is equal to 1." <EXPLAINS> """CODE.from lightning_app import LightningFlow, LightningWork
from lightning_app.core import List

class CounterWork(LightningWork):
    def __init__(self):
        super().__init__()
        self.counter = 0
    def run(self):
        self.counter += 1

class RootFlow(LightningFlow):
    def __init__(self):
        super().__init__()
        self.list = List(*[CounterWork(), CounterWork()])
    def run(self):
        for work in self.list:
            work.run()

flow = RootFlow()
flow.run()
assert flow.list[0].counter == 1
""" .

"DESCRIPTION.The code creates a RuntimeError exception object with a specific message and then raises the exception." <EXPLAINS> """CODE.e = RuntimeError('This is an exception of type RuntimeError')
st.exception(e)""" .

"DESCRIPTION.The code creates a RuntimeError object with a custom error message, then raises the exception using the st.exception() function." <EXPLAINS> """CODE.e = RuntimeError('This is an exception of type RuntimeError')
st.exception(e)""" .

"DESCRIPTION.The code creates a SQLite database named \"my_own_db.sql\" and saves the data from a dataset 'ds' into a table named \"data\" in the database." <EXPLAINS> """CODE.ds.to_sql("data", "sqlite:///my_own_db.sql")
import sqlite3
con = sqlite3.connect("my_own_db.sql")
with con:
...     ds.to_sql("data", con)
""" .

"DESCRIPTION.The code creates a Sequential model and adds two LocallyConnected2D layers to it, with the first layer having 64 filters of size 3x3 and taking an input shape of 32x32x3, and the second layer having 32 filters of size 3x3." <EXPLAINS> """CODE.model = Sequential()
model.add(LocallyConnected2D(64, (3, 3), input_shape=(32, 32, 3)))
model.add(LocallyConnected2D(32, (3, 3)))
""" .

"DESCRIPTION.The code creates a Sequential model in Python using the Keras library. It adds a Permute layer that permutes the dimensions of the input according to the specified order. In this case, it rearranges a tensor of shape (batch_size, 10, 64) to (batch_size, 64, 10)." <EXPLAINS> """CODE.model = Sequential()
model.add(Permute((2, 1), input_shape=(10, 64)))
# now: model.output_shape == (None, 64, 10)
# note: `None` is the batch dimension
""" .

"DESCRIPTION.The code creates a Sequential model in TensorFlow with a Dense layer of 16 units and adds a SyncBatchNormalization layer to the model." <EXPLAINS> """CODE.model = tf.keras.Sequential()
model.add(tf.keras.layers.Dense(16))
model.add(tf.keras.layers.experimental.SyncBatchNormalization())""" .

"DESCRIPTION.The code creates a Sequential model with a Dense layer of 32 units and input dimension 32. Then a RepeatVector layer is added to repeat the input 3 times along a new axis, resulting in an output shape of (None, 3, 32)." <EXPLAINS> """CODE.model = Sequential()
model.add(Dense(32, input_dim=32))
# now: model.output_shape == (None, 32)
# note: `None` is the batch dimension

model.add(RepeatVector(3))
# now: model.output_shape == (None, 3, 32)
""" .

"DESCRIPTION.The code creates a Sequential model with two AtrousConvolution1D layers. The first layer has 64 filters, a filter size of 3, and a dilation rate of 2 with padding set to 'same', and takes an input shape of (10, 32). The second layer has 32 filters, a filter size of 3, and a dilation rate of 2 with padding set to 'same'." <EXPLAINS> """CODE.model = Sequential()
model.add(AtrousConvolution1D(64, 3, atrous_rate=2,
                              border_mode='same',
                              input_shape=(10, 32)))
model.add(AtrousConvolution1D(32, 3, atrous_rate=2,
                              border_mode='same'))
""" .

"DESCRIPTION.The code creates a Sequential model with two LocallyConnected1D layers. The first layer has 64 filters of size 3 and takes input of shape (10, 32), while the second layer has 32 filters of size 3." <EXPLAINS> """CODE.model = Sequential()
model.add(LocallyConnected1D(64, 3, input_shape=(10, 32)))
model.add(LocallyConnected1D(32, 3))
""" .

"DESCRIPTION.The code creates a Series and a TimedeltaIndex using numpy and pandas, then calculates the total number of seconds for each element in the Series and Index, and returns the results in a float format." <EXPLAINS> """CODE.s = pd.Series(pd.to_timedelta(np.arange(5), unit='d'))
s
0   0 days
1   1 days
2   2 days
3   3 days
4   4 days
dtype: timedelta64[ns]

s.dt.total_seconds()
0         0.0
1     86400.0
2    172800.0
3    259200.0
4    345600.0
dtype: float64

idx = pd.to_timedelta(np.arange(5), unit='d')
idx
TimedeltaIndex(['0 days', '1 days', '2 days', '3 days', '4 days'],
               dtype='timedelta64[ns]', freq=None)

idx.total_seconds()
Float64Index([0.0, 86400.0, 172800.0, 259200.00000000003, 345600.0],
             dtype='float64')""" .

"DESCRIPTION.The code creates a SingleDeviceSharding object for the first JAX device, which enables sharding operations on a single device." <EXPLAINS> "CODE.single_device_sharding = jax.sharding.SingleDeviceSharding(jax.devices()[0])" .

"DESCRIPTION.The code creates a SpaceCardData object with specific attributes such as title, license, sdk, and duplicated_from, and then converts it into a dictionary format." <EXPLAINS> """CODE.from huggingface_hub import SpaceCardData
card_data = SpaceCardData(
...     title="Dreambooth Training",
...     license="mit",
...     sdk="gradio",
...     duplicated_from="multimodalart/dreambooth-training"
... )
card_data.to_dict()
{'title': 'Dreambooth Training', 'sdk': 'gradio', 'license': 'mit', 'duplicated_from': 'multimodalart/dreambooth-training'}
""" .

"DESCRIPTION.The code creates a Spark DataFrame with integer and string columns named \"id\" and \"name\", and then converts the DataFrame into an IterableDataset." <EXPLAINS> """CODE.df = spark.createDataFrame(
    data=[[1, "Elia"], [2, "Teo"], [3, "Fang"]],
    columns=["id", "name"],
)
ds = IterableDataset.from_spark(df)
""" .

"DESCRIPTION.The code creates a SplitBuilder object, submits a split generation task using the split_builder object, and waits for the result of the split generation task." <EXPLAINS> """CODE.split_builder = SplitBuilder(...)

with split_builder.maybe_beam_pipeline():
  split_info_future = split_builder.submit_split_generation(...)

split_info = split_info_future.result()
""" .

"DESCRIPTION.The code creates a Streamlit beta container and writes text inside it along with a bar chart displaying random data. It then creates another Streamlit beta container, writes text inside it, and writes more text inside the same container." <EXPLAINS> """CODE.with st.beta_container():
    st.write("This is inside the container")
    st.bar_chart(np.random.randn(50, 3))

container = st.beta_container()
container.write("This is inside the container")
container.write("This is inside too")
""" .

"DESCRIPTION.The code creates a TPUEstimator object with a specified embedding configuration, including optimization parameters using the Adagrad Momentum optimizer with a learning rate of 0.1." <EXPLAINS> """CODE.estimator = tf.estimator.tpu.TPUEstimator(
    ...
    embedding_spec=tf.estimator.tpu.experimental.EmbeddingConfigSpec(
        ...
        optimization_parameters=tf.tpu.experimental.AdagradMomentumParameters(0.1),
        ...))""" .

"DESCRIPTION.The code creates a TensorFlow constant array with values [-1.0, 0.0, 1.0] of data type float32 and then applies the softsign activation function to the array elements using Keras. Finally, it retrieves the numpy array representation of the resulting values after the activation function is applied." <EXPLAINS> """CODE.a = tf.constant([-1.0, 0.0, 1.0], dtype = tf.float32)
b = tf.keras.activations.softsign(a)
b.numpy()""" .

"DESCRIPTION.The code creates a TensorFlow variable initialized with a random 2x3 matrix, creates a new variable with the same shape and data type as the first variable but filled with ones, and then evaluates and returns the ones-filled variable." <EXPLAINS> """CODE.kvar = tf.keras.backend.variable(np.random.random((2,3)))
kvar_ones = tf.keras.backend.ones_like(kvar)
tf.keras.backend.eval(kvar_ones)""" .

"DESCRIPTION.The code creates a TensorFlow/Keras variable initialized with a 2x2 array of numbers (1, 2, 3, 4) as a float32 datatype, and then evaluates the variable to get the array values." <EXPLAINS> """CODE.kvar = tf.keras.backend.variable(np.array([[1, 2], [3, 4]]),
                                 dtype='float32')
tf.keras.backend.eval(kvar)""" .

"DESCRIPTION.The code creates a TensorSpec object with a shape of [1, 3, 224, 224] and a data type of Float." <EXPLAINS> """CODE.ts = TensorSpec(
    shape = [1, 3, 224, 224],
    dtype = ScalarType.Float
)""" .

"DESCRIPTION.The code creates a UniversalDetector object, feeds it with some bytes of data, closes the detector, and then retrieves the result of the detection process." <EXPLAINS> """CODE... code::

        u = UniversalDetector()
        u.feed(some_bytes)
        u.close()
        detected = u.result""" .

"""DESCRIPTION.The code creates a `RolloutWorker` object to simulate rollouts in reinforcement learning. The first example creates a worker for the "CartPole-v0" environment with a PGTFPolicy, and `worker.sample()` is called to generate a sample from the environment.

The second example creates a worker for the `MultiAgentTrafficGrid` environment with multiple policies for different agents (e.g., "car_policy1", "car_policy2", "traffic_light_policy"). The `policy_mapping_fn` function is provided to assign policies to agents based on their IDs. `worker.sample()` is called to generate a sample from the environment with multiple policies and agents.""" <EXPLAINS> """CODE.worker = RolloutWorker(
  env_creator=lambda _: gym.make("CartPole-v0"),
  policy=PGTFPolicy)
print(worker.sample())


worker = RolloutWorker(
  env_creator=lambda _: MultiAgentTrafficGrid(num_cars=25),
  policies={
      "car_policy1":
        (PGTFPolicy, Box(...), Discrete(...), {"gamma": 0.99}),
      "car_policy2":
        (PGTFPolicy, Box(...), Discrete(...), {"gamma": 0.95}),
      "traffic_light_policy":
        (PGTFPolicy, Box(...), Discrete(...), {}),
  },
  policy_mapping_fn=lambda agent_id:
    random.choice(["car_policy1", "car_policy2"])
    if agent_id.startswith("car_") else "traffic_light_policy")
print(worker.sample())
""" .

"DESCRIPTION.The code creates a bar plot using the data in the DataFrame, with the x-axis representing the labels specified in the code and the y-axis representing the corresponding values." <EXPLAINS> """CODE.df = pd.DataFrame({'lab':['A', 'B', 'C'], 'val':[10, 30, 20]})
ax = df.plot.bar(x='lab', y='val', rot=0)


speed = [0.1, 17.5, 40, 48, 52, 69, 88]
lifespan = [2, 8, 70, 1.5, 25, 12, 28]
index = ['snail', 'pig', 'elephant', 'rabbit', 'giraffe', 'coyote', 'horse']
df = pd.DataFrame({'speed': speed, 'lifespan': lifespan}, index=index)
ax = df.plot.bar(rot=0)


axes = df.plot.bar(rot=0, subplots=True)
axes[1].legend(loc=2)  # doctest: +SKIP


ax = df.plot.bar(y='speed', rot=0)


ax = df.plot.bar(x='lifespan', rot=0)
""" .

"DESCRIPTION.The code creates a callback object that can store event handler functions. It allows adding event handler functions to the callback object and firing the event by calling the `fire()` method." <EXPLAINS> """CODE.c = Callback(function)
c.fire()

c = Callback()
c += handler_function  # Add event handler.
c.fire()  # Fire event.
""" .

"DESCRIPTION.The code creates a class \"Duck\" with attributes \"attr1\" and \"attr2\" initialized to \"fizz\" and \"buzz\" respectively. It then creates an instance \"b\" of class \"SimpleMock\" with attribute \"attr1\" set to \"bar\". Checking the values of attributes in instance \"b\" and class \"Duck\" shows that the values match the expected values." <EXPLAINS> """CODE.a = type("Duck",(),{})
a.attr1,a.attr2 ="fizz","buzz"
b = SimpleMock(a,"attr1","bar")
b.attr1 == "bar" and b.attr2 == "buzz"
True
a.attr1 == "fizz" and a.attr2 == "buzz"
True""" .

"DESCRIPTION.The code creates a class called Example which registers an instance of itself with an object called closer when initialized, and unregisters itself from closer when closed or when the object is deleted." <EXPLAINS> """CODE.closer = Closer()
class Example(object):
    def __init__(self):
        self._id = closer.register(self)

    def close(self):
        # Probably worth making idempotent too!
        ...
        closer.unregister(self._id)

    def __del__(self):
        self.close()
""" .

"DESCRIPTION.The code creates a class method in the Foo class called with_args, which can be chained to set specific arguments x and y." <EXPLAINS> """CODE.Foo.with_args = classmethod(_with_args)
Foo.with_args(x=1).with_args(y=2)""" .

"DESCRIPTION.The code creates a constant tensor in TensorFlow with the values [[1, 2], [3, 4]] and then repeats it n=2 times using TensorFlow's backend function." <EXPLAINS> """CODE.tf.constant([[1, 2], [3, 4]])
tf.keras.backend.repeat(b, n=2)""" .

"DESCRIPTION.The code creates a container using Streamlit's beta_container function, and then writes some text and displays a bar chart inside the container. It also creates another container and writes text inside it." <EXPLAINS> """CODE.with st.beta_container():
    st.write("This is inside the container")
    st.bar_chart(np.random.randn(50, 3))

container = st.beta_container()
container.write("This is inside the container")
container.write("This is inside too")
""" .

"DESCRIPTION.The code creates a context manager to catch exceptions that occur within a thread. It stores the exception type, value, and traceback in the `args` attribute of the context manager. This information can be accessed after the thread is spawned to handle any exceptions that may have occurred. Once the context manager is exited, the `args` attribute is no longer accessible." <EXPLAINS> """CODE.class catch_threading_exception:
    def __enter__(self):
        self.args = None
        return self

    def __exit__(self, exc_type, exc_value, traceback):
        if exc_type is not None:
            self.args = (exc_type, exc_value, traceback)

with catch_threading_exception() as cm:
    # code spawning a thread which raises an exception
    ...
    # check the thread exception: use cm.args
    ...
# cm.args attribute no longer exists at this point
# (to break a reference cycle)
""" .

"DESCRIPTION.The code creates a context value cache using either an integer or lambda function as the value type. It then stores and retrieves values based on keys (None or a TensorFlow graph) in the cache, updating the values accordingly. The code also checks that the values are being stored and retrieved correctly." <EXPLAINS> """CODE.cache = ContextValueCache(int)
cache[None] += 2
cache[None] += 4
assert cache[None] == 6

with tf.Graph().as_default() as g:
  cache[None] += 5
  cache[g] += 3
assert cache[g] == 8

cache = ContextValueCache(lambda x: x + 1)
g = tf.get_default_graph()

value = cache.setdefault(key=g, kwargs={'x': 3})
assert cache[g] == 4""" .

"DESCRIPTION.The code creates a cross product between three sets of ragged tensors containing strings." <EXPLAINS> """CODE.tf.ragged.cross([tf.ragged.constant([['a'], ['b', 'c']]),
                  tf.ragged.constant([['d'], ['e']]),
                  tf.ragged.constant([['f'], ['g']])])""" .

"DESCRIPTION.The code creates a custom HTTP header dictionary called 'headers' with the initial key-value pair 'foo'='bar'. It then adds a new key-value pair 'Foo'='baz' to the 'headers' dictionary. Finally, it retrieves the value associated with the key 'foo'." <EXPLAINS> """CODE.headers = HTTPHeaderDict(foo='bar')
headers.add('Foo', 'baz')
headers['foo']""" .

"DESCRIPTION.The code creates a custom callback class, `InterruptingCallback`, that raises a RuntimeError exception when the epoch number is 4 during model training. This callback is used along with another callback, `BackupAndRestore`, in the model fitting process. The model is a simple neural network with one dense layer, compiled with SGD optimizer and mean squared error loss. The training is attempted twice, with the first attempt including both callbacks and the second attempt excluding the `InterruptingCallback`." <EXPLAINS> """CODE.class InterruptingCallback(tf.keras.callbacks.Callback):
    def on_epoch_begin(self, epoch, logs=None):
        if epoch == 4:
            raise RuntimeError('Interrupting!')

callback = tf.keras.callbacks.experimental.BackupAndRestore(
    backup_dir="/tmp/backup")

model = tf.keras.models.Sequential([tf.keras.layers.Dense(10)])
model.compile(tf.keras.optimizers.SGD(), loss='mse')

try:
    model.fit(np.arange(100).reshape(5, 20), np.zeros(5), epochs=10,
              batch_size=1, callbacks=[callback, InterruptingCallback()],
              verbose=0)
except:
    pass

history = model.fit(np.arange(100).reshape(5, 20), np.zeros(5), epochs=10,
                    batch_size=1, callbacks=[callback], verbose=0)""" .

"DESCRIPTION.The code creates a custom dictionary object called AttributeDictionary and assigns the value \"a\" to the key \"test\". It then accesses the value associated with the key \"test\" using attribute syntax, printing \"a\" to the console." <EXPLAINS> """CODE.d = AttributeDictionary()
d["test"] = "a"
print(d.test)  # prints "a"
""" .

"DESCRIPTION.The code creates a custom list data structure called \"plist\" and then adds the elements [3, 4] to the beginning of the list [1, 2]." <EXPLAINS> """CODE.plist([1, 2]).mcons([3, 4])
plist([4, 3, 1, 2])""" .

"DESCRIPTION.The code creates a data structure with two columns, \"version\" and \"project\", and fills it with values \"1 pandas\", \"2 pandas\", and \"1 numpy\"." <EXPLAINS> """CODE.s.struct.explode()
   version project
0        1  pandas
1        2  pandas
2        1   numpy""" .

"DESCRIPTION.The code creates a dataframe with columns 'a' and 'b', then formats the dataframe using TabularBuilder with column format 'lrc', and finally prints the formatted table." <EXPLAINS> """CODE.from pandas import DataFrame
from pandas.io.formats import format as fmt
df = DataFrame({"a": [1, 2], "b": ["b1", "b2"]})
formatter = fmt.DataFrameFormatter(df)
builder = TabularBuilder(formatter, column_format='lrc')
table = builder.get_result()
print(table)""" .

"DESCRIPTION.The code creates a dataset containing integers from 0 to 41, and then prints the cardinality of the dataset. It then repeats the dataset, calculates the cardinality again, and prints whether the cardinality equals to infinite cardinality. Finally, it filters the dataset by keeping all elements and checks if the cardinality is unknown, then prints the result." <EXPLAINS> """CODE.dataset = tf.data.Dataset.range(42)
print(dataset.cardinality().numpy())
dataset = dataset.repeat()
cardinality = dataset.cardinality()
print((cardinality == tf.data.INFINITE_CARDINALITY).numpy())
dataset = dataset.filter(lambda x: True)
cardinality = dataset.cardinality()
print((cardinality == tf.data.UNKNOWN_CARDINALITY).numpy())""" .

"DESCRIPTION.The code creates a dataset object using PaddlePaddle's distributed fleet module and sets a custom pipe command for data processing." <EXPLAINS> """CODE.import paddle
dataset = paddle.distributed.fleet.dataset.DatasetBase()
dataset._set_pipe_command("python my_script.py")""",
        """CODE.import paddle.base as base
dataset = base.DatasetFactory().create_dataset()
dataset.set_so_parser_name("./abc.so")""" .

"DESCRIPTION.The code creates a dataset object using PaddlePaddle's distributed fleet module and sets a pipe command to run a Python script called \"my_script.py\"." <EXPLAINS> """CODE.import paddle
dataset = paddle.distributed.fleet.dataset.DatasetBase()
dataset._set_pipe_command("python my_script.py")""",
        """CODE.import paddle.base as base
dataset = base.DatasetFactory().create_dataset()
dataset.set_pipe_command("python my_script.py")""" .

"DESCRIPTION.The code creates a dataset object using the PaddlePaddle framework and sets a download command for the dataset to read from AFS (Alibaba File System)." <EXPLAINS> """CODE.import paddle
dataset = paddle.distributed.fleet.DatasetBase()
dataset._set_download_cmd("./read_from_afs")""",
        """CODE.import paddle.base as base
dataset = base.DatasetFactory().create_dataset()
dataset.set_download_cmd("./read_from_afs")""" .

"DESCRIPTION.The code creates a dataset object using the PaddlePaddle framework and sets the batch size to 128 for the dataset." <EXPLAINS> """CODE.dataset = base.DatasetFactory().create_dataset()
dataset.set_batch_size(128)""",
        """CODE.import paddle
dataset = paddle.distributed.fleet.DatasetBase()
dataset._set_batch_size(128)""" .

"DESCRIPTION.The code creates a dataset object using the PaddlePaddle framework and sets the batch size to 128." <EXPLAINS> """CODE.import paddle
dataset = paddle.distributed.fleet.DatasetBase()
dataset._set_batch_size(128)""",
        """CODE.import paddle.fluid as fluid
dataset = fluid.DatasetFactory().create_dataset()
dataset.set_batch_size(128)""" .

"DESCRIPTION.The code creates a dataset object using the PaddlePaddle framework and sets the variables 'data' and 'label' for the dataset." <EXPLAINS> """CODE.import paddle
dataset = paddle.distributed.fleet.DatasetBase()
dataset._set_use_var([data, label])""",
        """CODE.import paddle.base as base
paddle.enable_static()
dataset = base.DatasetFactory().create_dataset()
data = paddle.static.data(name="data", shape=[None, 10, 10], dtype="int64")
label = paddle.static.data(name="label", shape=[None, 1], dtype="int64", lod_level=1)
dataset.set_use_var([data, label])""" .

"DESCRIPTION.The code creates a dataset object using the paddle.distributed.fleet.DatasetBase class and sets the number of threads to 12 for the dataset." <EXPLAINS> """CODE.import paddle
dataset = paddle.distributed.fleet.DatasetBase()
dataset._set_thread(12)""",
        """CODE.import paddle.base as base
dataset = base.DatasetFactory().create_dataset()
dataset.set_thread(12)""" .

"DESCRIPTION.The code creates a dataset using PaddlePaddle framework and sets a download command to read data from AFS (Alibaba Cloud File Storage)." <EXPLAINS> """CODE.import paddle
dataset = paddle.distributed.fleet.DatasetBase()
dataset._set_download_cmd("./read_from_afs")""",
        """CODE.import paddle.fluid as fluid
dataset = fluid.DatasetFactory().create_dataset()
dataset.set_download_cmd("./read_from_afs")""" .

"DESCRIPTION.The code creates a dataset using PaddlePaddle framework and sets a download command to read data from AFS (Alibaba File System)." <EXPLAINS> """CODE.import paddle
dataset = paddle.distributed.fleet.DatasetBase()
dataset._set_download_cmd("./read_from_afs")""",
        """CODE.import paddle.fluid as fluid
dataset = fluid.DatasetFactory().create_dataset()
dataset.set_download_cmd("./read_from_afs")""" .

"DESCRIPTION.The code creates a dataset using PaddlePaddle's fluid framework and sets a pipe command to run a Python script called \"my_script.py\" for data processing." <EXPLAINS> """CODE.import paddle
dataset = paddle.distributed.fleet.dataset.DatasetBase()
dataset._set_pipe_command("python my_script.py")""",
        """CODE.import paddle.fluid as fluid
dataset = fluid.DatasetFactory().create_dataset()
dataset.set_pipe_command("python my_script.py")""" .

"DESCRIPTION.The code creates a dataset using PaddlePaddle's fluid framework and sets the number of threads to 12 for the dataset." <EXPLAINS> """CODE.import paddle
dataset = paddle.distributed.fleet.DatasetBase()
dataset._set_thread(12)""",
        """CODE.import paddle.fluid as fluid
dataset = fluid.DatasetFactory().create_dataset()
dataset.set_thread(12)""" .

"DESCRIPTION.The code creates a dataset using PaddlePaddle's fluid library and sets a pipe command to run a Python script called \"my_script.py\" on the dataset." <EXPLAINS> """CODE.import paddle
dataset = paddle.distributed.fleet.dataset.DatasetBase()
dataset._set_pipe_command("python my_script.py")""",
        """CODE.import paddle.fluid as fluid
dataset = fluid.DatasetFactory().create_dataset()
dataset.set_pipe_command("python my_script.py")""" .

"DESCRIPTION.The code creates a dataset using PaddlePaddle, sets the rank offset to \"rank_offset\" for the created dataset." <EXPLAINS> """CODE.import paddle.fluid as fluid
dataset = fluid.DatasetFactory().create_dataset()
dataset.set_rank_offset("rank_offset")""" .

"DESCRIPTION.The code creates a dataset using Ray, performs groupby operations on the dataset, and then operates on the groups by calculating the sum of values in columns \"B\" and \"C\" relative to each group in a pandas DataFrame." <EXPLAINS> """CODE.import ray
import pandas as pd
import numpy as np

ds = ray.data.from_items([
    {"group": 1, "value": 1},
    {"group": 1, "value": 2},
    {"group": 2, "value": 3},
    {"group": 2, "value": 4}]
)

ds.groupby("group").map_groups(
    lambda g: {"result": np.array([g["value"][0]])}
)

df = pd.DataFrame(
    {"A": ["a", "a", "b"], "B": [1, 1, 3], "C": [4, 6, 5]}
)

ds = ray.data.from_pandas(df)

grouped = ds.groupby("A")

grouped.map_groups(
    lambda g: g.apply(
        lambda c: c / g[c.name].sum() if c.name in ["B", "C"] else c
    )
)""" .

"DESCRIPTION.The code creates a dataset using the PaddlePaddle framework and prints out the description of the dataset." <EXPLAINS> """CODE.import paddle
dataset = paddle.distributed.fleet.DatasetBase()
print(dataset._desc())""",
        """CODE.import paddle.fluid as fluid
dataset = fluid.DatasetFactory().create_dataset()
print(dataset.desc())""" .

"DESCRIPTION.The code creates a dataset using the PaddlePaddle framework and sets the HDFS configuration for the dataset with the specified file system name and user group information." <EXPLAINS> """CODE.import paddle
dataset = paddle.distributed.fleet.DatasetBase()
dataset._set_hdfs_config("my_fs_name", "my_fs_ugi")""",
        """CODE.import paddle.fluid as fluid
dataset = fluid.DatasetFactory().create_dataset()
dataset.set_hdfs_config("my_fs_name", "my_fs_ugi")""" .

"DESCRIPTION.The code creates a dataset using the PaddlePaddle framework and sets the HDFS configuration for the dataset." <EXPLAINS> """CODE.import paddle
dataset = paddle.distributed.fleet.DatasetBase()
dataset._set_hdfs_config("my_fs_name", "my_fs_ugi")""",
        """CODE.import paddle.fluid as fluid
dataset = fluid.DatasetFactory().create_dataset()
dataset.set_hdfs_config("my_fs_name", "my_fs_ugi")""" .

"DESCRIPTION.The code creates a dataset using the PaddlePaddle framework and sets the number of threads to 12 for data processing." <EXPLAINS> """CODE.import paddle
dataset = paddle.distributed.fleet.DatasetBase()
dataset._set_thread(12)""",
        """CODE.import paddle.fluid as fluid
dataset = fluid.DatasetFactory().create_dataset()
dataset.set_thread(12)""" .

"DESCRIPTION.The code creates a dataset using the PaddlePaddle framework and sets the variables 'data' and 'label' for the dataset to be used." <EXPLAINS> """CODE.import paddle
dataset = paddle.distributed.fleet.DatasetBase()
dataset._set_use_var([data, label])""",
        """CODE.import paddle.fluid as fluid
dataset = fluid.DatasetFactory().create_dataset()
dataset.set_use_var([data, label])""" .

"DESCRIPTION.The code creates a dataset using the PaddlePaddle framework and sets the variables 'data' and 'label' for the dataset to use." <EXPLAINS> """CODE.import paddle
dataset = paddle.distributed.fleet.DatasetBase()
dataset._set_use_var([data, label])""",
        """CODE.import paddle.fluid as fluid
dataset = fluid.DatasetFactory().create_dataset()
dataset.set_use_var([data, label])""" .

"DESCRIPTION.The code creates a dataset using the PaddlePaddle framework and sets the variables 'data' and 'label' for use in the dataset." <EXPLAINS> """CODE.import paddle
dataset = paddle.distributed.fleet.DatasetBase()
dataset._set_use_var([data, label])""",
        """CODE.import paddle.fluid as fluid
dataset = fluid.DatasetFactory().create_dataset()
dataset.set_use_var([data, label])""" .

"DESCRIPTION.The code creates a dataset using the PaddlePaddle framework and sets the variables to be used as data and labels for the dataset." <EXPLAINS> """CODE.import paddle
dataset = paddle.distributed.fleet.DatasetBase()
dataset._set_use_var([data, label])""",
        """CODE.import paddle.fluid as fluid
dataset = fluid.DatasetFactory().create_dataset()
dataset.set_use_var([data, label])""" .

"DESCRIPTION.The code creates a dataset using the PaddlePaddle framework and specifies a download command to read data from AFS (Alibaba File System)." <EXPLAINS> """CODE.import paddle.fluid as fluid
dataset = fluid.DatasetFactory().create_dataset()
dataset.set_download_cmd("./read_from_afs")""" .

"DESCRIPTION.The code creates a dataset with 5 blocks and 5 rows, where each row has an integer ID. It then creates an iterator for the dataset." <EXPLAINS> """CODE.import ray
ds = ray.data.range(5)
ds
Dataset(num_blocks=5, num_rows=5, schema={id: int64})
ds.iterator()
DataIterator(Dataset(num_blocks=5, num_rows=5, schema={id: int64}))""" .

"DESCRIPTION.The code creates a dataset with a batch size determined by the input context, repeats the dataset indefinitely, batches the data, and then shards the dataset based on the input context's number of input pipelines and input pipeline ID. The code then distributes the dataset across multiple replicas using a given strategy and runs a function on each batch of data." <EXPLAINS> """CODE.def dataset_fn(input_context):
  batch_size = input_context.get_per_replica_batch_size(global_batch_size)
  d = tf.data.Dataset.from_tensors([[1.]]).repeat().batch(batch_size)
  return d.shard(
      input_context.num_input_pipelines, input_context.input_pipeline_id)

inputs = strategy.distribute_datasets_from_function(dataset_fn)

for batch in inputs:
  replica_results = strategy.run(replica_fn, args=(batch,))""",
        """CODE.def dataset_fn(input_context):
  batch_size = input_context.get_per_replica_batch_size(global_batch_size)
  d = tf.data.Dataset.from_tensors([[1.]]).repeat().batch(batch_size)
  return d.shard(
      input_context.num_input_pipelines, input_context.input_pipeline_id)

inputs = strategy.experimental_distribute_datasets_from_function(dataset_fn)

for batch in inputs:
  replica_results = strategy.run(replica_fn, args=(batch,))""" .

"DESCRIPTION.The code creates a dataset with a batch size determined by the input context, repeats the dataset indefinitely, batches the data, and then shards the dataset based on the input context's number of input pipelines and input pipeline ID. The code then distributes the dataset across replicas using the provided strategy and runs a function on each batch using the strategy." <EXPLAINS> """CODE.def dataset_fn(input_context):
  batch_size = input_context.get_per_replica_batch_size(global_batch_size)
  d = tf.data.Dataset.from_tensors([[1.]]).repeat().batch(batch_size)
  return d.shard(
      input_context.num_input_pipelines, input_context.input_pipeline_id)

inputs = strategy.distribute_datasets_from_function(dataset_fn)

for batch in inputs:
  replica_results = strategy.run(replica_fn, args=(batch,))""",
        """CODE.def dataset_fn(input_context):
  batch_size = input_context.get_per_replica_batch_size(global_batch_size)
  d = tf.data.Dataset.from_tensors([[1.]]).repeat().batch(batch_size)
  return d.shard(
      input_context.num_input_pipelines, input_context.input_pipeline_id)

inputs = strategy.experimental_distribute_datasets_from_function(dataset_fn)

for batch in inputs:
  replica_results = strategy.run(replica_fn, args=(batch,))""" .

"DESCRIPTION.The code creates a dataset with a batch size determined by the input context, repeats the dataset indefinitely, batches the dataset, and then shards the dataset based on the input context's number of input pipelines and input pipeline ID. The code then distributes the dataset across replicas using a strategy and runs a function on each batch in parallel." <EXPLAINS> """CODE.def dataset_fn(input_context):
  batch_size = input_context.get_per_replica_batch_size(global_batch_size)
  d = tf.data.Dataset.from_tensors([[1.]]).repeat().batch(batch_size)
  return d.shard(
      input_context.num_input_pipelines, input_context.input_pipeline_id)

inputs = strategy.distribute_datasets_from_function(dataset_fn)

for batch in inputs:
  replica_results = strategy.run(replica_fn, args=(batch,))""",
        """CODE.def dataset_fn(input_context):
  batch_size = input_context.get_per_replica_batch_size(global_batch_size)
  d = tf.data.Dataset.from_tensors([[1.]]).repeat().batch(batch_size)
  return d.shard(
      input_context.num_input_pipelines, input_context.input_pipeline_id)

inputs = strategy.experimental_distribute_datasets_from_function(dataset_fn)

for batch in inputs:
  replica_results = strategy.run(replica_fn, args=(batch,))""" .

"DESCRIPTION.The code creates a dataset with a single element of value 42, creates an iterator for the dataset, and prints out the next element of the dataset through the iterator." <EXPLAINS> """CODE.dataset = tf.data.Dataset.from_tensors(42)
iterator = iter(dataset)
print(iterator.get_next())""" .

"DESCRIPTION.The code creates a dataset with numbers from 0 to 7, splits the dataset into batches of size 4, and then re-batches the dataset with a total of 3 replicas. Finally, it prints out each element of the dataset." <EXPLAINS> """CODE.ds = tf.data.Dataset.range(8)
ds = ds.batch(4)
ds = _LegacyRebatchDataset(ds, num_replicas=3)
for elem in ds:
  print(elem)
""" .

"DESCRIPTION.The code creates a dataset with values ranging from 0 to 9 using the Ray library and retrieves the first value from the dataset." <EXPLAINS> """CODE.import ray
dataset = ray.data.range(10)
next(iter(dataset.iterator().iter_rows()))""" .

"DESCRIPTION.The code creates a deep learning model with a convolutional layer with 64 filters, each of size 3x3, taking an input shape of 3 channels, each of size 32x32. It then adds a Flatten layer to the model." <EXPLAINS> """CODE.model = tf.keras.Sequential()
model.add(tf.keras.layers.Conv2D(64, 3, 3, input_shape=(3, 32, 32)))
model.output_shape
model.add(Flatten())
model.output_shape
""" .

"DESCRIPTION.The code creates a dense 5x5 tensor filled with random values, converts it to a sparse CSR format, and then returns the number of non-zero elements in the sparse tensor." <EXPLAINS> """CODE.dense = torch.randn(5, 5)
sparse = dense._to_sparse_csr()
sparse._nnz()
æªæ¾å°pythonä»£ç """,
        """CODE.dense = torch.randn(5, 5)
sparse = dense.to_sparse_csr()
sparse._nnz()""" .

"DESCRIPTION.The code creates a deque with elements [1, 2] and appends the element 3 to the end of the deque." <EXPLAINS> "CODE.pdeque([1, 2]).append(3)" .

"DESCRIPTION.The code creates a deque with elements [2, 1, 2] and then removes the first occurrence of the element 2 from the deque. The resulting deque will be [1, 2]." <EXPLAINS> """CODE.pdeque([2, 1, 2]).remove(2)
pdeque([1, 2])""" .

"DESCRIPTION.The code creates a device mesh object with XLA backend and a matrix [[1, 2, 3, 4]], then converts the device mesh object to XLA mesh." <EXPLAINS> """CODE.dt_mesh = DeviceMesh("xla", [[1, 2, 3, 4]])
mesh = convert_to_xla_mesh(dt_mesh)""" .

"DESCRIPTION.The code creates a dictionary 'values' with keys representing various metrics such as 'loss', 'acc', and 'metric_n'. It then logs these metrics using a method 'log_dict' in the 'result' object." <EXPLAINS> """CODE.values = {'loss': loss, 'acc': acc, ..., 'metric_n': metric_n}
result.log_dict(values)""" .

"DESCRIPTION.The code creates a dictionary containing key-value pairs representing loss, accuracy, and other metrics, and then logs the values using a function called log_dict." <EXPLAINS> """CODE.values = {'loss': loss, 'acc': acc, ..., 'metric_n': metric_n}
result.log_dict(values)""" .

"DESCRIPTION.The code creates a dictionary containing values for loss, accuracy, and other metrics, then logs these values." <EXPLAINS> """CODE.values = {'loss': loss, 'acc': acc, ..., 'metric_n': metric_n}
self.log_dict(values)""" .

"DESCRIPTION.The code creates a dictionary of convolutional layers using different dimensions (1D, 2D, and 3D) and then prints the keys of the layer dictionary." <EXPLAINS> """CODE.import paddle
from collections import OrderedDict

sublayers = OrderedDict([
    ('conv1d', paddle.nn.Conv1D(3, 2, 3)),
    ('conv2d', paddle.nn.Conv2D(3, 2, 3)),
    ('conv3d', paddle.nn.Conv3D(4, 6, (3, 3, 3))),
])

layer_dict = paddle.nn.LayerDict(sublayers=sublayers)
for k in layer_dict.keys():
    print(k)""" .

"DESCRIPTION.The code creates a dictionary of parameters including input features, hidden dimension, output features, dropout probability, learning rate, batch size, data root directory, and number of workers. It then initializes a LightningTemplateModel object with these parameters." <EXPLAINS> """CODE.params = dict(
    in_features=28 * 28,
    hidden_dim=1000,
    out_features=10,
    drop_prob=0.2,
    learning_rate=0.001 * 8,
    batch_size=2,
    data_root='./datasets',
    num_workers=4,
)
model = LightningTemplateModel(**params)""" .

"DESCRIPTION.The code creates a directed graph using the graphviz library in Python, with nodes representing various processes such as 'run', 'intr', 'kernel', 'zombie', 'sleep', 'swap', 'runswap', and 'new', and edges representing the flow of execution between these processes." <EXPLAINS> """CODE.import graphviz as graphviz

# Create a graphlib graph object
graph = graphviz.DiGraph()
graph.edge('run', 'intr')
graph.edge('intr', 'runbl')
graph.edge('runbl', 'run')
graph.edge('run', 'kernel')
graph.edge('kernel', 'zombie')
graph.edge('kernel', 'sleep')
graph.edge('kernel', 'runmem')
graph.edge('sleep', 'swap')
graph.edge('swap', 'runswap')
graph.edge('runswap', 'new')
graph.edge('runswap', 'runmem')
graph.edge('new', 'runmem')
graph.edge('sleep', 'runmem')

st.graphviz_chart(graph)

Or you can render the chart from the graph using GraphViz's Dot
language:

st.graphviz_chart('''
    digraph {
        run -> intr
        intr -> runbl
        runbl -> run
        run -> kernel
        kernel -> zombie
        kernel -> sleep
        kernel -> runmem
        sleep -> swap
        swap -> runswap
        runswap -> new
        runswap -> runmem
        new -> runmem
        sleep -> runmem
    }
''')""" .

"DESCRIPTION.The code creates a directed graph using the graphviz library to illustrate a series of connections between nodes representing different system processes and actions. The graph depicts relationships such as a process transitioning to another process, a process depending on another process, and resources being allocated or released. The graph visualization is displayed using Streamlit's graphviz_chart function." <EXPLAINS> """CODE.import streamlit as st
import graphviz as graphviz

# Create a graphlib graph object
graph = graphviz.Digraph()
graph.edge('run', 'intr')
graph.edge('intr', 'runbl')
graph.edge('runbl', 'run')
graph.edge('run', 'kernel')
graph.edge('kernel', 'zombie')
graph.edge('kernel', 'sleep')
graph.edge('kernel', 'runmem')
graph.edge('sleep', 'swap')
graph.edge('swap', 'runswap')
graph.edge('runswap', 'new')
graph.edge('runswap', 'runmem')
graph.edge('new', 'runmem')
graph.edge('sleep', 'runmem')

st.graphviz_chart(graph)

# Or you can render the chart from the graph using GraphViz's Dot language:
st.graphviz_chart('''
    digraph {
        run -> intr
        intr -> runbl
        runbl -> run
        run -> kernel
        kernel -> zombie
        kernel -> sleep
        kernel -> runmem
        sleep -> swap
        swap -> runswap
        runswap -> new
        runswap -> runmem
        new -> runmem
        sleep -> runmem
    }
''')""" .

"DESCRIPTION.The code creates a dispatch server for distributing data processing tasks and applies a distribution strategy to a dataset to execute processing tasks in parallel across multiple epochs." <EXPLAINS> """CODE.dispatcher = tf.data.experimental.service.DispatchServer(port=0)
dataset = tf.data.Dataset.range(10)
dataset = dataset.apply(tf.data.experimental.service.distribute(
    processing_mode="parallel_epochs", service=dispatcher.target))""" .

"DESCRIPTION.The code creates a dispatch server object with specified port number and sets the start parameter to False. It then starts the dispatch server." <EXPLAINS> """CODE.dispatcher = tf.data.experimental.service.DispatchServer(port=0, start=False)
dispatcher.start()""" .

"DESCRIPTION.The code creates a distributed dataset using TensorFlow's MirroredStrategy, where the data is repeated 100 times and batched with a global batch size of 16. It then iterates through the distributed dataset and specifies the tensor shapes and data types for the elements in the dataset. Additionally, it updates the MirroredStrategy to distribute across multiple GPUs and specifies the tensor shapes and data types for the per-replica data." <EXPLAINS> """CODE.global_batch_size = 16
strategy = tf.distribute.MirroredStrategy()
dataset = tf.data.Dataset.from_tensors(([1.],[2])).repeat(100).batch(global_batch_size)
distributed_iterator = iter(strategy.experimental_distribute_dataset(dataset))
distributed_iterator.element_spec
(TensorSpec(shape=(None, 1), dtype=tf.float32, name=None),
 TensorSpec(shape=(None, 1), dtype=tf.int32, name=None))

strategy = tf.distribute.MirroredStrategy(['/gpu:0', '/gpu:1'])
(PerReplicaSpec(TensorSpec(shape=(None, 1), dtype=tf.float32, name=None),
                TensorSpec(shape=(None, 1), dtype=tf.float32, name=None)),
 PerReplicaSpec(TensorSpec(shape=(None, 1), dtype=tf.int32, name=None),
                TensorSpec(shape=(None, 1), dtype=tf.int32, name=None))""" .

"DESCRIPTION.The code creates a frame containing a text area with the text 'Hello world!' inside." <EXPLAINS> """CODE.from prompt_toolkit.widgets import Frame, TextArea
print_container(
    Frame(TextArea(text='Hello world!')))""" .

"DESCRIPTION.The code creates a graph using the GraphViz library and defines a series of nodes and edges to represent a flow of events or relationships. The graph visualization is then displayed using the streamlit library." <EXPLAINS> """CODE.import streamlit as st
import graphviz as graphviz

# Create a graphlib graph object
graph = graphviz.Digraph()
graph.edge('run', 'intr')
graph.edge('intr', 'runbl')
graph.edge('runbl', 'run')
graph.edge('run', 'kernel')
graph.edge('kernel', 'zombie')
graph.edge('kernel', 'sleep')
graph.edge('kernel', 'runmem')
graph.edge('sleep', 'swap')
graph.edge('swap', 'runswap')
graph.edge('runswap', 'new')
graph.edge('runswap', 'runmem')
graph.edge('new', 'runmem')
graph.edge('sleep', 'runmem')

st.graphviz_chart(graph)

Or you can render the chart from the graph using GraphViz's Dot
language:

st.graphviz_chart('''
    digraph {
        run -> intr
        intr -> runbl
        runbl -> run
        run -> kernel
        kernel -> zombie
        kernel -> sleep
        kernel -> runmem
        sleep -> swap
        swap -> runswap
        runswap -> new
        runswap -> runmem
        new -> runmem
        sleep -> runmem
    }
''')""" .

"DESCRIPTION.The code creates a group of 2 workers and then executes a lambda function that returns the value 1. It then asserts that the length of the output is 2 and that all the values in the output are 1." <EXPLAINS> """CODE.worker_group = WorkerGroup(num_workers=2)
output = worker_group.execute(lambda: 1)
assert len(output) == 2
assert all(o == 1 for o in output)
""" .

"DESCRIPTION.The code creates a gym environment for the CartPole-v1 game, transforms observations by adding random noise, and resets the environment to start a new episode." <EXPLAINS> """CODE.import gym
env = gym.make('CartPole-v1')
env = TransformObservation(env, lambda obs: obs + 0.1*np.random.randn(*obs.shape))
env.reset()
array([-0.08319338,  0.04635121, -0.07394746,  0.20877492])""" .

"DESCRIPTION.The code creates a keras Model object and initializes two lists, `arr1` as a ListWrapper object and `arr2` as a regular, untracked python list, within a scope that disables automatic dependency tracking." <EXPLAINS> """CODE.model = tf.keras.Model()
model.arr1 = []  # Creates a ListWrapper object
with no_automatic_dependency_tracking_scope(model):
  model.arr2 = []  # Creates a regular, untracked python list
""" .

"DESCRIPTION.The code creates a keras model using the specified inputs and outputs, compiles the model using stochastic gradient descent ('sgd') optimizer and Hinge loss function." <EXPLAINS> """CODE.model = keras.Model(inputs, outputs)
model.compile('sgd', loss=keras.losses.Hinge())
""" .

"DESCRIPTION.The code creates a keras model with specified input and output layers, compiles the model using stochastic gradient descent optimizer with mean squared error loss, and uses sparse categorical accuracy as the evaluation metric." <EXPLAINS> """CODE.model = keras.Model(inputs, outputs)
model.compile(
    'sgd',
    loss='mse',
    metrics=[keras.metrics.SparseCategoricalAccuracy()])
""" .

"DESCRIPTION.The code creates a linear operator that acts on a batch matrix A with shape [..., M, N]. It then performs matrix multiplication between the operator and a batch matrix X with shape [..., N, R], resulting in a new batch matrix Y with shape [..., M, R]. The calculation involves taking the sum of the product of A and X along the last axis of X." <EXPLAINS> """CODE.# Make an operator acting like batch matrix A.  Assume A.shape = [..., M, N]
operator = LinearOperator(...)
operator.shape = [..., M, N]

X = ... # shape [..., N, R], batch matrix, R > 0.

Y = operator.matmul(X)
Y.shape
==> [..., M, R]

Y[..., :, r] = sum_j A[..., :, j] X[j, r]
""" .

"DESCRIPTION.The code creates a linear operator that behaves like a 10 x 2 x 2 matrix. It then solves linear systems for batches of length 10. It first solves one linear system (R=1) for every member of the batch with a RHS of shape 10 x 2 x 1. It then solves five linear systems (R=5) for every member of the batch with a RHS of shape 10 x 2 x 5. Finally, it accesses the solution to the linear system at index 3 for the third column of the RHS." <EXPLAINS> """CODE.# Create an operator acting like a 10 x 2 x 2 matrix.
operator = LinearOperator(...)
operator.shape # = 10 x 2 x 2

# Solve one linear system (R = 1) for every member of the length 10 batch.
RHS = ... # shape 10 x 2 x 1
X = operator.solve(RHS)  # shape 10 x 2 x 1

# Solve five linear systems (R = 5) for every member of the length 10 batch.
RHS = ... # shape 10 x 2 x 5
X = operator.solve(RHS)
X[3, :, 2]  # Solution to the linear system A[3, :, :] X = RHS[3, :, 2]
""" .

"DESCRIPTION.The code creates a linear operator, applies a rank 2 perturbation to it, computes the shape of the resulting operator, calculates the logarithm of its determinant, and applies the resulting operator to a given input tensor." <EXPLAINS> """CODE.# Create a 3 x 3 diagonal linear operator.
diag_operator = LinearOperatorDiag(
    diag_update=[1., 2., 3.], is_non_singular=True, is_self_adjoint=True,
    is_positive_definite=True)

# Perturb with a rank 2 perturbation
operator = LinearOperatorUDVHUpdate(
    operator=diag_operator,
    u=[[1., 2.], [-1., 3.], [0., 0.]],
    diag_update=[11., 12.],
    v=[[1., 2.], [-1., 3.], [10., 10.]])

operator.shape

operator.log_determinant()

x = ... Shape [3, 4] Tensor
operator.apply(x)
""" .

"DESCRIPTION.The code creates a list s2 with elements [1, 1, 2], removes the element 1 from s2 and assigns the result to s3, and then removes the element 2 from s3 and assigns the result to s4." <EXPLAINS> """CODE.s2 = pbag([1, 1, 2])
s3 = s2.remove(1)
s4 = s3.remove(2)""" .

"DESCRIPTION.The code creates a list s2 with elements [1, 1, 2], then removes the element 1 from s2 to create a new list s3. It then removes the element 2 from s3 to create a new list s4. Finally, the code returns a list with elements [1, 1]." <EXPLAINS> """CODE.s2 = pbag([1, 1, 2])
s3 = s2.remove(1)
s4 = s3.remove(2)
s4
pbag([1, 1])""" .

"DESCRIPTION.The code creates a logger object to record training information and fits a model using training data, with the logger being called back during the training process to save metrics such as loss and accuracy to a CSV file." <EXPLAINS> """CODE.csv_logger = CSVLogger('training.log')
model.fit(X_train, Y_train, callbacks=[csv_logger])
""" .

"DESCRIPTION.The code creates a logger using HFSummaryWriter from the huggingface_hub package. The logger automatically pushes logs every 15 minutes or when triggered manually using `logger.scheduler.trigger()`. Additionally, in the second code snippet, logs are automatically pushed every 5 minutes or when exiting the context manager." <EXPLAINS> """CODE.from huggingface_hub import HFSummaryWriter

# Logs are automatically pushed every 15 minutes
logger = HFSummaryWriter(repo_id="test_hf_logger", commit_every=15)
logger.add_scalar("a", 1)
logger.add_scalar("b", 2)
...

# You can also trigger a push manually
logger.scheduler.trigger()


from huggingface_hub import HFSummaryWriter

# Logs are automatically pushed every 5 minutes (default) + when exiting the context manager
with HFSummaryWriter(repo_id="test_hf_logger") as logger:
...     logger.add_scalar("a", 1)
...     logger.add_scalar("b", 2)
""" .

"DESCRIPTION.The code creates a main program using Fluid, defines a data variable \"rlt\" with shape [1, 1] and float32 data type, and then converts the main program to a string and prints it." <EXPLAINS> """CODE.import paddle.fluid as fluid

prog = fluid.default_main_program()
rlt = fluid.layers.data("fake_data", shape=[1,1], dtype='float32')
debug_str = prog.to_string(throw_on_error=True, with_details=False)
print(debug_str)""" .

"DESCRIPTION.The code creates a memory tracker object, starts tracking memory usage, executes some code, and then stops the memory tracker and updates metrics with the tracked memory usage." <EXPLAINS> """CODE.self._memory_tracker = TrainerMemoryTracker(self.args.skip_memory_metrics)
self._memory_tracker.start()
code ...
metrics = {"train_runtime": 10.5}
self._memory_tracker.stop_and_update_metrics(metrics)
""" .

"DESCRIPTION.The code creates a memory-mapped file 'temp.tif' with a shape of 256x256 and data type of float32. It then assigns a value of 1.0 to the element at position (255, 255), flushes the changes to the memory-mapped file, and retrieves the shape and data type of the memory-mapped file before finally deleting it. Later, it accesses the element at position (255, 255) of the memory-mapped file and returns the value 1.0." <EXPLAINS> """CODE.memmap('temp.tif', shape=(256, 256), dtype='float32')
im[255, 255] = 1.0
im.flush()
im.shape, im.dtype
((256, 256), dtype('float32'))
del im

memmap('temp.tif', page=0)
im[255, 255]
1.0""" .

"DESCRIPTION.The code creates a mirrored strategy for distributing computation across multiple GPUs. It then loads a TFRecord dataset, distributes it using the strategy, and defines a train_step function to train a model with the distributed dataset. Finally, it iterates over the distributed dataset and runs the train_step function on each element using the strategy." <EXPLAINS> """CODE.strategy = tf.distribute.MirroredStrategy()

# Create a dataset
dataset = dataset_ops.Dataset.TFRecordDataset([
  "/a/1.tfr", "/a/2.tfr", "/a/3.tfr", "/a/4.tfr"])

# Distribute that dataset
dist_dataset = strategy.experimental_distribute_dataset(dataset)

# Iterate over the distributed dataset
for x in dist_dataset:
  # process dataset elements
  strategy.run(train_step, args=(x,))


strategy = tf.distribute.MirroredStrategy()

# Create a dataset
dataset = dataset_ops.Dataset.TFRecordDataset([
  "/a/1.tfr", "/a/2.tfr", "/a/3.tfr", "/a/4.tfr"])

# Distribute that dataset
dist_dataset = strategy.experimental_distribute_dataset(dataset)

@tf.function(input_signature=[dist_dataset.element_spec])
def train_step(inputs):
  # train model with inputs
  return

# Iterate over the distributed dataset
for x in dist_dataset:
  # process dataset elements
  strategy.run(train_step, args=(x,))
""" .

"DESCRIPTION.The code creates a mirrored strategy for distributing training data across multiple GPUs. It then creates a dataset with numbers from 0 to 9 and batches them into groups of 2. The dataset is distributed using the mirrored strategy. Finally, the code iterates over the distributed dataset, running a train step function on each batch of data using the strategy's experimental_run_v2 method." <EXPLAINS> """CODE.strategy = tf.distribute.MirroredStrategy()

# Create a dataset
dataset = dataset_ops.Dataset.range(10).batch(2)

# Distribute that dataset
dist_dataset = strategy.experimental_distribute_dataset(dataset)
# Iterate over the distributed dataset
for x in dist_dataset:
  # process dataset elements
  strategy.experimental_run_v2(train_step, args=(x,))
""" .

"DESCRIPTION.The code creates a mock file system using MockFs and adds content to files '/path/to/file1' and '/path/to/file2'. It then checks if file '/path/to/file1' exists, writes content to '/path/to/file2', renames '/path/to/file1' to '/path/to/file1_moved', and asserts that the contents of the files in the mock file system match the expected values." <EXPLAINS> """CODE.fs = MockFs()
with fs.mock():

  fs.add_file('/path/to/file1', 'Content of file 1')

  assert tf.io.gfile.exists('/path/to/file1')
  with tf.io.gfile.GFile('/path/to/file2', 'w') as f:
    f.write('Content of file 2')
  tf.io.gfile.rename('/path/to/file1', '/path/to/file1_moved')

  assert fs.files == {
      '/path/to/file2': 'Content of file 2',
      '/path/to/file1_moved': 'Content of file 1',
  }
""" .

"DESCRIPTION.The code creates a mock filesystem and checks if the filesystem class is \"DummyTestFS\". It then asserts that the type of the filesystem is also \"DummyTestFS\" and prints out the files and directories in the filesystem." <EXPLAINS> """CODE.fs = mock_fs(["data/train.txt", "data.test.txt"])
assert fsspec.get_filesystem_class("mock").__name__ == "DummyTestFS"
assert type(fs).__name__ == "DummyTestFS"
print(fs.glob("**"))
["data", "data/train.txt", "data.test.txt"]
""" .

"DESCRIPTION.The code creates a model using the Keras library with specified inputs and outputs, compiles the model using stochastic gradient descent optimizer, mean squared error loss function, and true negatives metric." <EXPLAINS> """CODE.model = keras.Model(inputs, outputs)
model.compile('sgd', loss='mse', metrics=[keras.metrics.TrueNegatives()])
""" .

"DESCRIPTION.The code creates a monitored session with a BeholderHook to visualize training progress in the specified log directory and runs the training operation within the session." <EXPLAINS> """CODE.beholder_hook = BeholderHook(LOG_DIRECTORY)
with MonitoredSession(..., hooks=[beholder_hook]) as sess:
  sess.run(train_op)
""" .

"DESCRIPTION.The code creates a mutable dense hash table with int64 keys and values, sets a default value of -1, and an empty key of 0. It inserts keys and values into the table and then looks up query keys to retrieve their corresponding values from the table, finally printing the values." <EXPLAINS> """CODE.table = tf.contrib.lookup.MutableDenseHashTable(key_dtype=tf.int64,
                                                value_dtype=tf.int64,
                                                default_value=-1,
                                                empty_key=0)
sess.run(table.insert(keys, values))
out = table.lookup(query_keys)
print(out.eval())
""" .

"DESCRIPTION.The code creates a named tuple called \"Point\" with fields \"x\" and \"y\", then checks if the given input is a named tuple instance and returns True if it is, otherwise returns False." <EXPLAINS> """CODE.Point = namedtuple("Point", ["x", "y"])
p = Point(1, 2)
is_named_tuple(p)
True
is_named_tuple((1, 2))
False""" .

"DESCRIPTION.The code creates a neural network model that takes two inputs with different shapes, applies a Dense layer with a relu activation function to each input, adds the outputs of the two Dense layers together, applies another Dense layer with 4 units to the added output, and finally constructs a model with the specified inputs and output." <EXPLAINS> """CODE.input_shape = (2, 3, 4)
x1 = tf.random.normal(input_shape)
x2 = tf.random.normal(input_shape)
y = tf.keras.layers.add([x1, x2])
print(y.shape)

input1 = tf.keras.layers.Input(shape=(16,))
x1 = tf.keras.layers.Dense(8, activation='relu')(input1)
input2 = tf.keras.layers.Input(shape=(32,))
x2 = tf.keras.layers.Dense(8, activation='relu')(input2)
added = tf.keras.layers.add([x1, x2])
out = tf.keras.layers.Dense(4)(added)
model = tf.keras.models.Model(inputs=[input1, input2], outputs=out)""" .

"DESCRIPTION.The code creates a neural network model using Convolutional Neural Network (CNN) for image processing. The input shape of the images is specified as 3 channels with dimensions of 32x32. The model adds a convolutional layer with 64 filters of size 3x3, with padding to maintain the spatial dimensions as 32x32. After the convolutional layer, a Flatten layer is added to reshape the output into a single long vector with 65536 elements." <EXPLAINS> """CODE.model = Sequential()
model.add(Convolution2D(64, 3, 3,
                        border_mode='same',
                        input_shape=(3, 32, 32)))
# now: model.output_shape == (None, 64, 32, 32)

model.add(Flatten())
# now: model.output_shape == (None, 65536)
""" .

"DESCRIPTION.The code creates a neural network model using Keras Sequential API. The model includes cropping layers to remove parts of the input image. It then adds a 2D convolutional layer with 64 filters and specified padding. Another cropping layer is added to further crop the output of the convolutional layer." <EXPLAINS> """CODE.model = Sequential()
model.add(Cropping2D(cropping=((2, 2), (4, 4)),
                     input_shape=(28, 28, 3)))
model.add(Conv2D(64, (3, 3), padding='same))
model.add(Cropping2D(cropping=((2, 2), (2, 2)))
""" .

"DESCRIPTION.The code creates a neural network model using Keras and compiles it with stochastic gradient descent optimizer, mean squared error loss function, and false negatives as a metric." <EXPLAINS> """CODE.model = keras.Model(inputs, outputs)
model.compile('sgd', loss='mse', metrics=[keras.metrics.FalseNegatives()])
""" .

"DESCRIPTION.The code creates a neural network model using Keras framework, compiles it using stochastic gradient descent optimizer, and sets the loss function as Categorical Hinge loss." <EXPLAINS> """CODE.model = keras.Model(inputs, outputs)
model.compile('sgd', loss=keras.losses.CategoricalHinge())
""" .

"DESCRIPTION.The code creates a neural network model using Keras that takes inputs and generates outputs. It compiles the model using stochastic gradient descent optimizer with Kullback-Leibler Divergence as the loss function." <EXPLAINS> """CODE.model = keras.Model(inputs, outputs)
model.compile('sgd', loss=keras.losses.KLDivergence())
""" .

"DESCRIPTION.The code creates a neural network model using Keras with specified input and output layers, compiles the model using stochastic gradient descent optimizer, mean squared error loss function, and false positives metric." <EXPLAINS> """CODE.model = keras.Model(inputs, outputs)
model.compile('sgd', loss='mse', metrics=[keras.metrics.FalsePositives()])
""" .

"DESCRIPTION.The code creates a neural network model using Keras with specified inputs and outputs, and compiles the model using Stochastic Gradient Descent optimizer and Squared Hinge loss function." <EXPLAINS> """CODE.model = keras.Model(inputs, outputs)
model.compile('sgd', loss=keras.losses.SquaredHinge())
""" .

"DESCRIPTION.The code creates a neural network model using Keras with specified inputs and outputs, then compiles the model using stochastic gradient descent optimizer and Poisson loss function." <EXPLAINS> """CODE.model = keras.Model(inputs, outputs)
model.compile('sgd', loss=keras.losses.Poisson())
""" .

"DESCRIPTION.The code creates a neural network model using Keras with specified inputs and outputs, then compiles the model using stochastic gradient descent optimizer, mean squared error loss function, and false positives as the metric." <EXPLAINS> """CODE.model = keras.Model(inputs, outputs)
model.compile('sgd', loss='mse', metrics=[keras.metrics.FalsePositives()])
""" .

"DESCRIPTION.The code creates a neural network model using Keras, compiles it using stochastic gradient descent optimizer, mean squared error loss function, and categorical accuracy as the evaluation metric." <EXPLAINS> """CODE.model = keras.Model(inputs, outputs)
model.compile(
    'sgd',
    loss='mse',
    metrics=[keras.metrics.CategoricalAccuracy()])
""" .

"DESCRIPTION.The code creates a neural network model using Keras, compiles it with stochastic gradient descent optimization and hinge loss function." <EXPLAINS> """CODE.model = keras.Model(inputs, outputs)
model.compile('sgd', loss=keras.losses.Hinge())
""" .

"DESCRIPTION.The code creates a neural network model using Keras, compiles it with stochastic gradient descent optimizer and Kullback-Leibler Divergence loss function." <EXPLAINS> """CODE.model = keras.Model(inputs, outputs)
model.compile('sgd', loss=keras.losses.KLDivergence())
""" .

"DESCRIPTION.The code creates a neural network model using Keras, compiles it with stochastic gradient descent optimizer and hinge loss function." <EXPLAINS> """CODE.model = keras.Model(inputs, outputs)
model.compile('sgd', loss=keras.losses.Hinge())
""" .

"DESCRIPTION.The code creates a neural network model using Keras, compiles it with stochastic gradient descent optimizer and log cosh loss function." <EXPLAINS> """CODE.model = keras.Model(inputs, outputs)
model.compile('sgd', loss=keras.losses.LogCosh())
""" .

"DESCRIPTION.The code creates a neural network model using Keras, compiles the model using stochastic gradient descent optimizer and sparse top-k categorical accuracy as a metric." <EXPLAINS> """CODE.model = keras.Model(inputs, outputs)
model.compile(
    'sgd',
    metrics=[keras.metrics.SparseTopKCategoricalAccuracy()])
""" .

"DESCRIPTION.The code creates a neural network model using TensorFlow Keras that consists of an input layer, a dense layer with 2 units, and output layer(s). It compiles the model with Adam optimizer, mean squared error loss, and specified metrics. It then fits the model with random input and target data, and evaluates the model performance metrics including loss, mean absolute error (mae), and accuracy (acc) for multiple outputs." <EXPLAINS> """CODE.inputs = tf.keras.layers.Input(shape=(3,))
outputs = tf.keras.layers.Dense(2)(inputs)
model = tf.keras.models.Model(inputs=inputs, outputs=outputs)
model.compile(optimizer="Adam", loss="mse", metrics=["mae"])
model.metrics_names
[]
x = np.random.random((2, 3))
y = np.random.randint(0, 2, (2, 2))
model.fit(x, y)
model.metrics_names
['loss', 'mae']
inputs = tf.keras.layers.Input(shape=(3,))
d = tf.keras.layers.Dense(2, name='out')
output_1 = d(inputs)
output_2 = d(inputs)
model = tf.keras.models.Model(inputs=inputs, outputs=[output_1, output_2])
model.compile(optimizer="Adam", loss="mse", metrics=["mae", "acc"])
model.fit(x, (y, y))
model.metrics_names
['loss', 'out_loss', 'out_1_loss', 'out_mae', 'out_acc', 'out_1_mae', 'out_1_acc']""" .

"DESCRIPTION.The code creates a neural network model using TensorFlow Keras with multiple Reshape layers to modify the shape of input data." <EXPLAINS> """CODE.model = tf.keras.Sequential()
model.add(tf.keras.layers.Reshape((3, 4), input_shape=(12,)))
model.add(tf.keras.layers.Reshape((6, 2)))
model.add(tf.keras.layers.Reshape((-1, 2, 2)))""" .

"DESCRIPTION.The code creates a neural network model using TensorFlow and Keras to perform a regression task. It defines an input layer with shape (3,), adds a dense layer with 2 units, compiles the model with Adam optimizer and mean squared error loss, and uses mean absolute error as a metric. It then trains the model on random data, evaluates the metrics, and checks that the metrics are reset to 0 after calling model.reset_metrics()." <EXPLAINS> """CODE.inputs = tf.keras.layers.Input(shape=(3,))
outputs = tf.keras.layers.Dense(2)(inputs)
model = tf.keras.models.Model(inputs=inputs, outputs=outputs)
model.compile(optimizer="Adam", loss="mse", metrics=["mae"])

x = np.random.random((2, 3))
y = np.random.randint(0, 2, (2, 2))
_ = model.fit(x, y, verbose=0)
assert all(float(m.result()) for m in model.metrics)

model.reset_metrics()
assert all(float(m.result()) == 0 for m in model.metrics)""" .

"DESCRIPTION.The code creates a neural network model using TensorFlow for making predictions based on input features. It defines numeric and embedding feature columns, constructs a feature layer, parses input examples, creates dense layers with relu activation functions, and outputs a final prediction using a dense layer with one unit." <EXPLAINS> """CODE.price = tf.feature_column.numeric_column('price')
keywords_embedded = tf.feature_column.embedding_column(
    tf.feature_column.categorical_column_with_hash_bucket("keywords", 10K),
    dimension=16)
columns = [price, keywords_embedded, ...]
partitioner = tf.compat.v1.fixed_size_partitioner(num_shards=4)
feature_layer = tf.compat.v1.keras.layers.DenseFeatures(
    feature_columns=columns, partitioner=partitioner)

features = tf.io.parse_example(
    ..., features=tf.feature_column.make_parse_example_spec(columns))
dense_tensor = feature_layer(features)
for units in [128, 64, 32]:
  dense_tensor = tf.compat.v1.keras.layers.Dense(
                     units, activation='relu')(dense_tensor)
prediction = tf.compat.v1.keras.layers.Dense(1)(dense_tensor)
""" .

"DESCRIPTION.The code creates a neural network model using input and output layers, compiles it with stochastic gradient descent optimizer, mean squared error loss function, and precision metric for evaluation." <EXPLAINS> """CODE.model = keras.Model(inputs, outputs)
model.compile('sgd', loss='mse', metrics=[keras.metrics.Precision()])
""" .

"DESCRIPTION.The code creates a neural network model using inputs and outputs, compiles it using stochastic gradient descent (SGD) optimizer and log cosh error as the metric." <EXPLAINS> """CODE.model = keras.Model(inputs, outputs)
model.compile('sgd', metrics=[keras.metrics.LogCoshError()])
""" .

"DESCRIPTION.The code creates a neural network model using the Keras framework, with specified inputs and outputs, and compiles the model using stochastic gradient descent optimizer and root mean squared error as the evaluation metric." <EXPLAINS> """CODE.model = keras.Model(inputs, outputs)
model.compile('sgd', metrics=[keras.metrics.RootMeanSquaredError()])
""" .

"DESCRIPTION.The code creates a neural network model using the Keras framework, with stochastic gradient descent optimizer and categorical hinge loss function." <EXPLAINS> """CODE.model = keras.Model(inputs, outputs)
model.compile('sgd', loss=keras.losses.CategoricalHinge())""" .

"DESCRIPTION.The code creates a neural network model using the Keras library, compiles it with stochastic gradient descent optimizer and categorical hinge loss function." <EXPLAINS> """CODE.model = keras.Model(inputs, outputs)
model.compile('sgd', loss=keras.losses.CategoricalHinge())""" .

"DESCRIPTION.The code creates a neural network model using the Keras library, compiles the model using stochastic gradient descent optimizer with mean squared error loss function and true negatives as the metric." <EXPLAINS> """CODE.model = keras.Model(inputs, outputs)
model.compile('sgd', loss='mse', metrics=[keras.metrics.TrueNegatives()])
""" .

"DESCRIPTION.The code creates a neural network model using the Keras library. The model is compiled with the stochastic gradient descent optimizer ('sgd') and SparseTopKCategoricalAccuracy as the evaluation metric." <EXPLAINS> """CODE.model = keras.Model(inputs, outputs)
model.compile(
    'sgd',
    metrics=[keras.metrics.SparseTopKCategoricalAccuracy()])
""" .

"DESCRIPTION.The code creates a neural network model using the given inputs and outputs, compiles the model with stochastic gradient descent optimizer and TopK categorical accuracy metrics." <EXPLAINS> """CODE.model = keras.Model(inputs, outputs)
model.compile('sgd', metrics=[keras.metrics.TopKCategoricalAccuracy()])
""" .

"DESCRIPTION.The code creates a neural network model using two layers of locally connected 1D convolutional layers. The first layer has 64 filters, a kernel size of 3, and takes input of shape (10, 32). The second layer has 32 filters and a kernel size of 3." <EXPLAINS> """CODE.model = Sequential()
model.add(LocallyConnected1D(64, 3, input_shape=(10, 32)))
model.add(LocallyConnected1D(32, 3))
""" .

"DESCRIPTION.The code creates a neural network model with a Masking layer to ignore padded zeros in sequences, followed by an LSTM layer with 32 units for sequence prediction or classification tasks." <EXPLAINS> """CODE.    model = Sequential()
    model.add(Masking(mask_value=0., input_shape=(timesteps, features)))
    model.add(LSTM(32))
""" .

"DESCRIPTION.The code creates a neural network model with a linear layer of input size 50 and output size 50, followed by reshaping the output into a 2x5x5 tensor. This process is repeated multiple times with different configurations for reshaping the output tensor." <EXPLAINS> """CODE.m = nn.Sequential(
    nn.Linear(50, 50),
    nn.Unflatten(1, (2, 5, 5))
)
output = m(output)
output.size()

m = nn.Sequential(
    nn.Linear(50, 50),
    nn.Unflatten(1, torch.Size([2, 5, 5]))
)
output = m(output)
output.size()

m = nn.Sequential(
    nn.Linear(50, 50),
    nn.Unflatten('features', (('C', 2), ('H', 50), ('W',50)))
)
output = m(output)
output.size()
""" .

"DESCRIPTION.The code creates a neural network model with an Atrous Convolutional layer that takes an input of shape (3, 256, 256) and applies a 3x3 filter with an atrous rate of (2, 2) and 64 filters." <EXPLAINS> """CODE.model = Sequential()
model.add(AtrousConvolution2D(64, 3, 3, atrous_rate=(2,2),
                              border_mode='valid',
                              input_shape=(3, 256, 256)))
""" .

"DESCRIPTION.The code creates a neural network model with multiple layers of Dense units. It sets up the model architecture with different configurations such as input shape, batch input shape, and input dimension. The model is compiled with an optimizer and loss function, then trained on input data x and corresponding labels y for a specified number of epochs and batch size. It also displays the weights of the model, returning an empty list or a list of length 4 depending on the model setup." <EXPLAINS> """CODE.model = Sequential()
model.add(Dense(32, input_shape=(500,)))
model.add(Dense(32))
model = Sequential()
model.add(Dense(32, input_dim=500))
model = Sequential()
model.add(Dense(32, batch_input_shape=(None, 500)))
model = Sequential()
model.add(Dense(32))
model.add(Dense(32))
model.compile(optimizer=optimizer, loss=loss)
model.fit(x, y, batch_size=32, epochs=10)
model = Sequential()
model.add(Dense(32))
model.add(Dense(32))
model.weights  # returns []
model = Sequential()
model.add(Dense(32, input_shape=(500,)))
model.add(Dense(32))
model.weights  # returns list of length 4
model = Sequential()
model.add(Dense(32))
model.add(Dense(32))
model.build((None, 500))
model.weights  # returns list of length 4
""" .

"DESCRIPTION.The code creates a neural network model with one input layer of 3 neurons and one output layer of 5 neurons with a softmax activation function. It then converts the model configuration to JSON format and loads the model configuration to create a new model." <EXPLAINS> """CODE.model = tf.keras.Sequential([
    tf.keras.layers.Dense(5, input_shape=(3,)),
    tf.keras.layers.Softmax()])
config = model.to_json()
loaded_model = tf.keras.models.model_from_json(config)""" .

"DESCRIPTION.The code creates a neural network model with two convolutional layers. The first layer has 64 filters of size 3x3 with a padding of 'same' and takes input shape of (3, 256, 256). The second convolutional layer has 32 filters of size 3x3 with padding 'same'." <EXPLAINS> """CODE.model = Sequential()
model.add(Convolution2D(64, 3, 3,
                        border_mode='same',
                        input_shape=(3, 256, 256)))
model.add(Convolution2D(32, 3, 3, border_mode='same'))
""" .

"DESCRIPTION.The code creates a neural network model with two hidden layers, each containing 32 units, and an input layer with 500 input dimensions." <EXPLAINS> """CODE.model = Sequential()
model.add(Dense(32, input_dim=500))
model.add(Dense(32))

model = Sequential()
model.add(Dense(32, input_shape=(500,)))
model.add(Dense(32))

model = Sequential()
model.add(Dense(32, batch_input_shape=(None, 500)))
model.add(Dense(32))
""" .

"DESCRIPTION.The code creates a new distributed communication group with the specified ranks [2, 4, 6], and then retrieves this new group using the group ID gid.id." <EXPLAINS> """CODE.paddle.distributed.new_group([2,4,6])
paddle.distributed.get_group(gid.id)""" .

"DESCRIPTION.The code creates a new instance of a Cache object from the st module. If the Cache object is successfully created, data is fetched from a URL, processed, and assigned to the data attribute of the Cache object." <EXPLAINS> """CODE.c = st.Cache()
if c:
    # Fetch data from URL here, and then clean it up. Finally assign to c.
    c.data = ...

if c := st.Cache():
    # Fetch data from URL here, and then clean it up. Finally assign to c.
    c.data = ...""" .

"DESCRIPTION.The code creates a new instance of a structure type object \"s\" with attributes \"a\" and \"b\" set to 10 and 30 respectively. It then creates a copy of \"s\" called \"s2\". The code checks if the type of \"s2\" is the same as the structure type \"Struct\"." <EXPLAINS> """CODE.s = Struct(a=10,b=30)
s2 = s.copy()
type(s2) is Struct""" .

"DESCRIPTION.The code creates a new minimized instance of the Notepad application using the Windows Management Instrumentation (WMI)." <EXPLAINS> """CODE.import win32con
import wmi
c = wmi.WMI ()
startup = c.Win32_ProcessStartup.new (ShowWindow=win32con.SW_SHOWMINIMIZED)
pid, retval = c.Win32_Process.Create (
  CommandLine="notepad.exe",
  ProcessStartupInformation=startup
)""" .

"DESCRIPTION.The code creates a new model by making a copy of an existing model with freshly initialized weights." <EXPLAINS> """CODE.# Create a test Sequential model.
model = keras.Sequential([
    keras.Input(shape=(728,)),
    keras.layers.Dense(32, activation='relu'),
    keras.layers.Dense(1, activation='sigmoid'),
])
# Create a copy of the test model (with freshly initialized weights).
new_model = clone_model(model)

new_model = model.__class__.from_config(model.get_config())
""" .

"DESCRIPTION.The code creates a new process that launches the Notepad application in a minimized window on a Windows system." <EXPLAINS> """CODE.import win32con
import wmi
c = wmi.WMI ()
startup = c.Win32_ProcessStartup.new (ShowWindow=win32con.SW_SHOWMINIMIZED)
pid, retval = c.Win32_Process.Create (
  CommandLine="notepad.exe",
  ProcessStartupInformation=startup
)""" .

"DESCRIPTION.The code creates a new thread that sets the result of a torch.future object after a delay of 0.5 seconds. It then waits for the result to be set and prints the result, which should be a tensor containing two elements, both equal to 3." <EXPLAINS> """CODE.import threading
import time
import torch

def slow_set_future(fut, value):
    time.sleep(0.5)
    fut.set_result(value)

fut = torch.futures.Future()
t = threading.Thread(
    target=slow_set_future,
    args=(fut, torch.ones(2) * 3)
)
t.start()

print(fut.wait())  # tensor([3., 3.])
t.join()""" .

"DESCRIPTION.The code creates a pandas Categorical object \"s\" with values ['lama', 'cow', 'lama', 'beetle', 'lama', 'hippo']. It then checks if the values 'cow' and 'lama' are present in \"s\" and returns a boolean array indicating whether each value is in the Categorical object." <EXPLAINS> """CODE.s = pd.Categorical(['lama', 'cow', 'lama', 'beetle', 'lama', 'hippo'])
s.isin(['cow', 'lama'])
s.isin(['lama'])""" .

"DESCRIPTION.The code creates a pandas Categorical object from a list of characters and then calls the _reverse_indexer() method on it." <EXPLAINS> """CODE.c = pd.Categorical(list('aabca'))
c._reverse_indexer()""" .

"DESCRIPTION.The code creates a pandas DataFrame with 20 rows and 3 columns filled with random numbers. It then generates a line chart visualizing the data in the DataFrame." <EXPLAINS> """CODE.chart_data = pd.DataFrame(
...     np.random.randn(20, 3),
...     columns=['a', 'b', 'c'])
...
st.line_chart(chart_data)""" .

"DESCRIPTION.The code creates a pandas DataFrame with a single column containing relatively long strings of data. It then uses StataWriter117 to write the data into a .dta file, which is a Stata data file format." <EXPLAINS> """CODE.import pandas as pd
from pandas.io.stata import StataWriter117
data = pd.DataFrame([[1.0, 1, 'a']], columns=['a', 'b', 'c'])
writer = StataWriter117('./data_file.dta', data)
writer.write_file()

data = pd.DataFrame([['A relatively long string'], [''], ['']],
                    columns=['strls'])
writer = StataWriter117('./data_file_with_long_strings.dta', data,
                        convert_strl=['strls'])
writer.write_file()
""" .

"DESCRIPTION.The code creates a pandas DataFrame with columns for year, number of legs, and animal names. It then converts the DataFrame into a PyArrow Table and writes it to a Parquet file 'table_V2.parquet'. Later, it reads the Parquet file into a PyArrow Dataset, retrieves the 'n_legs' column, and accesses the metadata of the dataset." <EXPLAINS> """CODE.import pyarrow as pa
import pandas as pd
df = pd.DataFrame({'year': [2020, 2022, 2021, 2022, 2019, 2021],
                   'n_legs': [2, 2, 4, 4, 5, 100],
                   'animal': ["Flamingo", "Parrot", "Dog", "Horse",
                              "Brittle stars", "Centipede"]})
table = pa.Table.from_pandas(df)
import pyarrow.parquet as pq
pq.write_table(table, 'table_V2.parquet')
dataset = pq.ParquetDataset('table_V2.parquet',
                            use_legacy_dataset=False)
dataset.read_pandas(columns=["n_legs"])
dataset.read_pandas(columns=["n_legs"]).schema.pandas_metadata""" .

"DESCRIPTION.The code creates a pandas DataFrame with integer and string columns, then converts the DataFrame into a PyArrow Table." <EXPLAINS> """CODE.import pandas as pd
import pyarrow as pa
df = pd.DataFrame({
    'int': [1, 2],
    'str': ['a', 'b']
})
pa.Table.from_pandas(df)
""" .

"DESCRIPTION.The code creates a pandas DataFrame with random integer values, groups the DataFrame by the values in the first column, and squares each group's values in a progress-tracking manner." <EXPLAINS> """CODE.df = pd.DataFrame(np.random.randint(0, 100, (100000, 6)))
tqdm_pandas(tqdm())
df.groupby(0).progress_apply(lambda x: x**2)""" .

"DESCRIPTION.The code creates a pandas DataFrame with two columns named 'col1' and 'col2', and then writes this DataFrame to a file named 'df.orc' in ORC format. Finally, it reads the data from the 'df.orc' file back into a pandas DataFrame." <EXPLAINS> """CODE.df = pd.DataFrame(data={'col1': [1, 2], 'col2': [4, 3]})
df.to_orc('df.orc')
pd.read_orc('df.orc')""" .

"DESCRIPTION.The code creates a pandas DataFrame with two columns named 'first column' and 'second column' containing integer values. It then displays the DataFrame using a function called 'show'." <EXPLAINS> """CODE.dataframe = pd.DataFrame({
...     'first column': [1, 2, 3, 4],
...     'second column': [10, 20, 30, 40],
... }))
st.experimental_show(dataframe)""" .

"DESCRIPTION.The code creates a pandas DataFrame with two columns named 'first column' and 'second column' containing specified values, and then displays the DataFrame using a function called 'show'." <EXPLAINS> """CODE.dataframe = pd.DataFrame({
...     'first column': [1, 2, 3, 4],
...     'second column': [10, 20, 30, 40],
... }))
st.show(dataframe)""" .

"DESCRIPTION.The code creates a pandas Index object with the values 10, 20, 30, 40, and 50, and then calculates the differences between consecutive values in the Index." <EXPLAINS> """CODE.import pandas as pd
idx = pd.Index([10, 20, 30, 40, 50])
idx.diff()""" .

"DESCRIPTION.The code creates a pandas Series object with values [1, 2, 3, 4] and corresponding datetime index. It then resamples the data to a monthly frequency (starting of the month) and returns the maximum value for each month." <EXPLAINS> """CODE.ser = pd.Series([1, 2, 3, 4], index=pd.DatetimeIndex(
                ['2023-01-01', '2023-01-15', '2023-02-01', '2023-02-15']))
ser.resample('MS').max()""" .

"DESCRIPTION.The code creates a panel using the Panel library and serves it using the .servable() method." <EXPLAINS> """CODE.import panel as pn

pn.panel("Hello **Panel â¡** World").servable()


import lightning as L
from lightning.app.frontend.panel import PanelFrontend


class LitPanel(L.LightningFlow):
    def configure_layout(self):
        return PanelFrontend("panel_app_basic.py")


class LitApp(L.LightningFlow):
    def __init__(self):
        super().__init__()
        self.lit_panel = LitPanel()

    def configure_layout(self):
        return {"name": "home", "content": self.lit_panel}


app = L.LightningApp(LitApp())
""" .

"DESCRIPTION.The code creates a parallel environment using PaddlePaddle's dygraph module and prints out the trainer endpoints in the environment." <EXPLAINS> """CODE.import paddle.fluid as fluid
env = fluid.dygraph.ParallelEnv()
print("The trainer endpoints are %s" % env.trainer_endpoints)""" .

"DESCRIPTION.The code creates a parser object without adding help functionality, then adds arguments for a Trainer object to the parser, parses the arguments provided by the user, and creates a Trainer object based on the parsed arguments." <EXPLAINS> """CODE.parser = ArgumentParser(add_help=False)
parser = Trainer.add_argparse_args(parser)
args = Trainer.parse_argparser(parser.parse_args(""))
trainer = Trainer.from_argparse_args(args)""" .

"DESCRIPTION.The code creates a per-worker variable with an initial value of 0.0 within the scope of a distributed training strategy." <EXPLAINS> """CODE.with strategy.scope():
  var = tf.Variable(initial_value=0.0, per_worker_variable=True)

All per-worker values can be retrieved and read into a list via
PerWorkerVariable.read_all()
""" .

"DESCRIPTION.The code creates a persistent vector 'v1' with elements 1, 2, 3, 4, and 5. It then creates an evolver 'e' from 'v1', updates the second element of 'e' to 22, appends 6 to 'e', extends 'e' with [7, 8, 9], increments the ninth element of 'e' by 1, and finally creates a new persistent vector 'v2' from 'e' with updated elements." <EXPLAINS> """CODE.v1 = v(1, 2, 3, 4, 5)
e = v1.evolver()
e[1] = 22
e.append(6)
e.extend([7, 8, 9])
e[8] += 1
len(e)
9
v2 = e.pvector()
v2
pvector([1, 22, 3, 4, 5, 6, 7, 8, 10])""" .

"DESCRIPTION.The code creates a pivot table from a DataFrame 'df' with columns 'A', 'B', 'C', and 'D'. The pivot table summarizes the values in column 'D' by grouping the data based on columns 'A' and 'B' as rows, and column 'C' as columns, and then aggregating the values using the sum function." <EXPLAINS> """CODE.table = pivot_table(df, values='D', rows=['A', 'B'],
                     cols=['C'], aggfunc=np.sum)""" .

"DESCRIPTION.The code creates a placeholder array with shape (2, 4, 5) using TensorFlow backend, and then creates a variable array with values [[1, 2], [3, 4]]. It then calculates and returns the number of dimensions of the placeholder array and the variable array, which are 3 and 2 respectively." <EXPLAINS> """CODE.input = tf.keras.backend.placeholder(shape=(2, 4, 5))
val = np.array([[1, 2], [3, 4]])
kvar = tf.keras.backend.variable(value=val)
tf.keras.backend.ndim(input)
3
tf.keras.backend.ndim(kvar)
2""" .

"DESCRIPTION.The code creates a placeholder tensor with a shape of (2, 4, 5) and a variable tensor initialized with a numpy array. It then returns the number of dimensions of both tensors." <EXPLAINS> """CODE.    from keras import backend as K
    input = K.placeholder(shape=(2, 4, 5))
    val = np.array([[1, 2], [3, 4]])
    kvar = K.variable(value=val)
    K.ndim(input)
    K.ndim(kvar)
""" .

"DESCRIPTION.The code creates a placeholder tensor with a shape of (2, 4, 5) using the Keras backend." <EXPLAINS> """CODE.from keras import backend as K
input_ph = K.placeholder(shape=(2, 4, 5))
input_ph._keras_shape
input_ph
""" .

"DESCRIPTION.The code creates a placeholder tensor with shape (2, 4, 5) using Keras backend, retrieves and prints the shape of the placeholder tensor, creates a variable tensor initialized with a numpy array [[1, 2], [3, 4]], and retrieves and prints the shape of the variable tensor." <EXPLAINS> """CODE.from keras import backend as K
inputs = K.placeholder(shape=(2, 4, 5))
K.int_shape(inputs)
val = np.array([[1, 2], [3, 4]])
kvar = K.variable(value=val)
K.int_shape(kvar)
""" .

"DESCRIPTION.The code creates a program using PaddlePaddle's static mode with an IPU (In-Processing Unit) strategy specified, which includes settings for graph configuration, pipelining, gradient accumulation, and precision. It then compiles the program for execution on an IPU." <EXPLAINS> """CODE.import paddle
import paddle.static as static

paddle.enable_static()

a = static.data(name='data', shape=[None, 1], dtype='int32')
b = a + 1
main_prog = static.default_main_program()

ipu_strategy = static.IpuStrategy()
ipu_strategy.set_graph_config(num_ipus=1, is_training=True, micro_batch_size=1)
ipu_strategy.set_pipelining_config(enable_pipelining=False, batches_per_step=1, enable_gradient_accumulation=False, accumulation_factor=1)
ipu_strategy.set_precision_config(enable_fp16=False)

program = static.IpuCompiledProgram(
    main_prog,
    ipu_strategy=ipu_strategy).compile([a.name], [b.name])""" .

"DESCRIPTION.The code creates a proxy manager object with the proxy server located at 'http://localhost:3128/'. It then sends multiple GET requests using the proxy manager to different websites like 'http://google.com/', 'http://httpbin.org/', 'https://httpbin.org/', and 'https://twitter.com/'. The code also checks the number of proxy pools after each request." <EXPLAINS> """CODE.proxy = urllib3.ProxyManager('http://localhost:3128/')
r1 = proxy.request('GET', 'http://google.com/')
r2 = proxy.request('GET', 'http://httpbin.org/')
len(proxy.pools)
r3 = proxy.request('GET', 'https://httpbin.org/')
r4 = proxy.request('GET', 'https://twitter.com/')
len(proxy.pools)
""" .

"DESCRIPTION.The code creates a proxy manager using urllib3 for making HTTP requests through a proxy server. It sends GET requests to 'http://google.com/', 'http://httpbin.org/', 'https://httpbin.org/', and 'https://twitter.com/'. It also checks the number of connection pools used by the proxy manager." <EXPLAINS> """CODE.proxy = urllib3.ProxyManager('http://localhost:3128/')
r1 = proxy.request('GET', 'http://google.com/')
r2 = proxy.request('GET', 'http://httpbin.org/')
len(proxy.pools)
r3 = proxy.request('GET', 'https://httpbin.org/')
r4 = proxy.request('GET', 'https://twitter.com/')
len(proxy.pools)
""" .

"DESCRIPTION.The code creates a quantized addition operation between two quantized tensors with values 3.0 and 4.0." <EXPLAINS> """CODE.q_add = QFunctional('add')
a = torch.quantize_per_tensor(torch.tensor(3.0), 1.0, 0, torch.qint32)
b = torch.quantize_per_tensor(torch.tensor(4.0), 1.0, 0, torch.qint32)
q_add.add(a, b)  # Equivalent to ``torch.ops.quantized.add(3, 4)""" .

"DESCRIPTION.The code creates a queue with a maximum size of 100, then enqueues data in parallel and dequeues data concurrently in an asynchronous mode." <EXPLAINS> """CODE.queue = queue.Queue(100)
write_op = ParallelRollouts(...).for_each(Enqueue(queue))
read_op = Dequeue(queue)
combined_op = Concurrently([write_op, read_op], mode="async")
next(combined_op)""" .

"DESCRIPTION.The code creates a range of numbers from 0 to 3, multiplies each number by 2, and then gathers the results synchronously, returning a list of the multiplied numbers." <EXPLAINS> """CODE.next(from_range(4).for_each(lambda x: x * 2).gather_sync())
[0, 2, 4, 8]""" .

"DESCRIPTION.The code creates a reinforcement learning environment for the CartPole problem, initializes a replay buffer with a capacity of 10, and creates an agent that interacts with the environment and stores experience in the replay buffer." <EXPLAINS> """CODE.env = gym.make("CartPole-v1")
buffer = ReplayBuffer(10)
Agent(env, buffer)  # doctest: +ELLIPSIS""" .

"DESCRIPTION.The code creates a reinforcement learning environment for the CartPole-v1 game, initializes a replay buffer with a capacity of 10, and then creates an agent that interacts with the environment and stores experiences in the replay buffer." <EXPLAINS> """CODE.env = gym.make("CartPole-v1")
buffer = ReplayBuffer(10)
Agent(env, buffer)  # doctest: +ELLIPSIS""" .

"DESCRIPTION.The code creates a remote reference pointing to the master branch of the origin remote." <EXPLAINS> "CODE.remote.refs.master = RemoteReference('/refs/remotes/origin/master')" .

"DESCRIPTION.The code creates a replay buffer with a capacity of 5 and initializes a reinforcement learning dataset using this replay buffer." <EXPLAINS> "CODE.RLDataset(ReplayBuffer(5))" .

"DESCRIPTION.The code creates a replay buffer with a capacity of 5 and then creates a reinforcement learning dataset using that replay buffer." <EXPLAINS> "CODE.RLDataset(ReplayBuffer(5))" .

"DESCRIPTION.The code creates a repository named \"test-squash\" using the Hugging Face Hub API, uploads multiple files to the repository, and then squashes the commit history of the repository." <EXPLAINS> """CODE.from huggingface_hub import HfApi
api = HfApi()

# Create repo
repo_id = api.create_repo("test-squash").repo_id

# Make a lot of commits.
api.upload_file(repo_id=repo_id, path_in_repo="file.txt", path_or_fileobj=b"content")
api.upload_file(repo_id=repo_id, path_in_repo="lfs.bin", path_or_fileobj=b"content")
api.upload_file(repo_id=repo_id, path_in_repo="file.txt", path_or_fileobj=b"another_content")

# Squash history
api.super_squash_history(repo_id=repo_id)
""" .

"DESCRIPTION.The code creates a resource manager, requests resources (4 CPUs), waits until resources are ready, acquires the resources, binds to a remote task or actor, runs a remote function using the acquired resources, and frees the resources after use." <EXPLAINS> """CODE.# Create resource manager
resource_manager = ResourceManager()

# Create resource request
resource_request = ResourceRequest([{"CPU": 4}])

# Pass to resource manager
resource_manager.request_resources(resource_request)

# Wait until ready
while not resource_manager.has_resources_ready(resource_request):
    time.sleep(1)

# Once ready, acquire resources
acquired_resource = resource_manager.acquire_resources(resource_request)

# Bind to remote task or actor
annotated_remote_fn = acquired_resource.annotate_remote_entities(
    [remote_fn])

# Run remote function. This will use the acquired resources
ray.get(annotated_remote_fn.remote())

# After using the resources, free
resource_manager.free_resources(annotated_resources)
""" .

"DESCRIPTION.The code creates a scatter plot using Altair library, where the x-axis is represented by column 'a', the y-axis is represented by column 'b', the size of the circles is determined by column 'c', the color of the circles is also determined by column 'c', and the tooltip displays the values of columns 'a', 'b', and 'c'." <EXPLAINS> """CODE.import pandas as pd
import numpy as np
import altair as alt

df = pd.DataFrame(
    np.random.randn(200, 3),
    columns=['a', 'b', 'c'])

c = alt.Chart(df).mark_circle().encode(
    x='a', y='b', size='c', color='c', tooltip=['a', 'b', 'c'])

st._arrow_altair_chart(c, use_container_width=True)""" .

"DESCRIPTION.The code creates a schema with a constraint of maximum 2 items and validates a list [2, 3, 4] against this schema." <EXPLAINS> """CODE.schema = {"maxItems" : 2}
Draft3Validator(schema).validate([2, 3, 4])""" .

"DESCRIPTION.The code creates a sequential model in Keras with a dense layer of 32 units and an input dimension of 32, followed by a RepeatVector layer which repeats the input 3 times." <EXPLAINS> """CODE.model = Sequential()
model.add(Dense(32, input_dim=32))
model.add(RepeatVector(3))""" .

"DESCRIPTION.The code creates a sequential model in Python for a 1D convolutional neural network with atrous convolutions. Two atrous convolutional layers are added to the model, each with a specified number of filters, filter size, atrous rate, and border mode. The input shape for the first layer is set to (10, 32)." <EXPLAINS> """CODE.model = Sequential()
model.add(AtrousConvolution1D(64, 3, atrous_rate=2, border_mode='same', input_shape=(10, 32)))
model.add(AtrousConvolution1D(32, 3, atrous_rate=2, border_mode='same'))
""" .

"DESCRIPTION.The code creates a sequential model in Python using Convolution1D layers. The model first adds a Convolution1D layer with 64 filters, kernel size of 3, and 'same' border mode, taking input of shape (10, 32). Then it adds another Convolution1D layer with 32 filters, kernel size of 3, and 'same' border mode." <EXPLAINS> """CODE.model = Sequential()
model.add(Convolution1D(64, 3, border_mode='same', input_shape=(10, 32)))
model.add(Convolution1D(32, 3, border_mode='same'))
""" .

"DESCRIPTION.The code creates a sequential model in Python using Keras library. It adds a dense layer with 32 units and input dimension of 32, followed by a RepeatVector layer which repeats the input 3 times." <EXPLAINS> """CODE.model = Sequential()
model.add(Dense(32, input_dim=32))
model.add(RepeatVector(3))
""" .

"DESCRIPTION.The code creates a server object that dispatches data using TensorFlow's experimental service capability and listens on port 5050." <EXPLAINS> """CODE.
dispatcher = tf.data.experimental.service.DispatchServer(port=5050)
dispatcher.join()
""" .

"DESCRIPTION.The code creates a set named s1 with elements 1 and 2, and then adds the elements 3 and 4 to the set (duplicates are ignored)." <EXPLAINS> """CODE.s1 = s(1, 2)
s1.update([3, 4, 4])""" .

"DESCRIPTION.The code creates a set of feature columns containing 'feature_b', 'feature_c_bucketized', and the interaction between 'feature_a' and 'feature_c'. It then parses serialized examples using the specified feature columns provided in a dictionary format." <EXPLAINS> """CODE.feature_columns = set(
    [feature_b, feature_c_bucketized, feature_a_x_feature_c])
features = tf.parse_example(
    serialized=serialized_examples,
    features=make_parse_example_spec(feature_columns))


{
  "feature_a": parsing_ops.VarLenFeature(tf.string),
  "feature_b": parsing_ops.FixedLenFeature([1], dtype=tf.float32),
  "feature_c": parsing_ops.FixedLenFeature([1], dtype=tf.float32)
}
""" .

"DESCRIPTION.The code creates a set with an initial value of 1, then adds 1 and 2 to the set." <EXPLAINS> """CODE.s = pbag([1])
s.update([1, 2])""" .

"DESCRIPTION.The code creates a simple TensorFlow Keras model with one dense layer and input shape of 10. It then saves the model in the SavedModel format and loads the saved model back for inspection." <EXPLAINS> """CODE.import tensorflow as tf

# Create a tf.keras model.
model = tf.keras.Sequential()
model.add(tf.keras.layers.Dense(1, input_shape=[10]))
model.summary()

# Save the tf.keras model in the SavedModel format.
path = '/tmp/simple_keras_model'
tf.keras.experimental.export_saved_model(model, path)

# Load the saved keras model back.
new_model = tf.keras.experimental.load_from_saved_model(path)
new_model.summary()
""",
        """CODE.import tensorflow as tf

# Create a tf.keras model.
model = tf.keras.Sequential()
model.add(tf.keras.layers.Dense(1, input_shape=[10]))
model.summary()

# Save the tf.keras model in the SavedModel format.
saved_to_path = tf.contrib.saved_model.save_keras_model(
      model, '/tmp/my_simple_tf_keras_saved_model')

# Load the saved keras model back.
model_prime = tf.contrib.saved_model.load_keras_model(saved_to_path)
model_prime.summary()
""" .

"DESCRIPTION.The code creates a simple Tensorflow Keras model with one dense layer of size 1 and input shape of [10]. It then saves the model in the SavedModel format at the specified path '/tmp/simple_keras_model'. Finally, it loads the saved Keras model back and prints a summary of the new model." <EXPLAINS> """CODE.import tensorflow as tf

# Create a tf.keras model.
model = tf.keras.Sequential()
model.add(tf.keras.layers.Dense(1, input_shape=[10]))
model.summary()

# Save the tf.keras model in the SavedModel format.
path = '/tmp/simple_keras_model'
tf.compat.v1.keras.experimental.export_saved_model(model, path)

# Load the saved keras model back.
new_model = tf.compat.v1.keras.experimental.load_from_saved_model(path)
new_model.summary()
""" .

"DESCRIPTION.The code creates a simple line chart using the Bokeh library in Python and displays it using the Streamlit framework." <EXPLAINS> """CODE.import streamlit as st
from bokeh.plotting import figure

x = [1, 2, 3, 4, 5]
y = [6, 7, 2, 4, 5]

p = figure(
    title='simple line example',
    x_axis_label='x',
    y_axis_label='y')

p.line(x, y, legend='Trend', line_width=2)

st.bokeh_chart(p)""" .

"DESCRIPTION.The code creates a simple line plot using the Bokeh library in Streamlit, displaying the relationship between the x and y values provided in the lists." <EXPLAINS> """CODE.import streamlit as st
from bokeh.plotting import figure

x = [1, 2, 3, 4, 5]
y = [6, 7, 2, 4, 5]

p = figure(
    title='simple line example',
    x_axis_label='x',
    y_axis_label='y')

p.line(x, y, legend='Trend', line_width=2)

st.bokeh_chart(p, use_container_width=True)""" .

"DESCRIPTION.The code creates a simple neural network model using tf.keras, saves the model in the SavedModel format, and then loads the saved model back for use." <EXPLAINS> """CODE.import tensorflow as tf

# Create a tf.keras model.
model = tf.keras.Sequential()
model.add(tf.keras.layers.Dense(1, input_shape=[10]))
model.summary()

# Save the tf.keras model in the SavedModel format.
path = '/tmp/simple_keras_model'
tf.keras.experimental.export_saved_model(model, path)

# Load the saved keras model back.
new_model = tf.keras.experimental.load_from_saved_model(path)
new_model.summary()
""" .

"DESCRIPTION.The code creates a simple neural network model with one input layer of 3 units, one hidden layer of 5 units, and a softmax output layer. It then saves the model to a file and loads it back. It generates 10 random samples of input data with 3 features and checks if the predictions made by the original and loaded models are close." <EXPLAINS> """CODE.model = tf.keras.Sequential([
    tf.keras.layers.Dense(5, input_shape=(3,)),
    tf.keras.layers.Softmax()])
model.save('/tmp/model')
loaded_model = tf.keras.models.load_model('/tmp/model')
x = tf.random.uniform((10, 3))
assert np.allclose(model.predict(x), loaded_model.predict(x))""" .

"DESCRIPTION.The code creates a simple tf.keras model with one Dense layer and an input shape of 10. It then saves the model in the SavedModel format at a specified path. Finally, it loads the saved model back and displays a summary of the new model." <EXPLAINS> """CODE.import tensorflow as tf

# Create a tf.keras model.
model = tf.keras.Sequential()
model.add(tf.keras.layers.Dense(1, input_shape=[10]))
model.summary()

# Save the tf.keras model in the SavedModel format.
path = '/tmp/simple_keras_model'
tf.keras.experimental.export_saved_model(model, path)

# Load the saved keras model back.
new_model = tf.keras.experimental.load_from_saved_model(path)
new_model.summary()
""" .

"DESCRIPTION.The code creates a simple tf.keras model with one Dense layer and input shape of 10, then saves the model in the SavedModel format at a specified path. Finally, it loads the saved model back and displays its summary." <EXPLAINS> """CODE.import tensorflow as tf

# Create a tf.keras model.
model = tf.keras.Sequential()
model.add(tf.keras.layers.Dense(1, input_shape=[10]))
model.summary()

# Save the tf.keras model in the SavedModel format.
path = '/tmp/simple_keras_model'
tf.compat.v1.keras.experimental.export_saved_model(model, path)

# Load the saved keras model back.
new_model = tf.compat.v1.keras.experimental.load_from_saved_model(path)
new_model.summary()
""" .

"DESCRIPTION.The code creates a sparse identity matrix of size 3 using scipy.sparse library and converts it into a sparse DataFrame using pandas library." <EXPLAINS> """CODE.import scipy.sparse
import pandas as pd

mat = scipy.sparse.eye(3)
pd.DataFrame.sparse.from_spmatrix(mat)""" .

"DESCRIPTION.The code creates a sparse matrix A with values and coordinates specified, and then converts this sparse matrix into a SparseSeries object ss." <EXPLAINS> """CODE.from scipy import sparse
A = sparse.coo_matrix(([3.0, 1.0, 2.0], ([1, 0, 0], [0, 2, 3])), shape=(3, 4))
ss = pd.SparseSeries.from_coo(A)""" .

"DESCRIPTION.The code creates a sparse matrix using the COOrdinate format from the scipy library and then converts it into a SparseSeries data structure." <EXPLAINS> """CODE.from scipy import sparse
A = sparse.coo_matrix(([3.0, 1.0, 2.0], ([1, 0, 0], [0, 2, 3])), shape=(3, 4))
ss = SparseSeries.from_coo(A)""" .

"DESCRIPTION.The code creates a sparse tensor with a shape of (2, 2) using TensorFlow's Keras backend. It then checks if the tensor b is sparse, and converts it to a dense tensor c using the to_dense function. Finally, it checks if the tensor c is sparse." <EXPLAINS> """CODE.b = tf.keras.backend.placeholder((2, 2), sparse=True)
print(tf.keras.backend.is_sparse(b))
c = tf.keras.backend.to_dense(b)
print(tf.keras.backend.is_sparse(c) )""" .

"DESCRIPTION.The code creates a static hash table using TensorFlow to map keys to values, where keys are integers in the range of 0 to 99 and values are strings representing double the key value. It then looks up the values for keys 0, 1, and 2 in the hash table and asserts that the output values are [\"0\", \"2\", \"4\"]." <EXPLAINS> """CODE.keys = tf.data.Dataset.range(100)
values = tf.data.Dataset.range(100).map(
    lambda x: string_ops.as_string(x * 2))
ds = tf.data.Dataset.zip((keys, values))
init = tf.lookup.experimental.DatasetInitializer(ds)
table = tf.lookup.StaticHashTable(init, "")
output = table.lookup([0, 1, 2])
assertEquals(outputs, ["0", "2", "4"])""" .

"DESCRIPTION.The code creates a synthetic dataset using the _create_synth_kitti_dataset function with specified image dimensions. It then initializes a SegModel object with the dataset path, which contains a UNet neural network with specific layers for segmentation tasks." <EXPLAINS> """CODE.dataset_path = os.path.join(".", "Kitti")
_create_synth_kitti_dataset(dataset_path, image_dims=(1024, 512))
SegModel(dataset_path)  # doctest: +ELLIPSIS +NORMALIZE_WHITESPACE
SegModel(
  (net): UNet(
    (layers): ModuleList(
      (0): DoubleConv(...)
      (1): Down(...)
      (2): Down(...)
      (3): Up(...)
      (4): Up(...)
      (5): Conv2d(64, 19, kernel_size=(1, 1), stride=(1, 1))
    )
  )
)""" .

"DESCRIPTION.The code creates a table containing data about animals, including the year, number of legs, and animal names. It then writes this table to a Parquet dataset with a specified root path, partitions it by the 'year' column, and specifies to not use the legacy dataset. Finally, it reads the schema of the Parquet dataset." <EXPLAINS> """CODE.import pyarrow as pa
table = pa.table({'year': [2020, 2022, 2021, 2022, 2019, 2021],
                  'n_legs': [2, 2, 4, 4, 5, 100],
                  'animal': ["Flamingo", "Parrot", "Dog", "Horse",
                             "Brittle stars", "Centipede"]})
import pyarrow.parquet as pq
pq.write_to_dataset(table, root_path='dataset_v2_schema',
                    partition_cols=['year'],
                    use_legacy_dataset=False)
dataset = pq._ParquetDatasetV2('dataset_v2_schema/')

dataset.schema""" .

"DESCRIPTION.The code creates a table for looking up the indexes of words in a vocabulary file, then uses the table to lookup the indexes of the words \"emerson\", \"lake\", \"and\", and \"palmer\" in the vocabulary file. Finally, it initializes the table." <EXPLAINS> """CODE.features = tf.constant(["emerson", "lake", "and", "palmer"])
table = tf.contrib.lookup.index_table_from_file(
    vocabulary_file="test.txt", num_oov_buckets=1)
ids = table.lookup(features)
...
tf.tables_initializer().run()
""" .

"DESCRIPTION.The code creates a tensor object 'kvar' with a specified value, and then obtains its shape using the TensorFlow backend. It also creates a placeholder 'input' tensor with a specified shape and obtains its shape using the TensorFlow backend." <EXPLAINS> """CODE.val = np.array([[1, 2], [3, 4]])
kvar = tf.keras.backend.variable(value=val)
tf.keras.backend.shape(kvar)
input = tf.keras.backend.placeholder(shape=(2, 4, 5))
tf.keras.backend.shape(input)""" .

"DESCRIPTION.The code creates a tracing context named \"acontext\" using the JAX profiler and performs a dot product operation on matrix x and its transpose, blocking until the operation is completed." <EXPLAINS> """CODE.with jax.profiler.TraceContext("acontext"):
    jnp.dot(x, x.T).block_until_ready()""" .

"DESCRIPTION.The code creates a trainable object using a ResNet18 model, CIFAR dataset, specified optimizer, and CrossEntropyLoss function, then runs a hyperparameter tuning process to find the best learning rate values of 0.01 and 0.1 using 2 GPUs." <EXPLAINS> """CODE.TorchTrainable = TorchTrainer.as_trainable(
    model_creator=ResNet18,
    data_creator=cifar_creator,
    optimizer_creator=optimizer_creator,
    loss_creator=nn.CrossEntropyLoss,
    num_gpus=2
)
analysis = tune.run(
    TorchTrainable,
    config={"lr": tune.grid_search([0.01, 0.1])}
)""" .

"DESCRIPTION.The code creates a tuple containing an empty list and assigns it to the variable 'a'. It then checks if 'a' is hashable, which returns True. However, a custom function 'is_hashable' is called on 'a' which returns False." <EXPLAINS> """CODE.a = ([],)
isinstance(a, collections.Hashable)
True
is_hashable(a)
False""" .

"DESCRIPTION.The code creates a urllib3 context with the SSLv3 option disabled." <EXPLAINS> """CODE.from urllib3.util import ssl_
context = ssl_.create_urllib3_context()
context.options &= ~ssl_.OP_NO_SSLv3
""" .

"DESCRIPTION.The code creates a variable \"var\" with a 2D array, then uses tf.keras.backend.gather to gather specific rows from the variable \"var\" and evaluate the results using tf.keras.backend.eval." <EXPLAINS> """CODE.var = tf.keras.backend.variable([[1, 2, 3], [4, 5, 6]])
tf.keras.backend.eval(var)
array([[1., 2., 3.],
       [4., 5., 6.]], dtype=float32)
var_gathered = tf.keras.backend.gather(var, [0])
tf.keras.backend.eval(var_gathered)
array([[1., 2., 3.]], dtype=float32)
var_gathered = tf.keras.backend.gather(var, [1])
tf.keras.backend.eval(var_gathered)
array([[4., 5., 6.]], dtype=float32)
var_gathered = tf.keras.backend.gather(var, [0,1,0])
tf.keras.backend.eval(var_gathered)
array([[1., 2., 3.],
       [4., 5., 6.],
       [1., 2., 3.]], dtype=float32)""" .

"DESCRIPTION.The code creates a variable 'kvar' using the Keras backend with a numpy array 'val' as its value and then computes the size of the variable 'inputs' which results in a TensorFlow tensor with a shape of (), dtype of int32, and a value of 4." <EXPLAINS> """CODE.from keras import backend as K
val = np.array([[1, 2], [3, 4]])
kvar = K.variable(value=val)
K.size(inputs)
<tf.Tensor: id=9, shape=(), dtype=int32, numpy=4>
""" .

"DESCRIPTION.The code creates a variable 'kvar' using the values from the numpy array 'val' in the Keras backend, and then calculates the size of the variable 'inputs' in the Keras backend." <EXPLAINS> """CODE.from keras import backend as K
val = np.array([[1, 2], [3, 4]])
kvar = K.variable(value=val)
K.size(inputs)
<tf.Tensor: id=9, shape=(), dtype=int32, numpy=4>
""" .

"DESCRIPTION.The code creates a variable `kvar` with a 2x2 array of floats and then evaluates the variable `kvar` in the Keras backend." <EXPLAINS> """CODE.from keras import backend as K
kvar = K.variable(np.array([[1, 2], [3, 4]]), dtype='float32')
K.eval(kvar)
""" .

"DESCRIPTION.The code creates a variable with a 2x3 matrix, evaluates the variable, transposes the variable, evaluates the transposed variable, creates a placeholder with a 2x3 shape, and transposes the input placeholder." <EXPLAINS> """CODE.var = tf.keras.backend.variable([[1, 2, 3], [4, 5, 6])
tf.keras.backend.eval(var)
var_transposed = tf.keras.backend.transpose(var)
tf.keras.backend.eval(var_transposed)
input = tf.keras.backend.placeholder((2, 3))
input_transposed = tf.keras.backend.transpose(input)""" .

"DESCRIPTION.The code creates a vectorized environment for the CartPole-v1 gym environment with 3 parallel environments and resets the environments, returning the initial states of each environment as arrays." <EXPLAINS> """CODE.import gym
env = gym.vector.make('CartPole-v1', 3)
env.reset()
array([[-0.04456399,  0.04653909,  0.01326909, -0.02099827],
       [ 0.03073904,  0.00145001, -0.03088818, -0.03131252],
       [ 0.03468829,  0.01500225,  0.01230312,  0.01825218]],
      dtype=float32)""" .

"DESCRIPTION.The code creates a web server using Gradio and Hugging Face Hub that responds with a \"hello\" message when accessed via the \"/say_hello\" endpoint." <EXPLAINS> """CODE.import gradio as gr
from huggingface_hub import WebhooksServer, WebhookPayload

with gr.Blocks() as ui:
    ...

app = WebhooksServer(ui=ui, webhook_secret="my_secret_key")

@app.add_webhook("/say_hello")
async def hello(payload: WebhookPayload):
    return {"message": "hello"}

app.run()
""" .

"DESCRIPTION.The code creates an ActorPool object, submits a lambda function to double a value for processing by the actor pool, checks if there is a next result available, retrieves the next result from the pool, and then checks again if there is a next result available." <EXPLAINS> """CODE.pool = ActorPool(...)
pool.submit(lambda a, v: a.double.remote(v), 1)
print(pool.has_next())
print(pool.get_next())
print(pool.has_next())""" .

"DESCRIPTION.The code creates an Adam optimizer with a learning rate of 0.001 using PaddlePaddle's Fluid library. It then saves the optimizer's state dictionary to a file named \"opt_adam\" and loads the optimizer's state dictionary from the same file." <EXPLAINS> """CODE.import paddle.fluid as fluid

with fluid.dygraph.guard():
    adam = fluid.optimizer.Adam(0.001)

    state_dict = adam.state_dict()
    fluid.save_optimizer(state_dict, "opt_adam")

    fluid.load_optimizer("opt_adam")""" .

"DESCRIPTION.The code creates an Adam optimizer with a specified learning rate, sets up a fixed loss scale manager with a scale value of 5000, combines the optimizer with the loss scale manager to create a LossScaleOptimizer object, and then minimizes the loss function using this LossScaleOptimizer." <EXPLAINS> """CODE.
opt = tf.AdamOptimizer(learning_rate=...)

loss_scale_manger = tf.contrib.mixed_precision.FixedLossScaleManager(5000)

loss_scale_optimizer = LossScaleOptimizer(opt, loss_scale_manager)

train_op = loss_scale_optimizer.minimize(loss)
""" .

"DESCRIPTION.The code creates an Arrow array with values [\"a\", \"b\", \"c\", None, \"e\"] and a corresponding mask array with values [True, False, None, False, True]. It then filters the array based on the mask, first with the default behavior of filtering out null values, and then explicitly emits null values." <EXPLAINS> """CODE.import pyarrow as pa
arr = pa.array(["a", "b", "c", None, "e"])
mask = pa.array([True, False, None, False, True])
arr.filter(mask)
arr.filter(mask, null_selection_behavior='emit_null')
""" .

"DESCRIPTION.The code creates an AsyncManager object and then saves a checkpoint using the async-manager parameter provided to the save_checkpoint function." <EXPLAINS> """CODE.am = AsyncManager()
save_checkpoint(..., async_manager=am)""" .

"DESCRIPTION.The code creates an AttributeDict object with initial key-value pairs 'key1': 1 and 'key2': 'abc'. It then updates the AttributeDict with a new key-value pair 'my-key': 3.14 and another pair 'mew_key': 42. Finally, it updates the value of key 'key1' to 2 and prints out the AttributeDict object." <EXPLAINS> """CODE.ad = AttributeDict({'key1': 1, 'key2': 'abc'})
ad.update({'my-key': 3.14})
ad.update(mew_key=42)
ad.key1 = 2
ad""" .

"DESCRIPTION.The code creates an Embedding layer using PaddlePaddle's fluid.dygraph module, utilizes DataParallel to parallelize the process, saves the state dictionary of the Embedding layer to a file named \"paddle_dy\", loads the state dictionary from the file, and then updates the Embedding layer's parameters with the loaded dictionary." <EXPLAINS> """CODE.import paddle.fluid as fluid
with fluid.dygraph.guard():
    strategy=dygraph.parallel.prepare_context()
    emb = fluid.dygraph.Embedding([10, 10])
    emb = dygraph.parallel.DataParallel(emb, strategy)

    state_dict = emb.state_dict()
    fluid.save_dygraph( state_dict, "paddle_dy")

    para_state_dict, _ = fluid.load_dygraph( "paddle_dy")

    emb.load_dict( para_state_dict )""" .

"DESCRIPTION.The code creates an EvalContextModifier object with the option set to enable autoescaping." <EXPLAINS> "CODE.EvalContextModifier(options=[Keyword('autoescape', Const(True))])" .

"DESCRIPTION.The code creates an ExportArchive object to store an exported model. It tracks a given model and adds two endpoints: \"serve\" for serving the model with inference only, and \"call_inference\" and \"call_training\" for calling the model with inference and training, respectively. Finally, it writes out the ExportArchive object to a specified location." <EXPLAINS> """CODE.export_archive = ExportArchive()
export_archive.track(model)
export_archive.add_endpoint(
    name="serve",
    fn=model.call,
    input_signature=[tf.TensorSpec(shape=(None, 3), dtype=tf.float32)],
)
export_archive.write_out("path/to/location")

export_archive = ExportArchive()
export_archive.track(model)
export_archive.add_endpoint(
    name="call_inference",
    fn=lambda x: model.call(x, training=False),
    input_signature=[tf.TensorSpec(shape=(None, 3), dtype=tf.float32)],
)
export_archive.add_endpoint(
    name="call_training",
    fn=lambda x: model.call(x, training=True),
    input_signature=[tf.TensorSpec(shape=(None, 3), dtype=tf.float32)],
)
export_archive.write_out("path/to/location")
""" .

"DESCRIPTION.The code creates an HTTP header dictionary with the initial value of 'bar' for the key 'foo'. It then adds the value 'baz' to the key 'Foo' and retrieves the value for the key 'foo', returning 'bar, baz'." <EXPLAINS> """CODE.headers = HTTPHeaderDict(foo='bar')
headers.add('Foo', 'baz')
headers['foo']
'bar, baz'""" .

"DESCRIPTION.The code creates an InMemoryDataset using the PaddlePaddle framework, loads data from \"a.txt\" and \"b.txt\" into memory, and preprocesses each instance in the dataset." <EXPLAINS> """CODE.import paddle.fluid as fluid
dataset = fluid.DatasetFactory().create_dataset("InMemoryDataset")
filelist = ["a.txt", "b.txt"]
dataset.set_filelist(filelist)
dataset.load_into_memory()
dataset.preprocess_instance()""" .

"DESCRIPTION.The code creates an IntervalIndex object from a list of Interval objects." <EXPLAINS> """CODE.IntervalIndex.from_intervals([Interval(0, 1), Interval(1, 2)])
Index([Interval(0, 1), Interval(1, 2)])""" .

"DESCRIPTION.The code creates an IntervalIndex object from arrays representing the intervals [0, 1), [1, 2), and [2, 3)." <EXPLAINS> "CODE.IntervalIndex.from_arrays([0, 1, 2], [1, 2, 3])" .

"DESCRIPTION.The code creates an IpuStrategy object and sets the graph configuration parameters such as the number of IPUs, training status, micro batch size, and manual shard enablement." <EXPLAINS> """CODE.ipu_strategy = static.IpuStrategy()
ipu_strategy.set_graph_config(num_ipus=1,
                            is_training=True,
                            micro_batch_size=1,
                            enable_manual_shard=False)""" .

"DESCRIPTION.The code creates an RNN model using TensorFlow to process input sequences of categorical colors encoded as strings (R, G, B, Y) with the given vocabulary list. It uses a BasicRNNCell with a specified hidden size to process the input sequences and produce output states." <EXPLAINS> """CODE.colors = sequence_categorical_column_with_vocabulary_list(
    key='colors', vocabulary_list=('R', 'G', 'B', 'Y'),
    num_oov_buckets=2)
colors_embedding = embedding_column(colors, dimension=3)
columns = [colors_embedding]

features = tf.parse_example(..., features=make_parse_example_spec(columns))
input_layer, sequence_length = sequence_input_layer(features, columns)

rnn_cell = tf.nn.rnn_cell.BasicRNNCell(hidden_size)
outputs, state = tf.nn.dynamic_rnn(
    rnn_cell, inputs=input_layer, sequence_length=sequence_length)
""" .

"DESCRIPTION.The code creates an Xception model with specified input shape and number of classes, compiles it with a specific loss function and optimizer, generates dummy data for training, fits the model on 8 GPUs with batch size of 256 for 20 epochs, and saves the trained model as 'my_model.h5'." <EXPLAINS> """CODE.    import tensorflow as tf
    from keras.applications import Xception
    from keras.utils import multi_gpu_model
    import numpy as np

    num_samples = 1000
    height = 224
    width = 224
    num_classes = 1000

    # Instantiate the base model (or "template" model).
    # We recommend doing this with under a CPU device scope,
    # so that the model's weights are hosted on CPU memory.
    # Otherwise they may end up hosted on a GPU, which would
    # complicate weight sharing.
    with tf.device('/cpu:0'):
        model = Xception(weights=None,
                         input_shape=(height, width, 3),
                         classes=num_classes)

    # Replicates the model on 8 GPUs.
    # This assumes that your machine has 8 available GPUs.
    parallel_model = multi_gpu_model(model, gpus=8)
    parallel_model.compile(loss='categorical_crossentropy',
                           optimizer='rmsprop')

    # Generate dummy data.
    x = np.random.random((num_samples, height, width, 3))
    y = np.random.random((num_samples, num_classes))

    # This `fit` call will be distributed on 8 GPUs.
    # Since the batch size is 256, each GPU will process 32 samples.
    parallel_model.fit(x, y, epochs=20, batch_size=256)

    # Save model via the template model (which shares the same weights):
    model.save('my_model.h5')


     ..
     # Not needed to change the device scope for model definition:
     model = Xception(weights=None, ..)

     try:
         parallel_model = multi_gpu_model(model, cpu_relocation=True)
         print("Training using multiple GPUs..")
     except ValueError:
         parallel_model = model
         print("Training using single GPU or CPU..")
     parallel_model.compile(..)
     ..


     ..
     # Not needed to change the device scope for model definition:
     model = Xception(weights=None, ..)

     try:
         parallel_model = multi_gpu_model(model, cpu_merge=False)
         print("Training using multiple GPUs..")
     except:
         parallel_model = model
         print("Training using single GPU or CPU..")

     parallel_model.compile(..)
     ..
""" .

"DESCRIPTION.The code creates an agent object and trains it 10 times using a method called \"train\". Finally, it exports the agent's policy model to a directory named \"/tmp/export_dir\"." <EXPLAINS> """CODE.agent = MyAgent()
for _ in range(10):
    agent.train()
agent.export_policy_model("/tmp/export_dir")""" .

"DESCRIPTION.The code creates an agent object using the MyAgent class, then trains the agent for 10 iterations using the train method. Finally, it exports a policy checkpoint to the specified directory \"/tmp/export_dir\"." <EXPLAINS> """CODE.agent = MyAgent()
for _ in range(10):
    agent.train()
agent.export_policy_checkpoint("/tmp/export_dir")""" .

"DESCRIPTION.The code creates an area chart based on the data provided in the chart_data variable." <EXPLAINS> "CODE.st.area_chart(chart_data)" .

"DESCRIPTION.The code creates an empty placeholder in which the text \"Hello world!\" and an image represented by my_image_bytes can be displayed." <EXPLAINS> """CODE.my_placeholder = st.empty()
my_placeholder.text("Hello world!")
my_placeholder.image(my_image_bytes)""" .

"DESCRIPTION.The code creates an environment object for the BipedalWalker-v3 game in the Gym library with the hardcore mode enabled." <EXPLAINS> """CODE.import gym
env = gym.make("BipedalWalker-v3", hardcore=True)
""" .

"DESCRIPTION.The code creates an evaluation result object and stores two lists of data within it: one for IDs consisting of integers [0, 1, 2], and one for predictions consisting of strings ['cat', 'dog', 'dog']." <EXPLAINS> """CODE.result = pl.EvalResult()
result.write('ids', [0, 1, 2])
result.write('preds', ['cat', 'dog', 'dog'])""" .

"DESCRIPTION.The code creates an evaluation result object and writes two lists of values (ids and preds) to the result object. The 'ids' list contains integer values [0, 1, 2], while the 'preds' list contains string values ['cat', 'dog', 'dog']." <EXPLAINS> """CODE.result = pl.EvalResult()
result.write('ids', [0, 1, 2])
result.write('preds', ['cat', 'dog', 'dog'])""" .

"DESCRIPTION.The code creates an event manager and uses it to manage the lifecycle and tasks of an actor. It requests an actor with specific resource requirements, adds callback functions for actor lifecycle events, waits for the actor to start, schedules a task on the actor, and processes task futures." <EXPLAINS> """CODE.from ray.air.execution import ResourceRequest
from ray.air.execution._internal import EventType, RayEventManager

event_manager = RayEventManager()

# Request an actor
tracked_actor = event_manager.add_actor(
    ActorClass,
    kwargs={},
    resource_request=ResourceRequest([{"CPU": 1}])
)
tracked_actor.on_start(actor_start_callback)
tracked_actor.on_stop(actor_stop_callback)
tracked_actor.on_fail(actor_fail_callback)

# Yield control to event manager to start actor
event_manager.wait(timeout=1)

# Start task on the actor (ActorClass.foo.remote())
tracked_actor_task = event_manager.schedule_actor_task(
    tracked_actor,
    method_name="foo"
)
tracked_actor_task.on_result(task_result_callback)
tracked_actor_task.on_error(task_error_callback)

# Again yield control to event manager to process task futures
event_manager.wait(event_type=EventType.TASKS)
""" .

"DESCRIPTION.The code creates an in-memory dataset for distributed training with the ability to parse instance IDs." <EXPLAINS> """CODE.import paddle
paddle.enable_static()
dataset = paddle.distributed.InMemoryDataset()
dataset._set_parse_ins_id(True)""",
        """CODE.import paddle.base as base
dataset = base.DatasetFactory().create_dataset("InMemoryDataset")
dataset.set_parse_ins_id(True)""" .

"DESCRIPTION.The code creates an in-memory dataset using PaddlePaddle framework and enables data merging by sample ID." <EXPLAINS> """CODE.import paddle.fluid as fluid
dataset = fluid.DatasetFactory().create_dataset("InMemoryDataset")
dataset.set_merge_by_sid(True)""" .

"DESCRIPTION.The code creates an in-memory dataset using PaddlePaddle framework." <EXPLAINS> """CODE.import paddle
paddle.enable_static()
dataset = paddle.distributed.InMemoryDataset()
dataset._set_parse_content(True)""",
        """CODE.import paddle.fluid as fluid
dataset = fluid.DatasetFactory().create_dataset("InMemoryDataset")
dataset.set_parse_content(True)""" .

"DESCRIPTION.The code creates an in-memory dataset using PaddlePaddle with the ability to enable PV merge." <EXPLAINS> """CODE.import paddle.fluid as fluid
dataset = fluid.DatasetFactory().create_dataset("InMemoryDataset")
dataset.set_enable_pv_merge(True)""" .

"DESCRIPTION.The code creates an in-memory dataset using PaddlePaddle's fluid framework and enables parsing of instance IDs." <EXPLAINS> """CODE.import paddle
paddle.enable_static()
dataset = paddle.distributed.InMemoryDataset()
dataset._set_parse_ins_id(True)""",
        """CODE.import paddle.fluid as fluid
dataset = fluid.DatasetFactory().create_dataset("InMemoryDataset")
dataset.set_parse_ins_id(True)""" .

"DESCRIPTION.The code creates an in-memory dataset using PaddlePaddle's fluid framework and sets it to merge data by line ID." <EXPLAINS> """CODE.import paddle
paddle.enable_static()
dataset = paddle.distributed.InMemoryDataset()
dataset._set_merge_by_lineid()""",
        """CODE.import paddle.fluid as fluid
dataset = fluid.DatasetFactory().create_dataset("InMemoryDataset")
dataset.set_merge_by_lineid()""" .

"DESCRIPTION.The code creates an in-memory dataset using PaddlePaddle's fluid framework and sets the fleet send batch size to 800." <EXPLAINS> """CODE.import paddle
paddle.enable_static()
dataset = paddle.distributed.InMemoryDataset()
dataset._set_fleet_send_batch_size(800)""",
        """CODE.import paddle.fluid as fluid
dataset = fluid.DatasetFactory().create_dataset("InMemoryDataset")
dataset.set_fleet_send_batch_size(800)""" .

"DESCRIPTION.The code creates an in-memory dataset using PaddlePaddle's fluid framework and sets the queue number to 12." <EXPLAINS> """CODE.import paddle
paddle.enable_static()
dataset = paddle.distributed.InMemoryDataset()
dataset._set_queue_num(12)""",
        """CODE.import paddle.fluid as fluid
dataset = fluid.DatasetFactory().create_dataset("InMemoryDataset")
dataset.set_queue_num(12)""" .

"DESCRIPTION.The code creates an in-memory dataset using PaddlePaddle's fluid framework and sets the time interval for sending data to the distributed training fleet to 2 seconds." <EXPLAINS> """CODE.import paddle
paddle.enable_static()
dataset = paddle.distributed.InMemoryDataset()
dataset._set_fleet_send_sleep_seconds(2)""",
        """CODE.import paddle.fluid as fluid
dataset = fluid.DatasetFactory().create_dataset("InMemoryDataset")
dataset.set_fleet_send_sleep_seconds(2)""" .

"DESCRIPTION.The code creates an in-memory dataset using PaddlePaddle's fluid framework, loads files \"a.txt\" and \"b.txt\" into memory, and sets the current phase to 1." <EXPLAINS> """CODE.import paddle.fluid as fluid
dataset = fluid.DatasetFactory().create_dataset("InMemoryDataset")
filelist = ["a.txt", "b.txt"]
dataset.set_filelist(filelist)
dataset.load_into_memory()
dataset.set_current_phase(1)""" .

"DESCRIPTION.The code creates an in-memory dataset using PaddlePaddle's fluid library, sets a list of file names for the dataset, loads the dataset into memory, and then prints the size of the dataset." <EXPLAINS> """CODE.import paddle.fluid as fluid
dataset = fluid.DatasetFactory().create_dataset("InMemoryDataset")
filelist = ["a.txt", "b.txt"]
dataset.set_filelist(filelist)
dataset.load_into_memory()
print dataset.get_pv_data_size()""" .

"DESCRIPTION.The code creates an in-memory dataset using the PaddlePaddle framework and enables parsing of log keys for the dataset." <EXPLAINS> """CODE.import paddle.fluid as fluid
dataset = fluid.DatasetFactory().create_dataset("InMemoryDataset")
dataset.set_parse_logkey(True)""" .

"DESCRIPTION.The code creates an in-memory dataset using the PaddlePaddle framework and enables parsing of the dataset content." <EXPLAINS> """CODE.import paddle
paddle.enable_static()
dataset = paddle.distributed.InMemoryDataset()
dataset._set_parse_content(True)""",
        """CODE.import paddle.fluid as fluid
dataset = fluid.DatasetFactory().create_dataset("InMemoryDataset")
dataset.set_parse_content(True)""" .

"DESCRIPTION.The code creates an in-memory dataset using the PaddlePaddle framework with the ability to parse content." <EXPLAINS> """CODE.import paddle
paddle.enable_static()
dataset = paddle.distributed.InMemoryDataset()
dataset._set_parse_content(True)""",
        """CODE.import paddle.base as base
dataset = base.DatasetFactory().create_dataset("InMemoryDataset")
dataset.set_parse_content(True)""" .

"DESCRIPTION.The code creates an in-memory dataset using the PaddlePaddle framework with the option to parse instance IDs." <EXPLAINS> """CODE.import paddle
paddle.enable_static()
dataset = paddle.distributed.InMemoryDataset()
dataset._set_parse_ins_id(True)""",
        """CODE.import paddle.base as base
dataset = base.DatasetFactory().create_dataset("InMemoryDataset")
dataset.set_parse_ins_id(True)""" .

"DESCRIPTION.The code creates an in-memory dataset with 12 queues for distributed computing using PaddlePaddle framework." <EXPLAINS> """CODE.import paddle
paddle.enable_static()
dataset = paddle.distributed.InMemoryDataset()
dataset._set_queue_num(12)""",
        """CODE.import paddle.base as base
dataset = base.DatasetFactory().create_dataset("InMemoryDataset")
dataset.set_queue_num(12)""" .

"DESCRIPTION.The code creates an index object based on the input provided, which can be a list of single values, tuples, or lists." <EXPLAINS> """CODE._ensure_index(['a', 'b'])
Index(['a', 'b'], dtype='object')

_ensure_index([('a', 'a'),  ('b', 'c')])
Index([('a', 'a'), ('b', 'c')], dtype='object')

_ensure_index([['a', 'a'], ['b', 'c']])
MultiIndex(levels=[['a'], ['b', 'c']],
           labels=[[0, 0], [0, 1]])""",
        """CODE.ensure_index(['a', 'b'])
Index(['a', 'b'], dtype='object')

ensure_index([('a', 'a'),  ('b', 'c')])
Index([('a', 'a'), ('b', 'c')], dtype='object')

ensure_index([['a', 'a'], ['b', 'c']])
MultiIndex(levels=[['a'], ['b', 'c']],
           labels=[[0, 0], [0, 1]])""" .

"DESCRIPTION.The code creates an index object with duplicate values and then removes the duplicate values in three different ways: keeping the first occurrence, keeping the last occurrence, and removing all duplicates." <EXPLAINS> """CODE.idx = pd.Index(['lama', 'cow', 'lama', 'beetle', 'lama', 'hippo'])
idx.drop_duplicates(keep='first')
idx.drop_duplicates(keep='last')
idx.drop_duplicates(keep=False)""" .

"DESCRIPTION.The code creates an index using pandas with specified elements and checks if the index is categorical. Similarly, it creates another index with different elements and checks if it is categorical. Lastly, it creates a series and checks if its index is categorical." <EXPLAINS> """CODE.idx = pd.Index(["Watermelon", "Orange", "Apple",
...                 "Watermelon"]).astype("category")
idx.is_categorical()
True

idx = pd.Index([1, 3, 5, 7])
idx.is_categorical()
False

s = pd.Series(["Peter", "Victor", "Elisabeth", "Mar"])
s
0        Peter
1       Victor
2    Elisabeth
3          Mar
dtype: object
s.index.is_categorical()
False""" .

"DESCRIPTION.The code creates an index with the provided data in a specified format." <EXPLAINS> """CODE._ensure_index(['a', 'b'])
Index(['a', 'b'], dtype='object')

_ensure_index([('a', 'a'),  ('b', 'c')])
Index([('a', 'a'), ('b', 'c')], dtype='object')

_ensure_index([['a', 'a'], ['b', 'c']])
MultiIndex(levels=[['a'], ['b', 'c']],
           labels=[[0, 0], [0, 1]])""" .

"DESCRIPTION.The code creates an input object using PosixPipeInput and sends the text 'inputdata' to the input object." <EXPLAINS> """CODE.input = PosixPipeInput()
input.send_text('inputdata')""" .

"DESCRIPTION.The code creates an input tensor with a 5-dimensional shape and applies a 3D average pooling operation on the input, resulting in an output tensor with a shape of (batch_size, 10, 10, 10, 3)." <EXPLAINS> """CODE.inputs = tf.keras.Input(shape=(depth, height, width, input_channels))
layer = tf.keras.layers.AveragePooling3D(pool_size=3)
outputs = layer(inputs)  # Shape: (batch_size, 10, 10, 10, 3)
""" .

"DESCRIPTION.The code creates an instance of Data2VecTextConfig, initializes a Data2VecTextModel object with the configuration, and then retrieves the configuration from the model." <EXPLAINS> """CODE.from transformers import Data2VecTextModel, Data2VecTextConfig
configuration = Data2VecTextConfig()
model = Data2VecTextModel(configuration)
configuration = model.config
""" .

"DESCRIPTION.The code creates an instance of LocalFS from paddle.distributed.fleet.utils and checks if a file named \"test_is_exist\" exists." <EXPLAINS> """CODE.from paddle.distributed.fleet.utils import LocalFS
client = LocalFS()
ret = local_fs.is_exist("test_is_exist")""" .

"DESCRIPTION.The code creates an instance of LocalFS from the paddle.distributed.fleet.utils module and uses it to list the subdirectories and files within the specified directory \"./\"." <EXPLAINS> """CODE.from paddle.distributed.fleet.utils import LocalFS

client = LocalFS()
subdirs, files = client.ls_dir("./")""" .

"DESCRIPTION.The code creates an instance of MyTrainer and imports a policy model from a pre-trained weights file. It then runs the training process for the imported model 10 times." <EXPLAINS> """CODE.trainer = MyTrainer()
trainer.import_policy_model_from_h5("/tmp/weights.h5")
for _ in range(10):
    trainer.train()""" .

"DESCRIPTION.The code creates an instance of MyTrainer and trains it 10 times. After training, it exports the policy checkpoint to a specified directory." <EXPLAINS> """CODE.trainer = MyTrainer()
for _ in range(10):
    trainer.train()
trainer.export_policy_checkpoint("/tmp/export_dir")""" .

"DESCRIPTION.The code creates an instance of a FlavaMultimodalModel with a configuration, and then retrieves the configuration from the model." <EXPLAINS> """CODE.from transformers import FlavaMultimodalModel, FlavaMultimodalConfig
configuration = FlavaMultimodalConfig()
model = FlavaMultimodalModel(configuration)
configuration = model.config
""" .

"DESCRIPTION.The code creates an instance of a GroupViTText model and retrieves the configuration of the model." <EXPLAINS> """CODE.from transformers import GroupViTTextConfig, GroupViTTextModel
configuration = GroupViTTextConfig()
model = GroupViTTextModel(configuration)
configuration = model.config
""" .

"DESCRIPTION.The code creates an instance of a text summarization tool and uses it to generate a summary of a long text." <EXPLAINS> """CODE.from transformers.tools import TextSummarizationTool
summarizer = TextSummarizationTool()
summarizer(long_text)""" .

"DESCRIPTION.The code creates an instance of a trainer object, imports a policy model from a saved h5 file, and then trains the model 10 times." <EXPLAINS> """CODE.trainer = MyTrainer()
trainer.import_policy_model_from_h5("/tmp/weights.h5")
for _ in range(10):
    trainer.train()""" .

"DESCRIPTION.The code creates an instance of a training object called MyTrainer, runs the training process 10 times, and then exports the trained policy model to a specified directory." <EXPLAINS> """CODE.trainer = MyTrainer()
for _ in range(10):
    trainer.train()
trainer.export_policy_model("/tmp/export_dir")""" .

"DESCRIPTION.The code creates an instance of the Action class, which includes an instance of the User class as an attribute. The view method is called on the Action instance, and then the insert method is called, resulting in a PermissionError being raised because the User does not have permission to run the insert method." <EXPLAINS> """CODE.a = Action()
a.user = User()
a.view()
a.insert() # doctest: +IGNORE_EXCEPTION_DETAIL
Traceback (most recent call last):
   ...
PermissionError: User does not have the permission to run insert!""" .

"DESCRIPTION.The code creates an instance of the BooleanDtype class in pandas, which can be used to represent boolean data types in pandas data structures." <EXPLAINS> "CODE.pd.BooleanDtype()" .

"DESCRIPTION.The code creates an instance of the Int class with an initial value of 0, and then tags the instance with configuration attributes set to True, and synchronization attributes set to True." <EXPLAINS> "CODE.Int(0).tag(config=True, sync=True)" .

"DESCRIPTION.The code creates an instance of the LocalFS class from the paddle.distributed.fleet.utils module and uses it to list the directories and files in the current directory." <EXPLAINS> """CODE.from paddle.distributed.fleet.utils import LocalFS
client = LocalFS()
subdirs, files = client.ls_dir("./")""" .

"DESCRIPTION.The code creates an instance of the MNASNet model with 1000 classes and a width scale factor of 1.0. It then generates a random tensor \"x\" with shape (1, 3, 224, 224) and passes it through the model to obtain an output \"y\". It then calculates the dimensionality of \"y\" using the dim() method and the total number of elements in \"y\" using the nelement() method." <EXPLAINS> """CODE.model = MNASNet(1000, 1.0)
x = torch.rand(1, 3, 224, 224)
y = model(x)
y.dim()
y.nelement()""" .

"DESCRIPTION.The code creates an instance of the MyTrainer class, imports a policy model from a saved weights file, and then trains the model 10 times." <EXPLAINS> """CODE.trainer = MyTrainer()
trainer.import_policy_model_from_h5("/tmp/weights.h5")
for _ in range(10):
    trainer.train()""" .

"DESCRIPTION.The code creates an instance of the Version class with the string \"1.2.3.dev1\" and accesses the attribute dev." <EXPLAINS> "CODE.Version(\"1.2.3.dev1\").dev" .

"DESCRIPTION.The code creates an instance of the Version class with the version number \"1.2.3.dev1\" and accesses its dev attribute." <EXPLAINS> "CODE.Version(\"1.2.3.dev1\").dev" .

"DESCRIPTION.The code creates an iterator from a list of items [0, 1, 2], filters out elements that are less than or equal to 0, and yields the next item that meets the filter condition synchronously." <EXPLAINS> """CODE.it = from_items([0, 1, 2]).filter(lambda x: x > 0)
next(it.gather_sync())""" .

"DESCRIPTION.The code creates an object 'v1' and sets the values at specific nested keys within the object." <EXPLAINS> """CODE.v1 = v(1, 2, m(a=5, b=6))
v1.set_in((2, 'b'), 17)
v1.set_in((2, 'c', 'd'), 17)""" .

"DESCRIPTION.The code creates an object called \"obj\" of class Checkpointable, which has two attributes: \"has_dependency\" is a TensorFlow variable initialized to 0 with the name \"dep\", and \"no_dependency\" is an instance of class NoDependency with a TensorFlow variable initialized to 1 with the name \"nodep\". The code then asserts that the name of the \"no_dependency\" variable is \"nodep:0\"." <EXPLAINS> """CODE.obj = Checkpointable()
obj.has_dependency = tf.Variable(0., name="dep")
obj.no_dependency = NoDependency(tf.Variable(1., name="nodep"))
assert obj.no_dependency.name == "nodep:0"
""" .

"DESCRIPTION.The code creates an object of LocalFS class from paddle.distributed.fleet.utils module. It then creates a directory named \"test_mkdirs\" using the mkdirs() method of the client object, and subsequently deletes the \"test_mkdirs\" directory using the delete() method." <EXPLAINS> """CODE.from paddle.distributed.fleet.utils import LocalFS
client = LocalFS()
client.mkdirs("test_mkdirs")
client.delete("test_mkdirs")""" .

"DESCRIPTION.The code creates an object v1 with values 1, 2, 3, 4, 5, and then deletes the elements 1 and 3 from the object v1." <EXPLAINS> """CODE.v1 = v(1, 2, 3, 4, 5)
v1.delete(1)
v1.delete(1, 3)""" .

"DESCRIPTION.The code creates an optional object containing the value 42 and prints out the element specification of the optional object, which is a TensorSpec object specifying a scalar integer (int32) with no specific shape or name." <EXPLAINS> """CODE.optional = tf.experimental.Optional.from_value(42)
print(optional.element_spec)
tf.TensorSpec(shape=(), dtype=tf.int32, name=None)""" .

"DESCRIPTION.The code creates and checks various types of keras tensors and variables." <EXPLAINS> """CODE.np_var = np.array([1, 2])
tf.keras.backend.is_keras_tensor(np_var)
keras_var = tf.keras.backend.variable(np_var)
tf.keras.backend.is_keras_tensor(keras_var)
keras_placeholder = tf.keras.backend.placeholder(shape=(2, 4, 5))
tf.keras.backend.is_keras_tensor(keras_placeholder)
keras_input = tf.keras.layers.Input([10])
tf.keras.backend.is_keras_tensor(keras_input)
keras_layer_output = tf.keras.layers.Dense(10)(keras_input)
tf.keras.backend.is_keras_tensor(keras_layer_output)""" .

"DESCRIPTION.The code creates and commits a file named \"file.txt\" containing a JSON object {\"hey\": 8} to a repository named \"text-files\" cloned from <user>/text-files using an authentication token. It also creates a new PyTorch Transformer model, saves the model's state dictionary to a file named \"model.pt\", and commits it to a repository named \"torch-model\" cloned from <user>/torch-model using an authentication token." <EXPLAINS> """CODE.with Repository("text-files", clone_from="<user>/text-files", use_auth_token=True).commit("My first file :)"):
    with open("file.txt", "w+") as f:
        f.write(json.dumps({"hey": 8}))
import torch
model = torch.nn.Transformer()
with Repository("torch-model", clone_from="<user>/torch-model", use_auth_token=True).commit("My cool model :)"):
    torch.save(model.state_dict(), "model.pt")""" .

"DESCRIPTION.The code creates and compiles a neural network model using stochastic gradient descent optimization and root mean squared error as the evaluation metric." <EXPLAINS> """CODE.model = keras.Model(inputs, outputs)
model.compile('sgd', metrics=[keras.metrics.RootMeanSquaredError()])
""" .

"DESCRIPTION.The code creates and exports a model artifact, then reloads the artifact in a different context to make predictions on input data." <EXPLAINS> """CODE.# Create the artifact
model.export("path/to/location")

# Later, in a different process / environment...
reloaded_artifact = tf.saved_model.load("path/to/location")
predictions = reloaded_artifact.serve(input_data)
""" .

"DESCRIPTION.The code creates boxplots for each group in the DataFrame 'df' grouped by the specified levels." <EXPLAINS> """CODE.grouped = df.groupby(level='lvl1')
boxplot_frame_groupby(grouped)

grouped = df.unstack(level='lvl1').groupby(level=0, axis=1)
boxplot_frame_groupby(grouped, subplots=False)""" .

"DESCRIPTION.The code creates custom Iterable Datasets in Python using PaddlePaddle. The first dataset, RandomDataset, generates random images and labels. The second dataset, SplitedIterableDataset, splits a range of numbers among multiple workers. The third dataset, RangeIterableDataset, generates a range of numbers. The code then creates DataLoaders for each dataset with specified batch sizes, number of workers, and initialization functions, and prints the output of the DataLoaders." <EXPLAINS> """CODE.import numpy as np
from paddle.io import Dataset

# define a random dataset
class RandomDataset(Dataset):
    def __init__(self, num_samples):
        self.num_samples = num_samples

    def __iter__(self):
        for i in range(self.num_samples):
            image = np.random.random([784]).astype('float32')
            label = np.random.randint(0, 9, (1, )).astype('int64')
            yield image, label

dataset = RandomDataset(10)
for img, lbl in dataset:
    print(img, lbl)

import math
import numpy as np
import paddle.fluid as fluid
from paddle.io import IterableDataset, DataLoader, get_worker_info

class SplitedIterableDataset(IterableDataset):
    def __init__(self, start, end):
        self.start = start
        self.end = end

    def __iter__(self):
        worker_info = get_worker_info()
        if worker_info is None:
            iter_start = self.start
            iter_end = self.end
        else:
            per_worker = int(
                math.ceil((self.end - self.start) / float(
                    worker_info.num_workers)))
            worker_id = worker_info.id
            iter_start = self.start + worker_id * per_worker
            iter_end = min(iter_start + per_worker, self.end)

        for i in range(iter_start, iter_end):
            yield np.array([i])

place = fluid.CPUPlace()
with fluid.dygraph.guard(place):
    dataset = SplitedIterableDataset(start=2, end=9)
    dataloader = DataLoader(
        dataset,
        places=place,
        num_workers=2,
        batch_size=1,
        drop_last=True)

    print(list(dataloader))
    # outputs: [2, 5, 3, 6, 4, 7]

import math
import numpy as np
import paddle.fluid as fluid
from paddle.io import IterableDataset, DataLoader, get_worker_info

class RangeIterableDataset(IterableDataset):
    def __init__(self, start, end):
        self.start = start
        self.end = end

    def __iter__(self):
        for i in range(self.start, self.end):
            yield np.array([i])

place = fluid.CPUPlace()
with fluid.dygraph.guard(place):
    dataset = RangeIterableDataset(start=2, end=9)

    def worker_init_fn(worker_id):
        worker_info = get_worker_info()

        dataset = worker_info.dataset
        start = dataset.start
        end = dataset.end
        num_per_worker = int(
            math.ceil((end - start) / float(worker_info.num_workers)))

        worker_id = worker_info.id
        dataset.start = start + worker_id * num_per_worker
        dataset.end = min(dataset.start + num_per_worker, end)

    dataloader = DataLoader(
        dataset,
        places=place,
        num_workers=2,
        batch_size=1,
        drop_last=True,
        worker_init_fn=worker_init_fn)

    print(list(dataloader))
    # outputs: [2, 5, 3, 6, 4, 7]""" .

"DESCRIPTION.The code creates four instances of the ReplayActor class and then initializes a ParallelReplay object with these instances. Calling the next function on the ParallelReplay object returns a SampleBatch object." <EXPLAINS> """CODE.actors = [ReplayActor.remote() for _ in range(4)]
replay_op = ParallelReplay(actors)
next(replay_op)
SampleBatch(...)""" .

"DESCRIPTION.The code creates instances of OneHotCategorical distribution using probabilities or logits, and then calculates the probability of given samples under the distribution." <EXPLAINS> """CODE.p = [0.1, 0.5, 0.4]
dist = OneHotCategorical(probs=p)

logits = [-2, 2, 0]
dist = OneHotCategorical(logits=logits)

p = [0.1, 0.4, 0.5]
dist = OneHotCategorical(probs=p)
dist.prob([0,1,0])

samples = [[0,1,0], [1,0,0]]
dist.prob(samples)
""" .

"DESCRIPTION.The code creates instances of a Version class with a given version string, and accesses the local attribute of the instances. If the version string contains a local version identifier (after the '+'), the local attribute returns that identifier, otherwise it returns None." <EXPLAINS> """CODE.Version("1.2.3").local
None
Version("1.2.3+abc").local
'abc'""" .

"DESCRIPTION.The code creates instances of a Version class with different version numbers and then calls the release method on each instance." <EXPLAINS> """CODE.Version("1.2.3").release
Version("2.0.0").release
Version("1!2.0.0.post0").release""" .

"DESCRIPTION.The code creates linear operators and performs matrix operations such as matrix multiplication and calculation of determinant. It also demonstrates batch processing of linear operators." <EXPLAINS> """CODE.# Create a 4 x 4 linear operator composed of two 2 x 2 operators.
operator_1 = LinearOperatorFullMatrix([[1., 2.], [3., 4.]])
operator_2 = LinearOperatorFullMatrix([[1., 0.], [2., 1.]])
operator = LinearOperatorKronecker([operator_1, operator_2])

operator.to_dense()
==> [[1., 2., 0., 0.],
     [3., 4., 0., 0.],
     [2., 4., 1., 2.],
     [6., 8., 3., 4.]]

operator.shape
==> [4, 4]

operator.log_abs_determinant()
==> scalar Tensor

x = ... Shape [4, 2] Tensor
operator.matmul(x)
==> Shape [4, 2] Tensor

# Create a [2, 3] batch of 4 x 5 linear operators.
matrix_45 = tf.random_normal(shape=[2, 3, 4, 5])
operator_45 = LinearOperatorFullMatrix(matrix)

# Create a [2, 3] batch of 5 x 6 linear operators.
matrix_56 = tf.random_normal(shape=[2, 3, 5, 6])
operator_56 = LinearOperatorFullMatrix(matrix_56)

# Compose to create a [2, 3] batch of 20 x 30 operators.
operator_large = LinearOperatorKronecker([operator_45, operator_56])

# Create a shape [2, 3, 20, 2] vector.
x = tf.random_normal(shape=[2, 3, 6, 2])
operator_large.matmul(x)
==> Shape [2, 3, 30, 2] Tensor
""" .

"DESCRIPTION.The code creates linear operators, both diagonal and batch, and demonstrates operations such as converting to a dense matrix, accessing shape, computing the log determinant, applying the operator to a tensor, and solving an equation." <EXPLAINS> """CODE.# Create a 2 x 2 diagonal linear operator.
diag = [1., -1.]
operator = LinearOperatorDiag(diag)

operator.to_dense()
==> [[1.,  0.]
     [0., -1.]]

operator.shape
==> [2, 2]

operator.log_determinant()
==> scalar Tensor

x = ... Shape [2, 4] Tensor
operator.apply(x)
==> Shape [2, 4] Tensor

# Create a [2, 3] batch of 4 x 4 linear operators.
diag = tf.random_normal(shape=[2, 3, 4])
operator = LinearOperatorDiag(diag)

# Create a shape [2, 1, 4, 2] vector.  Note that this shape is compatible
# since the batch dimensions, [2, 1], are brodcast to
# operator.batch_shape = [2, 3].
y = tf.random_normal(shape=[2, 1, 4, 2])
x = operator.solve(y)
==> operator.apply(x) = y
""" .

"DESCRIPTION.The code creates meshgrid tensors using input data arrays in both static graph mode and dynamic graph mode in order to generate 2D grids from coordinate vectors." <EXPLAINS> """CODE.import paddle
import paddle.fluid as fluid
import numpy as np
x = fluid.data(name='x', shape=[100], dtype='int32')
y = fluid.data(name='y', shape=[200], dtype='int32')
input_1 = np.random.randint(0, 100, [100, ]).astype('int32')
input_2 = np.random.randint(0, 100, [200, ]).astype('int32')
exe = fluid.Executor(place=fluid.CPUPlace())
grid_x, grid_y = fluid.layers.meshgrid([x, y])
res_1, res_2 = exe.run(fluid.default_main_program(),
                         feed={'x': input_1,
                               'y': input_2},
                         fetch_list=[grid_x, grid_y])

#the shape of res_1 is (100, 200)
#the shape of res_2 is (100, 200)

#example 2: in dygraph mode
import paddle
import paddle.fluid as fluid
import numpy as np
input_3 = np.random.randint(0, 100, [100, ]).astype('int32')
input_4 = np.random.randint(0, 100, [200, ]).astype('int32')
with fluid.dygraph.guard():
    tensor_3 = fluid.dygraph.to_variable(input_3)
    tensor_4 = fluid.dygraph.to_variable(input_4)
    grid_x, grid_y = fluid.layers.meshgrid([tensor_3, tensor_4])
#the shape of grid_x is (100, 200)
#the shape of grid_y is (100, 200)    """ .

"DESCRIPTION.The code creates nested scopes named 'scope1' and 'scope2' using TensorFlow's name_scope method, and then prints the current name scope of the default graph using get_name_scope() method." <EXPLAINS> """CODE.with tf.name_scope('scope1'):
  with tf.name_scope('scope2'):
    print(tf.get_default_graph().get_name_scope())
""" .

"DESCRIPTION.The code creates period objects with specific year, month, and day values, then converts them to a different frequency. It accesses the ordinal value of the period object. It also creates a period index with specified years and quarters, as well as another period index with a specified start and end date range with a yearly frequency." <EXPLAINS> """CODE.i = Period(year=1,month=1,day=1,freq='D').asfreq('S', 'S')
i.ordinal
idx = PeriodIndex(year=year_arr, quarter=q_arr)
idx2 = PeriodIndex(start='2000', end='2010', freq='A')""" .

"DESCRIPTION.The code creates plots using the data stored in a pandas Series and DataFrame. It generates line plots for the Series and DataFrame, with options to plot on separate subplots, customize the x and y axes, and display the type of the plot object." <EXPLAINS> """CODE.s = pd.Series([1, 3, 2])
s.plot.line()

df = pd.DataFrame({
   'pig': [20, 18, 489, 675, 1776],
   'horse': [4, 25, 281, 600, 1900]
   }, index=[1990, 1997, 2003, 2009, 2014])
lines = df.plot.line()

axes = df.plot.line(subplots=True)
type(axes)

lines = df.plot.line(x='pig', y='horse')
""" .

"DESCRIPTION.The code creates polynomial features of degree 2 for the input data X and then creates polynomial features with only interaction terms for the same input data X." <EXPLAINS> """CODE.from sklearn.preprocessing import PolynomialFeatures
import numpy as np

X = np.arange(6).reshape(3, 2)
poly = PolynomialFeatures(2)
poly.fit_transform(X)

poly = PolynomialFeatures(interaction_only=True)
poly.fit_transform(X)
""" .

"DESCRIPTION.The code creates tensors of different shapes and values, and then uses the torch.atleast_3d() function to convert the tensors into 3-dimensional tensors." <EXPLAINS> """CODE.x = torch.tensor(0.5)
x
tensor(0.5000)
torch.atleast_3d(x)
tensor([[[0.5000]]])
y = torch.randn(2,2)
y
tensor([[-0.8079,  0.7460],
        [-1.1647,  1.4734]])
torch.atleast_3d(y)
tensor([[[-0.8079],
        [ 0.7460]],
        <BLANKLINE>
        [[-1.1647],
        [ 1.4734]]])
x = torch.randn(1,1,1)
x
tensor([[[-1.5689]]])
torch.atleast_3d(x)
tensor([[[-1.5689]]])
x = torch.tensor(0.5)
y = torch.tensor(1.)
torch.atleast_3d((x,y))
(tensor([[[0.5000]]]), tensor([[[1.]]]))""" .

"DESCRIPTION.The code creates three parameters with different shapes using XLA operations and then creates a tuple with these parameters. Finally, it retrieves and flattens the shapes of the tuple and the first parameter." <EXPLAINS> """CODE.c = xc.XlaBuilder("example")
p0 = xb.parameter(c, 1, xc.shape_from_pyval(jnp.ones([1])))
p1 = xb.parameter(c, 2, xc.shape_from_pyval(jnp.ones([2])))
p2 = xb.parameter(c, 3, xc.shape_from_pyval(jnp.ones([3])))
o = xops.Tuple(c, [p0, p1, p2])
flatten_shape(c.GetShape(o))
flatten_shape(c.GetShape(p0))""" .

"DESCRIPTION.The code creates two Actor objects, adds them to an ActorPool, and then uses map function to call the double method on each Actor object with a list of values as input, returning a list of doubled values." <EXPLAINS> """CODE.a1, a2 = Actor.remote(), Actor.remote()
pool = ActorPool([a1, a2])
print(pool.map(lambda a, v: a.double.remote(v), [1, 2, 3, 4]))
[2, 4, 6, 8]""" .

"DESCRIPTION.The code creates two DataFrames with random values and 20 columns each. It then displays the first DataFrame in a table and line chart using Streamlit. After that, it adds the second DataFrame to the table and updates the line chart with the data from the second DataFrame. Finally, it creates a Vega-Lite chart with the first DataFrame and updates it with the data from the second DataFrame using a named dataset." <EXPLAINS> """CODE.df1 = pd.DataFrame(
   np.random.randn(50, 20),
   columns=('col %d' % i for i in range(20)))
my_table = st.table(df1)

df2 = pd.DataFrame(
   np.random.randn(50, 20),
   columns=('col %d' % i for i in range(20)))
my_table.add_rows(df2)

my_chart = st.line_chart(df1)
my_chart.add_rows(df2)

my_chart = st.vega_lite_chart({
    'mark': 'line',
    'encoding': {'x': 'a', 'y': 'b'},
    'datasets': {
      'some_fancy_name': df1,  # <-- named dataset
     },
    'data': {'name': 'some_fancy_name'},
}),
my_chart.add_rows(some_fancy_name=df2)  # <-- name used as keyword
""" .

"DESCRIPTION.The code creates two DataFrames with random values, displays the first DataFrame in a table, and adds rows from the second DataFrame to the table. It also creates a line chart with the first DataFrame and adds rows from the second DataFrame to the chart. Additionally, it creates a Vega-Lite line chart with specific encoding and datasets, and adds rows from the second DataFrame to the chart using a named dataset." <EXPLAINS> """CODE.df1 = pd.DataFrame(
    np.random.randn(50, 20),
    columns=('col %d' % i for i in range(20)))

my_table = st.table(df1)

df2 = pd.DataFrame(
    np.random.randn(50, 20),
    columns=('col %d' % i for i in range(20)))

my_table.add_rows(df2)

my_chart = st.line_chart(df1)
my_chart.add_rows(df2)

my_chart = st.vega_lite_chart({
    'mark': 'line',
    'encoding': {'x': 'a', 'y': 'b'},
    'datasets': {
        'some_fancy_name': df1,  # <-- named dataset
    },
    'data': {'name': 'some_fancy_name'},
}),
my_chart.add_rows(some_fancy_name=df2)  # <-- name used as keyword
""",
        """CODE.df1 = pd.DataFrame(
   np.random.randn(50, 20),
   columns=('col %d' % i for i in range(20)))

my_table = st._arrow_table(df1)

df2 = pd.DataFrame(
   np.random.randn(50, 20),
   columns=('col %d' % i for i in range(20)))

my_table._arrow_add_rows(df2)

my_chart = st._arrow_line_chart(df1)
my_chart._arrow_add_rows(df2)

my_chart = st._arrow_vega_lite_chart({
    'mark': 'line',
    'encoding': {'x': 'a', 'y': 'b'},
    'datasets': {
        'some_fancy_name': df1,  # <-- named dataset
    },
    'data': {'name': 'some_fancy_name'},
}),
my_chart._arrow_add_rows(some_fancy_name=df2)  # <-- name used as keyword
""" .

"DESCRIPTION.The code creates two DataFrames, df1 and df2, with random values and 20 columns each. It then displays df1 in a table using Streamlit's st.table function and adds the rows of df2 to the table. Next, it creates a line chart using df1 and adds the rows of df2 to the chart. Finally, it creates a Vega-Lite chart with a line mark, x and y encoding, and a named dataset 'some_fancy_name' using df1. It then adds the rows of df2 to the Vega-Lite chart using the named dataset." <EXPLAINS> """CODE.df1 = pd.DataFrame(
    np.random.randn(50, 20),
    columns=('col %d' % i for i in range(20)))

my_table = st._legacy_table(df1)

df2 = pd.DataFrame(
    np.random.randn(50, 20),
    columns=('col %d' % i for i in range(20)))

my_table._legacy_add_rows(df2)

my_chart = st._legacy_line_chart(df1)
my_chart._legacy_add_rows(df2)

my_chart = st._legacy_vega_lite_chart({
    'mark': 'line',
    'encoding': {'x': 'a', 'y': 'b'},
    'datasets': {
        'some_fancy_name': df1,  # <-- named dataset
    },
    'data': {'name': 'some_fancy_name'},
}),
my_chart._legacy_add_rows(some_fancy_name=df2)  # <-- name used as keyword
""",
        """CODE.df1 = pd.DataFrame(
   np.random.randn(50, 20),
   columns=('col %d' % i for i in range(20)))

my_table = st.table(df1)

df2 = pd.DataFrame(
   np.random.randn(50, 20),
   columns=('col %d' % i for i in range(20)))

my_table.add_rows(df2)

my_chart = st.line_chart(df1)
my_chart.add_rows(df2)

my_chart = st.vega_lite_chart({
    'mark': 'line',
    'encoding': {'x': 'a', 'y': 'b'},
    'datasets': {
      'some_fancy_name': df1,  # <-- named dataset
     },
    'data': {'name': 'some_fancy_name'},
}),
my_chart.add_rows(some_fancy_name=df2)  # <-- name used as keyword
""" .

"DESCRIPTION.The code creates two Dense layers in a neural network, where layer_a has weights initialized to 1 and layer_b has weights initialized to 2. It then sets the weights of layer_b to be the same as the weights of layer_a." <EXPLAINS> """CODE.layer_a = tf.keras.layers.Dense(1,
...   kernel_initializer=tf.constant_initializer(1.))
a_out = layer_a(tf.convert_to_tensor([[1., 2., 3.]]))
layer_a.get_weights()
[array([[1.],
       [1.],
       [1.]], dtype=float32), array([0.], dtype=float32)]
layer_b = tf.keras.layers.Dense(1,
...   kernel_initializer=tf.constant_initializer(2.))
b_out = layer_b(tf.convert_to_tensor([[10., 20., 30.]]))
layer_b.get_weights()
[array([[2.],
       [2.],
       [2.]], dtype=float32), array([0.], dtype=float32)]
layer_b.set_weights(layer_a.get_weights())
layer_b.get_weights()
[array([[1.],
       [1.],
       [1.]], dtype=float32), array([0.], dtype=float32)]""" .

"DESCRIPTION.The code creates two Peephole LSTM cells with sizes 128 and 256, then creates a layer composed sequentially of these Peephole LSTM cells, and finally applies the layer to the input data." <EXPLAINS> """CODE.# Create 2 PeepholeLSTMCells
peephole_lstm_cells = [PeepholeLSTMCell(size) for size in [128, 256]]
# Create a layer composed sequentially of the peephole LSTM cells.
layer = RNN(peephole_lstm_cells)
input = keras.Input((timesteps, input_dim))
output = layer(input)
""" .

"DESCRIPTION.The code creates two PyTorch tensors with float data type and requires gradient tracking, and then creates a nested tensor using these two tensors. The final output indicates that the nested tensor is a leaf node in the computation graph." <EXPLAINS> """CODE.a = torch.arange(3, dtype=torch.float, requires_grad=True)
b = torch.arange(5, dtype=torch.float, requires_grad=True)
nt = torch.nested.nested_tensor([a, b], requires_grad=True)
nt.is_leaf
True
""" .

"DESCRIPTION.The code creates two Sequential models in PaddlePaddle's fluid framework, where each model is made up of Linear layers. The first model is created using an iterable of Layers while the second model is created using name-layer pairs. Both models are then used to perform forward pass on the input data array." <EXPLAINS> """CODE.import paddle.fluid as fluid
import numpy as np

data = np.random.uniform(-1, 1, [30, 10]).astype('float32')
with fluid.dygraph.guard():
    data = fluid.dygraph.to_variable(data)
    # create Sequential with iterable Layers
    model1 = fluid.dygraph.Sequential(
        fluid.Linear(10, 1), fluid.Linear(1, 2)
    )
    model1[0]  # access the first layer
    res1 = model1(data)  # sequential execution

    # create Sequential with name Layer pairs
    model2 = fluid.dygraph.Sequential(
        ('l1', fluid.Linear(10, 2)),
        ('l2', fluid.Linear(2, 3))
    )
    model2['l1']  # access l1 layer
    model2.add_sublayer('l3', fluid.Linear(3, 3))  # add sublayer
    res2 = model2(data)  # sequential execution""" .

"DESCRIPTION.The code creates two affine linear operators, each of which performs a linear transformation on input vector x and outputs y. The first operator applies a shift followed by a scaling operation using a diagonal matrix, while the second operator applies a shift followed by a lower triangular matrix multiplication." <EXPLAINS> """CODE.linalg = tf.contrib.linalg

x = [1., 2, 3]

shift = [-1., 0., 1]
diag = [1., 2, 3]
scale = linalg.LinearOperatorDiag(diag)
affine = AffineLinearOperator(shift, scale)
# In this case, `forward` is equivalent to:
# diag * scale + shift
y = affine.forward(x)  # [0., 4, 10]

shift = [2., 3, 1]
tril = [[1., 0, 0],
        [2, 1, 0],
        [3, 2, 1]]
scale = linalg.LinearOperatorTriL(tril)
affine = AffineLinearOperator(shift, scale)
# In this case, `forward` is equivalent to:
# np.squeeze(np.matmul(tril, np.expand_dims(x, -1)), -1) + shift
y = affine.forward(x)  # [3., 7, 11]
""" .

"DESCRIPTION.The code creates two arrays, y_left and y_right, filled with ones and negative ones respectively. It then stacks these arrays horizontally to create a stereo signal representation and prints the result. It finally stacks the arrays vertically to create another stereo signal representation and prints the result." <EXPLAINS> """CODE.y_left = np.ones(5)
y_right = -np.ones(5)
y_stereo = librosa.util.stack([y_left, y_right], axis=0)
y_stereo
array([[ 1.,  1.,  1.,  1.,  1.],
       [-1., -1., -1., -1., -1.]])
y_stereo.flags

y_stereo = librosa.util.stack([y_left, y_right], axis=-1)
y_stereo
array([[ 1., -1.],
       [ 1., -1.],
       [ 1., -1.],
       [ 1., -1.],
       [ 1., -1.]])
y_stereo.flags
""" .

"DESCRIPTION.The code creates two batches of VectorDiffeomixtures with specified mix_loc and mix_scale values. Each VectorDiffeomixture instance is defined with a Normal distribution, affine transformations involving loc and scale parameters, and a specified number of dimensions." <EXPLAINS> """CODE.ds = tf.contrib.distributions
la = tf.contrib.linalg

# Create two batches of VectorDiffeomixtures, one with mix_loc=[0.] and
# another with mix_loc=[1]. In both cases, `K=2` and the affine
# transformations involve:
# k=0: loc=zeros(dims)  scale=LinearOperatorScaledIdentity
# k=1: loc=[2.]*dims    scale=LinOpDiag
dims = 5
vdm = ds.VectorDiffeomixture(
    mix_loc=[[0.], [1]],
    mix_scale=[1.],
    distribution=ds.Normal(loc=0., scale=1.),
    loc=[
        None,  # Equivalent to `np.zeros(dims, dtype=np.float32)`.
        np.float32([2.]*dims),
    ],
    scale=[
        la.LinearOperatorScaledIdentity(
          num_rows=dims,
          multiplier=np.float32(1.1),
          is_positive_definite=True),
        la.LinearOperatorDiag(
          diag=np.linspace(2.5, 3.5, dims, dtype=np.float32),
          is_positive_definite=True),
    ],
    validate_args=True)
""" .

"DESCRIPTION.The code creates two classifier objects, a Logistic Regression model (clf1) and a Random Forest Classifier (clf2), and combines them into a Voting Classifier (eclf). It then sets the RandomForestClassifier parameter to None." <EXPLAINS> """CODE.clf1 = LogisticRegression()
clf2 = RandomForestClassifier()
eclf = VotingClassifier(estimators=[('lr', clf1), ('rf', clf2)]
eclf.set_params(rf=None)""" .

"DESCRIPTION.The code creates two classifiers, a Logistic Regression classifier and a Random Forest classifier. It then creates a Voting Classifier that combines the predictions of the two classifiers. The code sets the Random Forest classifier parameter to None." <EXPLAINS> """CODE.clf1 = LogisticRegression()
clf2 = RandomForestClassifier()
eclf = VotingClassifier(estimators=[('lr', clf1), ('rf', clf2)]
eclf.set_params(rf=None)""" .

"DESCRIPTION.The code creates two constant matrices \"a\" and \"b\" using TensorFlow, then stacks them along a new axis using tf.keras.backend.stack()." <EXPLAINS> """CODE.a = tf.constant([[1, 2],[3, 4]])
b = tf.constant([[10, 20],[30, 40]])
tf.keras.backend.stack((a, b))""" .

"DESCRIPTION.The code creates two constant tensors 'a' and 'b', and then stacks them together along a new axis using the tf.keras.backend.stack function." <EXPLAINS> """CODE.a = tf.constant([[1, 2],[3, 4]])
b = tf.constant([[10, 20],[30, 40]])
tf.keras.backend.stack((a, b))""" .

"DESCRIPTION.The code creates two data loaders with different batch sizes and then combines them using a custom CombinedLoader class. The CombinedLoader class has two modes of operation: 'max_size_cycle' mode cycles through batches until all loaders are exhausted, while 'min_size' mode combines batches by the minimum batch size of all loaders. The code then iterates through the combined loader and prints each item." <EXPLAINS> """CODE.loaders = {'a': torch.utils.data.DataLoader(range(6), batch_size=4),
            'b': torch.utils.data.DataLoader(range(15), batch_size=5)}
combined_loader = CombinedLoader(loaders, 'max_size_cycle')
for item in combined_loader:
    print(item)
combined_loader = CombinedLoader(loaders, 'min_size')
for item in combined_loader:
    print(item)""" .

"DESCRIPTION.The code creates two datasets from lists of numbers, zips them together into a single dataset as tuples, and then unzips the tuples back into two separate datasets. It then zips the datasets into a single dataset as a dictionary and unzips them back into two separate datasets." <EXPLAINS> """CODE.ds1 = tf.data.Dataset.from_tensor_slices([1, 2, 3])
ds2 = tf.data.Dataset.from_tensor_slices([4, 5, 6])
ds_zipped_tuple = tf.data.Dataset.zip((ds1, ds2))
ds_unzipped_tuple = _unzip_dataset(ds_zipped_tuple)
ds_zipped_dict = tf.data.Dataset.zip({'ds1': ds1, 'ds2': ds2})
ds_unzipped_dict = _unzip_dataset(ds_zipped_dict)""" .

"DESCRIPTION.The code creates two different types of files: one with a .txt extension containing \"line1\" and \"line2\" in the content, and another with a .ini extension containing configuration settings for pytest to display test 'rs' output." <EXPLAINS> """CODE.pytester.makefile(".txt", "line1", "line2")
pytester.makefile(".ini", pytest="[pytest]\\naddopts=-rs\\n")""" .

"DESCRIPTION.The code creates two future objects (fut0, fut1), collects them into a single future object (fut), sets results for fut0 and fut1, waits for the completion of all futures in fut, and prints the results of fut0 and fut1." <EXPLAINS> """CODE.import torch

fut0 = torch.futures.Future()
fut1 = torch.futures.Future()

fut = torch.futures.collect_all([fut0, fut1])

fut0.set_result(0)
fut1.set_result(1)

fut_list = fut.wait()
print(f"fut0 result = {fut_list[0].wait()}")
print(f"fut1 result = {fut_list[1].wait()}")""" .

"DESCRIPTION.The code creates two instances of LinearOperatorTridiag class from TensorFlow, which represents diagonal tridiagonal matrices. The first instance is created with specified superdiagonal, diagonal, and subdiagonal values, and the second instance is created with randomly generated diagonal values. It then calculates the determinant of the first matrix, solves a linear system for the second matrix, and returns the solution x." <EXPLAINS> """CODE.superdiag = [3., 4., 5.]
diag = [1., -1., 2.]
subdiag = [6., 7., 8]
operator = tf.linalg.LinearOperatorTridiag(
   [superdiag, diag, subdiag],
   diagonals_format='sequence')
operator.to_dense()
operator.shape
operator.log_abs_determinant()

diagonals = tf.random.normal(shape=[2, 3, 3, 4])
operator = tf.linalg.LinearOperatorTridiag(
   diagonals,
   diagonals_format='compact')

y = tf.random.normal(shape=[2, 1, 4, 2])
x = operator.solve(y)
x
""" .

"DESCRIPTION.The code creates two lists of linear layers, then extends the first list with the elements from the second list. Finally, it prints the length of the merged list and checks if the first element of the second list is the same as the eleventh element of the merged list." <EXPLAINS> """CODE.import paddle.fluid as fluid

with fluid.dygraph.guard():
    linears = fluid.dygraph.LayerList([fluid.dygraph.Linear(10, 10) for i in range(10)])
    another_list = fluid.dygraph.LayerList([fluid.dygraph.Linear(10, 10) for i in range(5)])
    linears.extend(another_list)
    print(len(linears))  # 15
    print(another_list[0] is linears[10])  # True""" .

"DESCRIPTION.The code creates two pandas Series, one with keys as strings and values as integers, and the other with keys as integers and values as strings. It then maps the values of the first Series to the values of the second Series based on matching keys." <EXPLAINS> """CODE.x = pd.Series({'one': 1, 'two': 2, 'three': 3})
y = pd.Series({1: 'foo', 2: 'bar', 3: 'baz'})
x.map(y)""" .

"DESCRIPTION.The code creates two placeholder tensors of shape (2, 2) with different sparsity settings, and then checks if each tensor is sparse or not." <EXPLAINS> """CODE.from keras import backend as K
a = K.placeholder((2, 2), sparse=False)
print(K.is_sparse(a))
False
b = K.placeholder((2, 2), sparse=True)
print(K.is_sparse(b))
True
""" .

"DESCRIPTION.The code creates two placeholders of shape (2, 2) using TensorFlow's Keras backend, with the first placeholder set as non-sparse (sparse=False) and the second placeholder set as sparse (sparse=True). The code then checks if each placeholder is sparse or not, and prints a boolean value indicating whether each placeholder is sparse or not." <EXPLAINS> """CODE.a = tf.keras.backend.placeholder((2, 2), sparse=False)
print(tf.keras.backend.is_sparse(a))
False
b = tf.keras.backend.placeholder((2, 2), sparse=True)
print(tf.keras.backend.is_sparse(b))
True""" .

"DESCRIPTION.The code creates two sets of data with shape [20] and adds them together to create a new set of data. It then waits for the default stream to finish before adding the original data1 set to the new data set." <EXPLAINS> """CODE.import paddle

paddle.set_device('custom_cpu')
s = paddle.device.Stream()
data1 = paddle.ones(shape=[20])
data2 = paddle.ones(shape=[20])
data3 = data1 + data2
with paddle.device.stream_guard(s):
    s.wait_stream(paddle.device.default_stream())
    data4 = data1 + data3""" .

"DESCRIPTION.The code creates two sparse columns using hash bucketing, then crosses the two sparse columns to create a new feature. A LinearEstimator is created with the feature columns consisting of the two sparse columns and the crossed feature. The estimator is trained and evaluated using input functions and used to make predictions." <EXPLAINS> """CODE.sparse_column_a = sparse_column_with_hash_bucket(...)
sparse_column_b = sparse_column_with_hash_bucket(...)

sparse_feature_a_x_sparse_feature_b = crossed_column(...)

estimator = LinearEstimator(
    feature_columns=[sparse_column_a, sparse_feature_a_x_sparse_feature_b],
    head=head_lib.poisson_regression_head())

# Input builders
def input_fn_train: # returns x, y
  ...
def input_fn_eval: # returns x, y
  ...
estimator.fit(input_fn=input_fn_train)
estimator.evaluate(input_fn=input_fn_eval)
estimator.predict(x=x)
""" .

"DESCRIPTION.The code creates two stream objects, s1 and s2, along with an event object e. The event e is recorded on stream s1, and stream s2 waits for the event to complete before continuing execution." <EXPLAINS> """CODE.import paddle
s1 = paddle.device.Stream()
s2 = paddle.device.Stream()
e = paddle.device.Event()
e.record(s1)
s2.wait_event(e)""" .

"DESCRIPTION.The code creates two streams, s1 and s2, and then makes s1 wait for s2 to finish before continuing." <EXPLAINS> """CODE.import paddle
paddle.set_device('custom_cpu')
s1 = paddle.device.Stream()
s2 = paddle.device.Stream()
s1.wait_stream(s2)""" .

"DESCRIPTION.The code crops the input 2D images or feature maps before passing them through a Convolutional Neural Network (CNN) model. Specifically, it crops the input images by removing specified number of rows and columns from the top, bottom, left, and right sides. " <EXPLAINS> """CODE.    # Crop the input 2D images or feature maps
    model = Sequential()
    model.add(Cropping2D(cropping=((2, 2), (4, 4)), input_shape=(3, 28, 28)))
    # now model.output_shape == (None, 3, 24, 20)
    model.add(Convolution2D(64, 3, 3, border_mode='same))
    model.add(Cropping2D(cropping=((2, 2), (2, 2))))
    # now model.output_shape == (None, 64, 20, 16)
""" .

"DESCRIPTION.The code crops the input 2D images or feature maps by removing specified rows and columns from the input shape, and then adds a Conv2D layer to the model." <EXPLAINS> """CODE.    # Crop the input 2D images or feature maps
    model = Sequential()
    model.add(Cropping2D(cropping=((2, 2), (4, 4)),
                         input_shape=(28, 28, 3)))
    # now model.output_shape == (None, 24, 20, 3)
    model.add(Conv2D(64, (3, 3), padding='same'))
    model.add(Cropping2D(cropping=((2, 2), (2, 2)))
    # now model.output_shape == (None, 20, 16, 64)
""" .

"DESCRIPTION.The code decodes a sequence of emissions using the CTC decoder and returns the final hypothesis after processing each emission step by step." <EXPLAINS> """CODE.decoder = torchaudio.models.decoder.ctc_decoder(...)
decoder.decode_begin()
decoder.decode_step(emission1)
decoder.decode_step(emission2)
decoder.decode_end()
result = decoder.get_final_hypothesis()""" .

"DESCRIPTION.The code defines a 2x2 numpy array \"val\", creates a Keras variable \"kvar\" using the array, and then checks the size of the variable in TensorFlow." <EXPLAINS> """CODE.from keras import backend as K
val = np.array([[1, 2], [3, 4]])
kvar = K.variable(value=val)
K.size(inputs)
<tf.Tensor: id=9, shape=(), dtype=int32, numpy=4>
""" .

"DESCRIPTION.The code defines a Bayesian inference process using Hamiltonian Monte Carlo (HMC) to sample from a joint distribution. It sets up a prior distribution on weights and a likelihood distribution based on the given factors and weights. It then computes the target log probability using the prior and likelihood distributions. HMC is used to sample from the joint distribution, and sample statistics like mean and variance are computed from the samples." <EXPLAINS> """CODE.tfd = tf.contrib.distributions

def make_likelihood(true_variances):
  return tfd.MultivariateNormalDiag(
      scale_diag=tf.sqrt(true_variances))

dims = 10
dtype = np.float32
true_variances = tf.linspace(dtype(1), dtype(3), dims)
likelihood = make_likelihood(true_variances)

states, kernel_results = hmc.sample_chain(
    num_results=1000,
    target_log_prob_fn=likelihood.log_prob,
    current_state=tf.zeros(dims),
    step_size=0.5,
    num_leapfrog_steps=2,
    num_burnin_steps=500)

# Compute sample stats.
sample_mean = tf.reduce_mean(states, axis=0)
sample_var = tf.reduce_mean(
    tf.squared_difference(states, sample_mean),
    axis=0)


tfd = tf.contrib.distributions

def make_prior(dims, dtype):
  return tfd.MultivariateNormalDiag(
      loc=tf.zeros(dims, dtype))

def make_likelihood(weights, factors):
  return tfd.MultivariateNormalDiag(
      loc=tf.tensordot(weights, factors, axes=[[0], [-1]]))

# Setup data.
num_weights = 10
num_factors = 4
num_chains = 100
dtype = np.float32

prior = make_prior(num_weights, dtype)
weights = prior.sample(num_chains)
factors = np.random.randn(num_factors, num_weights).astype(dtype)
x = make_likelihood(weights, factors).sample(num_chains)

def target_log_prob(w):
  # Target joint is: `f(w) = p(w, x | factors)`.
  return prior.log_prob(w) + make_likelihood(w, factors).log_prob(x)

# Get `num_results` samples from `num_chains` independent chains.
chains_states, kernels_results = hmc.sample_chain(
    num_results=1000,
    target_log_prob_fn=target_log_prob,
    current_state=tf.zeros([num_chains, dims], dtype),
    step_size=0.1,
    num_leapfrog_steps=2,
    num_burnin_steps=500)

# Compute sample stats.
sample_mean = tf.reduce_mean(chains_states, axis=[0, 1])
sample_var = tf.reduce_mean(
    tf.squared_difference(chains_states, sample_mean),
    axis=[0, 1])
""" .

"DESCRIPTION.The code defines a Beta distribution with parameters [0.5, 0.5] and [0.5, 0.5]. It then calculates the log probability of a value 0.2 in the distribution. Next, it reinterprets the Beta distribution as an Independent distribution along axis 1 and calculates the log probability of values [0.2, 0.2]." <EXPLAINS> """CODE.import paddle
from paddle.distribution import independent

beta = paddle.distribution.Beta(paddle.to_tensor([0.5, 0.5]), paddle.to_tensor([0.5, 0.5]))
print(beta.batch_shape, beta.event_shape)
# (2,) ()
print(beta.log_prob(paddle.to_tensor(0.2)))
# Tensor(shape=[2], dtype=float32, place=Place(gpu:0), stop_gradient=True,
#        [-0.22843921, -0.22843921])
reinterpreted_beta = independent.Independent(beta, 1)
print(reinterpreted_beta.batch_shape, reinterpreted_beta.event_shape)
# () (2,)
print(reinterpreted_beta.log_prob(paddle.to_tensor([0.2,  0.2])))
# Tensor(shape=[1], dtype=float32, place=Place(gpu:0), stop_gradient=True,
#        [-0.45687842])""" .

"DESCRIPTION.The code defines a Beta distribution with parameters [0.5, 0.5], calculates the log probability of a value 0.2 under this distribution, then creates an independent distribution reinterpreted from the original Beta distribution with the batch shape of () and event shape of (2). It finally calculates the log probability of two values [0.2, 0.2] under the reinterpreted distribution." <EXPLAINS> """CODE.import paddle
from paddle.distribution import independent

beta = paddle.distribution.Beta(paddle.to_tensor([0.5, 0.5]), paddle.to_tensor([0.5, 0.5]))
print(beta.batch_shape, beta.event_shape)
# (2,) ()
print(beta.log_prob(paddle.to_tensor(0.2)))
# Tensor(shape=[2], dtype=float32, place=Place(gpu:0), stop_gradient=True,
#        [-0.22843921, -0.22843921])
reinterpreted_beta = independent.Independent(beta, 1)
print(reinterpreted_beta.batch_shape, reinterpreted_beta.event_shape)
# () (2,)
print(reinterpreted_beta.log_prob(paddle.to_tensor([0.2,  0.2])))
# Tensor(shape=[1], dtype=float32, place=Place(gpu:0), stop_gradient=True,
#        [-0.45687842])""" .

"DESCRIPTION.The code defines a BinaryCrossentropy loss function and computes the loss between two sets of binary values. It then creates a model using stochastic gradient descent optimizer and the BinaryCrossentropy loss function for training." <EXPLAINS> """CODE.bce = keras.losses.BinaryCrossentropy()
loss = bce([0., 0., 1., 1.], [1., 1., 1., 0.])
model = keras.Model(inputs, outputs)
model.compile('sgd', loss=keras.losses.BinaryCrossentropy())
""" .

"DESCRIPTION.The code defines a CLI (Command Line Interface) application using Click library in Python. It accepts an input argument with a default value of 23. When the application is run, it returns the sum of the result (42) and the input value." <EXPLAINS> """CODE.@click.group()
@click.option('-i', '--input', default=23)
def cli(input):
    return 42

@cli.resultcallback()
def process_result(result, input):
    return result + input""" .

"DESCRIPTION.The code defines a CloseMatch object that compares a given pattern with a parsed string. It allows for a certain number of mismatches between the pattern and the string. The function parseString compares the parsed string with the pattern, and returns the parsed string along with information about any mismatches found, such as their positions." <EXPLAINS> """CODE.patt = CloseMatch("ATCATCGAATGGA")
patt.parseString("ATCATCGAAXGGA") # -> (['ATCATCGAAXGGA'], {'mismatches': [[9]], 'original': ['ATCATCGAATGGA']})
patt.parseString("ATCAXCGAAXGGA") # -> Exception: Expected 'ATCATCGAATGGA' (with up to 1 mismatches) (at char 0), (line:1, col:1)

# exact match
patt.parseString("ATCATCGAATGGA") # -> (['ATCATCGAATGGA'], {'mismatches': [[]], 'original': ['ATCATCGAATGGA']})

# close match allowing up to 2 mismatches
patt = CloseMatch("ATCATCGAATGGA", maxMismatches=2)
patt.parseString("ATCAXCGAAXGGA") # -> (['ATCAXCGAAXGGA'], {'mismatches': [[4, 9]], 'original': ['ATCATCGAATGGA']})""" .

"DESCRIPTION.The code defines a Color enum with values red, blue, and green. It creates a MyEntity class that has a color attribute, which can be set using values from the Color enum, color names as strings, scoped names as strings, or numbers. The code demonstrates setting the color attribute using different methods and asserts that the color attribute is green." <EXPLAINS> """CODE.    import enum
    from traitlets import HasTraits, UseEnum

    class Color(enum.Enum):
        red = 1         # -- IMPLICIT: default_value
        blue = 2
        green = 3

    class MyEntity(HasTraits):
        color = UseEnum(Color, default_value=Color.blue)

    entity = MyEntity(color=Color.red)
    entity.color = Color.green    # USE: Enum-value (preferred)
    entity.color = "green"        # USE: name (as string)
    entity.color = "Color.green"  # USE: scoped-name (as string)
    entity.color = 3              # USE: number (as int)
    assert entity.color is Color.green""" .

"DESCRIPTION.The code defines a CombinedStopper object that combines a MaximumIterationStopper with a TrialPlateauStopper. The MaximumIterationStopper limits the maximum number of iterations to 20, while the TrialPlateauStopper stops trials when a specified metric (\"my_metric\") stops improving. This combined stopper is then used in a tuning operation using the tune.run() function." <EXPLAINS> """CODE.from ray.tune.stopper import CombinedStopper, MaximumIterationStopper, TrialPlateauStopper

stopper = CombinedStopper(
    MaximumIterationStopper(max_iter=20),
    TrialPlateauStopper(metric="my_metric")
)

tune.run(train, stop=stopper)""" .

"DESCRIPTION.The code defines a ConvNetBuilder object and sets up a variable scope named 'cg' with a custom getter provided by the ConvNetBuilder object. Within this scope, convolutional operations are performed using the methods defined in the ConvNetBuilder object." <EXPLAINS> """CODE.network = ConvNetBuilder(...)
with tf.variable_scope('cg', custom_getter=network.get_custom_getter()):
  network.conv(...)
  # Call more methods of network here
""" .

"DESCRIPTION.The code defines a Convolutional Neural Network (CNN) model using TensorFlow's Keras API for image classification. The model consists of multiple convolutional and pooling layers to extract features from input images." <EXPLAINS> """CODE.import tensorflow as tf
model = tf.keras.Sequential()
model.add(tf.keras.layers.Conv2D(32, (3, 3), activation='elu', input_shape=(28, 28, 1))
model.add(tf.keras.layers.MaxPooling2D((2, 2))
model.add(tf.keras.layers.Conv2D(64, (3, 3), activation='elu'))
model.add(tf.keras.layers.MaxPooling2D((2, 2))
model.add(tf.keras.layers.Conv2D(64, (3, 3), activation='elu'))
""" .

"DESCRIPTION.The code defines a CounterModel class that represents a database table with columns 'id' and 'count'. It also defines a Flow class that inherits from LightningFlow, initializes a private token, a Database instance with CounterModel as a model, and a counter variable. The run method of the Flow class interacts with the database by running it, checking its status, updating, inserting, and deleting rows in the CounterModel table based on the value of the counter variable. The code also creates a LightningApp instance with an initialized Flow object." <EXPLAINS> """CODE.from typing import List
from sqlmodel import SQLModel, Field
from uuid import uuid4

from lightning_app import LightningFlow, LightningApp
from lightning_app.components.database import Database, DatabaseClient

class CounterModel(SQLModel, table=True):
    __table_args__ = {"extend_existing": True}

    id: int = Field(default=None, primary_key=True)
    count: int


class Flow(LightningFlow):

    def __init__(self):
        super().__init__()
        self._private_token = uuid4().hex
        self.db = Database(models=[CounterModel])
        self._client = None
        self.counter = 0

    def run(self):
        self.db.run(token=self._private_token)

        if not self.db.alive():
            return

        if self.counter == 0:
            self._client = DatabaseClient(
                model=CounterModel,
                db_url=self.db.url,
                token=self._private_token,
            )

        rows = self._client.select_all()

        print(f"{self.counter}: {rows}")

        if not rows:
            self._client.insert(CounterModel(count=0))
        else:
            row: CounterModel = rows[0]
            row.count += 1
            self._client.update(row)

        if self.counter >= 100:
            row: CounterModel = rows[0]
            self._client.delete(row)
            self._exit()

        self.counter += 1

app = LightningApp(Flow())

from typing import List
from sqlmodel import SQLModel, Field
from sqlalchemy import Column

from lightning_app.components.database.utilities import pydantic_column_type

class KeyValuePair(SQLModel):
    name: str
    value: str

class CounterModel(SQLModel, table=True):
    __table_args__ = {"extend_existing": True}

    name: int = Field(default=None, primary_key=True)

    # RIGHT THERE ! You need to use Field and Column with the `pydantic_column_type` utility.""" .

"DESCRIPTION.The code defines a DNNClassifier model in TensorFlow Estimator that takes two features 'feature1' and 'feature2'. It trains the model using the input function provided and then exports the trained model to be used for serving. The code also evaluates the model and makes predictions on new input data." <EXPLAINS> """CODE.feature1 = tf.feature_column.embedding_column(
    tf.feature_column.categorical_column_with_vocabulary_list(
        key='feature1', vocabulary_list=('green', 'yellow')), dimension=1)
feature2 = tf.feature_column.numeric_column(key='feature2', default_value=0.0)

classifier = tf.estimator.DNNClassifier(
    hidden_units=[4,2], feature_columns=[feature1, feature2])

def input_fn():
  features = {'feature1': tf.constant(['green', 'green', 'yellow']),
              'feature2': tf.constant([3.5, 4.2, 6.1])}
  label = tf.constant([1., 0., 0.])
  return tf.data.Dataset.from_tensors((features, label)).repeat()

classifier.train(input_fn=input_fn, steps=10)


supervised_input_receiver_fn = (
    tf.contrib.estimator.build_raw_supervised_input_receiver_fn(
        {'feature1': tf.placeholder(dtype=tf.string, shape=[None]),
         'feature2': tf.placeholder(dtype=tf.float32, shape=[None])},
        tf.placeholder(dtype=tf.float32, shape=[None])))

serving_input_receiver_fn = (
    tf.estimator.export.build_parsing_serving_input_receiver_fn(
        tf.feature_column.make_parse_example_spec([feature1, feature2])))

export_dir = tf.contrib.estimator.export_all_saved_models(
    classifier, '/tmp/export_all',
    {tf.estimator.ModeKeys.TRAIN: supervised_input_receiver_fn,
     tf.estimator.ModeKeys.EVAL: supervised_input_receiver_fn,
     tf.estimator.ModeKeys.PREDICT: serving_input_receiver_fn})

export_dir = classifier.export_savedmodel(
    '/tmp/export_predict', serving_input_receiver_fn)

est = tf.contrib.estimator.SavedModelEstimator(export_dir)

eval_results = est.evaluate(input_fn=input_fn, steps=1)
print(eval_results)

est.train(input_fn=input_fn, steps=20)

def predict_input_fn():
  example = tf.train.Example()
  example.features.feature['feature1'].bytes_list.value.extend(['yellow'])
  example.features.feature['feature2'].float_list.value.extend([1.])
  return {'inputs':tf.constant([example.SerializeToString()])}

predictions = est.predict(predict_input_fn)
print(next(predictions))
""" .

"DESCRIPTION.The code defines a Deep Q-Network (DQN) model for solving the CartPole-v1 environment in reinforcement learning. It includes a main network and a target network with identical architectures implemented as Sequential models." <EXPLAINS> """CODE.DQNLightning(env="CartPole-v1")
DQNLightning(
  (net): DQN(
    (net): Sequential(...)
  )
  (target_net): DQN(
    (net): Sequential(...)
  )
)""" .

"DESCRIPTION.The code defines a Deep Q-Network (DQN) model using PyTorch Lightning for solving the CartPole-v1 environment in reinforcement learning. It consists of a neural network (net) for the main DQN and a target_net for the target DQN." <EXPLAINS> """CODE.DQNLightning(env="CartPole-v1")
DQNLightning(
  (net): DQN(
    (net): Sequential(...)
  )
  (target_net): DQN(
    (net): Sequential(...)
  )
)""" .

"DESCRIPTION.The code defines a Deep Q-Network (DQN) with a neural network architecture." <EXPLAINS> """CODE.DQN(10, 5)
DQN(
  (net): Sequential(...)
)""" .

"DESCRIPTION.The code defines a Deep Q-Network (DQN) with a neural network model architecture consisting of sequential layers." <EXPLAINS> """CODE.DQN(10, 5)
DQN(
  (net): Sequential(...)
)""" .

"DESCRIPTION.The code defines a Deep Q-Network with an input size of 10 and an output size of 5. The network architecture is defined using a Sequential model." <EXPLAINS> """CODE.DQN(10, 5)
DQN(
  (net): Sequential(...)
)""" .

"DESCRIPTION.The code defines a Discriminator model with an input image shape of (1, 28, 28) using a Sequential neural network architecture." <EXPLAINS> """CODE.Discriminator(img_shape=(1, 28, 28))
Discriminator(
  (model): Sequential(...)
)""" .

"DESCRIPTION.The code defines a Discriminator model with input image shape of (1, 28, 28) and initializes it with a Sequential model." <EXPLAINS> """CODE.Discriminator(img_shape=(1, 28, 28))
Discriminator(
  (model): Sequential(...)
)""" .

"DESCRIPTION.The code defines a DoubleConv class that takes in two parameters (in this case, both set to 4) and initializes a Sequential neural network with unspecified layers inside it." <EXPLAINS> """CODE.DoubleConv(4, 4)
DoubleConv(
  (net): Sequential(...)
)""" .

"DESCRIPTION.The code defines a DoubleConv class with an input size of 4 and output size of 4. It contains a Sequential neural network model within the DoubleConv class." <EXPLAINS> """CODE.DoubleConv(4, 4)
DoubleConv(
  (net): Sequential(...)
)""" .

"DESCRIPTION.The code defines a FalseNegatives metric using TensorFlow's Keras module. It computes the number of false negatives in a classification problem by comparing predicted values with true labels, and can also consider sample weights for different instances. The metric can be used during model training to monitor the performance of the model in terms of false negatives." <EXPLAINS> """CODE.m = tf.keras.metrics.FalseNegatives()
m.update_state([0, 1, 1, 1], [0, 1, 0, 0])
m.result().numpy()

m.reset_state()
m.update_state([0, 1, 1, 1], [0, 1, 0, 0], sample_weight=[0, 0, 1, 0])
m.result().numpy()

model.compile(optimizer='sgd',
              loss='mse',
              metrics=[tf.keras.metrics.FalseNegatives()])
""" .

"DESCRIPTION.The code defines a FastAPI application named \"app\" and creates a deployment using the serve library for serving the app. It then defines a class \"App\" which inherits from the deployment and ingress decorators. Finally, it calls the deploy method on the App class." <EXPLAINS> """CODE.app = FastAPI()
@serve.deployment
    @serve.ingress(app)
    class App:
        pass
App.deploy()""" .

"DESCRIPTION.The code defines a GCS (Google Cloud Storage) subscriber object, subscribes to error messages, continuously polls for error messages while a certain condition \"running\" is met, and closes the subscriber after the loop ends." <EXPLAINS> """CODE.subscriber = GcsSubscriber()
subscriber.subscribe_error()
while running:
    error_id, error_data = subscriber.poll_error()
    ......
subscriber.close()""" .

"DESCRIPTION.The code defines a Generative Adversarial Network (GAN) with a generator and a discriminator. The generator creates new data samples while the discriminator tries to distinguish between real and generated samples, leading to a training process to improve the generator's ability to produce realistic data." <EXPLAINS> """CODE.GAN(img_shape=(1, 8, 8))
GAN(
  (generator): Generator(
    (model): Sequential(...)
  )
  (discriminator): Discriminator(
    (model): Sequential(...)
  )
)""" .

"DESCRIPTION.The code defines a Generative Adversarial Network (GAN) with a generator and discriminator components for generating and discriminating images respectively. The GAN takes an input image shape of 1 channel and 8x8 size." <EXPLAINS> """CODE.GAN(img_shape=(1, 8, 8))
GAN(
  (generator): Generator(
    (model): Sequential(...)
  )
  (discriminator): Discriminator(
    (model): Sequential(...)
  )
)""" .

"DESCRIPTION.The code defines a Generative Adversarial Network (GAN) with a generator and discriminator model. The generator model generates fake images based on random noise input, while the discriminator model distinguishes between real and fake images. The GAN is trained to improve the generator's ability to produce realistic images that can fool the discriminator." <EXPLAINS> """CODE.GAN(img_shape=(1, 8, 8))
(generator): Generator(
(model): Sequential(...)
)
(discriminator): Discriminator(
(model): Sequential(...)
)""" .

"DESCRIPTION.The code defines a Generator class with a model consisting of a Sequential neural network for generating images with a shape of 1x8x8." <EXPLAINS> """CODE.Generator(img_shape=(1, 8, 8))
Generator(
  (model): Sequential(...)
)""" .

"DESCRIPTION.The code defines a Generator model with input shape of 1 channel and 8x8 image size using a Sequential model for generating output images." <EXPLAINS> """CODE.Generator(img_shape=(1, 8, 8))
Generator(
  (model): Sequential(...)
)""" .

"DESCRIPTION.The code defines a ImageNetLightningModel class with a model attribute initialized as a ResNet model." <EXPLAINS> """CODE.ImageNetLightningModel(data_path='missing')
ImageNetLightningModel(
  (model): ResNet(...)
)""" .

"DESCRIPTION.The code defines a Keras model with specified inputs and outputs, compiles the model using stochastic gradient descent optimizer, mean squared error loss function, and sensitivity at a specific specificity metric." <EXPLAINS> """CODE.model = keras.Model(inputs, outputs)
model.compile(
    'sgd',
    loss='mse',
    metrics=[keras.metrics.SensitivityAtSpecificity()])
""" .

"DESCRIPTION.The code defines a LIT autoencoder model with an encoder and decoder components." <EXPLAINS> """CODE.LitAutoEncoder()  # doctest: +ELLIPSIS +NORMALIZE_WHITESPACE
LitAutoEncoder(
  (encoder): ...
  (decoder): ...
)""" .

"DESCRIPTION.The code defines a Laplace distribution with a mean of 0.0 and a scale of 1.0 using PaddlePaddle. It then calculates the inverse cumulative distribution function (icdf) for a given value of 0.1." <EXPLAINS> """CODE.import paddle

m = paddle.distribution.Laplace(paddle.to_tensor(0.0), paddle.to_tensor(1.0))
value = paddle.to_tensor(0.1)
m.icdf(value)""" .

"DESCRIPTION.The code defines a LinearOperatorCirculant using a given spectrum. The spectrum is used to generate a convolution kernel which is then used to create a dense representation of the operator. The code also demonstrates examples of creating operators with Hermitian spectrum and forcing the operator to have specific properties such as being self-adjoint, positive definite, or real." <EXPLAINS> """CODE.# spectrum is real ==> operator is self-adjoint
# spectrum is positive ==> operator is positive definite
spectrum = [6., 4, 2]

operator = LinearOperatorCirculant(spectrum)

# IFFT[spectrum]
operator.convolution_kernel()
==> [4 + 0j, 1 + 0.58j, 1 - 0.58j]

operator.to_dense()
==> [[4 + 0.0j, 1 - 0.6j, 1 + 0.6j],
     [1 + 0.6j, 4 + 0.0j, 1 - 0.6j],
     [1 - 0.6j, 1 + 0.6j, 4 + 0.0j]]

# Example of defining in terms of a real convolution kernel

# convolution_kernel is real ==> spectrum is Hermitian.
convolution_kernel = [1., 2., 1.]]
spectrum = tf.fft(tf.cast(convolution_kernel, tf.complex64))

# spectrum is Hermitian ==> operator is real.
# spectrum is shape [3] ==> operator is shape [3, 3]
# We force the input/output type to be real, which allows this to operate
# like a real matrix.
operator = LinearOperatorCirculant(spectrum, input_output_dtype=tf.float32)

operator.to_dense()
==> [[ 1, 1, 2],
     [ 2, 1, 1],
     [ 1, 2, 1]]

# Example of Hermitian spectrum

# spectrum is shape [3] ==> operator is shape [3, 3]
# spectrum is Hermitian ==> operator is real.
spectrum = [1, 1j, -1j]

operator = LinearOperatorCirculant(spectrum)

operator.to_dense()
==> [[ 0.33 + 0j,  0.91 + 0j, -0.24 + 0j],
     [-0.24 + 0j,  0.33 + 0j,  0.91 + 0j],
     [ 0.91 + 0j, -0.24 + 0j,  0.33 + 0j]]

# Example of forcing real `dtype` when spectrum is Hermitian

# spectrum is shape [4] ==> operator is shape [4, 4]
# spectrum is real ==> operator is self-adjoint
# spectrum is Hermitian ==> operator is real
# spectrum has positive real part ==> operator is positive-definite.
spectrum = [6., 4, 2, 4]

# Force the input dtype to be float32.
# Cast the output to float32. This is fine because the operator will be
# real due to Hermitian spectrum.
operator = LinearOperatorCirculant(spectrum, input_output_dtype=tf.float32)

operator.shape
==> [4, 4]

operator.to_dense()
==> [[4, 1, 0, 1],
     [1, 4, 1, 0],
     [0, 1, 4, 1],
     [1, 0, 1, 4]]

# convolution_kernel = tf.ifft(spectrum)
operator.convolution_kernel()
==> [4, 1, 0, 1]
""" .

"DESCRIPTION.The code defines a LitClassifier class with a backbone attribute set to an instance of the Backbone class." <EXPLAINS> """CODE.LitClassifier(Backbone())  # doctest: +NORMALIZE_WHITESPACE
LitClassifier(
  (backbone): ...
)""" .

"DESCRIPTION.The code defines a LitClassifier model with a Backbone model as its input." <EXPLAINS> """CODE.LitClassifier(Backbone())  # doctest: +ELLIPSIS +NORMALIZE_WHITESPACE
LitClassifier(
  (backbone): ...
)""" .

"DESCRIPTION.The code defines a LitClassifier object which takes a Backbone object as input." <EXPLAINS> """CODE.LitClassifier(Backbone())  # doctest: +ELLIPSIS +NORMALIZE_WHITESPACE
LitClassifier(
  (backbone): ...
)""" .

"DESCRIPTION.The code defines a Mean Squared Error (MSE) loss function using Keras, calculates the loss value between two arrays, creates a neural network model using Keras with specified inputs and outputs, and compiles the model using Stochastic Gradient Descent optimizer with the MSE loss function." <EXPLAINS> """CODE.mse = keras.losses.MeanSquaredError()
loss = mse([0., 0., 1., 1.], [1., 1., 1., 0.])
model = keras.Model(inputs, outputs)
model.compile('sgd', loss=keras.losses.MeanSquaredError())
""" .

"DESCRIPTION.The code defines a MirroredStrategy for distributed training across multiple GPUs. It creates a dataset with input and output tensors, repeats the dataset 100 times, and batches it using the global batch size of 16. The dataset is then distributed using the MirroredStrategy. The code also specifies the element specs for the distributed dataset, including the shape and data types of the tensors." <EXPLAINS> """CODE.global_batch_size = 16
strategy = tf.distribute.MirroredStrategy()
dataset = tf.data.Dataset.from_tensors(([1.],[2])).repeat(100).batch(global_batch_size)
dist_dataset = strategy.experimental_distribute_dataset(dataset)
dist_dataset.element_spec
(TensorSpec(shape=(None, 1), dtype=tf.float32, name=None),
 TensorSpec(shape=(None, 1), dtype=tf.int32, name=None))

strategy = tf.distribute.MirroredStrategy(['/gpu:0', '/gpu:1'])
(PerReplicaSpec(TensorSpec(shape=(None, 1), dtype=tf.float32, name=None),
                TensorSpec(shape=(None, 1), dtype=tf.float32, name=None)),
 PerReplicaSpec(TensorSpec(shape=(None, 1), dtype=tf.int32, name=None),
                TensorSpec(shape=(None, 1), dtype=tf.int32, name=None))""" .

"DESCRIPTION.The code defines a Mish activation function and applies it to a tensor x containing values -5.0, 0.0, and 5.0, resulting in the output tensor out with values approximately -0.03357624, 0.0, and 4.99955208." <EXPLAINS> """CODE.import paddle

x = paddle.to_tensor([-5., 0., 5.])
m = paddle.nn.Mish()
out = m(x) # [-0.03357624, 0., 4.99955208]""" .

"DESCRIPTION.The code defines a MultiDict object, sets a list of values for the key 'foo' in the MultiDict object, retrieves the first value for the key 'foo', and retrieves the full list of values for the key 'foo'." <EXPLAINS> """CODE.d = MultiDict()
d.setlist('foo', ['1', '2'])
d['foo']
'1'
d.getlist('foo')
['1', '2']""" .

"""DESCRIPTION.The code defines a MultiHeadAttention layer using Flax library in Python. It initializes the layer with a specific number of heads and features. It then generates random keys and shapes for queries, keys, and values, and initializes the layer with the random keys. The code applies the initialized layer with queries, keys, and values multiple times.

Additionally, the code defines a Module class that uses the MultiHeadAttention layer within its __call__ method. It initializes the module with attention related keyword arguments. It then applies the module with queries and dropout_rng multiple times, each time generating new random keys for dropout.""" <EXPLAINS> """CODE.import flax.linen as nn
import jax

layer = nn.MultiHeadAttention(num_heads=8, qkv_features=16)
key1, key2, key3, key4, key5, key6 = jax.random.split(jax.random.key(0), 6)
shape = (4, 3, 2, 5)
q, k, v = jax.random.uniform(key1, shape), jax.random.uniform(key2, shape), jax.random.uniform(key3, shape)
variables = layer.init(jax.random.key(0), q)

out = layer.apply(variables, q, k, v)
out = layer.apply(variables, q, k)
out = layer.apply(variables, q)

attention_kwargs = dict(
    num_heads=8,
    qkv_features=16,
    kernel_init=nn.initializers.ones,
    bias_init=nn.initializers.zeros,
    dropout_rate=0.5,
    deterministic=False,
    )
class Module(nn.Module):
  attention_kwargs: dict

  @nn.compact
  def __call__(self, x, dropout_rng=None):
    out1 = nn.MultiHeadAttention(**self.attention_kwargs)(x, dropout_rng=dropout_rng)
    out2 = nn.MultiHeadAttention(**self.attention_kwargs)(x, dropout_rng=dropout_rng)
    return out1, out2
module = Module(attention_kwargs)
variables = module.init({'params': key1, 'dropout': key2}, q)

out1, out2 = module.apply(variables, q, rngs={'dropout': key3})
out3, out4 = module.apply(variables, q, rngs={'dropout': key4})
out1, out2 = module.apply(variables, q, dropout_rng=key5)
out3, out4 = module.apply(variables, q, rngs={'dropout': key6}, dropout_rng=key5)
""" .

"DESCRIPTION.The code defines a Pandas Series object 's' containing nested lists of integers. It then flattens the nested lists into a single list." <EXPLAINS> """CODE.s = pd.Series(
...     [
...         [1, 2, 3],
...         [3],
...     ],
...     dtype=pd.ArrowDtype(pa.list_(
...         pa.int64()
...     ))
... )
s.list.flatten()""" .

"DESCRIPTION.The code defines a Parent class and a Child class that are generated based on a protocol message type. It also parses a protocol buffer file descriptor, creates a message descriptor, makes a class based on the descriptor, and initializes an instance of that class." <EXPLAINS> """CODE.class Parent(message.Message):
  __metaclass__ = GeneratedProtocolMessageType
  DESCRIPTOR = descriptor
  class Child(message.Message):
    __metaclass__ = GeneratedProtocolMessageType
    DESCRIPTOR = descriptor.nested_types[0]

file_descriptor = descriptor_pb2.FileDescriptorProto()
file_descriptor.ParseFromString(proto2_string)
msg_descriptor = descriptor.MakeDescriptor(file_descriptor.message_type[0])
msg_class = reflection.MakeClass(msg_descriptor)
msg = msg_class()""" .

"DESCRIPTION.The code defines a PlacerParams object with a hidden_size parameter set to 128 and a decay_steps parameter set to 50. It then accesses and prints the values of the hidden_size and decay_steps parameters from the PlacerParams object." <EXPLAINS> """CODE.params = PlacerParams(hidden_size=128, decay_steps=50)
hparams.hidden_size ==> 128
hparams.decay_steps ==> 50
""" .

"DESCRIPTION.The code defines a Python class called ExampleModule that inherits from DeviceDtypeModuleMixin. The class initializes with a weight tensor and registers it as a buffer. The code then demonstrates the usage of the ExampleModule class by creating an instance, changing the data type of the weight tensor, and moving the tensor to different devices." <EXPLAINS> """CODE.class ExampleModule(DeviceDtypeModuleMixin):
    def __init__(self, weight: torch.Tensor):
        super().__init__()
        self.register_buffer('weight', weight)
_ = torch.manual_seed(0)
module = ExampleModule(torch.rand(3, 4))
module.weight #doctest: +ELLIPSIS
tensor([[...]])
module.to(torch.double)
ExampleModule()
module.weight #doctest: +ELLIPSIS
tensor([[...]], dtype=torch.float64)
cpu = torch.device('cpu')
module.to(cpu, dtype=torch.half, non_blocking=True)
ExampleModule()
module.weight #doctest: +ELLIPSIS
tensor([[...]], dtype=torch.float16)
module.to(cpu)
ExampleModule()
module.weight #doctest: +ELLIPSIS
tensor([[...]], dtype=torch.float16)
""" .

"DESCRIPTION.The code defines a Python class called Flow which inherits from LightningFlow. Inside the class, there is a method called configure_layout which returns different types of frontends based on the configuration. It can return a StaticWebFrontend pointing to a specific folder, a StreamlitFrontend with a custom UI function, or a list of dictionaries each representing a tab with content such as child components or direct URLs." <EXPLAINS> """CODE.from lightning_app.frontend import StaticWebFrontend

class Flow(LightningFlow):
    ...

    def configure_layout(self):
        return StaticWebFrontend("path/to/folder/to/serve")

from lightning_app.frontend import StaticWebFrontend

class Flow(LightningFlow):
    ...

    def configure_layout(self):
        return StreamlitFrontend(render_fn=my_streamlit_ui)


def my_streamlit_ui(state):
    # add your streamlit code here!
    import streamlit as st

    st.button("Hello!")

class Flow(LightningFlow):
    ...

    def configure_layout(self):
        return [
            dict(name="First Tab", content=self.child0),
            dict(name="Second Tab", content=self.child1),
            # You can include direct URLs too
            dict(name="Lightning", content="https://lightning.ai"),
        ]
""" .

"DESCRIPTION.The code defines a Python function called \"my_cool_method\" that takes a string input \"repo_id\" and prints it. The function is decorated with the \"@validate_hf_hub_args\" decorator, which validates the input \"repo_id\" to ensure it does not contain certain characters like \"--\" or \"..\". If the input string violates the validation rules, an exception \"HFValidationError\" is raised." <EXPLAINS> """CODE.from huggingface_hub.utils import validate_hf_hub_args

@validate_hf_hub_args
... def my_cool_method(repo_id: str):
...     print(repo_id)

my_cool_method(repo_id="valid_repo_id")
valid_repo_id

my_cool_method("other..repo..id")
huggingface_hub.utils._validators.HFValidationError: Cannot have -- or .. in repo_id: 'other..repo..id'.

my_cool_method(repo_id="other..repo..id")
huggingface_hub.utils._validators.HFValidationError: Cannot have -- or .. in repo_id: 'other..repo..id'.
""" .

"DESCRIPTION.The code defines a Recurrent Neural Network (RNN) classifier using LSTM cells with sequence features represented by token embeddings. It trains the RNN classifier using a training input function, evaluates the model using an evaluation input function, and makes predictions using a prediction input function." <EXPLAINS> """CODE.token_sequence = sequence_categorical_column_with_hash_bucket(...)
token_emb = embedding_column(categorical_column=token_sequence, ...)

estimator = RNNClassifier(
    num_units=[32, 16], cell_type='lstm',
    sequence_feature_columns=[token_emb])

# Input builders
def input_fn_train: # returns x, y
  pass
estimator.train(input_fn=input_fn_train, steps=100)

def input_fn_eval: # returns x, y
  pass
metrics = estimator.evaluate(input_fn=input_fn_eval, steps=10)
def input_fn_predict: # returns x, None
  pass
predictions = estimator.predict(input_fn=input_fn_predict)
""" .

"DESCRIPTION.The code defines a RegNet model using a specified configuration and retrieves the configuration of the model." <EXPLAINS> """CODE.from transformers import RegNetConfig, RegNetModel
configuration = RegNetConfig()
model = RegNetModel(configuration)
configuration = model.config
""" .

"DESCRIPTION.The code defines a ResNet model configuration and initializes a ResNet model using that configuration. It then retrieves the configuration from the model." <EXPLAINS> """CODE.from transformers import ResNetConfig, ResNetModel
configuration = ResNetConfig()
model = ResNetModel(configuration)
configuration = model.config
""" .

"DESCRIPTION.The code defines a ReshapeTransform object that reshapes the input tensor from a shape of (1, 2, 3) to a shape of (2, 3). It then applies the transformation to the input tensor x and prints the output shape, the transformed tensor, the inverse of the transformation, and the log determinant of the Jacobian of the transformation." <EXPLAINS> """CODE.import paddle

x = paddle.ones((1,2,3))
reshape_transform = paddle.distribution.ReshapeTransform((2, 3), (3, 2))
print(reshape_transform.forward_shape((1,2,3)))
print(reshape_transform.forward(x))
print(reshape_transform.inverse(reshape_transform.forward(x)))
print(reshape_transform.forward_log_det_jacobian(x))""" .

"DESCRIPTION.The code defines a RootFlow class that contains two CounterWork instances within a Dict object. The CounterWork class has a counter attribute that is incremented by 1 every time its run method is called. The RootFlow class has a run method that iterates through the CounterWork instances in the Dict object and calls their run methods. After creating an instance of RootFlow and calling its run method, it asserts that the counter attribute of the first CounterWork instance is equal to 1." <EXPLAINS> """CODE.from lightning_app import LightningFlow, LightningWork
from lightning_app.core import Dict

class CounterWork(LightningWork):
    def __init__(self):
        super().__init__()
        self.counter = 0
    def run(self):
        self.counter += 1

class RootFlow(LightningFlow):
    def __init__(self):
        super().__init__()
        self.dict = Dict(**{"work_0": CounterWork(), "work_1": CounterWork()})
    def run(self):
        for work_name, work in self.dict.items():
            work.run()

flow = RootFlow()
flow.run()
assert flow.dict["work_0"].counter == 1
""" .

"DESCRIPTION.The code defines a RootFlow class which contains a dictionary of CounterWork instances. The CounterWork class has a counter attribute which gets incremented by 1 each time the run method is called. The RootFlow class has a run method that iterates over the CounterWork instances in the dictionary and calls their run methods. Finally, an instance of RootFlow is created and its run method is called, then it asserts that the counter attribute of the \"work_0\" CounterWork instance is equal to 1." <EXPLAINS> """CODE.from lightning_app import LightningFlow, LightningWork
from lightning_app.core import Dict

class CounterWork(LightningWork):
    def __init__(self):
        super().__init__()
        self.counter = 0
    def run(self):
        self.counter += 1

class RootFlow(LightningFlow):
    def __init__(self):
        super().__init__()
        self.dict = Dict(**{"work_0": CounterWork(), "work_1": CounterWork()})
    def run(self):
        for work_name, work in self.dict.items():
            work.run()

flow = RootFlow()
flow.run()
assert flow.dict["work_0"].counter == 1
""" .

"DESCRIPTION.The code defines a Sequential model and adds an Atrous Convolutional layer with 64 filters, 3x3 kernel size, and atrous rate of (2,2) to the model. The layer operates on input images with a shape of (3, 256, 256) without padding." <EXPLAINS> """CODE.model = Sequential()
model.add(AtrousConvolution2D(64, 3, 3, atrous_rate=(2,2), border_mode='valid', input_shape=(3, 256, 256)))
""" .

"DESCRIPTION.The code defines a Sequential model with two LocallyConnected1D layers. The first layer has 64 filters with a kernel size of 3 and takes input of shape (10, 32), while the second layer has 32 filters with a kernel size of 3." <EXPLAINS> """CODE.model = Sequential()
model.add(LocallyConnected1D(64, 3, input_shape=(10, 32)))
model.add(LocallyConnected1D(32, 3))
""" .

"DESCRIPTION.The code defines a ServableBoringModel class that inherits from BoringModel and ServableModule. The ServableBoringModel class provides methods to configure payload, serialization, serve step, and response. It also includes a serve callback and trainer for training the ServableBoringModel and asserts that the response JSON is equal to {\"output\": [0, 1]}." <EXPLAINS> """CODE.from typing import Dict, Any, Callable

import torch

from pytorch_lightning import Trainer
from pytorch_lightning.demos.boring_classes import BoringModel
from pytorch_lightning.serve.servable_module_validator import ServableModule, ServableModuleValidator


class ServableBoringModel(BoringModel, ServableModule):
    def configure_payload(self) -> Dict[str, Any]:
        return {"body": {"x": list(range(32))}}

    def configure_serialization(self) -> Tuple[Dict[str, Callable], Dict[str, Callable]]:
        def deserialize(x):
            return torch.tensor(x, dtype=torch.float)

        def serialize(x):
            return x.tolist()

        return {"x": deserialize}, {"output": serialize}

    def serve_step(self, x: torch.Tensor) -> Dict[str, torch.Tensor]:
        return {"output": torch.tensor([0, 1])}

    def configure_response(self):
        return {"output": [0, 1]}


serve_cb = ServableModuleValidator()
trainer = Trainer(
    max_epochs=1,
    limit_train_batches=2,
    limit_val_batches=0,
    callbacks=[serve_cb],
)
trainer.fit(ServableBoringModel())
assert serve_cb.resp.json() == {"output": [0, 1]}
""" .

"DESCRIPTION.The code defines a SiLU activation function and applies it to a randomly generated input tensor using PyTorch." <EXPLAINS> """CODE.m = nn.SiLU()
input = torch.randn(2)
output = m(input)
""" .

"DESCRIPTION.The code defines a SimpleRNN layer with 4 units. The first usage of the layer processes input data and produces an output tensor of shape `[32, 4]`. The second usage returns both the whole sequence output tensor of shape `[32, 10, 4]` and the final state tensor of shape `[32, 4]`." <EXPLAINS> """CODE.simple_rnn = tf.keras.layers.SimpleRNN(4)

output = simple_rnn(inputs)  # The output has shape `[32, 4]`.

simple_rnn = tf.keras.layers.SimpleRNN(
    4, return_sequences=True, return_state=True)

# whole_sequence_output has shape `[32, 10, 4]`.
# final_state has shape `[32, 4]`.
whole_sequence_output, final_state = simple_rnn(inputs)
""" .

"DESCRIPTION.The code defines a StickBreakingTransform object and performs forward, inverse, and forward_log_det_jacobian operations on the input tensor x using the defined transformation." <EXPLAINS> """CODE.import paddle

x = paddle.to_tensor([1.,2.,3.])
t = paddle.distribution.StickBreakingTransform()
print(t.forward(x))
print(t.inverse(t.forward(x)))
print(t.forward_log_det_jacobian(x))""" .

"DESCRIPTION.The code defines a SweepRun object with a history containing dictionaries of data points and a summary metrics dictionary. It then asserts that the maximum value of metric 'a' in the summary metrics is 50." <EXPLAINS> """CODE.run = SweepRun(history=[{'a': 1}, {'b': 3}, {'a': 2, 'b': 4}], summary_metrics={'a': 50})
assert run.metric_extremum('a', 'maximum') == 50""" .

"DESCRIPTION.The code defines a Tanh PyLayer class that performs forward and backward operations. In the forward operation, it calculates two values based on the input x. The first value, a, is the double of x, and the second value, b, is the triple of x. The forward operation marks a as non-differentiable and returns a and b. In the backward operation, it checks that the gradients of a and b match specific conditions and then returns the gradient of b. Finally, it creates an input x with a value of 1, sets it to be gradient-enabled, applies the Tanh PyLayer to x, computes the sum of b, and performs backward propagation on it." <EXPLAINS> """CODE.import paddle
from paddle.autograd import PyLayer
import numpy as np

class Tanh(PyLayer):
    @staticmethod
    def forward(ctx, x):
        a = x + x
        b = x + x + x
        ctx.mark_non_differentiable(a)
        return a, b

    @staticmethod
    def backward(ctx, grad_a, grad_b):
        assert np.equal(grad_a.numpy(), paddle.zeros([1]).numpy())
        assert np.equal(grad_b.numpy(), paddle.ones([1], dtype="float64").numpy())
        return grad_b

x = paddle.ones([1], dtype="float64")
x.stop_gradient = False
a, b = Tanh.apply(x)
b.sum().backward()""" .

"DESCRIPTION.The code defines a Tensor Processing Unit (TPU) Estimator with specific embedding and optimization parameters for training a machine learning model." <EXPLAINS> """CODE.estimator = tf.estimator.tpu.TPUEstimator(
    ...
    embedding_config_spec=tf.estimator.tpu.experimental.EmbeddingConfigSpec(
        ...
        optimization_parameters=tf.tpu.experimental.FtrlParameters(0.1),
        ...))""" .

"DESCRIPTION.The code defines a TensorFlow function \"f\" that takes an input tensor \"x\" and adds 1 to it. The function is compiled using just-in-time (JIT) compilation. Additionally, it also defines a TensorFlow function \"f\" that takes an input tensor \"x\" and adds a variable \"y\" (initialized as a tensor of zeros) to it. This function is also compiled using JIT compilation. The function \"f\" is then called to get the compiler intermediate representation (IR) in the HLO (High Level Optimizer) stage for the specified input shape." <EXPLAINS> """CODE.@tf.function(jit_compile=True)
def f(x):
    return x + 1

f.experimental_get_compiler_ir(tf.random.normal([10, 10])(stage='hlo')


y = tf.Variable(tf.zeros([10, 20], dtype=tf.float32))

@tf.function(jit_compile=True)
def f(x):
    return x + y

hlo_str = f.experimental_get_compiler_ir(tf.TensorSpec(shape=(10, 20)))(stage='hlo')
""" .

"DESCRIPTION.The code defines a TensorFlow function \"replica_fn\" that multiplies the input by 2.0, and uses a MirroredStrategy to distribute the computation across multiple devices. It then defines a \"run\" function that extracts the number of replicas in sync from the strategy, distributes values using experimental_distribute_values_from_function, and runs another replica function \"replica_fn2\" which multiplies the input by 2 on the distributed values. Finally, it executes the \"run\" function and stores the result." <EXPLAINS> """CODE.@tf.function
def replica_fn(input):
    return input*2.0

strategy = tf.distribute.MirroredStrategy()
tensor_input = tf.constant(3.0)
result = strategy.run(replica_fn, args=(tensor_input,))

@tf.function
def run():
    def value_fn(value_context):
        return value_context.num_replicas_in_sync
    distributed_values = (
        strategy.experimental_distribute_values_from_function(
            value_fn))
    def replica_fn2(input):
        return input*2
    return strategy.run(replica_fn2, args=(distributed_values,))

result = run()""" .

"DESCRIPTION.The code defines a TensorFlow function 'f' that adds 1 to the input tensor x. It also defines a TensorFlow variable 'y' initialized as a tensor of zeros with shape (10, 20). The function 'f' is then modified to add the variable 'y' to the input tensor x. The code then generates the compiler intermediate representation (IR) using the experimental_get_compiler_ir method for both cases." <EXPLAINS> """CODE.@tf.function(jit_compile=True)
def f(x):
    return x + 1

f.experimental_get_compiler_ir(tf.random.normal([10, 10])(stage='hlo')


y = tf.Variable(tf.zeros([10, 20], dtype=tf.float32))

@tf.function(jit_compile=True)
def f(x):
    return x + y

hlo_str = f.experimental_get_compiler_ir(tf.TensorSpec(shape=(10, 20)))(stage='hlo')
""" .

"DESCRIPTION.The code defines a TensorFlow model and a function called 'serve' that serves as a wrapper for the model. The 'serve' function takes any number of positional and keyword arguments, passes them to the model, applies postprocessing steps, and returns the outputs. The code also defines argument and keyword argument specifications for the model and saves the model at a specified path, specifying the 'serving_default' signature using the 'serve' function." <EXPLAINS> """CODE.model = tf.keras.Model(...)

@tf.function
def serve(*args, **kwargs):
  outputs = model(*args, **kwargs)
  # Apply postprocessing steps, or add additional outputs.
  ...
  return outputs

# arg_specs is `[tf.TensorSpec(...), ...]`. kwarg_specs, in this example, is
# an empty dict since functional models do not use keyword arguments.
arg_specs, kwarg_specs = model.save_spec()

model.save(path, signatures={
  'serving_default': serve.get_concrete_function(*arg_specs, **kwarg_specs)
})
""" .

"DESCRIPTION.The code defines a TensorFlow model, creates a function called serve using tf.function decorator, which takes arbitrary arguments and keyword arguments, applies postprocessing steps to the model outputs, and returns the modified outputs. It then saves the model to a specified path with a concrete function that specifies the input arguments and keyword arguments for serving the model." <EXPLAINS> """CODE.model = tf.keras.Model(...)

@tf.function
def serve(*args, **kwargs):
  outputs = model(*args, **kwargs)
  # Apply postprocessing steps, or add additional outputs.
  ...
  return outputs

# arg_specs is `[tf.TensorSpec(...), ...]`. kwarg_specs, in this example, is
# an empty dict since functional models do not use keyword arguments.
arg_specs, kwarg_specs = model.save_spec()

model.save(path, signatures={
  'serving_default': serve.get_concrete_function(*arg_specs, **kwarg_specs)
})
""" .

"DESCRIPTION.The code defines a Token class with attributes such as type, value, and position. It demonstrates creating Token objects, getting and setting their states, accessing their attributes like start position and string value, and comparing string values with Unicode characters." <EXPLAINS> """CODE.repr(Token(1, "test", (1, 1)))
"<Token: ('NAME', 'test', (1, 1))>"
Token(1, 'bar', (3, 4)).__getstate__()
(1, 'bar', 3, 4)
a = Token(0, 'baz', (0, 0))
a.__setstate__((1, 'foo', 3, 4))
a
<Token: ('NAME', 'foo', (3, 4))>
a.start_pos
(3, 4)
a.string
'foo'
a._start_pos_col
4
Token(1, u("ð·"), (1 ,1)).string + "p" == u("ð·p")
True""" .

"DESCRIPTION.The code defines a TransformedDistribution using a Normal distribution with mean 0 and standard deviation 1, along with an AffineTransform transformation with scale 1 and shift 2. It then samples 10 samples from this distribution and calculates the log probability of a value 0.5 under this distribution." <EXPLAINS> """CODE.import paddle
from paddle.distribution import transformed_distribution

d = transformed_distribution.TransformedDistribution(
    paddle.distribution.Normal(0., 1.),
    [paddle.distribution.AffineTransform(paddle.to_tensor(1.), paddle.to_tensor(2.))]
)

print(d.sample([10]))
# Tensor(shape=[10], dtype=float32, place=Place(gpu:0), stop_gradient=True,
#        [-0.10697651,  3.33609009, -0.86234951,  5.07457638,  0.75925219,
#         -4.17087793,  2.22579336, -0.93845034,  0.66054249,  1.50957513])
print(d.log_prob(paddle.to_tensor(0.5)))
# Tensor(shape=[1], dtype=float32, place=Place(gpu:0), stop_gradient=True,
#        [-1.64333570])""" .

"DESCRIPTION.The code defines a UNet neural network architecture with 3 encoding and decoding layers for binary classification tasks. The network consists of DoubleConv, Down, and Up modules for feature extraction and the final layer is a 1x1 Convolutional layer for outputting 2 classes." <EXPLAINS> """CODE.UNet(num_classes=2, num_layers=3)
(layers): ModuleList(
(0): DoubleConv(...)
(1): Down(...)
(2): Down(...)
(3): Up(...)
(4): Up(...)
(5): Conv2d(64, 2, kernel_size=(1, 1), stride=(1, 1))""" .

"DESCRIPTION.The code defines a Version class that extracts the base version number from a given version string." <EXPLAINS> """CODE.Version("1.2.3").base_version
Version("1.2.3+abc").base_version
Version("1!1.2.3+abc.dev1").base_version""" .

"DESCRIPTION.The code defines a Version class with a constructor that takes a version string as input. The release attribute in the Version class is accessed to retrieve the release version of each specified version string." <EXPLAINS> """CODE.Version("1.2.3").release
Version("2.0.0").release
Version("1!2.0.0.post0").release""" .

"DESCRIPTION.The code defines a `ScaleAndShift` bijector in TensorFlow, which performs a forward operation on input tensor `X` by scaling it with the `scale` tensor and then adding the `shift` tensor. The bijector can be instantiated with different values for `shift`, `scale`, `event_ndims`, `validate_args`, and `name`." <EXPLAINS> """CODE.# Instantiates the `ScaleAndShift` bijector.
# This `Bijector` is initialized with `scale` and `shift` `Tensors`, giving
# the forward operation:
# Y = g(X) = matmul(scale, X) + shift

import tensorflow as tf

class ScaleAndShift(tf.Module):
    def __init__(self, shift, scale, event_ndims=0, validate_args=False, name=None):
        self.shift = shift
        self.scale = scale
        self.event_ndims = event_ndims
        self.validate_args = validate_args
        self.name = name

    def forward(self, X):
        return tf.matmul(self.scale, X) + self.shift

# Args:
shift = tf.constant(1.0)
scale = tf.constant([[2.0, 0.0], [0.0, 2.0]])
event_ndims = 1
validate_args = False
name = "scale_and_shift_bijector"

bijector = ScaleAndShift(shift, scale, event_ndims, validate_args, name)

# Args:
shift = tf.constant(0.0)
scale = tf.constant(2.0)
event_ndims = 0
validate_args = True
name = "another_scale_and_shift_bijector"

bijector2 = ScaleAndShift(shift, scale, event_ndims, validate_args, name)
""" .

"DESCRIPTION.The code defines a backbone neural network architecture with two linear layers (l1 and l2)." <EXPLAINS> """CODE.Backbone()
Backbone(
  (l1): Linear(...)
  (l2): Linear(...)
)""" .

"DESCRIPTION.The code defines a backbone neural network architecture with two linear layers." <EXPLAINS> """CODE.Backbone()
(l1): Linear(...)
(l2): Linear(...)  """ .

"DESCRIPTION.The code defines a basic TensorFlow model for a language translation task using GRU cells for decoding. The model uses an embedding layer, an output layer, a greedy embedding helper, and a decoder with GRU cell for decoding. The dynamic_decode function is used to perform decoding based on the defined model." <EXPLAINS> """CODE.import paddle.fluid as fluid
import paddle.fluid.layers as layers
trg_emb = fluid.data(name="trg_emb",
                     shape=[None, None, 128],
                     dtype="float32")

trg_embeder = lambda x: fluid.embedding(
    x, size=[10000, 128], param_attr=fluid.ParamAttr(name="trg_embedding"))
output_layer = lambda x: layers.fc(x,
                                size=10000,
                                num_flatten_dims=len(x.shape) - 1,
                                param_attr=fluid.ParamAttr(name=
                                                        "output_w"),
                                bias_attr=False)
helper = layers.GreedyEmbeddingHelper(trg_embeder, start_tokens=0, end_token=1)
decoder_cell = layers.GRUCell(hidden_size=128)
decoder = layers.BasicDecoder(decoder_cell, helper, output_fn=output_layer)
outputs = layers.dynamic_decode(
    decoder=decoder, inits=decoder_cell.get_initial_states(encoder_output))""" .

"DESCRIPTION.The code defines a binary cross-entropy loss function, calculates the loss between two sets of binary values, creates a neural network model, and compiles the model using stochastic gradient descent optimizer and the binary cross-entropy loss function." <EXPLAINS> """CODE.bce = keras.losses.BinaryCrossentropy()
loss = bce([0., 0., 1., 1.], [1., 1., 1., 0.])
model = keras.Model(inputs, outputs)
model.compile('sgd', loss=keras.losses.BinaryCrossentropy())
""" .

"DESCRIPTION.The code defines a boolean configuration state called 'jax_enable_foo' with a default value of False and a help message 'Enable foo'. Then it uses a context manager enable_foo(True) to enable some functionality within the context." <EXPLAINS> """CODE.config.define_bool_state(
      name='jax_enable_foo',
      default=False,
      help='Enable foo.')

with enable_foo(True):
    ...""" .

"DESCRIPTION.The code defines a bounded tensor specification with shape (1, 2, 3) and float data type, with minimum value 0 and maximum value (5, 5, 5). It then converts the minimum and maximum values of the specified tensor to TensorFlow tensors. Lastly, it redefines the bounded tensor specification with shape (3, 5), integer data type, minimum value 0, and maximum value 2." <EXPLAINS> """CODE.spec = tensor_spec.BoundedTensorSpec((1, 2, 3), tf.float32, 0, (5, 5, 5))
tf_minimum = tf.convert_to_tensor(spec.minimum, dtype=spec.dtype)
tf_maximum = tf.convert_to_tensor(spec.maximum, dtype=spec.dtype)

spec = tensor_spec.BoundedTensorSpec((3, 5), tf.int32, 0, 2)
""" .

"DESCRIPTION.The code defines a callback function \"cb\" that takes input data and distributes it across multiple devices using JAX. The main function \"gda\" creates a GlobalDeviceArray object from the input data based on the provided callback function and device configuration." <EXPLAINS> """CODE.global_input_shape = (8, 2)
global_input_data = np.arange(prod(global_input_shape), dtype=np.float32).reshape(global_input_shape)
def cb(cb_inp):
    self.assertLen(cb_inp, len(global_mesh.local_devices))
    dbs = []
    for inp in cb_inp:
        index, devices = inp
        array = global_input_data[index]
        dbs.extend([jax.device_put(array, device) for device in devices])
    return dbs
gda = GlobalDeviceArray.from_batched_callback_with_devices(global_input_shape, global_mesh, mesh_axes, cb)""" .

"DESCRIPTION.The code defines a callback function that takes in an observation, the next observation, reward, done flag, and additional info. It then creates an environment plotter object with the callback function, a time step of 30*5, and a list of plotted data (in this case, just \"reward\"). Finally, it creates a gym environment for playing the game \"Pong-v3\" and uses the callback function from the environment plotter during gameplay." <EXPLAINS> """CODE.def callback(obs_t, obs_tp1, rew, done, info):
    return [rew,]

env_plotter = EnvPlotter(callback, 30 * 5, ["reward"])

env = gym.make("Pong-v3")
play(env, callback=env_plotter.callback)""" .

"DESCRIPTION.The code defines a categorical column 'colors' with a vocabulary list and default value, creates a feature column list including 'colors', parses features using 'make_parse_example_spec', and generates linear predictions using a linear model. Then, it creates embedding columns for 'colors' and other columns, parses features again, and generates a dense tensor using the input layer." <EXPLAINS> """CODE.colors = categorical_column_with_vocabulary_list(
    key='colors', vocabulary_list=('X', 'R', 'G', 'B', 'Y'), default_value=0)
columns = [colors, ...]
features = tf.parse_example(..., features=make_parse_example_spec(columns))
linear_prediction, _, _ = linear_model(features, columns)

columns = [embedding_column(colors, 3),...]
features = tf.parse_example(..., features=make_parse_example_spec(columns))
dense_tensor = input_layer(features, columns)
""" .

"DESCRIPTION.The code defines a chain of transformations using the AffineTransform and ExpTransform distributions in the PaddlePaddle library. It applies the forward and inverse transformations on the input tensor x, calculates the log determinant Jacobian of both forward and inverse transformations, and prints the results." <EXPLAINS> """CODE.import paddle
x = paddle.to_tensor([0., 1., 2., 3.])
chain = paddle.distribution.ChainTransform((
    paddle.distribution.AffineTransform(
        paddle.to_tensor(0.), paddle.to_tensor(1.)),
    paddle.distribution.ExpTransform()
))
print(chain.forward(x))
print(chain.inverse(chain.forward(x)))
print(chain.forward_log_det_jacobian(x))
print(chain.inverse_log_det_jacobian(chain.forward(x)))""" .

"DESCRIPTION.The code defines a class \"CounterWork\" that has a counter attribute initialized to 0 and a method \"run\" that increments the counter. Another class \"RootFlow\" is defined that contains a list of CounterWork instances. When the \"run\" method of \"RootFlow\" is called, it iterates through the CounterWork instances in the list and calls their \"run\" method. Finally, an instance of \"RootFlow\" is created and its \"run\" method is called. After running, it asserts that the counter of the first CounterWork instance in the list is equal to 1." <EXPLAINS> """CODE.from lightning_app import LightningFlow, LightningWork
from lightning_app.core import List

class CounterWork(LightningWork):
    def __init__(self):
        super().__init__()
        self.counter = 0
    def run(self):
        self.counter += 1

class RootFlow(LightningFlow):
    def __init__(self):
        super().__init__()
        self.list = List(*[CounterWork(), CounterWork()])
    def run(self):
        for work in self.list:
            work.run()

flow = RootFlow()
flow.run()
assert flow.list[0].counter == 1""" .

"DESCRIPTION.The code defines a class \"Vehicle\" with attributes \"top_speed\" and \"mpg\" as TensorFlow Tensors. It then creates a batch of Vehicle instances with given top speeds and miles per gallon values. Finally, it calculates the product of top speed and mpg for each vehicle in the batch using tf.map_fn and returns the result as a numpy array of integers." <EXPLAINS> """CODE.class Vehicle(BatchableExtensionType):
    top_speed: tf.Tensor
    mpg: tf.Tensor

batch = Vehicle([120, 150, 80], [30, 40, 12])
tf.map_fn(lambda vehicle: vehicle.top_speed * vehicle.mpg, batch,
          fn_output_signature=tf.int32).numpy()""" .

"DESCRIPTION.The code defines a class Foo that inherits from Managed. It then creates an instance f of Foo using a with statement, and asserts that f is equal to the current instance of Foo." <EXPLAINS> """CODE.class Foo(Managed):
    pass

with Foo() as f:
    assert f == Foo.current()""" .

"DESCRIPTION.The code defines a class FruitTraceType with a method _placeholder_value that returns an instance of the Fruit class. The code also includes a function foo(x) that can take the placeholder value as an argument." <EXPLAINS> """CODE.class FruitTraceType:
  def _placeholder_value():
    return Fruit()


@tf.function
def foo(x):
  # Here `x` can be the placeholder value
  ...
""",
        """CODE.class FruitTraceType:
  def placeholder_value(self, placeholder_context=None):
    return Fruit()


@tf.function
def foo(x):
  # Here `x` is be the placeholder value
  ...
""" .

"DESCRIPTION.The code defines a class Resolution with a method get_reso that takes a string argument. When the method is called with the argument 'second', it returns the class attribute RESO_SEC." <EXPLAINS> """CODE.Resolution.get_reso('second')
Resolution.get_reso('second') == Resolution.RESO_SEC""" .

"DESCRIPTION.The code defines a class SchedulerDAG that extends LightningFlow. It initializes a list called dags. It has a method run which checks if the schedule is set to \"hourly\", it will append a DAG object to the dags list and then iterate through the list to run each DAG object." <EXPLAINS> """CODE.from lightning_app import LightningFlow

class Flow(LightningFlow):
    def run(self):
        if self.schedule("hourly"):
            print("run some code every hour")


from lightning_app import LightningFlow
from lightning_app.structures import List

class SchedulerDAG(LightningFlow):
    def __init__(self):
        super().__init__()
        self.dags = List()

    def run(self):
        if self.schedule("hourly"):
            self.dags.append(DAG(...))

        for dag in self.dags:
            payload = dag.run()
""" .

"DESCRIPTION.The code defines a class TestInference that is used for inference and skips the gradient checking when running inference." <EXPLAINS> """CODE.@skip_check_grad_ci(reason="For inference, check_grad is not required.")
class TestInference(OpTest):""" .

"DESCRIPTION.The code defines a class Trainer with a constructor that takes a configuration dictionary as input. The Trainer class has a method train_epoch() that is expected to execute some training logic. An instance of Trainer is created with a configuration dictionary and a group of worker instances created using the ray library. The workers execute the train_epoch() method remotely, and the code asserts that the return values are as expected. Finally, the workers are shut down." <EXPLAINS> """CODE.class Trainer:
    def __init__(self, config):
        self.config = config

    def train_epoch(self):
        ...
        return 1

config = {"lr": 0.1}
trainer = Trainer(num_workers=2, backend="torch")
workers = trainer.to_worker_group(train_cls=Trainer, config=config)
futures = [w.train_epoch.remote() for w in workers]
assert ray.get(futures) == [1, 1]
assert ray.get(workers[0].train_epoch.remote()) == 1
workers.shutdown()
""" .

"DESCRIPTION.The code defines a class TransferableDataType and checks if different objects are instances of the TransferableDataType class." <EXPLAINS> """CODE.class TransferableDataType:
    pass

import torch

class CustomObject:
    def __init__(self):
        self.x = torch.rand(2, 2)
    def to(self, device):
        self.x = self.x.to(device)
        return self

isinstance(dict, TransferableDataType)
isinstance(torch.rand(2, 3), TransferableDataType)
isinstance(CustomObject(), TransferableDataType)""" .

"DESCRIPTION.The code defines a class Version with a constructor that takes a version string as input. It then accesses the public attribute of the Version instance to perform some functionality related to version management." <EXPLAINS> """CODE.Version("1.2.3").public
Version("1.2.3+abc").public
Version("1.2.3+abc.dev1").public""" .

"DESCRIPTION.The code defines a class `Affine` and creates instances of the `Affine` class with different parameters such as `shift`, `scale_identity_multiplier`, `scale_diag`, `scale_perturb_factor`, and `scale_perturb_diag`. It generates a matrix `scale` using these parameters and operations involving them." <EXPLAINS> """CODE.scale = (
  scale_identity_multiplier * tf.diag(tf.ones(d)) +
  tf.diag(scale_diag) +
  scale_tril +
  scale_perturb_factor @ diag(scale_perturb_diag) @
    tf.transpose([scale_perturb_factor])
)

b = Affine()

b = Affine(shift=[1., 2, 3])

b = Affine(shift=[1., 2, 3],
           scale_identity_multiplier=2.)

b = Affine(shift=[1., 2, 3],
           scale_diag=[-1., 2, 1])

b = Affine(shift=[1., 2, 3],
           scale_perturb_factor=[[1., 0],
                                 [0, 1],
                                 [1, 1]])

b = Affine(shift=[1., 2, 3],
           scale_diag=[1., 3, 3],
           scale_perturb_diag=[2., 1],
           scale_perturb_factor=[[1., 0],
                                 [0, 1],
                                 [1, 1]])
""" .

"DESCRIPTION.The code defines a class `Flow` that extends `LightningFlow`, and initializes a counter. It also includes a function `run` to print the current counter value, and a function `configure_layout` that configures the layout using JustPyFrontend. Additionally, it defines a `render_fn` function that creates a web page with a clickable div element that increments the counter and displays the old and new counter values when clicked. Lastly, it creates a `LightningApp` instance using the `Flow` class." <EXPLAINS> """CODE.from typing import Callable
from lightning import LightningApp, LightningFlow
from lightning.app.frontend import JustPyFrontend


class Flow(LightningFlow):
    def __init__(self):
        super().__init__()
        self.counter = 0

    def run(self):
        print(self.counter)

    def configure_layout(self):
        return JustPyFrontend(render_fn=render_fn)


def render_fn(get_state: Callable) -> Callable:
    import justpy as jp

    def my_click(self, *_):
        state = get_state()
        old_counter = state.counter
        state.counter += 1
        self.text = f"Click Me ! Old Counter: {old_counter} New Counter: {state.counter}"

    def webpage():
        wp = jp.WebPage()
        d = jp.Div(text="Hello ! Click Me!")
        d.on("click", my_click)
        wp.add(d)
        return wp

    return webpage


app = LightningApp(Flow())
""" .

"DESCRIPTION.The code defines a class `MyGraphConverter` that inherits from `GraphConverter`, which contains methods for converting, calibrating, and saving a graph model. The `get_rewriter_config` method returns a rewriter configuration. The code then creates an instance of `MyGraphConverter` with a specified input saved model directory and performs conversion, calibration, and optionally saves the converted model to an output directory." <EXPLAINS> """CODE.class MyGraphConverter(GraphConverter):
  ...

  def get_rewriter_config(self, rewriter_config_template=None):
    my_rewriter_config = ...
    return my_rewriter_config

my_converter = MyGraphConverter(input_saved_model_dir="my_dir")
converted_graph_def = my_converter.convert()
my_converter.save(output_saved_model_dir)  # Optional

my_converter = MyGraphConverter(input_saved_model_dir="my_dir")
my_converter.convert()

# Run calibration 10 times.
converted_graph_def = my_converter.calibrate(
    fetch_names=['output:0'],
    num_runs=10,
    feed_dict_fn=lambda: {'input:0': my_next_data()})

my_converter.save(output_saved_model_dir)  # Optional
""" .

"DESCRIPTION.The code defines a class `RootFlow` that inherits from `LightningFlow` and contains a method `run` that increments the `counter` attribute of the class instance by 1. It creates an instance of `RootFlow`, runs the `run` method on the instance, and asserts that the `counter` attribute is equal to 1 and that the value of `counter` in the `state` dictionary is also 1." <EXPLAINS> """CODE.from lightning import LightningFlow
class RootFlow(LightningFlow):
    def __init__(self):
        super().__init__()
        self.counter = 0
    def run(self):
        self.counter += 1

flow = RootFlow()
flow.run()
assert flow.counter == 1
assert flow.state["vars"]["counter"] == 1
""" .

"DESCRIPTION.The code defines a class `RootFlow` that inherits from `LightningFlow`. The class has a `counter` attribute initialized to 0 and a method `run` that increments the `counter` attribute by 1. An instance `flow` of `RootFlow` is created, its `run` method is called, and assertions are made to verify that the `counter` attribute is equal to 1 and that the state variable `counter` is also equal to 1." <EXPLAINS> """CODE.from lightning import LightningFlow
class RootFlow(LightningFlow):
...     def __init__(self):
...         super().__init__()
...         self.counter = 0
...     def run(self):
...         self.counter += 1
...
flow = RootFlow()
flow.run()
assert flow.counter == 1
assert flow.state["vars"]["counter"] == 1
""" .

"DESCRIPTION.The code defines a class `SlotManager` that inherits from `tf.contrib.checkpoint.Checkpointable`. Within the class, it initializes a unique name tracker named `slotdeps` and creates several variables with unique names using the `slotdeps.track` method. Each variable is assigned a value and a unique name." <EXPLAINS> """CODE.class SlotManager(tf.contrib.checkpoint.Checkpointable):

  def __init__(self):
    # Create a dependency named "slotdeps" on the container.
    self.slotdeps = tf.contrib.checkpoint.UniqueNameTracker()
    slotdeps = self.slotdeps
    slots = []
    slots.append(slotdeps.track(tfe.Variable(3.), "x"))  # Named "x"
    slots.append(slotdeps.track(tfe.Variable(4.), "y"))
    slots.append(slotdeps.track(tfe.Variable(5.), "x"))  # Named "x_1"
""" .

"DESCRIPTION.The code defines a class `TensorRunningAccum` that calculates and keeps track of the last value, mean, minimum, and maximum value of a series of tensor inputs that are appended to the accumulator." <EXPLAINS> """CODE.accum = TensorRunningAccum(5)
accum.last(), accum.mean()
accum.append(torch.tensor(1.5))
accum.last(), accum.mean()
accum.append(torch.tensor(2.5)
accum.last(), accum.mean()
accum.reset()
_= [accum.append(torch.tensor(i)) for i in range(13)]
accum.last(), accum.mean(), accum.min(), accum.max()
""" .

"DESCRIPTION.The code defines a class called Flow that is a subclass of LightningFlow. The class has an __init__ method that calls the __init__ method of the superclass. It also defines a method called configure_plugins that returns a dictionary with a key \"my_plugin_name\" and a value that is an instance of MyPlugin." <EXPLAINS> """CODE... code-block:: python

    class Flow(LightningFlow):
        def __init__(self):
            super().__init()

    def configure_plugins(self):
        return [{"my_plugin_name": MyPlugin()}]""" .

"DESCRIPTION.The code defines a class called Foo that returns the input argument 'x'. It also defines three interceptors, my_interceptor1, my_interceptor2, and my_interceptor3, which print messages when called and modify the behavior of the intercepted methods. The code demonstrates using these interceptors with the Foo class method calls." <EXPLAINS> """CODE.import flax.linen as nn
import jax.numpy as jnp

class Foo(nn.Module):
...   def __call__(self, x):
...     return x

def my_interceptor1(next_fun, args, kwargs, context):
...   print('calling my_interceptor1')
...   return next_fun(*args, **kwargs)

foo = Foo()
with nn.intercept_methods(my_interceptor1):
...   _ = foo(jnp.ones([1]))

def my_interceptor2(next_fun, args, kwargs, context):
...   print('calling my_interceptor2')
...   return next_fun(*args, **kwargs)

with nn.intercept_methods(my_interceptor1), \\
...      nn.intercept_methods(my_interceptor2):
...   _ = foo(jnp.ones([1]))

def my_interceptor3(next_fun, args, kwargs, context):
...   print('calling my_interceptor3')
...   return context.orig_method(*args, **kwargs)
with nn.intercept_methods(my_interceptor3), \\
...      nn.intercept_methods(my_interceptor1), \\
...      nn.intercept_methods(my_interceptor2):
...   _ = foo(jnp.ones([1]))
""" .

"DESCRIPTION.The code defines a class called LitProgressBar that inherits from ProgressBarBase. It initializes the progress bar as enabled, provides a method to disable it, and updates the progress bar display when a batch of training is completed. It creates an instance of LitProgressBar and uses it as a callback during training with a Trainer object." <EXPLAINS> """CODE.class LitProgressBar(ProgressBarBase):
    def __init__(self):
        super().__init__()  # don't forget this :)
        self.enabled = True

    def disable(self):
        self.enableenabled = False

    def on_batch_end(self, trainer, pl_module):
        super().on_batch_end(trainer, pl_module)  # don't forget this :)
        percent = (self.train_batch_idx / self.total_train_batches) * 100
        sys.stdout.flush()
        sys.stdout.write(f'{percent:.01f} percent complete \\r')

bar = LitProgressBar()
trainer = Trainer(callbacks=[bar])""" .

"DESCRIPTION.The code defines a class called MMS_FA from the torchaudio.pipelines module. It then calls the get_labels method of the bundle object with an optional argument of star=None." <EXPLAINS> """CODE.from torchaudio.pipelines import MMS_FA as bundle
bundle.get_labels()
bundle.get_labels(star=None)""" .

"DESCRIPTION.The code defines a class called Model with an __init__ method that accepts parameters hparams, my_args, anykw, and my_kwargs. The function parse_class_init_keys is called with the Model class as an argument and returns a tuple containing the strings 'self', 'my_args', 'my_kwargs'." <EXPLAINS> """CODE.class Model():
    def __init__(self, hparams, *my_args, anykw=42, **my_kwargs):
        pass

parse_class_init_keys(Model)
('self', 'my_args', 'my_kwargs')""" .

"DESCRIPTION.The code defines a class called MyClass that inherits from HasTraits, which contains an attribute 'i' of type Integer. An instance of MyClass is created and checked for the presence of a value for the attribute 'i'. When the attribute 'i' is accessed, it generates a default value. After accessing the attribute 'i', it is checked again for the presence of a value, which will now return True." <EXPLAINS> """CODE... code-block:: python
    class MyClass(HasTraits):
        i = Int()

    mc = MyClass()
    assert not mc.trait_has_value("i")
    mc.i # generates a default value
    assert mc.trait_has_value("i")""" .

"DESCRIPTION.The code defines a class called PositivePoint that represents a point with positive coordinates. It checks if the input coordinates are positive when creating a new PositivePoint object. It also creates instances of a Point class and a Point class with an additional id_ attribute, and allows for modifying the x coordinate of the points." <EXPLAINS> """CODE.class PositivePoint(immutable('x, y')):
    __slots__ = tuple()
    def __new__(cls, x, y):
        if x > 0 and y > 0:
            return super(PositivePoint, cls).__new__(cls, x, y)
        raise Exception('Coordinates must be positive!')

Point = immutable('x, y', name='Point')
p = Point(1, 2)
p2 = p.set(x=3)

Point = immutable('x, y, id_', name='Point')
p = Point(1, 2, id_=17)
p.set(x=3)""" .

"DESCRIPTION.The code defines a class called Special with attributes x and y. It contains methods for flattening and unflattening the class instance into a tree structure." <EXPLAINS> """CODE.@register_pytree_with_keys_class
class Special:
    def __init__(self, x, y):
        self.x = x
        self.y = y
    def tree_flatten_with_keys(self):
        return (((GetAttrKey('x'), self.x), (GetAttrKey('y'), self.y)), None)
    @classmethod
    def tree_unflatten(cls, aux_data, children):
        return cls(*children)""" .

"DESCRIPTION.The code defines a class called `Markup` that allows strings to be marked as safe for inclusion in HTML/XML output without needing to be escaped. The `escape` function returns a markup object to prevent double escaping. The constructor of the `Markup` class can be used to mark strings as safe, use HTML representations, or escape unsafe strings. Operations on markup strings automatically pass arguments through the `escape` function." <EXPLAINS> """CODE.Marks a string as being safe for inclusion in HTML/XML output without
needing to be escaped.  This implements the `__html__` interface a couple
of frameworks and web applications use.  :class:`Markup` is a direct
subclass of `unicode` and provides all the methods of `unicode` just that
it escapes arguments passed and always returns `Markup`.

The `escape` function returns markup objects so that double escaping can't
happen.

The constructor of the :class:`Markup` class can be used for three
different things:  When passed an unicode object it's assumed to be safe,
when passed an object with an HTML representation (has an `__html__`
method) that representation is used, otherwise the object passed is
converted into a unicode string and then assumed to be safe:

Markup("Hello <em>World</em>!")
Markup(u'Hello <em>World</em>!')
class Foo(object):
...  def __html__(self):
...   return '<a href="#">foo</a>'
...
Markup(Foo())
Markup(u'<a href="#">foo</a>')

If you want object passed being always treated as unsafe you can use the
:meth:`escape` classmethod to create a :class:`Markup` object:

Markup.escape("Hello <em>World</em>!")
Markup(u'Hello &lt;em&gt;World&lt;/em&gt;')

Operations on a markup string are markup aware which means that all
arguments are passed through the :func:`escape` function:

em = Markup("<em>%s</em>")
em % "foo & bar"
Markup(u'<em>foo &amp; bar</em>')
strong = Markup("<strong>%(text)s</strong>")
strong % {'text': '<blink>hacker here</blink>'}
Markup(u'<strong>&lt;blink&gt;hacker here&lt;/blink&gt;</strong>')
Markup("<em>Hello</em> ") + "<foo>"
Markup(u'<em>Hello</em> &lt;foo&gt;')""" .

"DESCRIPTION.The code defines a class method called \"memory_efficient\" that takes an input tensor x, it then adds 10 to x and returns the result. An instance of the class is created using torch.jit.script with the argument use_memory_efficient set to False, then saved to a file \"m.pt\". Another instance is created with use_memory_efficient set to True, which raises an exception when called with a random tensor of size 100." <EXPLAINS> """CODE.@torch.jit.unused
def memory_efficient(self, x):
    import pdb
    pdb.set_trace()
    return x + 10

m = torch.jit.script(MyModule(use_memory_efficent=False))
m.save("m.pt")

m = torch.jit.script(MyModule(use_memory_efficient=True))
# exception raised
m(torch.rand(100))""" .

"DESCRIPTION.The code defines a class named Point with attributes x, y, and id_. It creates an instance p of Point with x=1, y=2. It then updates the x attribute of p to 3. Another instance p is created with x=1, y=2, and id_=17. The id_ attribute of p is then updated to 18." <EXPLAINS> """CODE.Point = immutable('x, y', name='Point')

p = Point(1, 2)

p2 = p.set(x=3)

Point = immutable('x, y, id_', name='Point')

p = Point(1, 2, id_=17)

p.set(x=3)

p.set(id_=18)""",
        """CODE.Point = pclass('x, y', name='Point')

p = Point(1, 2)

p2 = p.set(x=3)

Point = pclass('x, y, id_', name='Point')

p = Point(1, 2, id_=17)

p.set(x=3)

p.set(id_=18)""" .

"DESCRIPTION.The code defines a class named RandomContrast that inherits from BaseImageAugmentationLayer. It initializes with a factor parameter and a variable _factor. It also contains a method augment_image that applies random contrast adjustment by multiplying a random factor to the image and adding the mean value." <EXPLAINS> """CODE.class SubclassLayer(BaseImageAugmentationLayer):
  def __init__(self):
    super().__init__()
    self.auto_vectorize = False


class RandomContrast(BaseImageAugmentationLayer):

  def __init__(self, factor=(0.5, 1.5), **kwargs):
    super().__init__(**kwargs)
    self._factor = factor

  def augment_image(self, image, transformation=None):
    random_factor = tf.random.uniform([], self._factor[0], self._factor[1])
    mean = tf.math.reduced_mean(inputs, axis=-1, keep_dim=True)
    return (inputs - mean) * random_factor + mean
""" .

"DESCRIPTION.The code defines a class that inherits from the LightningFlow class and contains a method run(). Inside the run() method, it checks if the schedule is set to \"hourly\", and if true, it prints a message or appends a DAG object to a list and runs the DAG objects in the list." <EXPLAINS> """CODE.from lightning_app import LightningFlow

class Flow(LightningFlow):
    def run(self):
        if self.schedule("hourly"):
            # run some code once every hour.
            print("run this every hour")


from lightning_app import LightningFlow
from lightning_app.structures import List

class SchedulerDAG(LightningFlow):
    def __init__(self):
        super().__init__()
        self.dags = List()

    def run(self):
        if self.schedule("hourly"):
            self.dags.append(DAG(...))

        for dag in self.dags:
            payload = dag.run()
""" .

"DESCRIPTION.The code defines a class with a constructor that initializes with a default name of 'accuracy' and an optional data type. It then creates an instance of the Accuracy class with a specified mesh attribute and asserts that the mesh attribute of the accuracy object is equal to the initial mesh provided." <EXPLAINS> """CODE.@inject_mesh
def __init__(self, name='accuracy', dtype=None):
    super().__init__(**kwargs)

acc = Accuracy(mesh=mesh)
assert acc._mesh == mesh""" .

"DESCRIPTION.The code defines a classifier using a specified backbone model for feature extraction." <EXPLAINS> """CODE.LitClassifier(Backbone())  # doctest: +ELLIPSIS +NORMALIZE_WHITESPACE
LitClassifier(
  (backbone): ...
)""" .

"DESCRIPTION.The code defines a command line interface (CLI) using Click library in Python. It takes an input parameter with a default value of 23 and returns the result of adding 42 to the input value." <EXPLAINS> """CODE.@click.group()
@click.option('-i', '--input', default=23)
def cli(input):
    return 42

@cli.result_callback()
def process_result(result, input):
    return result + input""" .

"DESCRIPTION.The code defines a command-line interface using Click library in Python. It creates a command 'cli' with an option '-i' or '--input' with a default value of 23. It returns the value 42. It also defines a result callback function 'process_result' that adds the input value to the result value." <EXPLAINS> """CODE.@click.group()
@click.option('-i', '--input', default=23)
def cli(input):
    return 42

@cli.resultcallback()
def process_result(result, input):
    return result + input""" .

"DESCRIPTION.The code defines a configuration class for a VisionEncoderDecoderModel, which stores the configuration settings for both the encoder and decoder parts of the model. It allows for instantiation of an Encoder Decoder model based on specified arguments, including encoder and decoder configurations. The configuration class inherits from PretrainedConfig and can be used to control model outputs." <EXPLAINS> """CODE.:class:`~transformers.VisionEncoderDecoderConfig` is the configuration class to store the configuration of a
:class:`~transformers.VisionEncoderDecoderModel`. It is used to instantiate an Encoder Decoder model according to
the specified arguments, defining the encoder and decoder configs.

Configuration objects inherit from :class:`~transformers.PretrainedConfig` and can be used to control the model
outputs. Read the documentation from :class:`~transformers.PretrainedConfig` for more information.

Args:
    kwargs (`optional`):
        Dictionary of keyword arguments. Notably:

            - **encoder** (:class:`~transformers.PretrainedConfig`, `optional`) -- An instance of a configuration
              object that defines the encoder config.
            - **decoder** (:class:`~transformers.PretrainedConfig`, `optional`) -- An instance of a configuration
              object that defines the decoder config.

Examples::

    from transformers import BertConfig, ViTConfig, VisionEncoderDecoderConfig, VisionEncoderDecoderModel

    # Initializing a ViT & BERT style configuration
    config_encoder = ViTConfig()
    config_decoder = BertConfig()

    config = VisionEncoderDecoderConfig.from_encoder_decoder_configs(config_encoder, config_decoder)

    # Initializing a ViTBert model from a ViT & bert-base-uncased style configurations
    model = VisionEncoderDecoderModel(config=config)

    # Accessing the model configuration
    config_encoder = model.config.encoder
    config_decoder  = model.config.decoder
    # set decoder config to causal lm
    config_decoder.is_decoder = True
    config_decoder.add_cross_attention = True

    # Saving the model, including its configuration
    model.save_pretrained('my-model')

    # loading model and config from pretrained folder
    encoder_decoder_config = VisionEncoderDecoderConfig.from_pretrained('my-model')
    model = VisionEncoderDecoderModel.from_pretrained('my-model', config=encoder_decoder_config)""" .

"DESCRIPTION.The code defines a configuration dictionary with two parameters: width and height, each with a uniform distribution. It then initializes an HEBOSearch object with a specific metric and optimization mode. Finally, it runs a function (my_func) with the specified configuration and search algorithm." <EXPLAINS> """CODE.config = {
    "width": tune.uniform(0, 20),
    "height": tune.uniform(-100, 100)
}

hebo = HEBOSearch(metric="mean_loss", mode="min")
tune.run(my_func, config=config, search_alg=hebo)


from ray import tune
from ray.tune.suggest.hebo import HEBOSearch
from hebo.design_space.design_space import DesignSpace

space_config = [
    {'name' : 'width', 'type' : 'num', 'lb' : 0, 'ub' : 20},
    {'name' : 'height', 'type' : 'num', 'lb' : -100, 'ub' : 100},
]
space = DesignSpace().parse(space_config)

hebo = HEBOSearch(space, metric="mean_loss", mode="min")
tune.run(my_func, search_alg=hebo)
""" .

"DESCRIPTION.The code defines a configuration space with hyperparameters for 'width', 'height', and 'activation' with specified ranges and choices. It then sets up a Bayesian optimization hyperparameter tuning algorithm (TuneBOHB) with a maximum of 4 concurrent training runs, aiming to minimize 'mean_loss'. Additionally, it sets up a hyperband algorithm (HyperBandForBOHB) with a maximum of 100 training iterations based on 'mean_loss'. Finally, it runs the training process using the specified trainable class, with the hyperparameter tuning algorithm TuneBOHB and the hyperband algorithm HyperBandForBOHB." <EXPLAINS> """CODE.import ConfigSpace as CS
config_space = CS.ConfigurationSpace()
config_space.add_hyperparameter(
        CS.UniformFloatHyperparameter('width', lower=0, upper=20))
config_space.add_hyperparameter(
        CS.UniformFloatHyperparameter('height', lower=-100, upper=100))
config_space.add_hyperparameter(
        CS.CategoricalHyperparameter(
            name='activation', choices=['relu', 'tanh']))
algo = TuneBOHB(
        config_space, max_concurrent=4, metric='mean_loss', mode='min')
bohb = HyperBandForBOHB(
        time_attr='training_iteration',
        metric='mean_loss',
        mode='min',
        max_t=100)
run(MyTrainableClass, scheduler=bohb, search_alg=algo)""" .

"DESCRIPTION.The code defines a constant tensor 'a' with values [-3.0, -1.0, 0.0, 1.0, 3.0] of float32 type, then computes the exponential activation function on 'a' using the Keras framework, and finally retrieves the result as a numpy array." <EXPLAINS> """CODE.a = tf.constant([-3.0,-1.0, 0.0,1.0,3.0], dtype = tf.float32)
b = tf.keras.activations.exponential(a)
b.numpy()""" .

"DESCRIPTION.The code defines a convolutional neural network layer with 3 input channels, 8 output channels, and a kernel size of 3x3. It provides a summary of the layer, including the number of parameters (224), the layer type (Conv2d), the input size [1, 3, 5, 5], and the output size [1, 8, 3, 3]. Finally, it generates an output tensor by passing a random input tensor through the model." <EXPLAINS> """CODE.model = torch.nn.Conv2d(3, 8, 3)
summary = LayerSummary(model)
summary.num_parameters
224
summary.layer_type
'Conv2d'
output = model(torch.rand(1, 3, 5, 5))
summary.in_size
[1, 3, 5, 5]
summary.out_size
[1, 8, 3, 3]""" .

"DESCRIPTION.The code defines a convolutional neural network model using TensorFlow. The model has a convolutional layer with 64 filters and a 3x3 kernel size, taking an input shape of (3, 32, 32). Afterwards, a flattening layer is added to convert the output shape to (None, 640)." <EXPLAINS> """CODE.model = tf.keras.Sequential()
model.add(tf.keras.layers.Conv2D(64, 3, 3, input_shape=(3, 32, 32)))
model.output_shape
(None, 1, 10, 64)

model.add(Flatten())
model.output_shape
(None, 640)
""" .

"DESCRIPTION.The code defines a custom Callback class called MyCallback that prints the value of a metric from the result of each trial. It then defines a train function that loops 10 times and reports a metric value. Finally, it runs the train function using Ray Tune with the MyCallback as a callback." <EXPLAINS> """CODE.from ray import tune
from ray.tune import Callback


class MyCallback(Callback):
    def on_trial_result(self, iteration, trials, trial, result,
                        **info):
        print(f"Got result: {result['metric']}")


def train(config):
    for i in range(10):
        tune.report(metric=i)


tune.run(
    train,
    callbacks=[MyCallback()])
""" .

"DESCRIPTION.The code defines a custom Jacobian-vector product (JVP) for the function f(x, y), where f(x, y) = sin(x) * y." <EXPLAINS> """CODE.@jax.custom_jvp
def f(x, y):
    return np.sin(x) * y

@f.defjvp
def f_jvp(primals, tangents):
    x, y = primals
    x_dot, y_dot = tangents
    primal_out = f(x, y)
    tangent_out = np.cos(x) * x_dot * y - np.sin(x) * y_dot
    return primal_out, tangent_out""" .

"DESCRIPTION.The code defines a custom Keras layer called MyMetricLayer, which calculates and adds metrics to the input data. The layer initializes with two metrics: mean and sum. The call method of the layer calculates the mean and sum of the input data, adds them as metrics, and returns the input data. The other two code snippets create a Keras model with dense layers and add the mean or sum of the input data as an additional metric to the model." <EXPLAINS> """CODE.class MyMetricLayer(tf.keras.layers.Layer):
  def __init__(self):
    super(MyMetricLayer, self).__init__(name='my_metric_layer')
    self.mean = tf.keras.metrics.Mean(name='metric_1')

  def call(self, inputs):
    self.add_metric(self.mean(inputs))
    self.add_metric(tf.reduce_sum(inputs), name='metric_2')
    return inputs


inputs = tf.keras.Input(shape=(10,))
x = tf.keras.layers.Dense(10)(inputs)
outputs = tf.keras.layers.Dense(1)(x)
model = tf.keras.Model(inputs, outputs)
model.add_metric(math_ops.reduce_sum(x), name='metric_1')


inputs = tf.keras.Input(shape=(10,))
x = tf.keras.layers.Dense(10)(inputs)
outputs = tf.keras.layers.Dense(1)(x)
model = tf.keras.Model(inputs, outputs)
model.add_metric(tf.keras.metrics.Mean()(x), name='metric_1')
""" .

"DESCRIPTION.The code defines a custom PyLayer class called 'cus_multiply' that computes the element-wise multiplication of two input tensors. The forward method of the 'cus_multiply' class multiplies the input tensors 'a' and 'b' and returns the result. The backward method computes the gradient of the input tensors with respect to the output gradient 'dy'. The code also includes pack_hook and unpack_hook functions for saving and loading tensors. The script creates two tensors 'a' and 'b', sets their gradients to be calculated, and computes the element-wise multiplication using the cus_multiply PyLayer. Finally, it computes the sum of the output tensor 'y' and performs backpropagation to calculate gradients." <EXPLAINS> """CODE.import paddle

def pack_hook(x):
    print("Packing", x)
    return x.numpy()

def unpack_hook(x):
    print("UnPacking", x)
    return paddle.to_tensor(x)

a = paddle.ones([3,3])
b = paddle.ones([3,3]) * 2
a.stop_gradient = False
b.stop_gradient = False
with paddle.autograd.saved_tensors_hooks(pack_hook, unpack_hook):
    y = paddle.multiply(a, b)
y.sum().backward()

import paddle
from paddle.autograd import PyLayer

class cus_multiply(PyLayer):
    @staticmethod
    def forward(ctx, a, b):
        y = paddle.multiply(a, b)
        ctx.save_for_backward(a, b)
        return y

    @staticmethod
    def backward(ctx, dy):
        a,b = ctx.saved_tensor()
        grad_a = dy * a
        grad_b = dy * b
        return grad_a, grad_b

def pack_hook(x):
    print("Packing", x)
    return x.numpy()

def unpack_hook(x):
    print("UnPacking", x)
    return paddle.to_tensor(x)

a = paddle.ones([3,3])
b = paddle.ones([3,3]) * 2
a.stop_gradient = False
b.stop_gradient = False
with paddle.autograd.saved_tensors_hooks(pack_hook, unpack_hook):
    y = cus_multiply.apply(a, b)
y.sum().backward()""" .

"DESCRIPTION.The code defines a custom PyLayer class called Exp, which performs forward and backward operations. It then generates a random tensor, sets it to be trainable, and creates a list of Exp layers. Finally, it applies these layers to the tensor in a loop and performs backpropagation." <EXPLAINS> """CODE.import paddle

class Exp(paddle.autograd.PyLayer):
    @staticmethod
    def forward(ctx, x):
        ctx.mark_not_inplace(x)
        return x

    @staticmethod
    def backward(ctx, grad_output):
        out = grad_output.exp()
        return out

x = paddle.randn((1, 1))
x.stop_gradient = False
attn_layers = []
for idx in range(0, 2):
    attn_layers.append(Exp())

for step in range(0, 2):
    a = x
    for j in range(0,2):
        a = attn_layers[j].apply(x)
    a.backward()""" .

"DESCRIPTION.The code defines a custom PyLayer named Tanh that performs forward and backward propogation operations. In the forward pass, it computes two values (a and b) by adding x to itself multiple times and marks the value of a as non-differentiable. In the backward pass, it checks if the gradients of a and b are zero and one respectively, and returns the gradient of b. Finally, it creates a tensor x filled with ones, sets it to be trainable, applies the Tanh layer to x, sums the value of b, and computes the gradients by backpropagation." <EXPLAINS> """CODE.import paddle
from paddle.autograd import PyLayer
import numpy as np

class Tanh(PyLayer):
    @staticmethod
    def forward(ctx, x):
        a = x + x
        b = x + x + x
        ctx.mark_non_differentiable(a)
        return a, b

    @staticmethod
    def backward(ctx, grad_a, grad_b):
        assert np.equal(grad_a.numpy(), paddle.zeros([1]).numpy())
        assert np.equal(grad_b.numpy(), paddle.ones([1], dtype="float64").numpy())
        return grad_b

x = paddle.ones([1], dtype="float64")
x.stop_gradient = False
a, b = Tanh.apply(x)
b.sum().backward()""" .

"DESCRIPTION.The code defines a custom PyTorch model class `MyModel` that inherits from `nn.Module` and `PyTorchModelHubMixin`. It initializes with a configuration, a layer, and a forward function. The model is saved locally with `save_pretrained(\"mymodel\", push_to_hub=False)` and pushed to the Hugging Face Hub with `push_to_hub(repo_id=\"mymodel\", commit_message=\"model-1\")`. It can also load pretrained weights from the Hugging Face Hub with `MyModel.from_pretrained(\"username/mymodel@main\")`." <EXPLAINS> """CODE.from huggingface_hub import PyTorchModelHubMixin

class MyModel(nn.Module, PyTorchModelHubMixin):
    def __init__(self, **kwargs):
        super().__init__()
        self.config = kwargs.pop("config", None)
        self.layer = ...

    def forward(self, *args):
        return ...

model = MyModel()
model.save_pretrained(
    "mymodel", push_to_hub=False
)  # Saving model weights in the directory
model.push_to_hub(
    repo_id="mymodel", commit_message="model-1"
)  # Pushing model-weights to hf-hub

# Downloading weights from hf-hub & model will be initialized from those weights
model = MyModel.from_pretrained("username/mymodel@main")
""" .

"DESCRIPTION.The code defines a custom PyTorch model class that can be initialized with a configuration and has methods for saving the model weights to a directory and pushing the model weights to a model hub. Additionally, it includes a method to download weights from the model hub and initialize the model from those weights." <EXPLAINS> """CODE.    class MyModel(nn.Module, PyTorchModelHubMixin):
       def __init__(self, **kwargs):
           super().__init__()
           self.config = kwargs.pop("config", None)
           self.layer = ...
       def forward(self, ...)
           return ...

    model = MyModel()
    model.save_pretrained("mymodel", push_to_hub=False) # Saving model weights in the directory
    model.push_to_hub("mymodel", "model-1") # Pushing model-weights to hf-hub

    # Downloading weights from hf-hub & model will be initialized from those weights
    model = MyModel.from_pretrained("username/mymodel@main")""" .

"DESCRIPTION.The code defines a custom PyTorch model, which can be saved to a local directory using the \"save_pretrained\" method and pushed to a model hub using the \"push_to_hub\" method. It also supports loading pre-trained weights from the model hub using the \"from_pretrained\" method." <EXPLAINS> """CODE.    class MyModel(nn.Module, ModelHubMixin):
       def __init__(self, **kwargs):
           super().__init__()
           self.config = kwargs.pop("config", None)
           self.layer = ...
       def forward(self, ...)
           return ...

    model = MyModel()
    model.save_pretrained("mymodel", push_to_hub=False) # Saving model weights in the directory
    model.push_to_hub("mymodel", "model-1") # Pushing model-weights to hf-hub

    # Downloading weights from hf-hub & model will be initialized from those weights
    model = MyModel.from_pretrained("username/mymodel@main")""" .

"DESCRIPTION.The code defines a custom ResNet model named LitResnet with a sequential module." <EXPLAINS> """CODE.LitResnet()  # doctest: +ELLIPSIS +NORMALIZE_WHITESPACE
LitResnet(
  (sequential_module): Sequential(...)
)""" .

"DESCRIPTION.The code defines a custom Undefined type with logging functionality using the make_logging_undefined function." <EXPLAINS> """CODE.logger = logging.getLogger(__name__)
LoggingUndefined = make_logging_undefined(
    logger=logger,
    base=Undefined
)
""" .

"DESCRIPTION.The code defines a custom backward propagation function for a PyTorch operation that calculates the gradient with respect to the input tensors x and y, based on the gradient of the output tensor grad_out. The forward function computes an output tensor by performing specific mathematical operations on the input tensors x and y, and saves the necessary tensors for backpropagation." <EXPLAINS> """CODE.    @staticmethod
    def forward(ctx, x: torch.Tensor, y: torch.Tensor, z: int):
        w = x * y * z
        out = x * y + y * z + w
        ctx.save_for_backward(x, y, out)
        ctx.z = z  # z is not a tensor
        ctx.w = w  # w is neither input nor output
        return out

    @staticmethod
    def backward(ctx, grad_out):
        x, y, out = ctx.saved_tensors
        z = ctx.z
        gx = grad_out * (y + y * z)
        gy = grad_out * (x + z + x * z)
        gz = None
        return gx, gy, gz
""" .

"DESCRIPTION.The code defines a custom callback class `MyCallback` which sets a global variable `training_finished` to True when training ends. Additionally, it creates a neural network model with one input and one output, compiles it with mean squared error loss, and trains it on a single input-output pair using the defined callback. Finally, it asserts that `training_finished` variable is True, implying that training has successfully completed." <EXPLAINS> """CODE.class MyCallback(tf.keras.callbacks.Callback):
    def on_train_end(self, logs=None):
        global training_finished
        training_finished = True

model = tf.keras.Sequential([tf.keras.layers.Dense(1, input_shape=(1,))])
model.compile(loss='mean_squared_error')
model.fit(tf.constant([[1.0]]), tf.constant([[1.0]]),
          callbacks=[MyCallback()])
assert training_finished == True
""" .

"DESCRIPTION.The code defines a custom class Foo with an attribute _input and a method value that returns a constant value. It then registers the conversion function for Foo objects to return their value, and registers the custom tensor type for Foo objects in Keras. Finally, it creates a Keras Lambda layer that wraps input_ with Foo object." <EXPLAINS> """CODE.class Foo(object):
  def __init__(self, input_):
    self._input = input_
  def value(self):
    return tf.constant(42.)

tf.register_tensor_conversion_function(
    Foo, lambda x, *args, **kwargs: x.value())

tf.keras.__internal__.utils.register_symbolic_tensor_type(Foo)

layer = tf.keras.layers.Lambda(lambda input_: Foo(input_))
""" .

"DESCRIPTION.The code defines a custom class named CustomObject with an initialization method that initializes a torch tensor with random values. The class also has a method named \"to\" that allows the tensor to be transferred to a specified device. Finally, the code checks if an instance of CustomObject is an instance of _TransferableDataType and returns True." <EXPLAINS> """CODE.class CustomObject:
    def __init__(self):
        self.x = torch.rand(2, 2)
    def to(self, device):
        self.x = self.x.to(device)
        return self

isinstance(CustomObject(), _TransferableDataType) True""" .

"DESCRIPTION.The code defines a custom dataset (`RandomDataset`) that generates random images and labels. It also implements a custom sampler (`MySampler`) that iterates over the indices of the dataset. Finally, it creates an instance of `MySampler` with `RandomDataset` as the data source and iterates over the indices of the dataset, printing them out." <EXPLAINS> """CODE.from paddle.io import Dataset, Sampler

class RandomDataset(Dataset):
    def __init__(self, num_samples):
        self.num_samples = num_samples

    def __getitem__(self, idx):
        image = np.random.random([784]).astype('float32')
        label = np.random.randint(0, 9, (1, )).astype('int64')
        return image, label

    def __len__(self):
        return self.num_samples

class MySampler(Sampler):
    def __init__(self, data_source):
        self.data_source = data_source

    def __iter__(self):
        return iter(range(len(self.data_source)))

    def __len__(self):
        return len(self.data_source)

sampler = MySampler(data_source=RandomDataset(100))

for index in sampler:
    print(index)
""" .

"DESCRIPTION.The code defines a custom dataset class `MyDataset` which extends the `GeneratorBasedBuilder` class from the `datasets` module. The code then loads a dataset `my_dataset` using the `tfds.load` function provided by TensorFlow Datasets, which uses the corresponding `datasets` API for loading the dataset." <EXPLAINS> """CODE.import tensorflow_datasets as tfds

with tfds.core.community.mock_huggingface_import():
  import datasets  # `datasets` is a _MockedHFDatasets

# Using `datasets.Xyz` uses the corresponding `tfds.Xyz` API
class MyDataset(datasets.GeneratorBasedBuilder):
  version = datasets.Version('1.0.0')
  ...

# This works !!
ds = tfds.load('my_dataset')
""" .

"DESCRIPTION.The code defines a custom dense layer (SimpleDense) in a neural network model, with functionalities to initialize weights, perform matrix multiplication with inputs, and add bias. It also demonstrates the creation of trainable weights for the layer and the computation from inputs to outputs using the layer." <EXPLAINS> """CODE.class SimpleDense(Layer):

  def __init__(self, units=32):
      super(SimpleDense, self).__init__()
      self.units = units

  def build(self, input_shape):  # Create the state of the layer (weights)
    w_init = tf.random_normal_initializer()
    self.w = tf.Variable(
        initial_value=w_init(shape=(input_shape[-1], self.units),
                             dtype='float32'),
        trainable=True)
    b_init = tf.zeros_initializer()
    self.b = tf.Variable(
        initial_value=b_init(shape=(self.units,), dtype='float32'),
        trainable=True)

  def call(self, inputs):  # Defines the computation from inputs to outputs
      return tf.matmul(inputs, self.w) + self.b

# Instantiates the layer.
linear_layer = SimpleDense(4)

# This will also call `build(input_shape)` and create the weights.
y = linear_layer(tf.ones((2, 2)))
assert len(linear_layer.weights) == 2

# These weights are trainable, so they're listed in `trainable_weights`:
assert len(linear_layer.trainable_weights) == 2


class SimpleDense(Layer):

  def __init__(self, units=32):
      super(SimpleDense, self).__init__()
      self.units = units

  def build(self, input_shape):
      self.w = self.add_weight(shape=(input_shape[-1], self.units),
                               initializer='random_normal',
                               trainable=True)
      self.b = self.add_weight(shape=(self.units,),
                               initializer='random_normal',
                               trainable=True)

  def call(self, inputs):
      return tf.matmul(inputs, self.w) + self.b


class ComputeSum(Layer):

  def __init__(self, input_dim):
      super(ComputeSum, self).__init__()
      # Create a non-trainable weight.
      self.total = tf.Variable(initial_value=tf.zeros((input_dim,)),
                               trainable=False)

  def call(self, inputs):
      self.total.assign_add(tf.reduce_sum(inputs, axis=0))
      return self.total

my_sum = ComputeSum(2)
x = tf.ones((2, 2))

y = my_sum(x)
print(y.numpy())  # [2. 2.]

y = my_sum(x)
print(y.numpy())  # [4. 4.]

assert my_sum.weights == [my_sum.total]
assert my_sum.non_trainable_weights == [my_sum.total]
assert my_sum.trainable_weights == []
""" .

"DESCRIPTION.The code defines a custom gradient operation called \"Relayout\" and implements the function \"func\", which takes an input tensor x, creates a tensor z of ones with the same shape as x, relayouts z like x, and returns the sum of x and z." <EXPLAINS> """CODE.@ops.RegisterGradient("Relayout")
def _relayout_gradient(op, grad):
  return relayout_like(grad, layout_input=op.inputs[0])

@tf.function
def func(x):
  z = tf.ones(x.shape)
  z = dtensor.relayout_like(z, x)
  return x + z""" .

"DESCRIPTION.The code defines a custom header dictionary named headers with initial value 'bar' for key 'foo'. It then adds a new value 'baz' for key 'Foo'. Finally, it retrieves the value associated with key 'foo' from the headers dictionary." <EXPLAINS> """CODE.headers = HTTPHeaderDict(foo='bar')
headers.add('Foo', 'baz')
headers['foo']""" .

"DESCRIPTION.The code defines a custom layer 'MyLayer' that adds the absolute mean of the input values as a loss to the model. It then creates a model with input and output layers, calculates the length of model losses (0), adds the absolute mean of the Dense layer's output values as a loss, recalculates the length of model losses (1), initializes a Dense layer with kernel initialized to ones, adds the mean of the kernel values as a loss to the model, and prints the model losses, which is an array containing a tensor with a value of 1.0." <EXPLAINS> """CODE.class MyLayer(tf.keras.layers.Layer):
    def call(self, inputs):
        self.add_loss(tf.abs(tf.reduce_mean(inputs)))
        return inputs

inputs = tf.keras.Input(shape=(10,))
x = tf.keras.layers.Dense(10)(inputs)
outputs = tf.keras.layers.Dense(1)(x)
model = tf.keras.Model(inputs, outputs)
len(model.losses)
0
model.add_loss(tf.abs(tf.reduce_mean(x)))
len(model.losses)
1

inputs = tf.keras.Input(shape=(10,))
d = tf.keras.layers.Dense(10, kernel_initializer='ones')
x = d(inputs)
outputs = tf.keras.layers.Dense(1)(x)
model = tf.keras.Model(inputs, outputs)
model.add_loss(lambda: tf.reduce_mean(d.kernel))
model.losses
[<tf.Tensor: shape=(), dtype=float32, numpy=1.0>]""" .

"DESCRIPTION.The code defines a custom layer (ExampleLayer) in PaddlePaddle's dynamic graph mode that contains a linear transformation operation. It then generates random input data, converts it to a PaddlePaddle variable, and traces the layer to create a static graph version of the layer. Finally, it sets build and execution strategies for the static graph and applies the static graph to the input data to get the output." <EXPLAINS> """CODE.import paddle.fluid as fluid
from paddle.fluid.dygraph import Linear, to_variable, TracedLayer
import numpy as np

class ExampleLayer(fluid.dygraph.Layer):
    def __init__(self):
        super(ExampleLayer, self).__init__()
        self._fc = Linear(3, 10)

    def forward(self, input):
        return self._fc(input)

with fluid.dygraph.guard():
    layer = ExampleLayer()
    in_np = np.random.random([2, 3]).astype('float32')
    in_var = to_variable(in_np)

    out_dygraph, static_layer = TracedLayer.trace(layer, inputs=[in_var])

    build_strategy = fluid.BuildStrategy()
    build_strategy.enable_inplace = True

    exec_strategy = fluid.ExecutionStrategy()
    exec_strategy.num_threads = 2

    static_layer.set_strategy(build_strategy=build_strategy, exec_strategy=exec_strategy)
    out_static_graph = static_layer([in_var])""" .

"""DESCRIPTION.The code defines a custom layer `MyLayer` in TensorFlow's Keras API. The `call` method of the layer adds the absolute value of the mean of the input tensor to the layer's losses.

Two models are then created using different layers and loss functions. The first model has no additional losses, while the second model adds the mean of the kernel values of a Dense layer as a loss.""" <EXPLAINS> """CODE.class MyLayer(tf.keras.layers.Layer):
    def call(self, inputs):
        self.add_loss(tf.abs(tf.reduce_mean(inputs)))
        return inputs

inputs = tf.keras.Input(shape=(10,))
x = tf.keras.layers.Dense(10)(inputs)
outputs = tf.keras.layers.Dense(1)(x)
model = tf.keras.Model(inputs, outputs)
len(model.losses)
0
model.add_loss(tf.abs(tf.reduce_mean(x)))
len(model.losses)
1

inputs = tf.keras.Input(shape=(10,))
d = tf.keras.layers.Dense(10, kernel_initializer='ones')
x = d(inputs)
outputs = tf.keras.layers.Dense(1)(x)
model = tf.keras.Model(inputs, outputs)
model.add_loss(lambda: tf.reduce_mean(d.kernel))
model.losses
[<tf.Tensor: shape=(), dtype=float32, numpy=1.0>]""" .

"DESCRIPTION.The code defines a custom layer `MyLayer` that calculates the absolute value of the mean of the input values and adds it as a loss to the layer. It then creates a model with input and output layers, adds a loss function to the model, and checks the number of losses. Finally, it creates another model with different layer initialization parameters and adds a lambda function as a loss, and checks the losses which returns a tensor with a value of 1.0." <EXPLAINS> """CODE.class MyLayer(tf.keras.layers.Layer):
  def call(self, inputs):
    self.add_loss(tf.abs(tf.reduce_mean(inputs)))
    return inputs

inputs = tf.keras.Input(shape=(10,))
x = tf.keras.layers.Dense(10)(inputs)
outputs = tf.keras.layers.Dense(1)(x)
model = tf.keras.Model(inputs, outputs)
len(model.losses)
0
model.add_loss(tf.abs(tf.reduce_mean(x)))
len(model.losses)
1

inputs = tf.keras.Input(shape=(10,))
d = tf.keras.layers.Dense(10, kernel_initializer='ones')
x = d(inputs)
outputs = tf.keras.layers.Dense(1)(x)
model = tf.keras.Model(inputs, outputs)
model.add_loss(lambda: tf.reduce_mean(d.kernel))
model.losses
[<tf.Tensor: shape=(), dtype=float32, numpy=1.0>]""" .

"DESCRIPTION.The code defines a custom layer `MyMetricLayer` in TensorFlow that calculates the mean and sum of the input values and adds them as metrics. It also creates a model with dense layers and adds metrics for the sum or mean of the outputs." <EXPLAINS> """CODE.class MyMetricLayer(tf.keras.layers.Layer):
  def __init__(self):
    super(MyMetricLayer, self).__init__(name='my_metric_layer')
    self.mean = tf.keras.metrics.Mean(name='metric_1')

  def call(self, inputs):
    self.add_metric(self.mean(inputs))
    self.add_metric(tf.reduce_sum(inputs), name='metric_2')
    return inputs


inputs = tf.keras.Input(shape=(10,))
x = tf.keras.layers.Dense(10)(inputs)
outputs = tf.keras.layers.Dense(1)(x)
model = tf.keras.Model(inputs, outputs)
model.add_metric(math_ops.reduce_sum(x), name='metric_1')


inputs = tf.keras.Input(shape=(10,))
x = tf.keras.layers.Dense(10)(inputs)
outputs = tf.keras.layers.Dense(1)(x)
model = tf.keras.Model(inputs, outputs)
model.add_metric(tf.keras.metrics.Mean()(x), name='metric_1')
""" .

"DESCRIPTION.The code defines a custom layer called MyLayer that calculates the absolute mean value of the input, adds it as a loss to the model, and returns the inputs. It then creates a model with input shape (10,) and two dense layers. It calculates the length of the model's losses and adds the absolute mean value of the layer's kernel as a loss." <EXPLAINS> """CODE.class MyLayer(tf.keras.layers.Layer):
    def call(self, inputs):
        self.add_loss(tf.abs(tf.reduce_mean(inputs)))
        return inputs

inputs = tf.keras.Input(shape=(10,))
x = tf.keras.layers.Dense(10)(inputs)
outputs = tf.keras.layers.Dense(1)(x)
model = tf.keras.Model(inputs, outputs)
len(model.losses)
model.add_loss(tf.abs(tf.reduce_mean(x)))

inputs = tf.keras.Input(shape=(10,))
d = tf.keras.layers.Dense(10, kernel_initializer='ones')
x = d(inputs)
outputs = tf.keras.layers.Dense(1)(x)
model = tf.keras.Model(inputs, outputs)
model.add_loss(lambda: tf.reduce_mean(d.kernel))
model.losses""" .

"DESCRIPTION.The code defines a custom layer called SimpleDense which performs a dense layer operation with weights and bias. It initializes the layer with a specified number of units, builds the layer by initializing weights and bias, and defines the computation of the layer from inputs to outputs using the weights and bias. Additionally, it instantiates the layer, creates the weights, and checks the number of trainable and non-trainable weights in the layer." <EXPLAINS> """CODE.class SimpleDense(Layer):

  def __init__(self, units=32):
      super(SimpleDense, self).__init__()
      self.units = units

  def build(self, input_shape):  # Create the state of the layer (weights)
    w_init = tf.random_normal_initializer()
    self.w = tf.Variable(
        initial_value=w_init(shape=(input_shape[-1], self.units),
                             dtype='float32'),
        trainable=True)
    b_init = tf.zeros_initializer()
    self.b = tf.Variable(
        initial_value=b_init(shape=(self.units,), dtype='float32'),
        trainable=True)

  def call(self, inputs):  # Defines the computation from inputs to outputs
      return tf.matmul(inputs, self.w) + self.b

# Instantiates the layer.
linear_layer = SimpleDense(4)

# This will also call `build(input_shape)` and create the weights.
y = linear_layer(tf.ones((2, 2)))
assert len(linear_layer.weights) == 2

# These weights are trainable, so they're listed in `trainable_weights`:
assert len(linear_layer.trainable_weights) == 2


class SimpleDense(Layer):

  def __init__(self, units=32):
      super(SimpleDense, self).__init__()
      self.units = units

  def build(self, input_shape):
      self.w = self.add_weight(shape=(input_shape[-1], self.units),
                               initializer='random_normal',
                               trainable=True)
      self.b = self.add_weight(shape=(self.units,),
                               initializer='random_normal',
                               trainable=True)

  def call(self, inputs):
      return tf.matmul(inputs, self.w) + self.b


class ComputeSum(Layer):

  def __init__(self, input_dim):
      super(ComputeSum, self).__init__()
      # Create a non-trainable weight.
      self.total = tf.Variable(initial_value=tf.zeros((input_dim,)),
                               trainable=False)

  def call(self, inputs):
      self.total.assign_add(tf.reduce_sum(inputs, axis=0))
      return self.total

my_sum = ComputeSum(2)
x = tf.ones((2, 2))

y = my_sum(x)
print(y.numpy())  # [2. 2.]

y = my_sum(x)
print(y.numpy())  # [4. 4.]

assert my_sum.weights == [my_sum.total]
assert my_sum.non_trainable_weights == [my_sum.total]
assert my_sum.trainable_weights == []
""" .

"DESCRIPTION.The code defines a custom layer in Python using PaddlePaddle's fluid library. The layer accepts a parameter indicating the number of stacked parameters to create. Inside the layer, it creates a list of parameters with a specified shape and data type, then performs matrix multiplication iteratively on the input tensor x with each parameter in the list. The final output is returned after all matrix multiplications. The code demonstrates the functionality by creating an instance of the custom layer, manipulating the parameters, and obtaining the resulting tensor shapes." <EXPLAINS> """CODE.import paddle.fluid as fluid
import numpy as np

class MyLayer(fluid.Layer):
    def __init__(self, num_stacked_param):
        super(MyLayer, self).__init__()
        # create ParameterList with iterable Parameters
        self.params = fluid.dygraph.ParameterList(
            [fluid.layers.create_parameter(
                shape=[2, 2], dtype='float32')] * num_stacked_param)

    def forward(self, x):
        for i, p in enumerate(self.params):
            tmp = self._helper.create_variable_for_type_inference('float32')
            self._helper.append_op(
                type="mul",
                inputs={"X": x,
                        "Y": p},
                outputs={"Out": tmp},
                attrs={"x_num_col_dims": 1,
                       "y_num_col_dims": 1})
            x = tmp
        return x

data_np = np.random.uniform(-1, 1, [5, 2]).astype('float32')
with fluid.dygraph.guard():
    x = fluid.dygraph.to_variable(data_np)
    num_stacked_param = 4
    model = MyLayer(num_stacked_param)
    print(len(model.params))  # 4
    res = model(x)
    print(res.shape)  # [5, 2]

    replaced_param = fluid.layers.create_parameter(shape=[2, 3], dtype='float32')
    model.params[num_stacked_param - 1] = replaced_param  # replace last param
    res = model(x)
    print(res.shape)  # [5, 3]
    model.params.append(fluid.layers.create_parameter(shape=[3, 4], dtype='float32'))  # append param
    print(len(model.params))  # 5
    res = model(x)
    print(res.shape)  # [5, 4]""" .

"DESCRIPTION.The code defines a custom layer using PaddlePaddle's dygraph API, which includes a linear transformation operation. It then generates random input data, converts it to a variable, and traces the layer to create a static version. Finally, it saves the static layer as an inference model and loads it for inference using the CPU. The model takes input of shape (2, 3) and outputs a tensor of shape (2, 10)." <EXPLAINS> """CODE.import paddle.fluid as fluid
from paddle.fluid.dygraph import Linear, to_variable, TracedLayer
import numpy as np

class ExampleLayer(fluid.dygraph.Layer):
    def __init__(self):
        super(ExampleLayer, self).__init__()
        self._fc = Linear(3, 10)

    def forward(self, input):
        return self._fc(input)

save_dirname = './saved_infer_model'
in_np = np.random.random([2, 3]).astype('float32')

with fluid.dygraph.guard():
    layer = ExampleLayer()
    in_var = to_variable(in_np)
    out_dygraph, static_layer = TracedLayer.trace(layer, inputs=[in_var])
    static_layer.save_inference_model(save_dirname, feed=[0], fetch=[0])

place = fluid.CPUPlace()
exe = fluid.Executor(place)
program, feed_vars, fetch_vars = fluid.io.load_inference_model(save_dirname,
                                        exe)

fetch, = exe.run(program, feed={feed_vars[0]: in_np}, fetch_list=fetch_vars)
print(fetch.shape) # (2, 10)""" .

"DESCRIPTION.The code defines a custom metric layer class in Python using TensorFlow's keras API. This custom metric layer calculates the mean and sum of the input data and adds them as metrics. Additionally, it implements a call method that calls these metric calculations and returns the input data as output." <EXPLAINS> """CODE.class MyMetricLayer(tf.keras.layers.Layer):
  def __init__(self):
    super(MyMetricLayer, self).__init__(name='my_metric_layer')
    self.mean = tf.keras.metrics.Mean(name='metric_1')

  def call(self, inputs):
    self.add_metric(self.mean(inputs))
    self.add_metric(tf.reduce_sum(inputs), name='metric_2')
    return inputs


inputs = tf.keras.Input(shape=(10,))
x = tf.keras.layers.Dense(10)(inputs)
outputs = tf.keras.layers.Dense(1)(x)
model = tf.keras.Model(inputs, outputs)
model.add_metric(math_ops.reduce_sum(x), name='metric_1')


inputs = tf.keras.Input(shape=(10,))
x = tf.keras.layers.Dense(10)(inputs)
outputs = tf.keras.layers.Dense(1)(x)
model = tf.keras.Model(inputs, outputs)
model.add_metric(tf.keras.metrics.Mean()(x), name='metric_1')
""" .

"DESCRIPTION.The code defines a custom metric layer in a neural network model. The `MyMetricLayer` class initializes two metrics: `metric_1` which calculates the mean of the inputs, and `metric_2` which calculates the sum of the inputs. The `call` method adds these metrics to the layer and returns the inputs." <EXPLAINS> """CODE.class MyMetricLayer(tf.keras.layers.Layer):
  def __init__(self):
    super(MyMetricLayer, self).__init__(name='my_metric_layer')
    self.mean = tf.keras.metrics.Mean(name='metric_1')

  def call(self, inputs):
    self.add_metric(self.mean(inputs))
    self.add_metric(tf.reduce_sum(inputs), name='metric_2')
    return inputs


inputs = tf.keras.Input(shape=(10,))
x = tf.keras.layers.Dense(10)(inputs)
outputs = tf.keras.layers.Dense(1)(x)
model = tf.keras.Model(inputs, outputs)
model.add_metric(math_ops.reduce_sum(x), name='metric_1')


inputs = tf.keras.Input(shape=(10,))
x = tf.keras.layers.Dense(10)(inputs)
outputs = tf.keras.layers.Dense(1)(x)
model = tf.keras.Model(inputs, outputs)
model.add_metric(tf.keras.metrics.Mean()(x), name='metric_1')
""" .

"DESCRIPTION.The code defines a custom model class \"MyModel\" that overrides the train_step method. This method performs a single training step by computing the predictions of the model, calculating the loss based on the predictions and true labels, computing gradients, updating the model's trainable variables using the optimizer, updating the model's metrics, and returning a dictionary of metric results." <EXPLAINS> """CODE.class MyModel(tf.keras.Model):

  def train_step(self, data):
    # If `sample_weight` is not provided, all samples will be weighted
    # equally.
    x, y, sample_weight = tf.keras.utils.unpack_x_y_sample_weight(data)

    with tf.GradientTape() as tape:
      y_pred = self(x, training=True)
      loss = self.compiled_loss(
        y, y_pred, sample_weight, regularization_losses=self.losses)
      trainable_variables = self.trainable_variables
      gradients = tape.gradient(loss, trainable_variables)
      self.optimizer.apply_gradients(zip(gradients, trainable_variables))

    self.compiled_metrics.update_state(y, y_pred, sample_weight)
    return {m.name: m.result() for m in self.metrics}
""" .

"DESCRIPTION.The code defines a custom model class that extends nn.Module and implements ModelHubMixin. It initializes the model with given configuration and layers. It also defines a forward method for the model. It provides the functionality to save model weights to a directory, upload model weights to the Hugging Face hub, and load model weights from the Hugging Face hub for model initialization." <EXPLAINS> """CODE.    class MyModel(nn.Module, ModelHubMixin):
       def __init__(self, **kwargs):
           super().__init__()
           self.config = kwargs.pop("config", None)
           self.layer = ...
       def forward(self, ...)
           return ...

    model = MyModel()
    model.save_pretrained("mymodel", push_to_hub=False) # Saving model weights in the directory
    model.push_to_hub("mymodel", "model-1") # Pushing model-weights to hf-hub

    # Downloading weights from hf-hub & model will be initialized from those weights
    model = MyModel.from_pretrained("username/mymodel@main")""" .

"DESCRIPTION.The code defines a custom model function \"_my_model_fn\" for an estimator using logistic regression head and Adagrad optimizer. It takes features, labels, and mode as inputs, computes logits using a keras model, and creates an estimator specification using the logistic regression head for the given features, labels, mode, optimizer, and logits. The function is then used to create an estimator using tf.estimator.Estimator." <EXPLAINS> """CODE.my_head = tf.contrib.estimator.logistic_regression_head()
my_estimator = tf.contrib.estimator.DNNEstimator(
    head=my_head,
    hidden_units=...,
    feature_columns=...)


def _my_model_fn(features, labels, mode):
  my_head = tf.contrib.estimator.logistic_regression_head()
  logits = tf.keras.Model(...)(features)

  return my_head.create_estimator_spec(
      features=features,
      mode=mode,
      labels=labels,
      optimizer=tf.AdagradOptimizer(learning_rate=0.1),
      logits=logits)
my_estimator = tf.estimator.Estimator(model_fn=_my_model_fn)
""" .

"DESCRIPTION.The code defines a custom model function `my_model_fn` for a logistic regression estimator. It also includes an input function `input_fn_train` for training data. The code then fits the estimator using the input function for training data and makes predictions using the trained estimator on input data `x`." <EXPLAINS> """CODE.  # See tf.contrib.learn.Estimator(...) for details on model_fn structure
  def my_model_fn(...):
    pass

  estimator = LogisticRegressor(model_fn=my_model_fn)

  # Input builders
  def input_fn_train:
    pass

  estimator.fit(input_fn=input_fn_train)
  estimator.predict(x=x)
""" .

"DESCRIPTION.The code defines a custom model function using the tf.contrib.learn.Estimator structure, creates a LogisticRegressor estimator with the custom model function, defines an input function for training data, fits the estimator using the input function, and makes predictions using the fitted estimator on input data x." <EXPLAINS> """CODE.  # See tf.contrib.learn.Estimator(...) for details on model_fn structure
  def my_model_fn(...):
    pass

  estimator = LogisticRegressor(model_fn=my_model_fn)

  # Input builders
  def input_fn_train:
    pass

  estimator.fit(input_fn=input_fn_train)
  estimator.predict(x=x)
""" .

"DESCRIPTION.The code defines a custom object scope for a custom object named \"MyObject\" and registers it. It then creates a dense layer with a weight regularizer parameter set to \"MyObject\". This allows for saving, loading, and other operations to recognize the custom object by name." <EXPLAINS> """CODE.    with CustomObjectScope({"MyObject":MyObject}):
        layer = Dense(..., W_regularizer="MyObject")
        # save, load, etc. will recognize custom object by name
""" .

"DESCRIPTION.The code defines a custom object scope that includes a custom object named \"MyObject\". It then creates a dense layer with a weight regularizer set to \"MyObject\". Any operations like saving or loading will recognize the custom object by name." <EXPLAINS> """CODE.    with CustomObjectScope({"MyObject":MyObject}):
        layer = Dense(..., W_regularizer="MyObject")
        # save, load, etc. will recognize custom object by name
""" .

"DESCRIPTION.The code defines a custom object scope which includes the object MyObject. It then creates a Dense layer with the kernel_regularizer set to 'MyObject'. This allows for saving, loading, or other operations to recognize the custom object by its name." <EXPLAINS> """CODE.    with CustomObjectScope({'MyObject':MyObject}):
        layer = Dense(..., kernel_regularizer='MyObject')
        # save, load, etc. will recognize custom object by name
""" .

"DESCRIPTION.The code defines a custom plugin \"MyPlugin\" that requires the presence of a specific AMP plugin \"MyCustomAMPPlugin\". When attempting to create a trainer object with plugins that include both \"MyPlugin\" and \"NativeAMPPlugin\", it will crash due to the enforcement of using \"MyCustomAMPPlugin\" by \"MyPlugin\"." <EXPLAINS> """CODE.class MyPlugin(DDPPlugin):
    def required_plugins(self):
        return [MyCustomAMPPlugin()]

# Will automatically add the necessary AMP plugin
trainer = Trainer(plugins=[MyPlugin()])

# Crash as MyPlugin enforces custom AMP plugin
trainer = Trainer(plugins=[MyPlugin(), NativeAMPPlugin()])""" .

"DESCRIPTION.The code defines a custom scope for objects in which a specific object named 'MyObject' is registered. This custom scope allows the Dense layer to use 'MyObject' as the kernel regularizer parameter, enabling functions like saving and loading to recognize and work with this custom object by name." <EXPLAINS> """CODE.    with custom_object_scope({'MyObject':MyObject}):
        layer = Dense(..., kernel_regularizer='MyObject')
        # save, load, etc. will recognize custom object by name
""" .

"DESCRIPTION.The code defines a custom sharded linear operation using torch.nn.functional.linear function, which takes input, weight, and bias tensors as arguments and performs a linear transformation on the input tensor using the weight and bias tensors." <EXPLAINS> """CODE.@custom_sharded_op_impl(torch.nn.functional.linear)
def my_custom_sharded_linear(types, args, kwargs, process_group):
  ....

input = torch.rand(10, 32)
weight = sharded_tensor.rand(32, 16)
bias = torch.rand(16)
# This will call 'my_custom_sharded_linear'
torch.nn.functional.linear(input, weight, bias)""" .

"DESCRIPTION.The code defines a custom stopper called TimeStopper that extends the Stopper class. It sets a start time and a deadline of 300 seconds. The stopper's __call__() method always returns False, and the stopper checks if the current time exceeds the deadline in the stop_all() method. The tune.run() function runs a trainable function with 200 samples and stops based on the TimeStopper object." <EXPLAINS> """CODE... code-block:: python

    import time
    from ray import tune
    from ray.tune import Stopper

    class TimeStopper(Stopper):
        def __init__(self):
            self._start = time.time()
            self._deadline = 300

        def __call__(self, trial_id, result):
            return False

        def stop_all(self):
            return time.time() - self._start > self.deadline

    tune.run(Trainable, num_samples=200, stop=TimeStopper())""" .

"DESCRIPTION.The code defines a custom transformer class that inherits from PyToPy, which has a method called transform_ast to transform AST nodes using ast.NodeTransformer classes. An instance of the transformer class is created, and the transform_function method is used to transform a given function f, resulting in a new function with an identical signature to f, along with a module and source map." <EXPLAINS> """CODE.class MyTransformer(PyToPy):

  def transform_ast(self, node, ctx):
    node = <<transform node, usually using ast.NodeTransformer classes>>
    return node

transformer = MyTransfomer()

new_f, module, source_map = transformer.transform_function(f, ...)
# new_f is a function with signature identical to f""" .

"DESCRIPTION.The code defines a custom type called CustomType that includes attributes x, y, and shape of type tf.Tensor, as well as a custom batch encoder for batch processing." <EXPLAINS> """CODE.class CustomBatchEncoder(ExtensionTypeBatchEncoder):
...   pass # Override batch(), unbatch(), encode(), and decode().

class CustomType(BatchableExtensionType):
...   x: tf.Tensor
...   y: tf.Tensor
...   shape: tf.TensorShape
...   __batch_encoder__ = CustomBatchEncoder()""" .

"DESCRIPTION.The code defines a data structure called \"features\" with a key 'x', which contains a 3-dimensional array of shape (1, 2, 3) with integer values of type int32." <EXPLAINS> """CODE.from datasets import Features
features = Features({'x': Array3D(shape=(1, 2, 3), dtype='int32')})
""" .

"DESCRIPTION.The code defines a dataclass called MyOwnBuildConfig that inherits from BuildConfig. It has a method build_commands that returns a list containing the command \"apt-get install libsparsehash-dev\"." <EXPLAINS> """CODE.from dataclasses import dataclass
from lightning_app import BuildConfig

@dataclass
class MyOwnBuildConfig(BuildConfig):

    def build_commands(self):
        return ["apt-get install libsparsehash-dev"]""" .

"DESCRIPTION.The code defines a dataset class for the MNIST dataset, where the dataset can be created either for training or testing. It allows for normalization of the dataset using specified mean and standard deviation values. It provides functionality to download the dataset from the internet and store it in a specified root directory if it is not already downloaded. The dataset class also supports getting the length of the dataset and calculating the count of each target in the dataset." <EXPLAINS> """CODE.Part of the code was copied from
https://github.com/pytorch/vision/blob/build/v0.5.0/torchvision/datasets/mnist.py

Args:
    root: Root directory of dataset where ``MNIST/processed/training.pt``
        and  ``MNIST/processed/test.pt`` exist.
    train: If ``True``, creates dataset from ``training.pt``,
        otherwise from ``test.pt``.
    normalize: mean and std deviation of the MNIST dataset.
    download: If true, downloads the dataset from the internet and
        puts it in root directory. If dataset is already downloaded, it is not
        downloaded again.

Examples:
    dataset = MNIST(download=True)
    len(dataset)
    60000
    torch.bincount(dataset.targets)
    tensor([5923, 6742, 5958, 6131, 5842, 5421, 5918, 6265, 5851, 5949])""" .

"DESCRIPTION.The code defines a dataset feature named 'x' with a shape of (1,2,2,3) and data type of 'int32'." <EXPLAINS> """CODE.from datasets import Features
features = Features({'x': Array4D(shape=(1, 2, 2, 3), dtype='int32')})
""" .

"DESCRIPTION.The code defines a decoder model for sequence generation using a GRU cell. The model takes an input sequence, embeds it using a target embedding layer, processes it through a GRU cell, and generates an output sequence using a fully connected layer. The decoder is initialized with the initial state from an encoder output and dynamically generates the output sequence." <EXPLAINS> """CODE.import paddle.fluid as fluid
import paddle.fluid.layers as layers
trg_emb = fluid.data(name="trg_emb",
                     shape=[None, None, 128],
                     dtype="float32")

trg_embeder = lambda x: fluid.embedding(
    x, size=[10000, 128], param_attr=fluid.ParamAttr(name="trg_embedding"))
output_layer = lambda x: layers.fc(x,
                                size=10000,
                                num_flatten_dims=len(x.shape) - 1,
                                param_attr=fluid.ParamAttr(name=
                                                        "output_w"),
                                bias_attr=False)
helper = layers.SampleEmbeddingHelper(trg_embeder, start_tokens=0, end_token=1)
decoder_cell = layers.GRUCell(hidden_size=128)
decoder = layers.BasicDecoder(decoder_cell, helper, output_fn=output_layer)
outputs = layers.dynamic_decode(
    decoder=decoder, inits=decoder_cell.get_initial_states(encoder_output))""" .

"DESCRIPTION.The code defines a decorator called underscore_memoization that can be used to memoize a function. In this specific case, the function being decorated simply returns the value 10." <EXPLAINS> """CODE.@underscore_memoization
def x(self):
    return 10""" .

"DESCRIPTION.The code defines a deployment using Ray Serve with specific logging configurations, including setting the log level to DEBUG and specifying the log directory. The deployment class simply returns the string \"Hello world!\" when called." <EXPLAINS> """CODE.        from ray import serve
        from ray.serve.schema import LoggingConfig
        # Set log level for the deployment.
        @serve.deployment(LoggingConfig(log_level="DEBUG")
        class MyDeployment:
            def __call__(self) -> str:
                return "Hello world!"
        # Set log directory for the deployment.
        @serve.deployment(LoggingConfig(logs_dir="/my_dir")
        class MyDeployment:
            def __call__(self) -> str:
                return "Hello world!\"""" .

"DESCRIPTION.The code defines a dictionary mapping characters to integer values, with optional exclusion of a specific character (\"*\")." <EXPLAINS> """CODE.from torchaudio.pipelines import MMS_FA as bundle
bundle.get_dict()
{'-': 0, 'a': 1, 'i': 2, 'e': 3, 'n': 4, 'o': 5, 'u': 6, 't': 7, 's': 8, 'r': 9, 'm': 10, 'k': 11, 'l': 12, 'd': 13, 'g': 14, 'h': 15, 'y': 16, 'b': 17, 'p': 18, 'w': 19, 'c': 20, 'v': 21, 'j': 22, 'z': 23, 'f': 24, "'": 25, 'q': 26, 'x': 27, '*': 28}
bundle.get_dict(star=None)
{'-': 0, 'a': 1, 'i': 2, 'e': 3, 'n': 4, 'o': 5, 'u': 6, 't': 7, 's': 8, 'r': 9, 'm': 10, 'k': 11, 'l': 12, 'd': 13, 'g': 14, 'h': 15, 'y': 16, 'b': 17, 'p': 18, 'w': 19, 'c': 20, 'v': 21, 'j': 22, 'z': 23, 'f': 24, "'": 25, 'q': 26, 'x': 27}
""" .

"DESCRIPTION.The code defines a discriminator model for a GAN (Generative Adversarial Network) with an input image shape of (1, 28, 28)." <EXPLAINS> """CODE.Discriminator(img_shape=(1, 28, 28))
Discriminator(
  (model): Sequential(...)
)""" .

"DESCRIPTION.The code defines a discriminator model for a Generative Adversarial Network (GAN) with an input image shape of 1 channel and 28x28 dimensions." <EXPLAINS> """CODE.Discriminator(img_shape=(1, 28, 28))
Discriminator(
  (model): Sequential(...)
)""" .

"DESCRIPTION.The code defines a discriminator model for a neural network with an input image shape of (1, 28, 28) for image classification tasks." <EXPLAINS> """CODE.Discriminator(img_shape=(1, 28, 28))
Discriminator(
  (model): Sequential(...)
)""" .

"DESCRIPTION.The code defines a dispatcher function 'f' that can register different types of functions. It registers an 'inc' function for integers that increments the input by 1, a 'dec' function for floats that decrements the input by 1, and a 'reverse' function for lists and tuples that reverses the input. It then calls the dispatcher function 'f' with different inputs." <EXPLAINS> """CODE.f = Dispatcher('f')
@f.register(int)
def inc(x):
    return x + 1
@f.register(float)
def dec(x):
    return x - 1
@f.register(list)
@f.register(tuple)
def reverse(x):
    return x[::-1]
f(1)
f(1.0)
f([1, 2, 3])""" .

"DESCRIPTION.The code defines a dispatcher that can add two numbers together based on their types (int or float). It then adds the integers 1 and 2 together using the dispatcher." <EXPLAINS> """CODE.D = Dispatcher('add')
D.add((int, int), lambda x, y: x + y)
D.add((float, float), lambda x, y: x + y)
D(1, 2)""" .

"DESCRIPTION.The code defines a feature dictionary object with keys 'a', 'b', and 'c', where 'a' is assigned the value of 'w', 'b' is assigned the value of 'x', and 'c' is a nested dictionary with keys 'd' and 'e' assigned values 'y' and 'z' respectively. Then, the code calls a method '_flatten' on the 'feature' object with a new dictionary argument to flatten the feature dictionary by updating values for keys 'b' and 'c'." <EXPLAINS> """CODE.feature = FeatureDict({
    'a': w,
    'b': x,
    'c': {
        'd': y,
        'e': z,
    },
})

feature._flatten({
    'b': X,
    'c': {
        'd': Y,
    },
})
""" .

"DESCRIPTION.The code defines a feature dictionary with keys 'agent_id' and 'episode', where 'episode' is a dataset containing images of observations and rewards. The code then yields a dictionary with keys 'agent_id' and 'episode', where 'episode' contains a generator that produces dictionaries of observations and rewards for 10 iterations." <EXPLAINS> """CODE.  features=tfds.features.FeatureDict({
   'agent_id': tf.string,
    'episode': tfds.features.Dataset({
      'observation': tfds.features.Image(),
      'reward': tfds.features.Image(),
    }),
  })


yield _, {
  'agent_id': agent_name
  'episode': ({'observation': ..., 'reward': ...} for _ in range(10)),
}
""" .

"DESCRIPTION.The code defines a filter that highlights specific names ('foo', 'bar', 'baz') in a piece of code as functions." <EXPLAINS> """CODE.filter = NameHighlightFilter(
    names=['foo', 'bar', 'baz'],
    tokentype=Name.Function,
)""" .

"DESCRIPTION.The code defines a forward_pre_hook function that modifies the input of a linear layer by multiplying it by 2 before the layer's forward operation is performed. It then registers this hook to a linear layer, applies it to input data, and verifies that the modified input leads to the same output as the original input." <EXPLAINS> """CODE.import paddle.fluid as fluid
import numpy as np

# the forward_post_hook change the input of the layer: input = input * 2
def forward_pre_hook(layer, input):
    # user can use layer and input for information statistis tasks

    # change the input
    input_return = (input[0] * 2)
    return input_return

with fluid.dygraph.guard():
    linear = fluid.Linear(13, 5, dtype="float32")

    # register the hook
    forward_pre_hook_handle = linear.register_forward_pre_hook(forward_pre_hook)

    value0 = np.arange(26).reshape(2, 13).astype("float32")
    in0 = fluid.dygraph.to_variable(value0)
    out0 = linear(in0)

    # remove the hook
    forward_pre_hook_handle.remove()

    value1 = value0 * 2
    in1 = fluid.dygraph.to_variable(value1)
    out1 = linear(in1)

    # hook change the linear's input to input * 2, so out0 is equal to out1.
    assert (out0.numpy() == out1.numpy()).any()""" .

"DESCRIPTION.The code defines a fully connected (FC) layer with 64 output units and 2 flatten dimensions, then feeds a randomly generated data array of shape [30, 10, 32] into the FC layer to obtain the output." <EXPLAINS> """CODE.fc = FC("fc", 2, num_flatten_dims=2)
out = fc(input=[data_1, data_2])

from paddle.fluid.dygraph.base import to_variable
import paddle.fluid as fluid
from paddle.fluid.dygraph import FC
import numpy as np

data = np.random.uniform(-1, 1, [30, 10, 32]).astype('float32')
with fluid.dygraph.guard():
    fc = FC("fc", 64, num_flatten_dims=2)
    data = to_variable(data)
    conv = fc(data)""" .

"DESCRIPTION.The code defines a function \"f\" that computes the dot product of a matrix \"x\" with its transpose using JAX, and blocks until the computation is completed. The function is annotated for profiling with JAX's profiler." <EXPLAINS> """CODE.@jax.profiler.annotate_function
def f(x):
    return jnp.dot(x, x.T).block_until_ready()

@partial(jax.profiler.trace_function, name="event_name")
def f(x):
    return jnp.dot(x, x.T).block_until_ready()""" .

"DESCRIPTION.The code defines a function 'f' that calculates the dot product of a matrix 'x' with its transpose using jax library, and then waits for the computation to finish before returning the result. The second function 'f' additionally traces the function with a custom event name using jax profiler." <EXPLAINS> """CODE.@jax.profiler.trace_function
def f(x):
    return jnp.dot(x, x.T).block_until_ready()

@partial(jax.profiler.trace_function, name="event_name")
def f(x):
    return jnp.dot(x, x.T).block_until_ready()""" .

"DESCRIPTION.The code defines a function _shape_repr that takes a tuple as input parameter and returns a string representation of the shape of the tuple. It handles tuples with different number of elements, including empty tuples." <EXPLAINS> """CODE._shape_repr((1, 2))
_shape_repr((one, 2 * one))
_shape_repr((1,))
_shape_repr(())""" .

"DESCRIPTION.The code defines a function `backend_factory` that creates a requests session with specified proxy settings for both HTTP and HTTPS connections. This function is then used to configure an HTTP backend with the provided proxy settings. Finally, the function `get_session()` is called to get the configured session." <EXPLAINS> """CODE.def backend_factory() -> requests.Session:
    session = requests.Session()
    session.proxies = {"http": "http://10.10.1.10:3128", "https": "https://10.10.1.11:1080"}
    return session

configure_http_backend(backend_factory=backend_factory)

session = get_session()
""" .

"""DESCRIPTION.The code defines a function `cb` that takes a list of tuples as input, where each tuple contains an index and a list of devices. It then retrieves arrays from `global_input_data` based on the provided indices and creates device arrays using `jax.device_put` for each device in the list. Finally, it returns a list of device arrays.

The code then uses `GlobalDeviceArray.from_batched_callback_with_devices` to create a `GlobalDeviceArray` instance `gda` by passing in `global_input_shape`, `global_mesh`, `mesh_axes`, and the previously defined `cb` function.

Lastly, it retrieves the shape of the addressable data at index 0 from `gda` and returns `(1, 2)`.""" <EXPLAINS> """CODE.def cb(cb_inp):
    dbs = []
    for inp in cb_inp:
        index, devices = inp
        array = global_input_data[index]
        dbs.extend([jax.device_put(array, device) for device in devices])
    return dbs

gda = GlobalDeviceArray.from_batched_callback_with_devices(
    global_input_shape, global_mesh, mesh_axes, cb)

gda.addressable_data(0).shape
(1, 2)
""" .

"DESCRIPTION.The code defines a function `f(x)` that adds 1 to the input `x`, and then calls this function with the input value of 1. The code captures the graph generated during the function call and allows for inspection of the graph." <EXPLAINS> """CODE.@def_function.function
def f(x):
  return x + constant_op.constant(1.)

with context.collect_graphs() as graphs:
  with ops.device("CPU:0"):
    f(constant_op.constant(1.))

graph, = graphs  # `graph` contains a single GraphDef for inspection
""" .

"DESCRIPTION.The code defines a function `f` that takes an input array, applies the sine function element-wise, inspects the sharding of the array, squares the resulting array element-wise, and utilizes parallel just-in-time (pjit) compilation." <EXPLAINS> """CODE.import jax
import jax.numpy as jnp
from jax.experimental.maps import Mesh
from jax.experimental.pjit import PartitionSpec, pjit

x = jnp.arange(8, dtype=jnp.float32)
def f_(x):
    x = jnp.sin(x)
    jax.debug.inspect_array_sharding(x, callback=print)
    return jnp.square(x)
f = pjit(f_, in_axis_resources=PartitionSpec('dev'),
         out_axis_resources=PartitionSpec('dev'))
with Mesh(jax.devices(), ('dev',)):
    f.lower(x).compile()  # doctest: +SKIP
""" .

"DESCRIPTION.The code defines a function `foo` that takes a tensor `a` and an integer `b`, and returns the sum of `a` and `b`. Another function `bar` uses `torch.jit.fork` to asynchronously execute `foo` with input `a` and constant `b=2`, then returns the result after waiting for the computation to finish. The code also includes a class `SubMod` and a class `Mod` that uses `torch.jit.fork` to asynchronously execute the `forward` method of `SubMod` with input `a` and constant `b=2`, then return the result after waiting for the computation to finish." <EXPLAINS> """CODE.import torch
from torch import Tensor
def foo(a : Tensor, b : int) -> Tensor:
    return a + b
def bar(a):
    fut : torch.jit.Future[Tensor] = torch.jit.fork(foo, a, b=2)
    return torch.jit.wait(fut)
script_bar = torch.jit.script(bar)
input = torch.tensor(2)
# only the scripted version executes asynchronously
assert script_bar(input) == bar(input)
# trace is not run asynchronously, but fork is captured in IR
graph = torch.jit.trace(bar, (input,)).graph
assert "fork" in str(graph)

import torch
from torch import Tensor
class SubMod(torch.nn.Module):
    def forward(self, a: Tensor, b : int):
        return a + b
class Mod(torch.nn.Module):
    def __init__(self):
        super(self).__init__()
        self.mod = SubMod()
    def forward(self, input):
        fut = torch.jit.fork(self.mod, a, b=2)
        return torch.jit.wait(fut)
input = torch.tensor(2)
mod = Mod()
assert mod(input) == torch.jit.script(mod).forward(input)
""" .

"DESCRIPTION.The code defines a function `func_returning_tuples` that takes in three torch tensors `x`, `y`, and `z`, performs arithmetic operations on them, and returns a tuple containing the manipulated values of `x`, `y`, and `z`. The code then applies the function to create a torch onnx program, which adapts the output of the function for onnx format by removing the nested tuple structure and returning a list of tensors." <EXPLAINS> """CODE.import torch
import torch.onnx
def func_returning_tuples(x, y, z):
...     x = x + y
...     y = y + z
...     z = x + y
...     return (x, (y, z))
x = torch.tensor(1.)
y = torch.tensor(2.)
z = torch.tensor(3.)
onnx_program = torch.onnx.dynamo_export(func_returning_tuples, x, y, z)
pt_output = func_returning_tuples(x, y, z)
print(pt_output)
(tensor(3.), (tensor(5.), tensor(8.)))
print(onnx_program.adapt_torch_outputs_to_onnx(pt_output, model_with_state_dict=func_returning_tuples))
[tensor(3.), tensor(5.), tensor(8.)]
""" .

"DESCRIPTION.The code defines a function `func` and a list `x` with elements 'a', 'b', 'c'. It then evaluates the variables in the scope of the function and accesses the first four pairs of names in the scope using the Jedi library." <EXPLAINS> """CODE.from jedi._compatibility import u
from jedi.parser import Parser
parser = Parser(u('''
x = ['a', 'b', 'c']
def func():
    y = None
'''))
scope = parser.module.subscopes[0]

from jedi.evaluate import Evaluator
pairs = list(get_names_of_scope(Evaluator(), scope))
pairs[0]
pairs[1]
pairs[2]
pairs[3]                                        #doctest: +ELLIPSIS
""" .

"DESCRIPTION.The code defines a function called \"step\" that takes an iterator, retrieves the next element from the iterator, performs some operation on the data, and returns the result. It then creates a TensorFlow graph function called \"tf_step\" based on the \"step\" function. The code then iterates through epochs and steps of a TensorFlow dataset, calling the \"step\" function on each element of the iterator." <EXPLAINS> """CODE.  def step(iterator):
    data = next(iterator)
    # result <= Do something with data
    return result
  tf_step = tf.function(step, reduce_retracing=True)

  # Assume x is a tf.data Dataset.
  data_handler = data_adapter.get_data_handler(x=x)
  # Epoch iteration
  for epo_idx, iterator in data_handler.enumerate_epochs():
      # Stop on dataset exhaustion.
      with data_handler.catch_stop_iteration():
        for step in data_handler.steps(): # Step iteration
            step_result = step(iterator)
""" .

"DESCRIPTION.The code defines a function called 'print_log_msg' that prints a message. It then creates a logs socket using a client function 'create_cluster_logs_socket' with specific parameters. The socket runs continuously in a loop to receive log messages. Finally, it closes the logs socket when finished." <EXPLAINS> """CODE.def print_log_msg(ws_app, msg):
    print(msg)

logs_socket = client.create_cluster_logs_socket("cluster_id", 1661100000, 1661101000, print_log_msg)
logs_socket.run_forever()


def print_log_msg(ws_app, msg):
    print(msg)

logs_socket = client.create_cluster_logs_socket("cluster_id", 1661100000, 1661101000, print_log_msg)

logs_thread = Thread(target=cluster_logs_socket.run_forever)

logs_thread.start()
# .......

logs_socket.close()
""" .

"DESCRIPTION.The code defines a function called dataset_fn that creates a dataset batched and repeated from input data and then shards it among input pipelines. It then distributes the dataset across replicas using experimental_distribute_datasets_from_function. Another function replica_fn_with_signature is defined with a specified input signature using tf.function, which is used to train the model with the distributed dataset in each step using strategy.run." <EXPLAINS> """CODE.def dataset_fn(input_context):
  batch_size = input_context.get_per_replica_batch_size(global_batch_size)
  d = tf.data.Dataset.from_tensors([[1.]]).repeat().batch(batch_size)
  return d.shard(
      input_context.num_input_pipelines, input_context.input_pipeline_id)

inputs = strategy.experimental_distribute_datasets_from_function(dataset_fn)

for batch in inputs:
  replica_results = strategy.run(replica_fn, args=(batch,))

# If you want to specify `input_signature` for a `tf.function` you must
# first create the iterator.
iterator = iter(inputs)

@tf.function(input_signature=[iterator.element_spec])
def replica_fn_with_signature(inputs):
  # train the model with inputs
  return

for _ in range(steps):
  strategy.run(replica_fn_with_signature,
      args=(next(iterator),))
""" .

"DESCRIPTION.The code defines a function called dyfunc that takes an input x, calculates the mean of x using PaddlePaddle's fluid.layers.mean function, and based on the mean value, it either subtracts 1 from x or adds 1 to x. The converted function new_func applies this logic to the input x, creates a 3x3 tensor filled with zeros using fluid.layers.fill_constant, runs the function using fluid.Executor on CPUPlace, and prints the output tensor which contains 1.0 values in a 3x3 matrix." <EXPLAINS> """CODE.import paddle.fluid as fluid
from paddle.fluid.dygraph.dygraph_to_static import convert_call

def dyfunc(x):
    if fluid.layers.mean(x) < 0:
        x_v = x - 1
    else:
        x_v = x + 1

    return x_v

new_func = convert_call(dyfunc)
x = fluid.layers.fill_constant(shape=[3, 3], value=0, dtype='float64')
x_v = new_func(x)
exe = fluid.Executor(fluid.CPUPlace())
out = exe.run(fetch_list=[x_v])
print(out[0])
# [[1. 1. 1.]
#  [1. 1. 1.]
#  [1. 1. 1.]]""" .

"DESCRIPTION.The code defines a function called epsilon2 that takes either one or two arguments and prints them. If only one argument is provided, it prints that one argument twice. If two arguments are provided, it prints both arguments." <EXPLAINS> """CODE.epsilon2(10, 20)
epsilon2(30)""" .

"DESCRIPTION.The code defines a function called foo that takes two input parameters, x and y, adds them together, and returns the result. It then uses the to_static decorator to convert the foo function into a static graph representation. The first print statement outputs the concrete program representation of the decorated foo function with specific input specifications. The second print statement executes the decorated foo function with random input values and outputs its concrete program representation." <EXPLAINS> """CODE.import paddle
from paddle.jit import to_static
from paddle.static import InputSpec

paddle.disable_static()

def foo(x, y):
    z = x + y
    return z

decorated_foo = to_static(foo, input_spec=[InputSpec([10], name='x'), InputSpec([10], name='y')])
print(decorated_foo.concrete_program)

decorated_foo = to_static(foo)
out_foo = decorated_foo(paddle.rand([10]), paddle.rand([10]))
print(decorated_foo.concrete_program)""" .

"DESCRIPTION.The code defines a function called from_key_val_list that takes an object as input. If the input object is a dictionary, the function returns an ordered dictionary with the same key-value pairs. Otherwise, it raises a ValueError stating that more than one value is needed to unpack. The code also includes examples of calling the function with a list of tuples and a dictionary." <EXPLAINS> """CODE.from collections import OrderedDict

def from_key_val_list(obj):
    if isinstance(obj, dict):
        return OrderedDict(obj)
    else:
        raise ValueError('need more than 1 value to unpack')

# äº¤äºå¼å½ä»¤è½¬æ¢æèæ¬ä¸­çä»£ç 
from_key_val_list([('key', 'val')])
from_key_val_list({'key': 'val'})""" .

"DESCRIPTION.The code defines a function called image_description_dict that takes a byte string as input and returns a dictionary containing the shape information of an image with dimensions 256x256x3 and the axes as \"YXS\"." <EXPLAINS> """CODE.image_description_dict(b'shape=(256, 256, 3)')
description = b'{"shape": [256, 256, 3], "axes": "YXS"}'
image_description_dict(description)  # doctest: +SKIP""" .

"DESCRIPTION.The code defines a function called one_step that takes an input and returns it. It then runs the one_step function multiple times using a strategy object, passing in the next item from a distributed dataset iterator as an argument each time." <EXPLAINS> """CODE.@tf.function
def one_step(input):
    return input

for _ in range(step_num):
    strategy.run(one_step, args=(dist_dataset_iterator.get_next(),))""" .

"DESCRIPTION.The code defines a function called some_function which does not take any action and is decorated with a clean up signal decorator." <EXPLAINS> """CODE.@_clean_up_sig
def some_function(self, unused_element_argument, stuff):
    pass""" .

"DESCRIPTION.The code defines a function called supported_instruction_sets() that returns a set of supported instruction sets for different architectures. It provides sets of instruction sets for x86, PPC, and ARM architectures respectively." <EXPLAINS> """CODE.supported_instruction_sets()  # for x86
{"SSE2", "AVX2", ...}
supported_instruction_sets()  # for PPC
{"VSX", "VSX2", ...}
supported_instruction_sets()  # for ARM
{"NEON", "ASIMD", ...}""" .

"DESCRIPTION.The code defines a function called test_tifffile, which takes an optional boolean argument verbose." <EXPLAINS> "CODE.test_tifffile(verbose=False)" .

"DESCRIPTION.The code defines a function called tree_structure that takes in a parameter representing a tree structure and returns a PyTreeSpec object describing the structure of the tree. The PyTreeSpec object shows the mapping of keys to values in the tree structure, with '*' representing any value and 'NoneIsLeaf' indicating that nodes without children are leaf nodes." <EXPLAINS> """CODE.tree = {'b': (2, [3, 4]), 'a': 1, 'c': None, 'd': 5}
tree_structure(tree)
PyTreeSpec({'a': *, 'b': (*, [*, *]), 'c': *, 'd': *}, NoneIsLeaf)
tree_structure(1)
PyTreeSpec(*, NoneIsLeaf)
tree_structure(None)
PyTreeSpec(*, NoneIsLeaf)""" .

"DESCRIPTION.The code defines a function check_error that takes an Error object as input and calls its throw method, which can raise a ValueError. It then jits a function checked_f with jax and assigns the error and output of the function to error and out variables respectively. Finally, it calls the check_error function from the checkify module with the error variable as input." <EXPLAINS> """CODE.def check_error(err: Error) -> None:
    err.throw()  # can raise ValueError

error, out = jax.jit(checked_f)(x)
checkify.check_error(error)""" .

"DESCRIPTION.The code defines a function coerce_odd() that takes a value as input and checks if the value is odd. If the value is odd, it returns the value. Otherwise, it returns None. Additionally, it creates a GraphQLScalarType named 'Odd' with the serialize function set to coerce_odd." <EXPLAINS> """CODE.def coerce_odd(value):
    if value % 2 == 1:
        return value
    return None

OddType = GraphQLScalarType(name='Odd', serialize=coerce_odd)""" .

"DESCRIPTION.The code defines a function f that takes two input parameters x and y, and returns 0.5 times x minus 0.5 times y. It then creates a linear transpose of the function f_transpose using JAX, with input parameters of shape () and dtype np.float32. Finally, it calls the f_transpose function with an input value of 1.0." <EXPLAINS> """CODE.f = lambda x, y: 0.5 * x - 0.5 * y
scalar = types.SimpleNamespace(shape=(), dtype=np.float32)
f_transpose = jax.linear_transpose(f, scalar, scalar)
f_transpose(1.0)""" .

"DESCRIPTION.The code defines a function f(x) that checks if x is not equal to 0, throws an error if x is 0, and then returns the result of 1/x. It then applies a checker to the function f and uses jax.jit to compile the function. Finally, it calls the compiled function with an input of 0, catches the error, and throws it." <EXPLAINS> """CODE.def f(x):
    checkify.check(x!=0, "cannot be zero!")
    return 1/x

checked_f = checkify.checkify(f)
err, out = jax.jit(checked_f)(0)
err.throw()  # doctest: +IGNORE_EXCEPTION_DETAIL""" .

"DESCRIPTION.The code defines a function foo that raises a RuntimeWarning with the message \"bar\". The code then sets a simple filter for warnings to 'once' and calls foo within the context manager. After that, it imports the assert_warns function from numpy.testing and calls foo again. Finally, it uses the all_warnings context manager to check if the assert_warns function correctly raises a RuntimeWarning when calling foo." <EXPLAINS> """CODE.import warnings
def foo():
    warnings.warn(RuntimeWarning("bar"))

with warnings.catch_warnings():
    warnings.simplefilter('once')
    foo()

from numpy.testing import assert_warns
foo()

with all_warnings():
    assert_warns(RuntimeWarning, foo)""" .

"DESCRIPTION.The code defines a function named \"cond\" that takes a predicate, true function, false function, and operand as arguments. It then evaluates the predicate and calls either the true function or false function on the operand based on the result of the predicate evaluation." <EXPLAINS> """CODE.def cond(pred, true_fun, false_fun, operand):
    if pred:
        return true_fun(operand)
    else:
        return false_fun(operand)

jax.lax.cond(
    get_predicate_value(),
    lambda _: 23,
    lambda _: 42,
    operand=None)""" .

"DESCRIPTION.The code defines a function named \"f\" that takes an integer input and returns twice the input value. It then deploys the function as a Ray Serve application under the name \"my_app\". Finally, it retrieves the handle to the deployed application and asserts that when the function is remotely called with the input value of 3, it returns the expected result of 6." <EXPLAINS> """CODE.import ray
from ray import serve

@serve.deployment
def f(val: int) -> int:
    return val * 2

serve.run(f.bind(), name="my_app")
handle = serve.get_app_handle("my_app")
assert handle.remote(3).result() == 6
""" .

"DESCRIPTION.The code defines a function named \"f\" with two parameters, \"a\" and \"b\" with a default value of 1. It then creates a Script object with source, starting position of the function definition, and file name. Later, it retrieves and prints the docstring of the function \"f\"." <EXPLAINS> """CODE.def f(a, b=1):
    "Document for function f."

script = Script(source, 1, len('def f'), 'example.py')
doc = script.goto_definitions()[0].docstring()
print(doc)
print(script.goto_definitions()[0].docstring(raw=True))""" .

"DESCRIPTION.The code defines a function named `my_function` decorated with `@experimental`, which when called, prints \"Hello world!\" to the console." <EXPLAINS> """CODE.@experimental
def my_function():
    print("Hello world!")

my_function()
""" .

"""DESCRIPTION.The code defines a function named cb which takes a list of tuples as input. It iterates through each tuple, extracts the index and list of devices, retrieves an array from global_input_data using the index, and then creates a device buffer for each device in the list. Finally, it returns a list of device buffers.

The code then utilizes the cb function to create a GlobalDeviceArray object using a specific shape, mesh, mesh axes, and the cb function.

Finally, the code accesses the shape of the addressable data at index 0 in the GlobalDeviceArray object, which returns a shape of (1, 2).""" <EXPLAINS> """CODE.def cb(cb_inp):
    dbs = []
    for inp in cb_inp:
        index, devices = inp
        array = global_input_data[index]
        dbs.extend([jax.device_put(array, device) for device in devices])
    return dbs

gda = GlobalDeviceArray.from_batched_callback_with_devices(
    global_input_shape, global_mesh, mesh_axes, cb)

gda.addressable_data(0).shape
(1, 2)
""" .

"DESCRIPTION.The code defines a function named extract_from_ast that takes an abstract syntax tree node and an optional argument babel_style. The function extracts translations from the AST node based on the specified style (default to Babel style). The function returns a list of extracted translations." <EXPLAINS> """CODE.from jinja2 import Environment
env = Environment()
node = env.parse('{{ (_("foo"), _(), ngettext("foo", "bar", 42)) }}')

def extract_from_ast(node, babel_style=True):
    # implementation here
    pass

list(extract_from_ast(node))
list(extract_from_ast(node, babel_style=False))
""" .

"DESCRIPTION.The code defines a function named f with two parameters, a and b with a default value of 1. The function has a documentation string \"Document for function f.\" and it prints out the docstring of the function f using different methods." <EXPLAINS> """CODE.def f(a, b=1):
    "Document for function f."

print(doc)
print(script.infer(1, len('def f'))[0].docstring())
print(script.infer(1, len('def f'))[0].docstring(raw=True))""" .

"DESCRIPTION.The code defines a function named f with two parameters, a and b, where b has a default value of 1. The code then prints out the docstring of the function f and the docstring without formatting characters." <EXPLAINS> """CODE.def f(a, b=1):
    "Document for function f."

print(doc)
print(script.infer(1, len('def f'))[0].docstring())
print(script.infer(1, len('def f'))[0].docstring(raw=True))""" .

"DESCRIPTION.The code defines a function named print_something that prints the process index specified in the input argument using an f-string. It then calls the function print_something." <EXPLAINS> """CODE.@state.on_process(process_index=2)
def print_something():
    print(f"Printed on process {state.process_index}")

print_something()""" .

"DESCRIPTION.The code defines a function that calculates the dot product of a matrix with its transpose and then waits for the result to be ready. The function is then called with a matrix of ones with dimensions 1000x1000." <EXPLAINS> """CODE.@jax.profiler.annotate_function
def f(x):
    return jnp.dot(x, x.T).block_until_ready()

@partial(jax.profiler.trace_function, name="event_name")
def f(x):
    return jnp.dot(x, x.T).block_until_ready()""",
        """CODE.@jax.profiler.trace_function
def f(x):
    return jnp.dot(x, x.T).block_until_ready()

f(jnp.ones((1000, 1000))

@partial(jax.profiler.trace_function, name="event_name")
def f(x):
    return jnp.dot(x, x.T).block_until_ready()

f(jnp.ones((1000, 1000))""" .

"DESCRIPTION.The code defines a function that checks the shapes of the input arrays `a` and `b`. If the shape of `a` is not 2-dimensional or the shape of `b` is not 1-dimensional, it raises a ValueError with a specific message. The function then calculates the dot product of `a` and `b` using JAX's numpy implementation. Finally, the code applies the `raises` function to the input arrays `a` and `b` using a BatchApply function, reshapes the arrays accordingly, and asserts that the output matches the expected result after reshaping." <EXPLAINS> """CODE.import jax, jax.numpy as jnp

a = jax.random.normal(jax.random.key(0), [2, 3, 4])
b = jax.random.normal(jax.random.key(1), [4])

def raises(a, b):
  if len(a.shape) != 2:
    raise ValueError("a must be shape 2")
  if len(b.shape) != 1:
    raise ValueError("b must be shape 1")
  return jnp.dot(a, b)

out = BatchApply(raises)(a, b)
expected_merged_leading = raises(a.reshape(2*3, 4), b)
expected = expected_merged_leading.reshape((2, 3) + expected_merged_leading.shape[1:])
np.testing.assert_array_equal(out, expected)
""" .

"DESCRIPTION.The code defines a function that computes the dot product of the transpose of a sparse matrix M and a vector v, then calls the function with a specific sparse matrix M and vector v." <EXPLAINS> """CODE.@sparse.sparsify
def f(M, v):
  return 2 * M.T @ v

M = sparse.BCOO.fromdense(jnp.arange(12).reshape(3, 4))

v = jnp.array([3, 4, 2])

f(M, v)""" .

"DESCRIPTION.The code defines a function that creates an experiment using a DNNClassifier estimator with specified hidden units, train and evaluation input functions. It then creates a tuner using a study configuration and objective key, and runs the tuner to optimize hyperparameters for the experiment." <EXPLAINS> """CODE.  def _create_my_experiment(config, hparams):
    hidden_units = [hparams.unit_per_layer] * hparams.num_hidden_layers

    return tf.contrib.learn.Experiment(
        estimator=DNNClassifier(config=config, hidden_units=hidden_units),
        train_input_fn=my_train_input,
        eval_input_fn=my_eval_input)

  tuner = create_tuner(study_configuration, objective_key)

  learn_runner.tune(experiment_fn=_create_my_experiment, tuner)
""" .

"DESCRIPTION.The code defines a function that logs metrics using MLflow during a training process. It also utilizes Ray Tune for hyperparameter tuning by specifying a search space for the parameters \"a\" and \"b\". The function trains a model with the given configuration and logs the loss metric to the specified MLflow experiment." <EXPLAINS> """CODE.from ray.tune.integration.mlflow import mlflow_mixin

@mlflow_mixin
def train_fn(config):
    ...
    mlflow.log_metric(...)


from ray.tune.integration.mlflow import mlflow_mixin

@mlflow_mixin
def train_fn(config):
    mlflow.autolog()
    xgboost_results = xgb.train(config, ...)


from ray import tune
from ray.tune.integration.mlflow import mlflow_mixin

import mlflow

# Create the MlFlow expriment.
mlflow.create_experiment("my_experiment")

@mlflow_mixin
def train_fn(config):
    for i in range(10):
        loss = self.config["a"] + self.config["b"]
        mlflow.log_metric(key="loss", value=loss})
    tune.report(loss=loss, done=True)

tune.run(
    train_fn,
    config={
        # define search space here
        "a": tune.choice([1, 2, 3]),
        "b": tune.choice([4, 5, 6]),
        # mlflow configuration
        "mlflow": {
            "experiment_name": "my_experiment",
            "tracking_uri": mlflow.get_tracking_uri()
        }
    })
""" .

"DESCRIPTION.The code defines a function that runs a given replica function on the input data, distributed across multiple replicas using a TensorFlow strategy. The replica function simply multiplies the input by 2." <EXPLAINS> """CODE.@tf.function
def run():
  def value_fn(value_context):
    return value_context.num_replicas_in_sync
  distributed_values = (
      strategy.experimental_distribute_values_from_function(value_fn))
  def replica_fn(input):
    return input * 2
  return strategy.run(replica_fn, args=(distributed_values,))""" .

"DESCRIPTION.The code defines a function that scales a given input tensor by a learnable parameter and returns the result. It then uses automatic differentiation to calculate the gradients of the scaled output tensor with respect to the input tensor and the learnable parameter." <EXPLAINS> """CODE.def learn_scale(scope, x):
    p = scope.param('scale', nn.initializers.zeros, ())
    return p * x

def f(scope, x):
    y, bwd = lift.vjp(learn_scale, scope, x)
    params_grad, x_grad = bwd(jnp.ones(y.shape))
    return y, params_grad, x_grad""" .

"DESCRIPTION.The code defines a function that takes an input array, converts it to a variable, checks if the mean value of the array is greater than 0, and then subtracts 1 from the array if the mean is greater than 0, otherwise it adds 1 to the array. Finally, it returns the modified array." <EXPLAINS> """CODE.import paddle.fluid as fluid
import numpy as np

@fluid.dygraph.jit.declarative
def func(x):
    x = fluid.dygraph.to_variable(x)
    if fluid.layers.mean(x) > 0:
        x_v = x - 1
    else:
        x_v = x + 1
    return x_v

prog_trans = fluid.dygraph.ProgramTranslator()
prog_trans.enable(False)

x = np.ones([1, 2])
# The declarative is disabled so the func is run in dygraph
with fluid.dygraph.guard():
    print(func(x).numpy()) # [[2. 2.]]""" .

"DESCRIPTION.The code defines a function that takes an input x and performs calculations based on the value of z, where z is the result of applying sin function to the sin function of 3.0. If z is greater than 0, the function returns the sin of x; otherwise, it returns the cosine of x. The code also includes JAX library imports and uses JAX's JIT compiler for optimization." <EXPLAINS> """CODE.import jax
import jax.numpy as jnp

@jax.jit
def f(x):
  with jax.ensure_compile_time_eval():
    y = jnp.sin(3.0)
    z = jnp.sin(y)
  if z > 0:  # the value of z is availble and can be used in control flow
    return jnp.sin(x)
  else:
    return jnp.cos(x)

import jax
import jax.numpy as jnp
from jax import random

@jax.jit
def jax_fn(x):
  with jax.ensure_compile_time_eval():
    y = random.randint(random.PRNGKey(0), (1000,1000), 0, 100)
  y2 = y @ y
  x2 = jnp.sum(y2) * x
  return x2""" .

"DESCRIPTION.The code defines a function train_fn that takes in a distributed_iterator as input. It iterates for a specified number of steps (steps_per_loop) and gets optional data from the distributed iterator. If data is available, it runs the step function on the data using a strategy and prints the result." <EXPLAINS> """CODE.def step_fn(x):
...   return x
@tf.function
... def train_fn(distributed_iterator):
...   for _ in tf.range(steps_per_loop):
...     optional_data = distributed_iterator.get_next_as_optional()
...     if not optional_data.has_value():
...       break
...     tf.print(strategy.run(step_fn, args=(optional_data.get_value(),)))
train_fn(distributed_iterator)""" .

"DESCRIPTION.The code defines a function with a docstring that contains the actual code for the function." <EXPLAINS> "CODE.fn_with_code_in_docstring()" .

"DESCRIPTION.The code defines a generator function that yields dictionaries with \"text\" and \"label\" keys. Then it creates an IterableDataset from the generator function." <EXPLAINS> """CODE.def gen():
    yield {"text": "Good", "label": 0}
    yield {"text": "Bad", "label": 1}

ds = IterableDataset.from_generator(gen)
""" .

"DESCRIPTION.The code defines a generator model for a Generative Adversarial Network (GAN) with an input image shape of 1 channel and 8x8 pixels. The generator model is implemented as a sequential neural network." <EXPLAINS> """CODE.Generator(img_shape=(1, 8, 8))
Generator(
  (model): Sequential(...)
)""" .

"DESCRIPTION.The code defines a generator model for generating images with a specified shape, and includes a sequential neural network model." <EXPLAINS> """CODE.Generator(img_shape=(1, 8, 8))
Generator(
  (model): Sequential(...)
)""" .

"DESCRIPTION.The code defines a global input data array of shape (8, 2) and a callback function cb that returns a specific element from the global input data array based on the given index. It then creates a GlobalDeviceArray object gda from the callback function, global input shape, global mesh, and mesh axes." <EXPLAINS> """CODE.global_input_shape = (8, 2)
global_input_data = np.arange(prod(global_input_shape)).reshape(global_input_shape)
def cb(index):
    return global_input_data[index]
gda = GlobalDeviceArray.from_callback(global_input_shape, global_mesh, mesh_axes, cb)""" .

"DESCRIPTION.The code defines a global input shape and data array in the form of a numpy array. It also defines a batched callback function that verifies the length of input indices and returns the corresponding data from the global input data array. Finally, it creates a GlobalDeviceArray object using the batched callback function and other specified parameters." <EXPLAINS> """CODE.global_input_shape = (8, 2)
global_input_data = np.arange(
    prod(global_input_shape)).reshape(global_input_shape)
def batched_cb(indices):
    self.assertEqual(len(indices),len(global_mesh.local_devices))
    return [global_input_data[index] for index in indices]
gda = GlobalDeviceArray.from_batched_callback(global_input_shape, global_mesh, mesh_axes, batched_cb)""" .

"DESCRIPTION.The code defines a global input shape, mesh axes, global mesh, global input data, and a batched callback function. It then creates a GlobalDeviceArray object from the batched callback function, global input shape, global mesh, and mesh axes. Finally, it accesses the shape of the addressable data at index 0, which is (2, 2)." <EXPLAINS> """CODE.global_input_shape = (8, 2)
mesh_axes = P('x')
global_mesh = Mesh(np.array(jax.devices()).reshape(4, 2), ('x', 'y'))
global_input_data = np.arange(math.prod(global_input_shape)).reshape(global_input_shape)

def batched_cb(indices):
  assert len(indices) == len(global_mesh.local_devices)
  return [global_input_data[index] for index in indices]

gda = GlobalDeviceArray.from_batched_callback(global_input_shape, global_mesh, mesh_axes, batched_cb)
gda.addressable_data(0).shape
(2, 2)
""" .

"DESCRIPTION.The code defines a global input shape, mesh axes, global mesh, global input data, and a batched callback function. The batched callback function receives a list of indices and returns values from the global input data corresponding to those indices. Finally, a GlobalDeviceArray is created from the batched callback function, global input shape, global mesh, and mesh axes, and the shape of the addressable data at index 0 is retrieved." <EXPLAINS> """CODE.global_input_shape = (8, 2)
mesh_axes = P('x')
global_mesh = Mesh(np.array(jax.devices()).reshape(4, 2), ('x', 'y'))
global_input_data = np.arange(math.prod(global_input_shape)).reshape(global_input_shape)

def batched_cb(indices):
    assert len(indices) == len(global_mesh.local_devices)
    return [global_input_data[index] for index in indices]

gda = GlobalDeviceArray.from_batched_callback(global_input_shape, global_mesh, mesh_axes, batched_cb)
gda.addressable_data(0).shape
""" .

"DESCRIPTION.The code defines a grammar for parsing either a word consisting of alphabetic characters or a number consisting of digits. It also sets debugging mode for the word parser. Finally, it parses a string containing alternating words and numbers." <EXPLAINS> """CODE.wd = Word(alphas).setName("alphaword")
integer = Word(nums).setName("numword")
term = wd | integer

# turn on debugging for wd
wd.setDebug()

OneOrMore(term).parseString("abc 123 xyz 890")""" .

"DESCRIPTION.The code defines a gym environment space with two different components, 'position' and 'velocity', each with specific shape and data type. It then samples items from this environment space and iterates through the sampled items, returning a dictionary with arrays representing the 'position' and 'velocity' values at each iteration." <EXPLAINS> """CODE.from gym.spaces import Box, Dict
space = Dict({
... 'position': Box(low=0, high=1, shape=(2, 3), dtype=np.float32),
... 'velocity': Box(low=0, high=1, shape=(2, 2), dtype=np.float32)})
items = space.sample()
it = iterate(space, items)
next(it)
{'position': array([-0.99644893, -0.08304597, -0.7238421 ], dtype=float32),
'velocity': array([0.35848552, 0.1533453 ], dtype=float32)}
next(it)
{'position': array([-0.67958736, -0.49076623,  0.38661423], dtype=float32),
'velocity': array([0.7975036 , 0.93317133], dtype=float32)}
next(it)""" .

"DESCRIPTION.The code defines a hyperparameter named 'a' with possible values of 1, 2, and 3. It then asserts that the method value_to_int() in the HyperParameter class should return 1 when given the input 2." <EXPLAINS> """CODE.parameter = HyperParameter('a', {'values': [1, 2, 3]})
assert parameter.value_to_int(2) == 1""" .

"DESCRIPTION.The code defines a keras model using specified inputs and outputs, compiles the model using stochastic gradient descent optimizer and squared hinge loss function." <EXPLAINS> """CODE.model = keras.Model(inputs, outputs)
model.compile('sgd', loss=keras.losses.SquaredHinge())
""" .

"DESCRIPTION.The code defines a language model using the GRU cell. It takes an input sequence of embeddings, applies an embedding layer, and uses the GRU cell to generate the output sequence. The output layer is a fully connected layer that maps the GRU cell output to a sequence of 10000 classes. The decoding process is performed dynamically using the BasicDecoder with a SampleEmbeddingHelper to assist in generating the output sequence." <EXPLAINS> """CODE.import paddle.fluid as fluid
import paddle.fluid.layers as layers
trg_emb = fluid.data(name="trg_emb",
                     shape=[None, None, 128],
                     dtype="float32")

trg_embeder = lambda x: fluid.embedding(
    x, size=[10000, 128], param_attr=fluid.ParamAttr(name="trg_embedding"))
output_layer = lambda x: layers.fc(x,
                                size=10000,
                                num_flatten_dims=len(x.shape) - 1,
                                param_attr=fluid.ParamAttr(name=
                                                        "output_w"),
                                bias_attr=False)
helper = layers.SampleEmbeddingHelper(trg_embeder, start_tokens=0, end_token=1)
decoder_cell = layers.GRUCell(hidden_size=128)
decoder = layers.BasicDecoder(decoder_cell, helper, output_fn=output_layer)
outputs = layers.dynamic_decode(
    decoder=decoder, inits=decoder_cell.get_initial_states(encoder_output))""" .

"DESCRIPTION.The code defines a layout mapping for the weights and biases of different layers in neural network models created using different paradigms (subclass model, functional model, and sequential model). The layout mappings are applied to the weights and biases of the layers in each model to ensure consistency and facilitate resource optimization in training and inference processes." <EXPLAINS> """CODE.layout_map = layout_map_lib.LayoutMap(mesh=self.mesh)
layout_map['d1.kernel'] = layout_1
layout_map['d1.bias'] = layout_2
layout_map['d2.kernel'] = layout_3
layout_map['d2.bias'] = layout_4

## Subclassed model
class SubclassModel(tf.keras.Model):

  def __init__(self, name=None):
    super().__init__(name=name)
    self.d1 = tf.keras.layers.Dense(1000)
    self.d2 = tf.keras.layers.Dense(1000)

  def call(self, inputs):
    x = self.d1(inputs)
    return self.d2(x)

with layout_map_scope(layout_map):
  model = SubclassModel()
# Triggering the creation of weights within or outside of the scope works
inputs = tf.zeros((10, 10))
results = model(inputs)

model.d1.kernel.layout == layout_1
model.d1.bias.layout == layout_2
model.d2.kernel.layout == layout_3
model.d2.bias.layout == layout_4

## Functional model
with layout_map_scope(layout_map):
  inputs = tf.keras.Input((10,), batch_size=10)
  x = tf.keras.layers.Dense(20, name='d1')(inputs)
  output = tf.keras.layers.Dense(30, name='d2')(x)

  model = tf.keras.Model(inputs, output)

d1 = model.layers[1]
d2 = model.layers[2]

d1.kernel.layout == layout_1
d1.bias.layout == layout_2
d1.kernel.layout == layout_3
d1.bias.layout == layout_4

## Sequential model
with layout_map_scope(layout_map):
  model = tf.keras.Sequential([
      tf.keras.layers.Dense(20, name='d1', input_shape=(10,)),
      tf.keras.layers.Dense(30, name='d2')
  ])

d1 = model.layers[0]
d2 = model.layers[1]

d1.kernel.layout == layout_1
d1.bias.layout == layout_2
d1.kernel.layout == layout_3
d1.bias.layout == layout_4
""" .

"DESCRIPTION.The code defines a learning rate and decay parameters for a gradient descent optimizer. It uses an inverse time decay strategy to adjust the learning rate based on the global step. The optimizer is then used to minimize a loss function with the updated learning rate." <EXPLAINS> """CODE.learning_rate = 0.1
decay_steps = 1.0
decay_rate = 0.5
global_step = tf.Variable(0, trainable=False)
learning_rate = tf.compat.v1.train.inverse_time_decay(learning_rate, global_step, decay_steps, decay_rate)
learning_step = (
    tf.compat.v1.train.GradientDescentOptimizer(learning_rate)
    .minimize(...my loss..., global_step=global_step)
)
""" .

"DESCRIPTION.The code defines a learning rate schedule using TensorFlow where the learning rate changes at specific global steps according to the values specified in the boundaries and values lists." <EXPLAINS> """CODE.global_step = tf.Variable(0, trainable=False)
boundaries = [100000, 110000]
values = [1.0, 0.5, 0.1]
learning_rate = tf.compat.v1.train.piecewise_constant(global_step, boundaries, values)
""" .

"DESCRIPTION.The code defines a linear neural network layer with 10 input and 10 output units, generates random input data of shape 10x10, passes the input through the linear layer to get an output, calculates the mean of the output as the loss, initializes momentum optimizer with specified parameters, performs backpropagation to calculate gradients, updates the model weights using momentum optimizer, and clears the calculated gradients. Additionally, the code defines a convolutional layer with 10 input and 10 output channels, using L2 weight decay regularization with coefficient 0.01." <EXPLAINS> """CODE.import paddle
from paddle.regularizer import L1Decay
import numpy as np
linear = paddle.nn.Linear(10, 10)
inp = paddle.rand(shape=[10, 10], dtype="float32")
out = linear(inp)
loss = paddle.mean(out)
beta1 = paddle.to_tensor([0.9], dtype="float32")
beta2 = paddle.to_tensor([0.99], dtype="float32")
momentum = paddle.optimizer.Momentum(
    learning_rate=0.1,
    parameters=linear.parameters(),
    weight_decay=L1Decay(0.0001))
back = out.backward()
momentum.step()
momentum.clear_grad()

my_conv2d = Conv2D(
        in_channels=10,
        out_channels=10,
        kernel_size=1,
        stride=1,
        padding=0,
        weight_attr=ParamAttr(regularizer=L2Decay(coeff=0.01)),
        bias_attr=False)""" .

"DESCRIPTION.The code defines a linear neural network model using PaddlePaddle framework, trains the model on a random dataset with specified batch size and epoch number, implements forward propagation, calculates loss using CrossEntropyLoss, optimizes using Adam optimizer, and saves the trained model." <EXPLAINS> """CODE.import numpy as np
import paddle
import paddle.nn as nn
import import paddle.optimizer as opt

BATCH_SIZE = 16
BATCH_NUM = 4
EPOCH_NUM = 4

IMAGE_SIZE = 784
CLASS_NUM = 10

# define a random dataset
class RandomDataset(paddle.io.Dataset):
    def __init__(self, num_samples):
        self.num_samples = num_samples

    def __getitem__(self, idx):
        image = np.random.random([IMAGE_SIZE]).astype('float32')
        label = np.random.randint(0, CLASS_NUM - 1, (1, )).astype('int64')
        return image, label

    def __len__(self):
        return self.num_samples

class LinearNet(nn.Layer):
    def __init__(self):
        super(LinearNet, self).__init__()
        self._linear = nn.Linear(IMAGE_SIZE, CLASS_NUM)

    @paddle.jit.to_static
    def forward(self, x):
        return self._linear(x)

def train(layer, loader, loss_fn, opt):
    for epoch_id in range(EPOCH_NUM):
        for batch_id, (image, label) in enumerate(loader()):
            out = layer(image)
            loss = loss_fn(out, label)
            loss.backward()
            opt.step()
            opt.clear_grad()
            print("Epoch {} batch {}: loss = {}".format(
                epoch_id, batch_id, np.mean(loss.numpy())))

# create network
layer = LinearNet()
loss_fn = nn.CrossEntropyLoss()
adam = opt.Adam(learning_rate=0.001, parameters=layer.parameters())

# create data loader
dataset = RandomDataset(BATCH_NUM * BATCH_SIZE)
loader = paddle.io.DataLoader(dataset,
    batch_size=BATCH_SIZE,
    shuffle=True,
    drop_last=True,
    num_workers=2)

# train
train(layer, loader, loss_fn, adam)

# save
model_path = "linear.example.model"
paddle.jit.save(layer, model_path)

# load
translated_layer = paddle.jit.load(model_path)

# get program
program = translated_layer.program()""" .

"DESCRIPTION.The code defines a linear operator acting like a batch matrix, solving one linear system for every member of the batch, and verifying the correctness of the solution by performing a matrix-vector multiplication." <EXPLAINS> """CODE.# Make an operator acting like batch matrix A.  Assume A.shape = [..., M, N]
operator = LinearOperator(...)
operator.shape = [..., M, N]

# Solve one linear system for every member of the batch.
RHS = ... # shape [..., M]

X = operator.solvevec(RHS)
# X is the solution to the linear system
# sum_j A[..., :, j] X[..., j] = RHS[..., :]

operator.matvec(X)
==> RHS
""" .

"DESCRIPTION.The code defines a linear operator and applies it to a batch of vectors X, resulting in a new batch of vectors Y. The shape of Y is expected to be [..., M], where M is determined by the linear transformation performed by the operator on X. The calculation is done by multiplying each element of the batch with a corresponding transformation matrix A and summing the results." <EXPLAINS> """CODE.operator = LinearOperator(...)

X = ... # shape [..., N], batch vector

Y = operator.matvec(X)
Y.shape
==> [..., M]

Y[..., :] = sum_j A[..., :, j] X[..., j]
""" .

"DESCRIPTION.The code defines a mapping between regular expressions matching layer names and their corresponding layout configurations. It assigns specific layout configurations (such as 2D or 4D layouts) to different types of layers (dense or conv2d) in a neural network model. The mappings are then used to retrieve the layout configurations for specific layer names, allowing for easy access to the layout information of each layer in the model." <EXPLAINS> """CODE.map = LayoutMap(mesh=None)
map['.*dense.*kernel'] = layout_2d
map['.*dense.*bias'] = layout_1d
map['.*conv2d.*kernel'] = layout_4d
map['.*conv2d.*bias'] = layout_1d

layout_1 = map['dense_1.kernel']    #   layout_1 == layout_2d
layout_2 = map['dense_1.bias']      #   layout_2 == layout_1d
layout_3 = map['dense_2.kernel']    #   layout_3 == layout_2d
layout_4 = map['dense_2.bias']      #   layout_4 == layout_1d
layout_5 = map['my_model/conv2d_123/kernel']    #   layout_5 == layout_4d
layout_6 = map['my_model/conv2d_123/bias']      #   layout_6 == layout_1d
""" .

"DESCRIPTION.The code defines a method called _foo_changed that is triggered when the 'name' attribute changes. It utilizes the observe and observe_compat decorators for monitoring changes. The super() function is used to call the base class's _foo_changed method with the provided arguments." <EXPLAINS> """CODE.@observe('name')
@observe_compat
def _foo_changed(self, change):
    ...
super()._foo_changed(self, name, old, new)""" .

"""DESCRIPTION.The code defines a method called `prepare_data` that contains some good practices such as downloading data, tokenizing, and running other appropriate functions. It also has some bad practices like assigning values directly to class attributes.

Additionally, the code shows the instantiation of a `Trainer` object with different arguments based on the `prepare_data_per_node` parameter value. It also calls methods on a `model` object for data preparation, setup, and data loading for training, validation, and testing.""" <EXPLAINS> """CODE.def prepare_data(self):
    # good
    download_data()
    tokenize()
    etc()

    # bad
    self.split = data_split
    self.some_state = some_other_state()

# DEFAULT
# called once per node on LOCAL_RANK=0 of that node
Trainer(prepare_data_per_node=True)

# call on GLOBAL_RANK=0 (great for shared file systems)
Trainer(prepare_data_per_node=False)

model.prepare_data()
    if ddp/tpu: init()
model.setup(stage)
model.train_dataloader()
model.val_dataloader()
model.test_dataloader()
""" .

"DESCRIPTION.The code defines a method called `thing` which is decorated to be cached per instance. It lazily computes the value of `thing` using the `compute_thing` function and caches the result to avoid recomputation." <EXPLAINS> """CODE.@property
@tracking.cached_per_instance
def thing(self):
  # `thing` is expensive to compute (and may not even be requested), so we
  # want to lazily compute it and then cache it.
  return compute_thing(self)""" .

"DESCRIPTION.The code defines a minimal RNN (Recurrent Neural Network) cell as a layer subclass in Python. The RNN cell consists of a kernel and a recurrent kernel, and it performs matrix operations on input and previous output to produce an output. The code also demonstrates how to use the minimal RNN cell in a single-layer RNN and a stacked RNN structure." <EXPLAINS> """CODE.    # First, let's define a RNN Cell, as a layer subclass.

    class MinimalRNNCell(keras.layers.Layer):

        def __init__(self, units, **kwargs):
            self.units = units
            self.state_size = units
            super(MinimalRNNCell, self).__init__(**kwargs)

        def build(self, input_shape):
            self.kernel = self.add_weight(shape=(input_shape[-1], self.units),
                                          initializer='uniform',
                                          name='kernel')
            self.recurrent_kernel = self.add_weight(
                shape=(self.units, self.units),
                initializer='uniform',
                name='recurrent_kernel')
            self.built = True

        def call(self, inputs, states):
            prev_output = states[0]
            h = K.dot(inputs, self.kernel)
            output = h + K.dot(prev_output, self.recurrent_kernel)
            return output, [output]

    # Let's use this cell in a RNN layer:

    cell = MinimalRNNCell(32)
    x = keras.Input((None, 5))
    layer = RNN(cell)
    y = layer(x)

    # Here's how to use the cell to build a stacked RNN:

    cells = [MinimalRNNCell(32), MinimalRNNCell(64)]
    x = keras.Input((None, 5))
    layer = RNN(cells)
    y = layer(x)
""" .

"DESCRIPTION.The code defines a minimal RNN cell as a layer subclass in Keras, which includes initializing the cell with a given number of units, building the cell with input shape and weights, and implementing the cell's call method to perform operations on inputs and previous outputs. The code also demonstrates how to use the defined cell in a single RNN layer and in a stacked RNN with multiple cells." <EXPLAINS> """CODE.    # First, let's define a RNN Cell, as a layer subclass.

    class MinimalRNNCell(keras.layers.Layer):

        def __init__(self, units, **kwargs):
            self.units = units
            self.state_size = units
            super(MinimalRNNCell, self).__init__(**kwargs)

        def build(self, input_shape):
            self.kernel = self.add_weight(shape=(input_shape[-1], self.units),
                                          initializer='uniform',
                                          name='kernel')
            self.recurrent_kernel = self.add_weight(
                shape=(self.units, self.units),
                initializer='uniform',
                name='recurrent_kernel')
            self.built = True

        def call(self, inputs, states):
            prev_output = states[0]
            h = K.dot(inputs, self.kernel)
            output = h + K.dot(prev_output, self.recurrent_kernel)
            return output, [output]

    # Let's use this cell in a RNN layer:

    cell = MinimalRNNCell(32)
    x = keras.Input((None, 5))
    layer = RNN(cell)
    y = layer(x)

    # Here's how to use the cell to build a stacked RNN:

    cells = [MinimalRNNCell(32), MinimalRNNCell(64)]
    x = keras.Input((None, 5))
    layer = RNN(cells)
    y = layer(x)
""" .

"DESCRIPTION.The code defines a minimal RNN cell as a subclass of a layer. The RNN cell contains a kernel and recurrent kernel which are used to perform calculations on inputs and previous outputs. The cell can be used in a single RNN layer or in a stacked RNN by specifying the number of units in each cell." <EXPLAINS> """CODE.# First, let's define a RNN Cell, as a layer subclass.

class MinimalRNNCell(keras.layers.Layer):

    def __init__(self, units, **kwargs):
        self.units = units
        self.state_size = units
        super(MinimalRNNCell, self).__init__(**kwargs)

    def build(self, input_shape):
        self.kernel = self.add_weight(shape=(input_shape[-1], self.units),
                                      initializer='uniform',
                                      name='kernel')
        self.recurrent_kernel = self.add_weight(
            shape=(self.units, self.units),
            initializer='uniform',
            name='recurrent_kernel')
        self.built = True

    def call(self, inputs, states):
        prev_output = states[0]
        h = backend.dot(inputs, self.kernel)
        output = h + backend.dot(prev_output, self.recurrent_kernel)
        return output, [output]

# Let's use this cell in a RNN layer:

cell = MinimalRNNCell(32)
x = keras.Input((None, 5))
layer = RNN(cell)
y = layer(x)

# Here's how to use the cell to build a stacked RNN:

cells = [MinimalRNNCell(32), MinimalRNNCell(64)]
x = keras.Input((None, 5))
layer = RNN(cells)
y = layer(x)
""" .

"DESCRIPTION.The code defines a minimal model index with information about a model named \"my-cool-model\" that was evaluated for image classification on a dataset named \"Beans\" with an accuracy metric value of 0.9. The code then retrieves the model name as \"my-cool-model\" and the evaluation results which include the task type as \"image-classification\" and the metric type as \"accuracy\"." <EXPLAINS> """CODE.from huggingface_hub.repocard_data import model_index_to_eval_results
# Define a minimal model index
model_index = [
...     {
...         "name": "my-cool-model",
...         "results": [
...             {
...                 "task": {
...                     "type": "image-classification"
...                 },
...                 "dataset": {
...                     "type": "beans",
...                     "name": "Beans"
...                 },
...                 "metrics": [
...                     {
...                         "type": "accuracy",
...                         "value": 0.9
...                     }
...                 ]
...             }
...         ]
...     }
... ]
model_name, eval_results = model_index_to_eval_results(model_index)
model_name
'my-cool-model'
eval_results[0].task_type
'image-classification'
eval_results[0].metric_type
'accuracy'
""" .

"DESCRIPTION.The code defines a model for ImageNet classification using a ResNet architecture." <EXPLAINS> """CODE.ImageNetLightningModel(data_path='missing')
ImageNetLightningModel(
  (model): ResNet(...)
)""" .

"DESCRIPTION.The code defines a model for training on the ImageNet dataset using the Lightning framework. The model appears to be a ResNet model." <EXPLAINS> """CODE.ImageNetLightningModel(data_path='missing')
ImageNetLightningModel(
  (model): ResNet(...)
)""" .

"DESCRIPTION.The code defines a model using different input formats (such as a list or dictionary), tracks the model using an ExportArchive object, and creates an endpoint named \"serve\" that calls the model with specific input signatures using TensorFlow TensorSpec. Additionally, it includes a traced TensorFlow function serving_fn that must be called at least once, which is then added as an endpoint to the ExportArchive object." <EXPLAINS> """CODE.export_archive = ExportArchive()
export_archive.track(model)
export_archive.add_endpoint(
    name="serve",
    fn=model.call,
    input_signature=[tf.TensorSpec(shape=(None, 3), dtype=tf.float32)],
)


export_archive = ExportArchive()
export_archive.track(model)
export_archive.add_endpoint(
    name="serve",
    fn=model.call,
    input_signature=[
        tf.TensorSpec(shape=(None, 3), dtype=tf.float32),
        tf.TensorSpec(shape=(None, 4), dtype=tf.float32),
    ],
)


model = keras.Model(inputs=[x1, x2], outputs=outputs)

export_archive = ExportArchive()
export_archive.track(model)
export_archive.add_endpoint(
    name="serve",
    fn=model.call,
    input_signature=[
        [
            tf.TensorSpec(shape=(None, 3), dtype=tf.float32),
            tf.TensorSpec(shape=(None, 4), dtype=tf.float32),
        ],
    ],
)


model = keras.Model(inputs={"x1": x1, "x2": x2}, outputs=outputs)

export_archive = ExportArchive()
export_archive.track(model)
export_archive.add_endpoint(
    name="serve",
    fn=model.call,
    input_signature=[
        {
            "x1": tf.TensorSpec(shape=(None, 3), dtype=tf.float32),
            "x2": tf.TensorSpec(shape=(None, 4), dtype=tf.float32),
        },
    ],
)


@tf.function()
def serving_fn(x):
    return model(x)

# The function must be traced, i.e. it must be called at least once.
serving_fn(tf.random.normal(shape=(2, 3)))

export_archive = ExportArchive()
export_archive.track(model)
export_archive.add_endpoint(name="serve", fn=serving_fn)
""" .

"DESCRIPTION.The code defines a model using the Keras library with specified input and output layers, compiles the model using stochastic gradient descent optimizer and a hinge loss function." <EXPLAINS> """CODE.model = keras.Model(inputs, outputs)
model.compile('sgd', loss=keras.losses.Hinge())
""" .

"DESCRIPTION.The code defines a model using the input and output layers, compiles the model using Stochastic Gradient Descent optimizer, and sets the loss function as Squared Hinge loss." <EXPLAINS> """CODE.model = keras.Model(inputs, outputs)
model.compile('sgd', loss=keras.losses.SquaredHinge())
""" .

"DESCRIPTION.The code defines a module, parallelizes it using PairwiseParallel, transforms it using pre_dp_module_transform, and then applies DistributedDataParallel (DDP) to the module." <EXPLAINS> """CODE.# xdoctest: +SKIP("distributed")
    from torch.distributed.tensor.parallel import parallelize_module, PairwiseParallel
    from torch.nn.parallel import DistributedDataParallel as DDP
    from torch.distributed.tensor.parallel.ddp import pre_dp_module_transform

    # Define the module.
    m = module(...)
    parallelize_module(m, PairwiseParallel())
    m = pre_dp_module_transform(m)
    m = DDP(m)""" .

"DESCRIPTION.The code defines a multi-input, multi-output neural network model with different branches for each input. Each branch consists of one or more dense layers. The model is created using the function 'get_multi_io_model' with the specified input branches and shared output branch or shared input branch." <EXPLAINS> """CODE.
branch_a = [Input(shape=(2,), name='a'), Dense(), Dense()]
branch_b = [Input(shape=(3,), name='b'), Dense(), Dense()]

model = get_multi_io_model(branch_a, branch_b)



input_branch_a = [Input(shape=(2,), name='a'), Dense(), Dense()]
input_branch_b = [Input(shape=(3,), name='b'), Dense(), Dense()]
shared_output_branch = [Concatenate(), Dense(), Dense()]

model = get_multi_io_model(input_branch_a, input_branch_b,
                           shared_output_branch=shared_output_branch)



shared_input_branch = [Input(shape=(2,), name='in'), Dense(), Dense()]
output_branch_a = [Dense(), Dense()]
output_branch_b = [Dense(), Dense()]

model = get_multi_io_model(output__branch_a, output_branch_b,
                           shared_input_branch=shared_input_branch)
""" .

"DESCRIPTION.The code defines a neural network backbone with two linear layers." <EXPLAINS> """CODE.Backbone()
(l1): Linear(...)
(l2): Linear(...)  """ .

"DESCRIPTION.The code defines a neural network model called \"Foo\" with three Dense layers. During training, the second Dense layer's parameters are normalized using spectral normalization. The model is initialized with some input data and then trained using an optimization step to minimize the L2 loss between the model predictions and target values. Finally, the trained model is used for inference or evaluation on new input data." <EXPLAINS> """CODE.class Foo(nn.Module):
  @nn.compact
  def __call__(self, x, train):
    x = nn.Dense(3)(x)
    # only spectral normalize the params of the second Dense layer
    x = nn.SpectralNorm(nn.Dense(4))(x, update_stats=train)
    x = nn.Dense(5)(x)
    return x

# init
x = jnp.ones((1, 2))
y = jnp.ones((1, 5))
model = Foo()
variables = model.init(jax.random.PRNGKey(0), x, train=False)

# train
def train_step(variables, x, y):
  def loss_fn(params):
    logits, updates = model.apply(
        {'params': params, 'batch_stats': variables['batch_stats']},
        x,
        train=True,
        mutable=['batch_stats'],
    )
    loss = jnp.mean(optax.l2_loss(predictions=logits, targets=y))
    return loss, updates

  (loss, updates), grads = jax.value_and_grad(loss_fn, has_aux=True)(
      variables['params']
  )
  return {
      'params': jax.tree_map(
          lambda p, g: p - 0.1 * g, variables['params'], grads
      ),
      'batch_stats': updates['batch_stats'],
  }, loss
for _ in range(10):
  variables, loss = train_step(variables, x, y)

# inference / eval
out = model.apply(variables, x, train=False)
""" .

"DESCRIPTION.The code defines a neural network model class M with a linear layer, initializes a floating-point model, defines a calibration function to evaluate the model on calibration data, captures the pre-autograd graph of the model, applies quantization using a specified quantizer, and runs calibration on the quantized model." <EXPLAINS> """CODE.import torch
from torch.ao.quantization.quantize_pt2e import prepare_pt2e
from torch._export import capture_pre_autograd_graph
from torch.ao.quantization.quantizer import (
    XNNPACKQuantizer,
    get_symmetric_quantization_config,
)

class M(torch.nn.Module):
    def __init__(self):
        super().__init__()
        self.linear = torch.nn.Linear(5, 10)

   def forward(self, x):
       return self.linear(x)

# initialize a floating point model
float_model = M().eval()

# define calibration function
def calibrate(model, data_loader):
    model.eval()
    with torch.no_grad():
        for image, target in data_loader:
            model(image)

# Step 1. program capture
# NOTE: this API will be updated to torch.export API in the future, but the captured
# result shoud mostly stay the same
m = capture_pre_autograd_graph(m, *example_inputs)
# we get a model with aten ops

# Step 2. quantization
# backend developer will write their own Quantizer and expose methods to allow
# users to express how they
# want the model to be quantized
quantizer = XNNPACKQuantizer().set_global(get_symmetric_quantization_config())
m = prepare_pt2e(m, quantizer)

# run calibration
# calibrate(m, sample_inference_data)""" .

"DESCRIPTION.The code defines a neural network model consisting of two linear layers with input sizes 16 and 8, and output sizes 8 and 4 respectively. A dropout layer is added between the linear layers. The model is then moved to the GPU specified by 'cuda:1' and is split into 8 chunks for processing." <EXPLAINS> """CODE.fc1 = nn.Linear(16, 8).cuda(0)
fc2 = nn.Linear(8, 4).cuda(1)
dropout = nn.Dropout()

# Dropout does not have any parameters/buffers, but we want to
# run it on cuda:1 to avoid any GPU to CPU transfers.
model = nn.Sequential(fc1, fc2, WithDevice(dropout, 'cuda:1'))
model = Pipe(model, chunks=8)""" .

"DESCRIPTION.The code defines a neural network model that includes a lambda layer to square the input and another lambda layer called antirectifier." <EXPLAINS> """CODE.model.add(Lambda(lambda x: x ** 2))
model.add(Lambda(antirectifier))
""" .

"DESCRIPTION.The code defines a neural network model that permutes the dimensions of the input data from (10, 64) to (64, 10) before passing it through the model." <EXPLAINS> """CODE.model = Sequential()
model.add(Permute((2, 1), input_shape=(10, 64)))
# now: model.output_shape == (None, 64, 10)
# note: `None` is the batch dimension
""" .

"DESCRIPTION.The code defines a neural network model that takes input data of shape (3,) and has two dense layers with different activation functions, the first layer has 4 units with a ReLU activation function and the second layer has 5 units with a softmax activation function. The model is compiled using the input and output layers defined." <EXPLAINS> """CODE.import tensorflow as tf

inputs = tf.keras.Input(shape=(3,))
x = tf.keras.layers.Dense(4, activation=tf.nn.relu)(inputs)
outputs = tf.keras.layers.Dense(5, activation=tf.nn.softmax)(x)
model = tf.keras.Model(inputs=inputs, outputs=outputs)


import tensorflow as tf

class MyModel(tf.keras.Model):

  def __init__(self):
    super(MyModel, self).__init__()
    self.dense1 = tf.keras.layers.Dense(4, activation=tf.nn.relu)
    self.dense2 = tf.keras.layers.Dense(5, activation=tf.nn.softmax)

  def call(self, inputs):
    x = self.dense1(inputs)
    return self.dense2(x)


import tensorflow as tf

class MyModel(tf.keras.Model):

  def __init__(self):
    super(MyModel, self).__init__()
    self.dense1 = tf.keras.layers.Dense(4, activation=tf.nn.relu)
    self.dense2 = tf.keras.layers.Dense(5, activation=tf.nn.softmax)
    self.dropout = tf.keras.layers.Dropout(0.5)

  def call(self, inputs, training=False):
    x = self.dense1(inputs)
    if training:
      x = self.dropout(x, training=training)
    return self.dense2(x)
""" .

"DESCRIPTION.The code defines a neural network model using Flax with 3 dense layers and ReLU activation functions. It then manipulates the parameters of the model by finding and updating specific parameters in the second dense layer. Finally, it ensures that the changes made to the parameters are correct and that the original parameters remain unchanged." <EXPLAINS> """CODE.import flax.linen as nn
from flax.cursor import cursor
import jax
import jax.numpy as jnp

class Model(nn.Module):
  @nn.compact
  def __call__(self, x):
    x = nn.Dense(3)(x)
    x = nn.relu(x)
    x = nn.Dense(3)(x)
    x = nn.relu(x)
    x = nn.Dense(3)(x)
    x = nn.relu(x)
    return x

params = Model().init(jax.random.PRNGKey(0), jnp.empty((1, 2)))['params']

def cond_fn(path, value):
  '''Find the second dense layer params.'''
  return 'Dense_1' in path

new_params = cursor(params).find(cond_fn)['bias'].set(params['Dense_1']['bias'] + 1)

for layer in ('Dense_0', 'Dense_1', 'Dense_2'):
  if layer == 'Dense_1':
    assert (new_params[layer]['bias'] == params[layer]['bias'] + 1).all()
  else:
    assert (new_params[layer]['bias'] == params[layer]['bias']).all()

c = cursor(params)
c2 = c.find(cond_fn)
c2['kernel'] += 2
c2['bias'] += 2
new_params = c.build()

for layer in ('Dense_0', 'Dense_1', 'Dense_2'):
  if layer == 'Dense_1':
    assert (new_params[layer]['kernel'] == params[layer]['kernel'] + 2).all()
    assert (new_params[layer]['bias'] == params[layer]['bias'] + 2).all()
  else:
    assert (new_params[layer]['kernel'] == params[layer]['kernel']).all()
    assert (new_params[layer]['bias'] == params[layer]['bias']).all()

assert jax.tree_util.tree_all(
      jax.tree_util.tree_map(
          lambda x, y: (x == y).all(),
          params,
          Model().init(jax.random.PRNGKey(0), jnp.empty((1, 2)))[
              'params'
          ],
      )
  ) # make sure original params are unchanged""" .

"DESCRIPTION.The code defines a neural network model using Flax, which consists of two dense layers. The model takes input data 'x' of shape (16, 9), passes it through the defined dense layers, and returns the output." <EXPLAINS> """CODE.import jax
import jax.numpy as jnp
import flax.linen as nn

class Foo(nn.Module):
    @nn.compact
    def __call__(self, x):
        h = nn.Dense(4)(x)
        return nn.Dense(2)(h)

x = jnp.ones((16, 9))
tabulate_fn = nn.tabulate(Foo(), jax.random.PRNGKey(0))

print(tabulate_fn(x))
""" .

"DESCRIPTION.The code defines a neural network model using Keras library with an input shape of 784. It includes a RandomFourierFeatures layer with output dimension of 4096 and a dense layer with 10 units and softmax activation function. The model is compiled using Adam optimizer, categorical crossentropy loss function, and categorical accuracy metric." <EXPLAINS> """CODE.model = keras.Sequential([
  keras.Input(shape=(784,)),
  RandomFourierFeatures(
      output_dim=4096,
      scale=10.,
      kernel_initializer='gaussian'),
  layers.Dense(units=10, activation='softmax'),
])
model.compile(
    optimizer='adam',
    loss='categorical_crossentropy',
    metrics=['categorical_accuracy']
)


model = keras.Sequential([
  keras.Input(shape=(784,)),
  RandomFourierFeatures(
      output_dim=4096,
      scale=10.,
      kernel_initializer='gaussian'),
  layers.Dense(units=10),
])
model.compile(
    optimizer='adam',
    loss='hinge',
    metrics=['categorical_accuracy']
)
""" .

"DESCRIPTION.The code defines a neural network model using Keras with two input layers of different shapes. Each input is passed through a dense layer with ReLU activation function. The output of the two dense layers is then subtracted element-wise. Finally, the result is passed through another dense layer to produce the output of shape (4)." <EXPLAINS> """CODE.import keras

input1 = keras.layers.Input(shape=(16,))
x1 = keras.layers.Dense(8, activation='relu')(input1)
input2 = keras.layers.Input(shape=(32,))
x2 = keras.layers.Dense(8, activation='relu')(input2)
# Equivalent to subtracted = keras.layers.subtract([x1, x2])
subtracted = keras.layers.Subtract()([x1, x2])

out = keras.layers.Dense(4)(subtracted)
model = keras.models.Model(inputs=[input1, input2], outputs=out)
""" .

"DESCRIPTION.The code defines a neural network model using Keras, compiles the model using stochastic gradient descent as optimizer, mean squared error as loss function, and categorical accuracy as the evaluation metric." <EXPLAINS> """CODE.model = keras.Model(inputs, outputs)
model.compile(
    'sgd',
    loss='mse',
    metrics=[keras.metrics.CategoricalAccuracy()])
""" .

"DESCRIPTION.The code defines a neural network model using LSTM cells for sequence classification. It uses categorical columns and embedding layers for feature representation. It then trains the model using a training input function, evaluates the model using an evaluation input function, and makes predictions using a prediction input function." <EXPLAINS> """CODE.token_sequence = sequence_categorical_column_with_hash_bucket(...)
token_emb = embedding_column(categorical_column=token_sequence, ...)

estimator = RNNClassifier(
    sequence_feature_columns=[token_emb],
    num_units=[32, 16], cell_type='lstm')

# Input builders
def input_fn_train: # returns x, y
  pass
estimator.train(input_fn=input_fn_train, steps=100)

def input_fn_eval: # returns x, y
  pass
metrics = estimator.evaluate(input_fn=input_fn_eval, steps=10)
def input_fn_predict: # returns x, None
  pass
predictions = estimator.predict(input_fn=input_fn_predict)
""" .

"DESCRIPTION.The code defines a neural network model using TensorFlow's Keras API with an input layer of shape (3,) and a dense layer with 2 units. It then compiles the model with the Adam optimizer, mean squared error loss, and mean absolute error metrics. Random input and target data are generated and used to train the model. After training, it checks that all model metrics have a non-zero result and then resets the metrics and checks that their results are all zero." <EXPLAINS> """CODE.inputs = tf.keras.layers.Input(shape=(3,))
outputs = tf.keras.layers.Dense(2)(inputs)
model = tf.keras.models.Model(inputs=inputs, outputs=outputs)
model.compile(optimizer="Adam", loss="mse", metrics=["mae"])

x = np.random.random((2, 3))
y = np.random.randint(0, 2, (2, 2))
_ = model.fit(x, y, verbose=0)
assert all(float(m.result()) for m in model.metrics)

model.reset_metrics()
assert all(float(m.result()) == 0 for m in model.metrics)""" .

"DESCRIPTION.The code defines a neural network model using TensorFlow's Keras API, compiles the model, converts it to an Estimator, and trains the model using a specified input function for a single step." <EXPLAINS> """CODE.keras_model = tf.keras.Model(...)
keras_model.compile(...)

estimator = tf.keras.estimator.model_to_estimator(keras_model)

def input_fn():
  return dataset_ops.Dataset.from_tensors(
      ({'features': features, 'sample_weights': sample_weights},
       targets))

estimator.train(input_fn, steps=1)


inputs = {'a': tf.keras.Input(..., name='a'),
          'b': tf.keras.Input(..., name='b')}
outputs = {'c': tf.keras.layers.Dense(..., name='c')(inputs['a']),
           'd': tf.keras.layers.Dense(..., name='d')(inputs['b'])}
keras_model = tf.keras.Model(inputs, outputs)
keras_model.compile(...)
export_outputs = {'c': tf.estimator.export.RegressionOutput,
                  'd': tf.estimator.export.ClassificationOutput}

estimator = tf.keras.estimator.model_to_estimator(
    keras_model, export_outputs=export_outputs)

def input_fn():
  return dataset_ops.Dataset.from_tensors(
      ({'features': features, 'sample_weights': sample_weights},
       targets))

estimator.train(input_fn, steps=1)
""" .

"DESCRIPTION.The code defines a neural network model using TensorFlow/Keras to perform regression or classification tasks. The model has an input layer with shape (3,) and a dense layer with 2 units. It is compiled with Adam optimizer, mean squared error loss, and mean absolute error metric. Training data (x) and target data (y) are generated randomly, and the model is trained on the data. After training, the model's metrics include loss, mean absolute error, and accuracy for both the overall model and individual output layers." <EXPLAINS> """CODE.inputs = tf.keras.layers.Input(shape=(3,))
outputs = tf.keras.layers.Dense(2)(inputs)
model = tf.keras.models.Model(inputs=inputs, outputs=outputs)
model.compile(optimizer="Adam", loss="mse", metrics=["mae"])
model.metrics_names
[]
x = np.random.random((2, 3))
y = np.random.randint(0, 2, (2, 2))
model.fit(x, y)
model.metrics_names
['loss', 'mae']
inputs = tf.keras.layers.Input(shape=(3,))
d = tf.keras.layers.Dense(2, name='out')
output_1 = d(inputs)
output_2 = d(inputs)
model = tf.keras.models.Model(inputs=inputs, outputs=[output_1, output_2])
model.compile(optimizer="Adam", loss="mse", metrics=["mae", "acc"])
model.fit(x, (y, y))
model.metrics_names
['loss', 'out_loss', 'out_1_loss', 'out_mae', 'out_acc', 'out_1_mae', 'out_1_acc']""" .

"DESCRIPTION.The code defines a neural network model using Xception architecture for image classification with 1000 classes. It then replicates and optimizes the model on 8 GPUs using multi_gpu_model from Keras, generates dummy data for training, and fits the model on the data for 20 epochs with a batch size of 256 on the 8 GPUs. Finally, it saves the model as 'my_model.h5'." <EXPLAINS> """CODE.    import tensorflow as tf
    from keras.applications import Xception
    from keras.utils import multi_gpu_model
    import numpy as np

    num_samples = 1000
    height = 224
    width = 224
    num_classes = 1000

    # Instantiate the base model (or "template" model).
    # We recommend doing this with under a CPU device scope,
    # so that the model's weights are hosted on CPU memory.
    # Otherwise they may end up hosted on a GPU, which would
    # complicate weight sharing.
    with tf.device('/cpu:0'):
        model = Xception(weights=None,
                         input_shape=(height, width, 3),
                         classes=num_classes)

    # Replicates the model on 8 GPUs.
    # This assumes that your machine has 8 available GPUs.
    parallel_model = multi_gpu_model(model, gpus=8)
    parallel_model.compile(loss='categorical_crossentropy',
                           optimizer='rmsprop')

    # Generate dummy data.
    x = np.random.random((num_samples, height, width, 3))
    y = np.random.random((num_samples, num_classes))

    # This `fit` call will be distributed on 8 GPUs.
    # Since the batch size is 256, each GPU will process 32 samples.
    parallel_model.fit(x, y, epochs=20, batch_size=256)

    # Save model via the template model (which shares the same weights):
    model.save('my_model.h5')


     ..
     # Not needed to change the device scope for model definition:
     model = Xception(weights=None, ..)

     try:
         model = multi_gpu_model(model, cpu_relocation=True)
         print("Training using multiple GPUs..")
     except:
         print("Training using single GPU or CPU..")

     model.compile(..)
     ..


     ..
     # Not needed to change the device scope for model definition:
     model = Xception(weights=None, ..)

     try:
         model = multi_gpu_model(model, cpu_merge=False)
         print("Training using multiple GPUs..")
     except:
         print("Training using single GPU or CPU..")
     model.compile(..)
     ..
""" .

"DESCRIPTION.The code defines a neural network model using a sequential architecture with Bidirectional Long Short-Term Memory (LSTM) layers. The model is compiled with categorical crossentropy loss and rmsprop optimizer. The first model has two Bidirectional LSTM layers with 10 units each, followed by a Dense layer with 5 units and a softmax activation function. The second model includes a custom backward layer with a different activation function (relu) and go_backwards set to True in the Bidirectional layer." <EXPLAINS> """CODE.model = Sequential()
model.add(Bidirectional(LSTM(10, return_sequences=True), input_shape=(5, 10)))
model.add(Bidirectional(LSTM(10)))
model.add(Dense(5))
model.add(Activation('softmax'))
model.compile(loss='categorical_crossentropy', optimizer='rmsprop')

# With custom backward layer
model = Sequential()
forward_layer = LSTM(10, return_sequences=True)
backward_layer = LSTM(10, activation='relu', return_sequences=True,
                      go_backwards=True)
model.add(Bidirectional(forward_layer, backward_layer=backward_layer,
                      input_shape=(5, 10)))
model.add(Dense(5))
model.add(Activation('softmax'))
model.compile(loss='categorical_crossentropy', optimizer='rmsprop')
""" .

"DESCRIPTION.The code defines a neural network model using flax that consists of three modules - Baz, Bar, and Foo. Baz is a simple module with a single dense layer. Bar is a module that includes multiple operations, including using Baz module, multiple dense layers, and l2-normalization of parameters. Foo is the main module that comprises dense layers, l2-normalization of parameters, and l2-normalization of specific kernels and parameters in Bar and Baz submodules. The initialization of the model creates and initializes the parameters for the defined modules." <EXPLAINS> """CODE.class Baz(nn.Module):
  @nn.compact
  def __call__(self, x):
    return nn.Dense(2)(x)

class Bar(nn.Module):
  @nn.compact
  def __call__(self, x):
    x = Baz()(x)
    x = nn.Dense(3)(x)
    x = Baz()(x)
    x = nn.Dense(3)(x)
    return x

class Foo(nn.Module):
  @nn.compact
  def __call__(self, x):
    x = nn.Dense(3)(x)
    # l2-normalize all params of the second Dense layer
    x = nn.WeightNorm(nn.Dense(4), variable_filter=None)(x)
    x = nn.Dense(5)(x)
    # l2-normalize all kernels in the Bar submodule and all params in the
    # Baz submodule
    x = nn.WeightNorm(Bar(), variable_filter={'kernel', 'Baz'})(x)
    return x

# init
x = jnp.ones((1, 2))
model = Foo()
variables = model.init(jax.random.key(0), x)

variables
# {
#   params: {
#     ...
#     WeightNorm_0: {
#         Dense_1/bias/scale: Array([1., 1., 1., 1.], dtype=float32),
#         Dense_1/kernel/scale: Array([1., 1., 1., 1.], dtype=float32),
#     },
#     ...
#     WeightNorm_1: {
#         Bar_0/Baz_0/Dense_0/bias/scale: Array([1., 1.], dtype=float32),
#         Bar_0/Baz_0/Dense_0/kernel/scale: Array([1., 1.], dtype=float32),
#         Bar_0/Baz_1/Dense_0/bias/scale: Array([1., 1.], dtype=float32),
#         Bar_0/Baz_1/Dense_0/kernel/scale: Array([1., 1.], dtype=float32),
#         Bar_0/Dense_0/kernel/scale: Array([1., 1., 1.], dtype=float32),
#         Bar_0/Dense_1/kernel/scale: Array([1., 1., 1.], dtype=float32),
#     },
#     ...
#   }
# }
""" .

"DESCRIPTION.The code defines a neural network model using keras, compiles the model using stochastic gradient descent optimizer and mean squared error loss function, and adds recall as a metric for model evaluation." <EXPLAINS> """CODE.model = keras.Model(inputs, outputs)
model.compile('sgd', loss='mse', metrics=[keras.metrics.Recall()])
""" .

"DESCRIPTION.The code defines a neural network model using the Keras library and compiles it with stochastic gradient descent optimizer and LogCosh loss function." <EXPLAINS> """CODE.model = keras.Model(inputs, outputs)
model.compile('sgd', loss=keras.losses.LogCosh())
""" .

"DESCRIPTION.The code defines a neural network model using the Keras library, compiles it using stochastic gradient descent (SGD) as the optimizer, mean squared error (MSE) as the loss function, and True Positives as the evaluation metric." <EXPLAINS> """CODE.model = keras.Model(inputs, outputs)
model.compile('sgd', loss='mse', metrics=[keras.metrics.TruePositives()])
""" .

"DESCRIPTION.The code defines a neural network model using two Bidirectional LSTM layers with 10 units each, followed by a Dense layer with 5 units and a softmax activation function. The model is compiled with a categorical crossentropy loss function and the RMSprop optimizer." <EXPLAINS> """CODE.model = Sequential()
model.add(Bidirectional(LSTM(10, return_sequences=True), input_shape=(5, 10)))
model.add(Bidirectional(LSTM(10)))
model.add(Dense(5))
model.add(Activation('softmax'))
model.compile(loss='categorical_crossentropy', optimizer='rmsprop')
""" .

"DESCRIPTION.The code defines a neural network model which consists of a sequential module with a MaxPool2d layer followed by a DoubleConv module. The MaxPool2d layer performs max pooling operation on the input data, reducing its spatial dimensions. The DoubleConv module applies a sequential operation on the input data, which likely involves multiple convolutional layers." <EXPLAINS> """CODE.Down(
  (net): Sequential(
    (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (1): DoubleConv(
      (net): Sequential(...)
    )
  )
)""" .

"DESCRIPTION.The code defines a neural network model with 3 dense layers followed by ReLU activation. It then initializes the model parameters and defines an update function that multiplies all dense kernel parameters by 2 and adds 1, while subtracting the bias parameter of the second dense layer by 1. Finally, it applies the update function to the parameters and verifies the correctness of the updated parameters with some assertions." <EXPLAINS> """CODE.import flax.linen as nn
from flax.cursor import cursor
import jax
import jax.numpy as jnp

class Model(nn.Module):
  @nn.compact
  def __call__(self, x):
    x = nn.Dense(3)(x)
    x = nn.relu(x)
    x = nn.Dense(3)(x)
    x = nn.relu(x)
    x = nn.Dense(3)(x)
    x = nn.relu(x)
    return x

params = Model().init(jax.random.PRNGKey(0), jnp.empty((1, 2)))['params']

def update_fn(path, value):
  '''Multiply all dense kernel params by 2 and add 1.
  Subtract the Dense_1 bias param by 1.'''
  if 'kernel' in path:
    return value * 2 + 1
  elif 'Dense_1' in path and 'bias' in path:
    return value - 1
  return value

c = cursor(params)
new_params = c.apply_update(update_fn).build()
for layer in ('Dense_0', 'Dense_1', 'Dense_2'):
  assert (new_params[layer]['kernel'] == 2 * params[layer]['kernel'] + 1).all()
  if layer == 'Dense_1':
    assert (new_params[layer]['bias'] == jnp.array([-1, -1, -1])).all()
  else:
    assert (new_params[layer]['bias'] == params[layer]['bias']).all()

assert jax.tree_util.tree_all(
      jax.tree_util.tree_map(
          lambda x, y: (x == y).all(),
          params,
          Model().init(jax.random.PRNGKey(0), jnp.empty((1, 2)))[
              'params'
          ],
      )
  ) # make sure original params are unchanged""" .

"DESCRIPTION.The code defines a neural network model with a sequential structure. It first applies max pooling with a kernel size of 2 and a stride of 2. Then it applies a double convolution operation." <EXPLAINS> """CODE.Down(
  (net): Sequential(
    (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (1): DoubleConv(
      (net): Sequential(...)
    )
  )
)""" .

"DESCRIPTION.The code defines a neural network model with a single hidden layer of 10 nodes using TensorFlow's Keras API. It then compiles the model, prepares training and testing datasets, and trains the model for 10 epochs." <EXPLAINS> """CODE.strategy = utils.get_strategy()
with strategy.scope():
  model = tf.keras.Sequential([tf.keras.layers.Dense(10)])

model.compile(...)
train_ds, test_ds = ...
model.fit(train_ds, validation_data=test_ds, epochs=10)
""" .

"DESCRIPTION.The code defines a neural network model with a single input layer of shape 28x28x1 and a max pooling layer that reduces the size of the input by applying max pooling with pool size 2 and strides 2." <EXPLAINS> """CODE. y = tf.compat.v1.layers.max_pooling1d(x, pool_size=2, strides=2)
 x = tf.keras.Input((28, 28, 1))
 y = tf.keras.layers.MaxPooling1D(pool_size=2, strides=2)(x)
 model = tf.keras.Model(x, y)
""" .

"DESCRIPTION.The code defines a neural network model with a single layer of 10 neurons and compiles it using stochastic gradient descent (SGD) as the optimizer and mean squared error (mse) as the loss function. It also creates a dataset function to handle input data processing, sets up input options for distributed training, and fits the model using a dataset with 10 epochs and 10 steps per epoch. Finally, it employs a ParameterServerStrategy for distributing training across multiple devices." <EXPLAINS> """CODE.model = tf.keras.Sequential([tf.keras.layers.Dense(10)])
model.compile(tf.keras.optimizers.SGD(), loss="mse")

def dataset_fn(input_context):
  global_batch_size = 64
  batch_size = input_context.get_per_replica_batch_size(global_batch_size)
  dataset = tf.data.Dataset.from_tensors(([1.], [1.])).repeat()
  dataset = dataset.shard(
      input_context.num_input_pipelines, input_context.input_pipeline_id)
  dataset = dataset.batch(batch_size)
  dataset = dataset.prefetch(2)
  return dataset

input_options = tf.distribute.InputOptions(
    experimental_fetch_to_device=True,
    experimental_per_replica_buffer_size=2)
model.fit(tf.keras.utils.experimental.DatasetCreator(
    dataset_fn, input_options=input_options), epochs=10, steps_per_epoch=10)

strategy = tf.distribute.experimental.ParameterServerStrategy(
    cluster_resolver)
with strategy.scope():
  model = tf.keras.Sequential([tf.keras.layers.Dense(10)])
model.compile(tf.keras.optimizers.SGD(), loss="mse")
""" .

"DESCRIPTION.The code defines a neural network model with a time distributed layer that applies a dense or convolutional operation with a specific input shape." <EXPLAINS> """CODE.model = Sequential()
model.add(TimeDistributed(Dense(8), input_shape=(10, 16)))


model = Sequential()
model.add(TimeDistributed(Convolution2D(64, 3, 3), input_shape=(10, 3, 299, 299)))
""" .

"DESCRIPTION.The code defines a neural network model with an input layer of shape 32 and a dense layer of 32 units. It then creates a model using these layers and compiles it with a GradientDescentOptimizer with a learning rate of 1.0. Finally, the code converts the model to run on a TPU with 2 replicas." <EXPLAINS> """CODE.a = Input(shape=(32,))
b = Dense(32)(a)
model = Model(inputs=a, outputs=b)
model = keras_support.tpu_model(model)
model.compile(
    optimizer=tf.train.GradientDescentOptimizer(learning_rate=1.0),
    ...)
a = Input(shape=(32,))
b = Dense(32)(a)
model = Model(inputs=a, outputs=b)
model = keras_support.tpu_model(model, replicas=2)
model.compile(
    optimizer=tf.train.GradientDescentOptimizer(learning_rate=1.0),
    ...)""" .

"DESCRIPTION.The code defines a neural network model with multiple layers using TensorFlow's Keras API. The model takes inputs with a shape of (3,), applies a Dense layer with 4 units and ReLU activation function, followed by another Dense layer with 5 units and softmax activation function. The model is constructed and returned." <EXPLAINS> """CODE.import tensorflow as tf

inputs = tf.keras.Input(shape=(3,))
x = tf.keras.layers.Dense(4, activation=tf.nn.relu)(inputs)
outputs = tf.keras.layers.Dense(5, activation=tf.nn.softmax)(x)
model = tf.keras.Model(inputs=inputs, outputs=outputs)


import tensorflow as tf

class MyModel(tf.keras.Model):

  def __init__(self):
    super(MyModel, self).__init__()
    self.dense1 = tf.keras.layers.Dense(4, activation=tf.nn.relu)
    self.dense2 = tf.keras.layers.Dense(5, activation=tf.nn.softmax)

  def call(self, inputs):
    x = self.dense1(inputs)
    return self.dense2(x)


import tensorflow as tf

class MyModel(tf.keras.Model):

  def __init__(self):
    super(MyModel, self).__init__()
    self.dense1 = tf.keras.layers.Dense(4, activation=tf.nn.relu)
    self.dense2 = tf.keras.layers.Dense(5, activation=tf.nn.softmax)
    self.dropout = tf.keras.layers.Dropout(0.5)

  def call(self, inputs, training=False):
    x = self.dense1(inputs)
    if training:
      x = self.dropout(x, training=training)
    return self.dense2(x)
""" .

"DESCRIPTION.The code defines a neural network model with two input layers of different shapes, each connected to a dense layer with ReLU activation function. The output of these layers are then multiplied element-wise, followed by another dense layer with 4 units. Finally, a model is created with the defined inputs and outputs." <EXPLAINS> """CODE.x1 = np.arange(3.0)
x2 = np.arange(3.0)
tf.keras.layers.multiply([x1, x2])

input1 = tf.keras.layers.Input(shape=(16,))
x1 = tf.keras.layers.Dense(8, activation='relu')(input1)
input2 = tf.keras.layers.Input(shape=(32,))
x2 = tf.keras.layers.Dense(8, activation='relu')(input2)
out = tf.keras.layers.multiply([x1,x2])
out = tf.keras.layers.Dense(4)(out)
model = tf.keras.models.Model(inputs=[input1, input2], outputs=out)""" .

"DESCRIPTION.The code defines a neural network model with two layers of locally connected 2D convolutional layers, one with 64 filters of size 3x3 and the other with 32 filters of size 3x3. The input shape of the model is (32, 32, 3)." <EXPLAINS> """CODE.model = Sequential()
model.add(LocallyConnected2D(64, (3, 3), input_shape=(32, 32, 3)))
model.add(LocallyConnected2D(32, (3, 3)))
""" .

"DESCRIPTION.The code defines a no-op decoder function in TensorFlow datasets that decodes a feature normally. The function is then used as a decoder for the 'image' feature when loading the 'mnist' dataset with the specified split for training." <EXPLAINS> """CODE.@tfds.decode.make_decoder(output_dtype=tf.string)
def no_op_decoder(example, feature):
  \"\"\"Decoder simply decoding feature normally.\"\"\"
  return feature.decode_example(example)

tfds.load('mnist', split='train', decoders: {
    'image': no_op_decoder(),
})""" .

"DESCRIPTION.The code defines a pandas Series with structured data containing version and project information. The data type of the Series is specified using pyarrow, and it consists of integer and string fields." <EXPLAINS> """CODE.import pyarrow as pa
import pandas as pd

s = pd.Series(
    [
        {"version": 1, "project": "pandas"},
        {"version": 2, "project": "pandas"},
        {"version": 1, "project": "numpy"},
    ],
    dtype=pd.ArrowDtype(pa.struct(
        [("version", pa.int64()), ("project", pa.string())]
    ))
)

s.struct.dtypes
version     int64[pyarrow]
project    string[pyarrow]
dtype: object""" .

"DESCRIPTION.The code defines a parser for parsing ISO8601 formatted datetime strings and converts the parsed string into a datetime object." <EXPLAINS> """CODE.dt_expr = pyparsing_common.iso8601_datetime.copy()
dt_expr.setParseAction(pyparsing_common.convertToDatetime())""" .

"DESCRIPTION.The code defines a parser that counts the number of elements in an array and then parses a string to extract the elements based on the count, where the elements are alphanumeric words. It also converts binary constants to integers before parsing." <EXPLAINS> """CODE.countedArray(Word(alphas)).parseString('2 ab cd ef')
binaryConstant = Word('01').setParseAction(lambda t: int(t[0], 2))
countedArray(Word(alphas), intExpr=binaryConstant).parseString('10 ab cd ef')""" .

"DESCRIPTION.The code defines a parser to handle arguments without adding help, adds custom argument \"--my_custom_arg\" with a default value of 'something', then parses the arguments and creates a Trainer object using the parsed arguments with logger disabled." <EXPLAINS> """CODE.parser = ArgumentParser(add_help=False)
parser = Trainer.add_argparse_args(parser)
parser.add_argument('--my_custom_arg', default='something')  # doctest: +SKIP
args = Trainer.parse_argparser(parser.parse_args(""))
trainer = Trainer.from_argparse_args(args, logger=False)""" .

"DESCRIPTION.The code defines a pattern to match one or more alphabetical words in a string. It then parses the input string to return a list of matched words. It also ignores C-style comments in the input string while parsing." <EXPLAINS> """CODE.patt = OneOrMore(Word(alphas))
patt.parseString('ablaj /* comment */ lskjd') # -> ['ablaj']

patt.ignore(cStyleComment)
patt.parseString('ablaj /* comment */ lskjd') # -> ['ablaj', 'lskjd']""" .

"DESCRIPTION.The code defines a piecewise constant learning rate decay schedule based on the given boundaries and values, and calculates the learning rate based on the current step during optimization." <EXPLAINS> """CODE.step = tf.Variable(0, trainable=False)
boundaries = [100000, 110000]
values = [1.0, 0.5, 0.1]
learning_rate_fn = keras.optimizers.schedules.PiecewiseConstantDecay(
    boundaries, values)

# Later, whenever we perform an optimization step, we pass in the step.
learning_rate = learning_rate_fn(step)
""" .

"DESCRIPTION.The code defines a placeholder for input data with a shape of (2, 4, 5)." <EXPLAINS> """CODE.input_ph = tf.keras.backend.placeholder(shape=(2, 4, 5))
input_ph
""" .

"DESCRIPTION.The code defines a placeholder tensor of shape (2, 3) with a data type of float32, then casts the placeholder tensor to have a data type of float16." <EXPLAINS> """CODE.from keras import backend as K
input = K.placeholder((2, 3), dtype='float32')
K.cast(input, dtype='float16')
input = K.cast(input, dtype='float16')
""" .

"DESCRIPTION.The code defines a placeholder tensor with shape (2, 4, 5) using the Keras backend." <EXPLAINS> """CODE.from keras import backend as K
input_ph = K.placeholder(shape=(2, 4, 5))
input_ph._keras_shape
input_ph
""" .

"DESCRIPTION.The code defines a random dataset with images and labels, where each image is an array of random floating point numbers and each label is a random integer between 0 and 9. It creates two instances of this random dataset, each with 10 samples, and chains them together into a single dataset. Finally, it iterates over the dataset to print each image and its corresponding label." <EXPLAINS> """CODE.import numpy as np
import paddle
from paddle.io import IterableDataset, ChainDataset

# define a random dataset
class RandomDataset(IterableDataset):
    def __init__(self, num_samples):
        self.num_samples = num_samples

    def __iter__(self):
        for i in range(10):
            image = np.random.random([32]).astype('float32')
            label = np.random.randint(0, 9, (1, )).astype('int64')
            yield image, label

dataset = ChainDataset([RandomDataset(10), RandomDataset(10)])
for image, label in iter(dataset):
    print(image, label)""" .

"DESCRIPTION.The code defines a remote actor class called MyActor that inherits from RayServeMixin. It specifies a serve method 'my_method' which can be called with an argument." <EXPLAINS> """CODE.@ray.remote
class MyActor(RayServeMixin):
    # This is optional, by default it is "__call__"
    serve_method = 'my_method'

    def my_method(self, arg):
        ...""" .

"DESCRIPTION.The code defines a scheduler function that adjusts the learning rate based on the epoch. If the epoch is less than 10, the learning rate remains the same, otherwise it decays exponentially. It then creates a Sequential model with a Dense layer of 10 units, compiles the model using stochastic gradient descent optimizer and mean squared error loss. A LearningRateScheduler callback is defined using the scheduler function to adjust the learning rate during training. Finally, the model is trained for 15 epochs and the learning rate of the optimizer is rounded to 5 decimal places after training." <EXPLAINS> """CODE.def scheduler(epoch, lr):
    if epoch < 10:
        return lr
    else:
        return lr * tf.math.exp(-0.1)

model = tf.keras.models.Sequential([tf.keras.layers.Dense(10)])
model.compile(tf.keras.optimizers.SGD(), loss='mse')
round(model.optimizer.lr.numpy(), 5)

callback = tf.keras.callbacks.LearningRateScheduler(scheduler)
history = model.fit(np.arange(100).reshape(5, 20), np.zeros(5),
                    epochs=15, callbacks=[callback], verbose=0)
round(model.optimizer.lr.numpy(), 5)""" .

"DESCRIPTION.The code defines a sequential model and adds layers to it using the TimeDistributed layer wrapper. The first block of code adds a TimeDistributed Dense layer with an output shape of (None, 10, 8), and the second block adds another TimeDistributed Dense layer to the model, changing the output shape to (None, 10, 32). The third block adds a TimeDistributed Conv2D layer to the model, with input shape (10, 299, 299, 3), creating a convolutional neural network with a specific architecture." <EXPLAINS> """CODE.    model = Sequential()
    model.add(TimeDistributed(Dense(8), input_shape=(10, 16)))
    # now model.output_shape == (None, 10, 8)


    model.add(TimeDistributed(Dense(32)))
    # now model.output_shape == (None, 10, 32)


    model = Sequential()
    model.add(TimeDistributed(Conv2D(64, (3, 3)),
                              input_shape=(10, 299, 299, 3)))
""" .

"DESCRIPTION.The code defines a sequential model in Keras using locally connected 1D layers with 64 and 32 filters respectively and a kernel size of 3. The input shape of the first layer is (10, 32)." <EXPLAINS> """CODE.model = Sequential()
model.add(LocallyConnected1D(64, 3, input_shape=(10, 32)))
model.add(LocallyConnected1D(32, 3))
""" .

"DESCRIPTION.The code defines a sequential model in Keras with two locally connected 2D layers. The first layer takes an input shape of (32, 32, 3) and has 64 filters with a (3, 3) kernel size. The second locally connected layer has 32 filters with a (3, 3) kernel size." <EXPLAINS> """CODE.model = Sequential()
model.add(LocallyConnected2D(64, (3, 3), input_shape=(32, 32, 3)))
model.add(LocallyConnected2D(32, (3, 3)))
""" .

"DESCRIPTION.The code defines a sequential model in Python using keras library. The model consists of two dense layers, the first layer has 32 neurons and expects input arrays of shape (*, 16) and outputs arrays of shape (*, 32). The second layer also has 32 neurons, and the size of the input is automatically inferred from the output of the first layer." <EXPLAINS> """CODE.# as first layer in a sequential model:
model = Sequential()
model.add(Dense(32, input_shape=(16,)))
# now the model will take as input arrays of shape (*, 16)
# and output arrays of shape (*, 32)

# after the first layer, you don't need to specify
# the size of the input anymore:
model.add(Dense(32))
""" .

"DESCRIPTION.The code defines a series of test functions using different combinations of parameters and decorators based on certain conditions. Each test function is used to test different scenarios or properties of functions, operations, modules, or objects in the codebase." <EXPLAINS> """CODE.@decorateIf(unittest.skip, lambda params: params["x"] == 2)
@parametrize("x", range(5))
def test_foo(self, x):
    ...

@parametrize("x,y", [(1, 'foo'), (2, 'bar'), (3, 'baz')])
@decorateIf(
    unittest.expectedFailure,
    lambda params: params["x"] == 3 and params["y"] == "baz"
)
def test_bar(self, x, y):
    ...

@decorateIf(
    unittest.expectedFailure,
    lambda params: params["op"].name == "add" and params["dtype"] == torch.float16
)
@ops(op_db)
def test_op_foo(self, device, dtype, op):
    ...

@decorateIf(
    unittest.skip,
    lambda params: params["module_info"].module_cls is torch.nn.Linear and params["device"] == "cpu"
)
@modules(module_db)
def test_module_foo(self, device, dtype, module_info):
    ...""" .

"DESCRIPTION.The code defines a set of hyperparameters, saves them to a CSV file, loads the hyperparameters from the CSV file, compares the original hyperparameters with the loaded hyperparameters, and finally deletes the CSV file." <EXPLAINS> """CODE.hparams = Namespace(batch_size=32, learning_rate=0.001, data_root='./any/path/here')
path_csv = './testing-hparams.csv'
save_hparams_to_tags_csv(path_csv, hparams)
hparams_new = load_hparams_from_tags_csv(path_csv)
vars(hparams) == hparams_new
os.remove(path_csv)""" .

"DESCRIPTION.The code defines a simple neural network model using PyTorch, consisting of a single linear layer with 64 input features and 4 output features. The model takes input data, applies a ReLU activation function to the output of the linear layer, and returns the result. The code then saves the model in ONNX format to a temporary file and checks if the file exists." <EXPLAINS> """CODE.class SimpleModel(LightningModule):
    def __init__(self):
        super().__init__()
        self.l1 = torch.nn.Linear(in_features=64, out_features=4)

    def forward(self, x):
        return torch.relu(self.l1(x.view(x.size(0), -1)))

with tempfile.NamedTemporaryFile(suffix='.onnx', delete=False) as tmpfile:
    model = SimpleModel()
    input_sample = torch.randn((1, 64))
    model.to_onnx(tmpfile.name, input_sample, export_params=True)
    os.path.isfile(tmpfile.name)""" .

"DESCRIPTION.The code defines a simple neural network model with one linear layer. It then converts the model to TorchScript and saves it to a file named \"model.pt\". Finally, it checks if the file \"model.pt\" exists." <EXPLAINS> """CODE.class SimpleModel(LightningModule):
    def __init__(self):
        super().__init__()
        self.l1 = torch.nn.Linear(in_features=64, out_features=4)

    def forward(self, x):
        return torch.relu(self.l1(x.view(x.size(0), -1)))

model = SimpleModel()
torch.jit.save(model.to_torchscript(), "model.pt")
os.path.isfile("model.pt")
""" .

"DESCRIPTION.The code defines a sparse feature embedding for input data with dimensions 1024x64. It sets up data input placeholders for 'show', 'click', and 'ins', and creates a distributed ShowClickEntry object. The code then creates a sparse embedding layer using the input data, the specified dimensions, and initialization parameters, without actually running the embedding layer for inference." <EXPLAINS> """CODE.import paddle
paddle.enable_static()

sparse_feature_dim = 1024
embedding_size = 64

shows = paddle.static.data(name='show', shape=[1], dtype='int64')
clicks = paddle.static.data(name='click', shape=[1], dtype='int64')
input = paddle.static.data(name='ins', shape=[1], dtype='int64')

entry = paddle.distributed.ShowClickEntry("show", "click")

emb = paddle.static.nn.sparse_embedding(
    input=input,
    size=[sparse_feature_dim, embedding_size],
    is_test=False,
    entry=entry,
    param_attr=paddle.ParamAttr(name="SparseFeatFactors",
                               initializer=paddle.nn.initializer.Uniform()))""" .

"DESCRIPTION.The code defines a sparse lookup operation for embeddings using the \"_pull_sparse_v2\" function in the PaddlePaddle deep learning framework. It takes input data, specified as a sequence of integers, and retrieves corresponding embeddings from a sparse embedding table with a specified size and accessor class for DownpourCtr." <EXPLAINS> """CODE.import paddle.fluid as fluid
data = fluid.layers.data(name='sequence', shape=[1], dtype='int64', lod_level=1)
emb = fluid.layers.nn._pull_sparse_v2(
    input=data, size=11, table_id=0, accessor_class="DownpourCtrAccessor")""" .

"DESCRIPTION.The code defines a sparse placeholder tensor named \"b\" with dimensions 2x2 using Keras backend. It then checks if tensor \"b\" is sparse, converts it to a dense tensor named \"c\", and checks if \"c\" is sparse." <EXPLAINS> """CODE.    from keras import backend as K
    b = K.placeholder((2, 2), sparse=True)
    print(K.is_sparse(b))
    c = K.to_dense(b)
    print(K.is_sparse(c))
""" .

"DESCRIPTION.The code defines a structure for a autoencoder model, with an encoder and a decoder." <EXPLAINS> """CODE.LitAutoEncoder()  # doctest: +ELLIPSIS +NORMALIZE_WHITESPACE
LitAutoEncoder(
  (encoder): ...
  (decoder): ...
)""" .

"DESCRIPTION.The code defines a subclass of tf.keras.Model called SubclassModel. The SubclassModel has two Dense layers with 10 and 20 units respectively. The call method of the SubclassModel feeds the input through the first Dense layer and returns the output after passing through the second Dense layer. The code then creates an instance of SubclassModel, calls it with zeros input, and retrieves the weight paths of the model. Additionally, the code defines a Functional Model using tf.keras.Input and two Dense layers with 20 and 30 units respectively. It retrieves the Dense layers, d1 and d2, from the model and retrieves the weight paths of the model." <EXPLAINS> """CODE.class SubclassModel(tf.keras.Model):

  def __init__(self, name=None):
    super().__init__(name=name)
    self.d1 = tf.keras.layers.Dense(10)
    self.d2 = tf.keras.layers.Dense(20)

  def call(self, inputs):
    x = self.d1(inputs)
    return self.d2(x)

model = SubclassModel()
model(tf.zeros((10, 10)))
weight_paths = model.get_weight_paths()

inputs = tf.keras.Input((10,), batch_size=10)
x = tf.keras.layers.Dense(20, name='d1')(inputs)
output = tf.keras.layers.Dense(30, name='d2')(x)
model = tf.keras.Model(inputs, output)
d1 = model.layers[1]
d2 = model.layers[2]
weight_paths = model.get_weight_paths()
""" .

"DESCRIPTION.The code defines a sweep configuration with a grid search method and a parameter 'a' with values [1, 2, 3]. It then creates a HyperParameterSet object using the 'parameters' key from the sweep configuration." <EXPLAINS> """CODE.sweep_config = {'method': 'grid', 'parameters': {'a': {'values': [1, 2, 3]}}}
hps = HyperParameterSet.from_config(sweep_config['parameters'])""" .

"DESCRIPTION.The code defines a template `t` containing a macro named `foo` that returns the value `'42'`. When accessing the `module` attribute of `t` and calling the `foo` macro, it returns `'23'` and `'42'` respectively." <EXPLAINS> """CODE.t = Template('{% macro foo() %}42{% endmacro %}23')
unicode(t.module)
u'23'
t.module.foo()
u'42'""" .

"DESCRIPTION.The code defines a test class MyTests that inherits from testing_utils.KerasTestCase. It includes a test method test_foo that creates a small MLP model, compiles it with RMSPropOptimizer, mse loss function, and mae metrics, prepares input and target data, creates a dataset from the data, repeats the dataset 100 times, batches the dataset, and fits the model to the dataset for one epoch with 2 steps per epoch and verbose output. The test method is decorated with run_with_all_model_types decorator that excludes 'sequential' model type." <EXPLAINS> """CODE.class MyTests(testing_utils.KerasTestCase):

  @testing_utils.run_with_all_model_types(
    exclude_models = ['sequential'])
  def test_foo(self):
    model = testing_utils.get_small_mlp(1, 4, input_dim=3)
    optimizer = RMSPropOptimizer(learning_rate=0.001)
    loss = 'mse'
    metrics = ['mae']
    model.compile(optimizer, loss, metrics=metrics)

    inputs = np.zeros((10, 3))
    targets = np.zeros((10, 4))
    dataset = dataset_ops.Dataset.from_tensor_slices((inputs, targets))
    dataset = dataset.repeat(100)
    dataset = dataset.batch(10)

    model.fit(dataset, epochs=1, steps_per_epoch=2, verbose=1)

@testing_utils.run_with_all_model_types(exclude_models = ['sequential'])
class MyTests(testing_utils.KerasTestCase):

  def test_foo(self):
    model = testing_utils.get_small_mlp(1, 4, input_dim=3)
    optimizer = RMSPropOptimizer(learning_rate=0.001)
    loss = 'mse'
    metrics = ['mae']
    model.compile(optimizer, loss, metrics=metrics)

    inputs = np.zeros((10, 3))
    targets = np.zeros((10, 4))
    dataset = dataset_ops.Dataset.from_tensor_slices((inputs, targets))
    dataset = dataset.repeat(100)
    dataset = dataset.batch(10)

    model.fit(dataset, epochs=1, steps_per_epoch=2, verbose=1)
""" .

"DESCRIPTION.The code defines a test function that is decorated with a custom decorator `@expect_deprecation(\"push_to_hub\")`. Inside the test function, there is a call to the function `push_to_hub` with a specified `repo_url`, which is expected to raise a FutureWarnings deprecation warning." <EXPLAINS> """CODE.@expect_deprecation("push_to_hub"):
def test_push_to_hub_git_version(self):
    (...)
    push_to_hub(repo_url="something") <- Should warn with FutureWarnings
    (...)
""" .

"DESCRIPTION.The code defines a test function that saves a keras Sequential model with specified layers and compiles it with specified loss and optimizer. The saved model is then loaded back into memory." <EXPLAINS> """CODE.@testing_utils.run_with_all_saved_model_formats
def test_foo(self):
    save_format = testing_utils.get_save_format()
    saved_model_dir = '/tmp/saved_model/'
    model = keras.models.Sequential()
    model.add(keras.layers.Dense(2, input_shape=(3,)))
    model.add(keras.layers.Dense(3))
    model.compile(loss='mse', optimizer='sgd', metrics=['acc'])

    keras.models.save_model(model, saved_model_dir, save_format=save_format)
    model = keras.models.load_model(saved_model_dir)""" .

"DESCRIPTION.The code defines a text and loads an audio example from the HuggingFace Hub using the datasets library. It then initializes a ClvpProcessor and ClvpModelForConditionalGeneration from the pretrained models. It generates processor output and extracts speech features from the model based on the provided text and audio input." <EXPLAINS> """CODE.import datasets
from transformers import ClvpProcessor, ClvpModelForConditionalGeneration

# Define the Text and Load the Audio (We are taking an audio example from HuggingFace Hub using `datasets` library)
text = "This is an example text."
ds = datasets.load_dataset("hf-internal-testing/librispeech_asr_dummy", "clean", split="validation")
ds = ds.cast_column("audio", datasets.Audio(sampling_rate=22050))
_, audio, sr = ds.sort("id").select(range(1))[:1]["audio"][0].values()

# Define processor and model
processor = ClvpProcessor.from_pretrained("susnato/clvp_dev")
model = ClvpModelForConditionalGeneration.from_pretrained("susnato/clvp_dev")

# Generate processor output and model output
processor_output = processor(raw_speech=audio, sampling_rate=sr, text=text, return_tensors="pt")
speech_embeds = model.get_speech_features(
...     input_ids=processor_output["input_ids"], input_features=processor_output["input_features"]
... )
""" .

"DESCRIPTION.The code defines a text data, initializes a CLVP processor and a CLVP model for conditional generation, generates processor output, and retrieves text embeddings using the model." <EXPLAINS> """CODE.from transformers import ClvpProcessor, ClvpModelForConditionalGeneration

# Define the Text
text = "This is an example text."

# Define processor and model
processor = ClvpProcessor.from_pretrained("susnato/clvp_dev")
model = ClvpModelForConditionalGeneration.from_pretrained("susnato/clvp_dev")

# Generate processor output and text embeds
processor_output = processor(text=text, return_tensors="pt")
text_embeds = model.get_text_features(input_ids=processor_output["input_ids"])
""" .

"DESCRIPTION.The code defines a training loop for a neural network model using distributed training with Horovod. It initializes the Horovod environment, loads the training dataset shard, defines a neural network model, sets up the optimizer with distributed optimizer from Horovod, trains the model for multiple epochs using the dataset shard, and saves the model checkpoints. Finally, it sets up a HorovodTrainer object with the training loop function, scaling configuration, and dataset, and fits the trainer to train the model." <EXPLAINS> """CODE.def train_loop_per_worker():
        ...

def train_loop_per_worker(config: Dict):
        ...

def train_loop_per_worker():
        # Report intermediate results for callbacks or logging.
        train.report(...)

        # Checkpoints the provided args as restorable state.
        train.save_checkpoint(...)

        # Returns dict of last saved checkpoint.
        train.load_checkpoint()

        # Returns the Ray Dataset shard for the given key.
        train.get_dataset_shard("my_dataset")

        # Returns the total number of workers executing training.
        train.get_world_size()

        # Returns the rank of this worker.
        train.get_world_rank()

        # Returns the rank of the worker on the current node.
        train.get_local_rank()

import ray
import ray.train as train
import ray.train.torch. # Need this to use `train.torch.get_device()`
import horovod.torch as hvd
import torch
import torch.nn as nn
from ray.ml.train.integrations.horovod import HorovodTrainer

input_size = 1
layer_size = 15
output_size = 1
num_epochs = 3

class NeuralNetwork(nn.Module):
    def __init__(self):
        super(NeuralNetwork, self).__init__()
        self.layer1 = nn.Linear(input_size, layer_size)
        self.relu = nn.ReLU()
        self.layer2 = nn.Linear(layer_size, output_size)
    def forward(self, input):
        return self.layer2(self.relu(self.layer1(input)))

def train_loop_per_worker():
    hvd.init()
    dataset_shard = train.get_dataset_shard("train")
    model = NeuralNetwork()
    device = train.torch.get_device()
    model.to(device)
    loss_fn = nn.MSELoss()
    lr_scaler = 1
    optimizer = torch.optim.SGD(model.parameters(), lr=0.1 * lr_scaler)
    # Horovod: wrap optimizer with DistributedOptimizer.
    optimizer = hvd.DistributedOptimizer(
        optimizer,
        named_parameters=model.named_parameters(),
        op=hvd.Average,
    )
    for epoch in range(num_epochs):
        model.train()
        for inputs, labels in iter(
            dataset_shard.to_torch(
                label_column="y",
                label_column_dtype=torch.float,
                feature_column_dtypes=torch.float,
                batch_size=32,
            )
        ):
            inputs.to(device)
            labels.to(device)
            outputs = model(inputs)
            loss = loss_fn(outputs, labels)
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
            print(f"epoch: {epoch}, loss: {loss.item()}")
        train.save_checkpoint(model=model.state_dict())
train_dataset = ray.data.from_items([{"x": x, "y": x + 1} for x in range(32)])
scaling_config = {"num_workers": 3}
# If using GPUs, use the below scaling config instead.
# scaling_config = {"num_workers": 3, "use_gpu": True}
trainer = HorovodTrainer(
    train_loop_per_worker=train_loop_per_worker,
    scaling_config={"num_workers": 3},
    datasets={"train": train_dataset},
)
result = trainer.fit()
""" .

"DESCRIPTION.The code defines a transformation function 't' composed of an exponential transform and a power transform with base 2. It then applies the forward transformation to the input 'x', calculates the inverse transformation of the forward-transformed 'x', and computes the forward log determinant Jacobian of 'x'." <EXPLAINS> """CODE.import paddle

x = paddle.stack(
    (paddle.to_tensor([1., 2., 3.]), paddle.to_tensor([1, 2., 3.])), 1)
t = paddle.distribution.StackTransform(
    (paddle.distribution.ExpTransform(),
    paddle.distribution.PowerTransform(paddle.to_tensor(2.))),
    1
)
print(t.forward(x))
print(t.inverse(t.forward(x)))
print(t.forward_log_det_jacobian(x))""" .

"DESCRIPTION.The code defines a variable \"features\" that specifies the features of a dataset, including a feature named 'x' with a 5-dimensional array of shape (1, 2, 2, 3, 3) and data type 'int32'." <EXPLAINS> """CODE.from datasets import Features
features = Features({'x': Array5D(shape=(1, 2, 2, 3, 3), dtype='int32')})
""" .

"DESCRIPTION.The code defines a variable \"v\" using tensorflow, and checks if the variable was created within a specific scope defined by the strategy object. The first block of code returns True because the variable was indeed created within the scope, while the second block of code returns False because the variable was created outside of the scope." <EXPLAINS> """CODE.with strategy.scope():
    v = tf.Variable(1.)
strategy.variable_created_in_scope(v)
True

v = tf.Variable(1.)
strategy.variable_created_in_scope(v)
False""" .

"DESCRIPTION.The code defines a wav2vec2 base model with 32 output units and loads the pre-trained weights from the \"wav2vec2-base-960h.pt\" file." <EXPLAINS> """CODE.wav2vec2_base(num_out=32)
model.load_state_dict(torch.load("wav2vec2-base-960h.pt"))""" .

"DESCRIPTION.The code defines a webhook endpoint 'trigger_training' which is used to trigger a training job if a dataset is updated. The function 'trigger_training' takes a WebhookPayload object as input, checks if the payload corresponds to an update action on a dataset in a repository, and then triggers a training job. The server is automatically started at the end of the first script, while the second script manually starts the server by calling the 'run' method on the 'trigger_training' function." <EXPLAINS> """CODE.from huggingface_hub import webhook_endpoint, WebhookPayload

@webhook_endpoint
async def trigger_training(payload: WebhookPayload):
    if payload.repo.type == "dataset" and payload.event.action == "update":
        # Trigger a training job if a dataset is updated
        ...

# Server is automatically started at the end of the script.


from huggingface_hub import webhook_endpoint, WebhookPayload

@webhook_endpoint
async def trigger_training(payload: WebhookPayload):
    if payload.repo.type == "dataset" and payload.event.action == "update":
        # Trigger a training job if a dataset is updated
        ...

# Start the server manually
trigger_training.run()
""" .

"DESCRIPTION.The code defines a worker server for distributed TensorFlow data processing, connects to a dispatcher, creates a dataset with a range of 10 elements, distributes the dataset for parallel epoch processing, and prints the dataset elements. The worker server with port 5051 joins the dispatcher server with address \"grpc://localhost:5050\"." <EXPLAINS> """CODE.worker = tf.data.experimental.service.WorkerServer(
    port=0, dispatcher_address=dispatcher_address)
dataset = tf.data.Dataset.range(10)
dataset = dataset.apply(tf.data.experimental.service.distribute(
    processing_mode="parallel_epochs", service=dispatcher.target))
print(list(dataset.as_numpy_iterator()))

worker = tf.data.experimental.service.WorkerServer(
    port=5051, dispatcher_address="grpc://localhost:5050")
worker.join()""" .

"DESCRIPTION.The code defines an Absolute Value transformation using the paddle library in Python. It calculates the forward, inverse, and inverse log-det-jacobian of the absolute value transformation for input tensors, handling special cases like 0." <EXPLAINS> """CODE.import paddle

abs = paddle.distribution.AbsTransform()

print(abs.forward(paddle.to_tensor([-1., 0., 1.])))
# Tensor(shape=[3], dtype=float32, place=Place(gpu:0), stop_gradient=True,
#        [1., 0., 1.])

print(abs.inverse(paddle.to_tensor(1.)))
# (Tensor(shape=[1], dtype=float32, place=Place(gpu:0), stop_gradient=True,
#        [-1.]), Tensor(shape=[1], dtype=float32, place=Place(gpu:0), stop_gradient=True,
#        [1.]))

# The |dX/dY| is constant 1. So Log|dX/dY| == 0
print(abs.inverse_log_det_jacobian(paddle.to_tensor(1.)))
# (Tensor(shape=[], dtype=float32, place=Place(gpu:0), stop_gradient=True,
#        0.), Tensor(shape=[], dtype=float32, place=Place(gpu:0), stop_gradient=True,
#        0.))

#Special case handling of 0.
print(abs.inverse(paddle.to_tensor(0.))
# (Tensor(shape=[1], dtype=float32, place=Place(gpu:0), stop_gradient=True,
#        [0.]), Tensor(shape=[1], dtype=float32, place=Place(gpu:0), stop_gradient=True,
#        [0.]))
print(abs.inverse_log_det_jacobian(paddle.to_tensor(0.))
# (Tensor(shape=[], dtype=float32, place=Place(gpu:0), stop_gradient=True,
#        0.), Tensor(shape=[], dtype=float32, place=Place(gpu:0), stop_gradient=True,
#        0.))""" .

"DESCRIPTION.The code defines an AutoEncoder class using Flax library to construct an autoencoder neural network model. It consists of an encoder and a decoder, both using Dense layers. The encoder takes input data and encodes it into a lower-dimensional representation, while the decoder reconstructs the original data from the encoded representation. The code also initializes the model, binds the variables, and demonstrates encoding and decoding operations on sample data." <EXPLAINS> """CODE.import jax
import jax.numpy as jnp
import flax.linen as nn

class AutoEncoder(nn.Module):
  def setup(self):
    self.encoder = nn.Dense(3)
    self.decoder = nn.Dense(5)

  def __call__(self, x):
    return self.decoder(self.encoder(x))

x = jnp.ones((16, 9))
ae = AutoEncoder()
variables = ae.init(jax.random.PRNGKey(0), x)
model = ae.bind(variables)
z = model.encoder(x)
x_reconstructed = model.decoder(z)
""" .

"DESCRIPTION.The code defines an AutoEncoder class using Flax's nn.Module. The AutoEncoder consists of an encoder and a decoder, each utilizing a Dense layer. The encode function takes an input x and returns the output of the decoder after processing through the encoder. The code then initializes the AutoEncoder, binds the variables, and applies the encoder and decoder to the input x to reconstruct it." <EXPLAINS> """CODE.import jax
import jax.numpy as jnp
import flax.linen as nn

class AutoEncoder(nn.Module):
  def setup(self):
    self.encoder = nn.Dense(3)
    self.decoder = nn.Dense(5)

  def __call__(self, x):
    return self.decoder(self.encoder(x))

x = jnp.ones((16, 9))
ae = AutoEncoder()
variables = ae.init(jax.random.PRNGKey(0), x)
model = ae.bind(variables)
z = model.encoder(x)
x_reconstructed = model.decoder(z)
""" .

"DESCRIPTION.The code defines an IPU (Intelligent Processing Unit) strategy using PaddlePaddle framework and retrieves the number of IPUs configured in the strategy." <EXPLAINS> """CODE.import paddle
import paddle.static as static

paddle.enable_static()

ipu_strategy = static.IpuStrategy()
num_ipus = ipu_strategy.get_option('num_ipus')""" .

"DESCRIPTION.The code defines an IPU strategy for running operations in single precision floating point format." <EXPLAINS> """CODE.import paddle
import paddle.static as static

paddle.enable_static()

ipu_strategy = static.IpuStrategy()
ipu_strategy.set_precision_config(enable_fp16=False)""" .

"DESCRIPTION.The code defines an IPU strategy that includes a custom operation mapping between a PaddlePaddle RELU function and a Popart RELU function." <EXPLAINS> """CODE.import paddle
import paddle.static as static

paddle.enable_static()

ipu_strategy = static.IpuStrategy()
ipu_strategy.add_custom_op('paddle_relu', 'popart_relu')""" .

"DESCRIPTION.The code defines an ImageNetLightningModel class and initializes it with a data path parameter. It also includes a ResNet model as part of the ImageNetLightningModel class." <EXPLAINS> """CODE.ImageNetLightningModel(data_path='missing')
ImageNetLightningModel(
  (model): ResNet(...)
)""" .

"DESCRIPTION.The code defines an RNN Estimator using LSTM cells for regression tasks. It includes setting up the token embedding columns, defining the RNN structure with specified number of units, and providing input functions for training, evaluation, and prediction." <EXPLAINS> """CODE.token_sequence = sequence_categorical_column_with_hash_bucket(...)
token_emb = embedding_column(categorical_column=token_sequence, ...)

estimator = RNNEstimator(
    head=tf.contrib.estimator.regression_head(),
    sequence_feature_columns=[token_emb],
    num_units=[32, 16], cell_type='lstm')

# Or with custom RNN cell:
def rnn_cell_fn(mode):
  cells = [ tf.contrib.rnn.LSTMCell(size) for size in [32, 16] ]
  if mode == tf.estimator.ModeKeys.TRAIN:
    cells = [ tf.contrib.rnn.DropoutWrapper(cell, input_keep_prob=0.5)
                  for cell in cells ]
  return tf.contrib.rnn.MultiRNNCell(cells)

estimator = RNNEstimator(
    head=tf.contrib.estimator.regression_head(),
    sequence_feature_columns=[token_emb],
    rnn_cell_fn=rnn_cell_fn)

# Input builders
def input_fn_train: # returns x, y
  pass
estimator.train(input_fn=input_fn_train, steps=100)

def input_fn_eval: # returns x, y
  pass
metrics = estimator.evaluate(input_fn=input_fn_eval, steps=10)
def input_fn_predict: # returns x, None
  pass
predictions = estimator.predict(input_fn=input_fn_predict)
""" .

"DESCRIPTION.The code defines an affine transformation with scale factor 1 and shift factor 0, applies the forward transformation to the input tensor x, calculates the inverse transformation of the forward transformation applied to x, and computes the forward log determinant of the Jacobian matrix for the transformation at x." <EXPLAINS> """CODE.import paddle

x = paddle.to_tensor([1., 2.])
affine = paddle.distribution.AffineTransform(paddle.to_tensor(0.), paddle.to_tensor(1.))

print(affine.forward(x))
print(affine.inverse(affine.forward(x)))
print(affine.forward_log_det_jacobian(x))""" .

"DESCRIPTION.The code defines an asynchronous function named interact that prompts the user with a yes/no dialog, allows the user to type something, and then prints out the user's input. The interact function is passed as an argument to PromptToolkitSSHServer which is used to create a SSH server with the specified host keys. The server is then run on the specified port indefinitely." <EXPLAINS> """CODE.async def interact() -> None:
    await yes_no_dialog("my title", "my text").run_async()

    prompt_session = PromptSession()
    text = await prompt_session.prompt_async("Type something: ")
    print_formatted_text('You said: ', text)

server = PromptToolkitSSHServer(interact=interact)
loop = get_event_loop()
loop.run_until_complete(
    asyncssh.create_server(
        lambda: MySSHServer(interact),
        "",
        port,
        server_host_keys=["/etc/ssh/..."],
    )
)
loop.run_forever()""" .

"DESCRIPTION.The code defines an environment specification tree and adds a new environment specification for a specific environment. It then asserts that the added environment specification can be accessed correctly both using the key and indexing syntax." <EXPLAINS> """CODE.specs = EnvSpecTree()

specs["My/Env-v0"] = EnvSpec(...)
assert specs["My/Env-v0"] == EnvSpec(...)

assert specs.tree["My"]["Env"]["0"] == specs["My/Env-v0"]
""" .

"DESCRIPTION.The code defines an immutable data structure called Point with attributes x, y, and id_. It creates instances of Point with specified attribute values and allows for updating the attribute values using the set method." <EXPLAINS> """CODE.Point = immutable('x, y', name='Point')

p = Point(1, 2)

p2 = p.set(x=3)

Point(x=1, y=2)

Point(x=3, y=2)

Point = immutable('x, y, id_', name='Point')

p = Point(1, 2, id_=17)

p.set(x=3)

Point(x=3, y=2, id_=17)

p.set(id_=18)""",
        """CODE.Point = pclass('x, y', name='Point')

p = Point(1, 2)

p2 = p.set(x=3)

Point = pclass('x, y, id_', name='Point')

p = Point(1, 2, id_=17)

p.set(x=3)

p.set(id_=18)""" .

"DESCRIPTION.The code defines an input data class with features represented as a torch tensor and a bias as an integer. It also defines an output data class with a result represented as a torch tensor. The function \"fn\" takes an instance of the InputDataClass as input, calculates the sum of the feature and bias, and returns an instance of the OutputDataClass with the result. Lastly, it exports the function using torch.export.export and prints the result." <EXPLAINS> """CODE.@dataclass
class InputDataClass:
    feature: torch.Tensor
    bias: int

class OutputDataClass:
    res: torch.Tensor

torch.export.register_dataclass(InputDataClass)
torch.export.register_dataclass(OutputDataClass)

def fn(o: InputDataClass) -> torch.Tensor:
    res = res=o.feature + o.bias
    return OutputDataClass(res=res)

ep = torch.export.export(fn, (InputDataClass(torch.ones(2, 2), 1), ))
print(ep)""" .

"DESCRIPTION.The code defines an instance of a LitClassifier with a Backbone model as its argument." <EXPLAINS> """CODE.LitClassifier(Backbone())  # doctest: +ELLIPSIS +NORMALIZE_WHITESPACE
LitClassifier(
  (backbone): ...
)""" .

"DESCRIPTION.The code defines and applies different types of regularization techniques (L1, L2, Custom L1, Custom L2) to a dense layer in a neural network. It calculates and aggregates the regularization losses associated with each regularization technique." <EXPLAINS> """CODE.layer = tf.keras.layers.Dense(
    5, input_dim=5,
    kernel_initializer='ones',
    kernel_regularizer=tf.keras.regularizers.L1(0.01),
    activity_regularizer=tf.keras.regularizers.L2(0.01))
tensor = tf.ones(shape=(5, 5)) * 2.0
out = layer(tensor)
tf.math.reduce_sum(layer.losses)
tf.keras.regularizers.L1(0.3)
tf.keras.regularizers.L2(0.1)
tf.keras.regularizers.L1L2(l1=0.01, l2=0.01)
regularizer = tf.keras.regularizers.L2(2.)
tensor = tf.ones(shape=(5, 5))
regularizer(tensor)
@tf.keras.utils.register_keras_serializable(package='Custom', name='l1')
def l1_reg(weight_matrix):
   return 0.01 * tf.math.reduce_sum(tf.math.abs(weight_matrix))
layer = tf.keras.layers.Dense(5, input_dim=5,
    kernel_initializer='ones', kernel_regularizer=l1_reg)
tensor = tf.ones(shape=(5, 5))
out = layer(tensor)
layer.losses
@tf.keras.utils.register_keras_serializable(package='Custom', name='l2')
class L2Regularizer(tf.keras.regularizers.Regularizer):
  def __init__(self, l2=0.):  # pylint: disable=redefined-outer-name
    self.l2 = l2

  def __call__(self, x):
    return self.l2 * tf.math.reduce_sum(tf.math.square(x))

  def get_config(self):
    return {'l2': float(self.l2)}
layer = tf.keras.layers.Dense(
  5, input_dim=5, kernel_initializer='ones',
  kernel_regularizer=L2Regularizer(l2=0.5))
tensor = tf.ones(shape=(5, 5))
out = layer(tensor)
layer.losses
""" .

"DESCRIPTION.The code defines and applies two instances of the LeakyReLU activation function with different alpha values to a list of input values, which returns the output after applying the activation function. The LeakyReLU function returns the input values if they are positive, and multiplies negative input values by a small gradient (alpha) to prevent the dying ReLU problem." <EXPLAINS> """CODE.layer = tf.keras.layers.LeakyReLU()
output = layer([-3.0, -1.0, 0.0, 2.0])
list(output.numpy())
[-0.9, -0.3, 0.0, 2.0]
layer = tf.keras.layers.LeakyReLU(alpha=0.1)
output = layer([-3.0, -1.0, 0.0, 2.0])
list(output.numpy())
[-0.3, -0.1, 0.0, 2.0]
""" .

"DESCRIPTION.The code defines and checks the data type of placeholders and variables in a Keras backend." <EXPLAINS> """CODE.from keras import backend as K
K.dtype(K.placeholder(shape=(2,4,5)))
K.dtype(K.placeholder(shape=(2,4,5), dtype='float32'))
K.dtype(K.placeholder(shape=(2,4,5), dtype='float64'))
kvar = K.variable(np.array([[1, 2], [3, 4]]))
K.dtype(kvar)
kvar = K.variable(np.array([[1, 2], [3, 4]]), dtype='float32')
K.dtype(kvar)
""" .

"DESCRIPTION.The code defines and checks the types of variables, placeholders, and tensors in a Keras model." <EXPLAINS> """CODE.from keras import backend as K
np_var = numpy.array([1, 2])
K.is_keras_tensor(np_var)
keras_var = K.variable(np_var)
K.is_keras_tensor(keras_var)  # A variable is not a Tensor.
keras_placeholder = K.placeholder(shape=(2, 4, 5))
K.is_keras_tensor(keras_placeholder)  # A placeholder is a Tensor.
""" .

"DESCRIPTION.The code defines and initializes a LitClassifier object with a Backbone model." <EXPLAINS> """CODE.LitClassifier(Backbone())  # doctest: +ELLIPSIS +NORMALIZE_WHITESPACE
LitClassifier(
  (backbone): ...
)""" .

"DESCRIPTION.The code defines and initializes a VectorExponential distribution with specific parameters for single and batch observations in a multi-variate space. It then computes the probability density function of the given observations and returns the result as a scalar or vector depending on the input shape." <EXPLAINS> """CODE.ds = tf.contrib.distributions
la = tf.contrib.linalg

# Initialize a single 2-variate VectorExponential, supported on
# {(x, y) in R^2 : x > 0, y > 0}.

# The first component has pdf exp{-x}, the second 0.5 exp{-x / 2}
vex = ds.VectorExponentialDiag(scale_diag=[1., 2.])

# Compute the pdf of an`R^2` observation; return a scalar.
vex.prob([3., 4.]).eval()  # shape: []

# Initialize a 2-batch of 3-variate Vector Exponential's.
loc = [[1., 2, 3],
       [1., 0, 0]]              # shape: [2, 3]
scale_diag = [[1., 2, 3],
              [0.5, 1, 1.5]]     # shape: [2, 3]

vex = ds.VectorExponentialDiag(loc, scale_diag)

# Compute the pdf of two `R^3` observations; return a length-2 vector.
x = [[1.9, 2.2, 3.1],
     [10., 1.0, 9.0]]     # shape: [2, 3]
vex.prob(x).eval()    # shape: [2]
""" .

"DESCRIPTION.The code defines and initializes different models for classification, quantized classification, object detection, and segmentation tasks using MobileNetV3 architecture. It also saves the model weights to different files after initialization." <EXPLAINS> """CODE.from torchvision import models as M

# Classification
model = M.mobilenet_v3_large(pretrained=False)
print(store_model_weights(model, './class.pth'))

# Quantized Classification
model = M.quantization.mobilenet_v3_large(pretrained=False, quantize=False)
model.fuse_model()
model.qconfig = torch.quantization.get_default_qat_qconfig('qnnpack')
_ = torch.quantization.prepare_qat(model, inplace=True)
print(store_model_weights(model, './qat.pth'))

# Object Detection
model = M.detection.fasterrcnn_mobilenet_v3_large_fpn(pretrained=False, pretrained_backbone=False)
print(store_model_weights(model, './obj.pth'))

# Segmentation
model = M.segmentation.deeplabv3_mobilenet_v3_large(pretrained=False, pretrained_backbone=False, aux_loss=True)
print(store_model_weights(model, './segm.pth', strict=False))
""" .

"DESCRIPTION.The code defines and initializes multivariate Gaussian distributions using TensorFlow, computes the probability density function (pdf) for given observations, and handles both single and batched observations. It leverages linear operator classes from TensorFlow to parameterize the distributions and compute their covariance matrices." <EXPLAINS> """CODE.ds = tf.contrib.distributions
la = tf.contrib.linalg

# Initialize a single 3-variate Gaussian.
mu = [1., 2, 3]
cov = [[ 0.36,  0.12,  0.06],
       [ 0.12,  0.29, -0.13],
       [ 0.06, -0.13,  0.26]]
scale = tf.cholesky(cov)
# ==> [[ 0.6,  0. ,  0. ],
#      [ 0.2,  0.5,  0. ],
#      [ 0.1, -0.3,  0.4]])

mvn = ds.MultivariateNormalLinearOperator(
    loc=mu,
    scale=la.LinearOperatorTriL(scale))

# Covariance agrees with cholesky(cov) parameterization.
mvn.covariance().eval()
# ==> [[ 0.36,  0.12,  0.06],
#      [ 0.12,  0.29, -0.13],
#      [ 0.06, -0.13,  0.26]]

# Compute the pdf of an`R^3` observation; return a scalar.
mvn.prob([-1., 0, 1]).eval()  # shape: []

# Initialize a 2-batch of 3-variate Gaussians.
mu = [[1., 2, 3],
      [11, 22, 33]]              # shape: [2, 3]
scale_diag = [[1., 2, 3],
              [0.5, 1, 1.5]]     # shape: [2, 3]

mvn = ds.MultivariateNormalLinearOperator(
    loc=mu,
    scale=la.LinearOperatorDiag(scale_diag))

# Compute the pdf of two `R^3` observations; return a length-2 vector.
x = [[-0.9, 0, 0.1],
     [-10, 0, 9]]     # shape: [2, 3]
mvn.prob(x).eval()    # shape: [2]
""" .

"DESCRIPTION.The code defines and initializes two linear layers using the PaddlePaddle framework, performs forward pass calculations, computes gradients, and checks if the gradients of the weights are None after the backward pass. Additionally, it includes a function test_layer() that defines and utilizes two additional linear layers for a forward pass calculation using specific input data." <EXPLAINS> """CODE.import numpy as np
import paddle.fluid as fluid

data = np.array([[2, 3], [4, 5]]).astype('float32')
with fluid.dygraph.guard():
    l0 = fluid.Linear(2, 2)
    l1 = fluid.Linear(2, 2)
    with fluid.dygraph.no_grad():
        tmp = l1.weight * 2
    x = fluid.dygraph.to_variable(data)
    y = l0(x) + tmp
    o = l1(y)
    o.backward()
    print(tmp.gradient() is None)
    print(l0.weight.gradient() is None)

@fluid.dygraph.no_grad
def test_layer():
    with fluid.dygraph.guard():
        inp = np.ones([3, 1024], dtype='float32')
        t = fluid.dygraph.base.to_variable(inp)
        linear1 = fluid.Linear(1024, 4, bias_attr=False)
        linear2 = fluid.Linear(4, 4)
        ret = linear1(t)
        dy_ret = linear2(ret)

test_layer()""" .

"DESCRIPTION.The code defines and operates on a global mesh of hosts and devices. It creates a GlobalDeviceArray object from global input data, slices the data per-local device, and performs a matrix multiplication operation using parallelized JAX functions. Finally, it demonstrates accessing and manipulating the output data shards on local devices within the global mesh." <EXPLAINS> """CODE.# Logical mesh is (hosts, devices)
assert global_mesh.shape == {'x': 4, 'y': 8}

global_input_shape = (64, 32)
mesh_axes = P('x', 'y')

# Dummy example data; in practice we wouldn't necessarily materialize global data
# in a single process.
global_input_data = np.arange(
    np.prod(global_input_shape)).reshape(global_input_shape)

def get_local_data_slice(index):
  # index will be a tuple of slice objects, e.g. (slice(0, 16), slice(0, 4))
  # This method will be called per-local device from the GSDA constructor.
  return global_input_data[index]

gda = GlobalDeviceArray.from_callback(
        global_input_shape, global_mesh, mesh_axes, get_local_data_slice)

f = pjit(lambda x: x @ x.T, out_axis_resources = P('y', 'x'))

with mesh(global_mesh.shape, global_mesh.axis_names):
  out = f(gda)

print(type(out))  # GlobalDeviceArray
print(out.shape)  # global shape == (64, 64)
print(out.local_shards[0].data)  # Access the data on a single local device,
                                 # e.g. for checkpointing
print(out.local_shards[0].data.shape)  # per-device shape == (8, 16)
print(out.local_shards[0].index) # Numpy-style index into the global array that
                                 # this data shard corresponds to

# `out` can be passed to another pjit call, out.local_shards can be used to
# export the data to non-jax systems (e.g. for checkpointing or logging), etc.
""" .

"DESCRIPTION.The code defines and performs operations on LogNormal distributions with specified locations and scales. The functionality includes sampling from the distribution, calculating entropy, log probability, probability, and Kullback-Leibler divergence between two LogNormal distributions." <EXPLAINS> """CODE.import paddle
from paddle.distribution import LogNormal

# Define a single scalar LogNormal distribution.
dist = LogNormal(loc=0., scale=3.)
# Define a batch of two scalar valued LogNormals.
# The underlying Normal of first has mean 1 and standard deviation 11, the underlying Normal of second 2 and 22.
dist = LogNormal(loc=[1., 2.], scale=[11., 22.])
# Get 3 samples, returning a 3 x 2 tensor.
dist.sample((3, ))

# Define a batch of two scalar valued LogNormals.
# Their underlying Normal have mean 1, but different standard deviations.
dist = LogNormal(loc=1., scale=[11., 22.])

# Complete example
value_tensor = paddle.to_tensor([0.8], dtype="float32")

lognormal_a = LogNormal([0.], [1.])
lognormal_b = LogNormal([0.5], [2.])
sample = lognormal_a.sample((2, ))
# a random tensor created by lognormal distribution with shape: [2, 1]
entropy = lognormal_a.entropy()
# [1.4189385] with shape: [1]
lp = lognormal_a.log_prob(value_tensor)
# [-0.72069150] with shape: [1]
p = lognormal_a.probs(value_tensor)
# [0.48641577] with shape: [1]
kl = lognormal_a.kl_divergence(lognormal_b)
# [0.34939718] with shape: [1]""" .

"DESCRIPTION.The code defines and prints the data types of placeholders and variables in the Keras backend, including specifying the shape and data types for the placeholders and variables." <EXPLAINS> """CODE.from keras import backend as K
print(K.dtype(K.placeholder(shape=(2,4,5))))
print(K.dtype(K.placeholder(shape=(2,4,5), dtype='float32')))
print(K.dtype(K.placeholder(shape=(2,4,5), dtype='float64')))
kvar = K.variable(np.array([[1, 2], [3, 4]]))
print(K.dtype(kvar))
kvar = K.variable(np.array([[1, 2], [3, 4]]), dtype='float32')
print(K.dtype(kvar))
""" .

"DESCRIPTION.The code defines and trains a neural network model using TensorFlow's Keras API." <EXPLAINS> """CODE.model = tf.keras.Sequential()
model.add(tf.keras.layers.Dense(8, input_shape=(16,)))
model.add(tf.keras.layers.Dense(4))

model = tf.keras.Sequential()
model.add(tf.keras.Input(shape=(16,)))
model.add(tf.keras.layers.Dense(8))

model = tf.keras.Sequential()
model.add(tf.keras.layers.Dense(8))
model.add(tf.keras.layers.Dense(4))

model = tf.keras.Sequential()
model.add(tf.keras.layers.Dense(8, input_shape=(16,)))
model.add(tf.keras.layers.Dense(4))
len(model.weights)

model = tf.keras.Sequential()
model.add(tf.keras.layers.Dense(8))
model.add(tf.keras.layers.Dense(4))
model.build((None, 16))
len(model.weights)

model = tf.keras.Sequential()
model.add(tf.keras.layers.Dense(8))
model.add(tf.keras.layers.Dense(1))
model.compile(optimizer='sgd', loss='mse')
model.fit(x, y, batch_size=32, epochs=10)
""" .

"DESCRIPTION.The code defines and trains a wide and deep model using linear and deep neural network components separately. It first creates a linear model and compiles it with the optimizer 'adagrad' and loss function 'mse', then fits the model using linear inputs and target values. Similarly, it creates a deep neural network model, compiles it with 'rmsprop' optimizer and 'mse' loss function, and fits the model using deep neural inputs and target values. Finally, a combined wide and deep model is created by combining the linear and dnn models, compiled with multiple optimizers and loss functions, and trained using both linear and dnn inputs." <EXPLAINS> """CODE.linear_model = LinearModel()
dnn_model = keras.Sequential([keras.layers.Dense(units=64),
                             keras.layers.Dense(units=1)])
combined_model = WideDeepModel(linear_model, dnn_model)
combined_model.compile(optimizer=['sgd', 'adam'], 'mse', ['mse'])
# define dnn_inputs and linear_inputs as separate numpy arrays or
# a single numpy array if dnn_inputs is same as linear_inputs.
combined_model.fit([linear_inputs, dnn_inputs], y, epochs)
# or define a single `tf.data.Dataset` that contains a single tensor or
# separate tensors for dnn_inputs and linear_inputs.
dataset = tf.data.Dataset.from_tensors(([linear_inputs, dnn_inputs], y))
combined_model.fit(dataset, epochs)


linear_model = LinearModel()
linear_model.compile('adagrad', 'mse')
linear_model.fit(linear_inputs, y, epochs)
dnn_model = keras.Sequential([keras.layers.Dense(units=1)])
dnn_model.compile('rmsprop', 'mse')
dnn_model.fit(dnn_inputs, y, epochs)
combined_model = WideDeepModel(linear_model, dnn_model)
combined_model.compile(optimizer=['sgd', 'adam'], 'mse', ['mse'])
combined_model.fit([linear_inputs, dnn_inputs], y, epochs)
""" .

"DESCRIPTION.The code defines and transposes a tensor variable, and then defines and transposes a placeholder tensor." <EXPLAINS> """CODE.var = K.variable([[1, 2, 3], [4, 5, 6]])
K.eval(var)
var_transposed = K.transpose(var)
K.eval(var_transposed)


inputs = K.placeholder((2, 3))
inputs
input_transposed = K.transpose(inputs)
input_transposed
""" .

"DESCRIPTION.The code defines and utilizes a HParams object to specify and override hyperparameters for a model. It allows users to set hyperparameter values through command line flags or by parsing JSON format." <EXPLAINS> """CODE.# Create a HParams object specifying names and values of the model
# hyperparameters:
hparams = HParams(learning_rate=0.1, num_hidden_units=100)

# The hyperparameter are available as attributes of the HParams object:
hparams.learning_rate ==> 0.1
hparams.num_hidden_units ==> 100

# Define a command line flag to pass name=value pairs.
# For example using argparse:
import argparse
parser = argparse.ArgumentParser(description='Train my model.')
parser.add_argument('--hparams', type=str,
                    help='Comma seperated list of "name=value" pairs.')
args = parser.parse_args()
...
def my_program():
  # Create a HParams object specifying the names and values of the
  # model hyperparameters:
  hparams = tf.HParams(learning_rate=0.1, num_hidden_units=100,
                       activations=['relu', 'tanh'])

  # Override hyperparameters values by parsing the command line
  hparams.parse(args.hparams)

  # If the user passed `--hparams=learning_rate=0.3` on the command line
  # then 'hparams' has the following attributes:
  hparams.learning_rate ==> 0.3
  hparams.num_hidden_units ==> 100
  hparams.activations ==> ['relu', 'tanh']

  # If the hyperparameters are in json format use parse_json:
  hparams.parse_json('{"learning_rate": 0.3, "activations": "relu"}')
""" .

"DESCRIPTION.The code defines arguments for searching datasets, setting author to 'huggingface' and language to 'en'." <EXPLAINS> """CODE.args = DatasetSearchArguments()
args.author.huggingface
args.language.en
""" .

"DESCRIPTION.The code defines bijectors and constructs a transformed distribution using TensorFlow Probability. The ScaleTriL bijector is used with different parameters such as Exp, Identity, and a Chain of bijectors including AffineScalar and Softplus. The TransformedDistribution is created with a Normal distribution transformed by CholeskyOuterProduct and ScaleTriL bijectors." <EXPLAINS> """CODE.tfb = tf.contrib.distributions.bijectors
b = tfb.ScaleTriL(
     diag_bijector=tfb.Exp(),
     diag_shift=None)
b.forward(x=[0., 0., 0.])
b.inverse(y=[[1., 0],
             [.5, 2]])
dist = tfd.TransformedDistribution(
        tfd.Normal(tf.zeros(6), tf.ones(6)),
        tfb.Chain([tfb.CholeskyOuterProduct(), tfb.ScaleTriL()]))
b = tfb.ScaleTriL(
     diag_bijector=tfb.Identity(),
     diag_shift=None)
b = tfb.ScaleTriL(
     diag_bijector=tfb.Chain([
       tfb.AffineScalar(shift=1e-3),
       tfb.Softplus(),
       tfb.AffineScalar(shift=0.5413)]),
     diag_shift=None)""" .

"DESCRIPTION.The code defines classes FooType and DummyTransformer, where DummyTransformer is a subclass of NodeStateTracker and ast.NodeTransformer. The DummyTransformer class overrides the visit_If method to track the innermost enclosing If statement node and assign it to the foo_property attribute of the FooType instance stored in the state. The visit_Name method access the foo_property attribute of the FooType instance." <EXPLAINS> """CODE.  class FooType(object):

    def __init__(self):
      self.foo_property = None

  class DummyTransformer(NodeStateTracker, ast.NodeTransformer):

    def visit_If(self, node):
      self.state[FooType].enter()
      self.state[FooType].foo_property = node
      node = self.veneric_visit(node)
      self.state[FooType].exit()
      return node

    def visit_Name(self, node):
      self.state[FooType].foo_property  # will hold the innermost enclosing if

    def visit_If(self, node):
      with self.state[FooType] as foo:
        foo.foo_property = node
        return self.generic_visit(node)
""" .

"DESCRIPTION.The code defines command line arguments for a PyTorch Lightning Trainer model training process, allowing for customization of a custom argument '--my_custom_arg' with a default value of 'something'. It then parses the arguments and initializes a Trainer object with the provided configuration." <EXPLAINS> """CODE.from pytorch_lightning import Trainer
parser = ArgumentParser(add_help=False)
parser = Trainer.add_argparse_args(parser)
parser.add_argument('--my_custom_arg', default='something')  # doctest: +SKIP
args = Trainer.parse_argparser(parser.parse_args(""))
trainer = Trainer.from_argparse_args(args, logger=False)""" .

"DESCRIPTION.The code defines configurations for a hybrid model that combines text and vision inputs. It then initializes an encoder-decoder model using these configurations, saves the model to a file named 'my-model', and reloads the model using the saved configuration." <EXPLAINS> """CODE.from transformers import BertConfig, CLIPConfig, HybridCLIPConfig, FlaxHybridCLIP

config_text = BertConfig()
config_vision = CLIPConfig()

config = HybridCLIPConfig.from_text_vision_configs(config_text, config_vision, projection_dim=512)

model = EncoderDecoderModel(config=config)

config_text = model.config.text_config
config_vision = model.config.vision_config

model.save_pretrained('my-model')

encoder_decoder_config = HybridCLIPConfig.from_pretrained('my-model')
model = FlaxHybridCLIP.from_pretrained('my-model', config=encoder_decoder_config)
""" .

"DESCRIPTION.The code defines custom layers and models in TensorFlow for neural network with activity and weight regularization. The first code snippet adds activity regularization to the model, calculating the absolute mean of the inputs. The second code snippet adds activity regularization to the model, calculating the absolute mean of the layer output. The third code snippet adds weight regularization to the model, calculating the mean of the layer kernel." <EXPLAINS> """CODE.class MyLayer(tf.keras.layers.Layer):
  def call(self, inputs):
    self.add_loss(tf.abs(tf.reduce_mean(inputs)))
    return inputs


inputs = tf.keras.Input(shape=(10,))
x = tf.keras.layers.Dense(10)(inputs)
outputs = tf.keras.layers.Dense(1)(x)
model = tf.keras.Model(inputs, outputs)
# Activity regularization.
model.add_loss(tf.abs(tf.reduce_mean(x)))


inputs = tf.keras.Input(shape=(10,))
d = tf.keras.layers.Dense(10)
x = d(inputs)
outputs = tf.keras.layers.Dense(1)(x)
model = tf.keras.Model(inputs, outputs)
# Weight regularization.
model.add_loss(lambda: tf.reduce_mean(d.kernel))
""" .

"DESCRIPTION.The code defines custom layers and models in TensorFlow using Keras API. The first one adds regularization based on the absolute mean of the inputs to the model. The second one adds activity regularization by taking the absolute mean of the x values. The third one adds weight regularization by calculating the mean of the kernel values." <EXPLAINS> """CODE.class MyLayer(tf.keras.layers.Layer):
  def call(self, inputs):
    self.add_loss(tf.abs(tf.reduce_mean(inputs)))
    return inputs


inputs = tf.keras.Input(shape=(10,))
x = tf.keras.layers.Dense(10)(inputs)
outputs = tf.keras.layers.Dense(1)(x)
model = tf.keras.Model(inputs, outputs)
# Activity regularization.
model.add_loss(tf.abs(tf.reduce_mean(x)))


inputs = tf.keras.Input(shape=(10,))
d = tf.keras.layers.Dense(10)
x = d(inputs)
outputs = tf.keras.layers.Dense(1)(x)
model = tf.keras.Model(inputs, outputs)
# Weight regularization.
model.add_loss(lambda: tf.reduce_mean(d.kernel))
""" .

"DESCRIPTION.The code defines different Arrow data types using PyArrow library in Python." <EXPLAINS> """CODE.import pyarrow as pa
pd.ArrowDtype(pa.int64())
int64[pyarrow]
pd.ArrowDtype(pa.timestamp("s", tz="America/New_York"))
timestamp[s, tz=America/New_York][pyarrow]
pd.ArrowDtype(pa.list_(pa.int64()))
list<item: int64>[pyarrow]""" .

"DESCRIPTION.The code defines different methods for preprocessing different types of input features (float, integer, and string) in a machine learning model. These methods include standardization, rescaling, discretization, indexing, and hashing, with the option of one-hot encoding the resulting discrete representations." <EXPLAINS> """CODE.# Plain float values.
FeatureSpace.float(name=None)

# Float values to be preprocessed via featurewise standardization
# (i.e. via a `keras.layers.Normalization` layer).
FeatureSpace.float_normalized(name=None)

# Float values to be preprocessed via linear rescaling
# (i.e. via a `keras.layers.Rescaling` layer).
FeatureSpace.float_rescaled(scale=1., offset=0., name=None)

# Float values to be discretized. By default, the discrete
# representation will then be one-hot encoded.
FeatureSpace.float_discretized(
    num_bins, bin_boundaries=None, output_mode="one_hot", name=None)

# Integer values to be indexed. By default, the discrete
# representation will then be one-hot encoded.
FeatureSpace.integer_categorical(
    max_tokens=None, num_oov_indices=1, output_mode="one_hot", name=None)

# String values to be indexed. By default, the discrete
# representation will then be one-hot encoded.
FeatureSpace.string_categorical(
    max_tokens=None, num_oov_indices=1, output_mode="one_hot", name=None)

# Integer values to be hashed into a fixed number of bins.
# By default, the discrete representation will then be one-hot encoded.
FeatureSpace.integer_hashed(num_bins, output_mode="one_hot", name=None)

# String values to be hashed into a fixed number of bins.
# By default, the discrete representation will then be one-hot encoded.
FeatureSpace.string_hashed(num_bins, output_mode="one_hot", name=None)
""" .

"DESCRIPTION.The code defines functions \"mish\" and \"softplus\" to calculate the Mish activation function and the softplus function respectively. The Mish activation function is defined as x multiplied by the hyperbolic tangent of the softplus of x. The softplus function calculates the logarithm of the exponential of x plus 1." <EXPLAINS> """CODE.def mish(x):
    return x * tanh(softplus(x))

def softplus(x):
    return log(exp(x) + 1)
""" .

"DESCRIPTION.The code defines functions for creating a neural network model, optimizer, and dataset. It then creates a PyTorchTrainer object using the defined functions and trains the model using Mean Squared Error loss and stochastic gradient descent optimization with a specified learning rate." <EXPLAINS> """CODE.def model_creator(config):
    return nn.Linear(1, 1)


def optimizer_creator(model, config):
    return torch.optim.SGD(
        model.parameters(), lr=config.get("lr", 1e-4))


def data_creator(config):
    return LinearDataset(2, 5), LinearDataset(2, 5, size=400)

trainer = PyTorchTrainer(
    model_creator,
    data_creator,
    optimizer_creator,
    loss_creator=nn.MSELoss,
    use_gpu=True
)
trainer.train()
""" .

"DESCRIPTION.The code defines functions that return specific constant values or arrays of values with different data types, shapes, and structures." <EXPLAINS> """CODE.def f(...):
  return tf.constant(37.0)
result = dataset.map(f)
result.output_classes == tf.Tensor
result.output_types == tf.float32
result.output_shapes == []  # scalar

def g(...):
  return tf.constant(37.0), tf.constant(["Foo", "Bar", "Baz"])
result = dataset.map(g)
result.output_classes == (tf.Tensor, tf.Tensor)
result.output_types == (tf.float32, tf.string)
result.output_shapes == ([], [3])

def h(...):
  return 37.0, ["Foo", "Bar", "Baz"], np.array([1.0, 2.0] dtype=np.float64)
result = dataset.map(h)
result.output_classes == (tf.Tensor, tf.Tensor, tf.Tensor)
result.output_types == (tf.float32, tf.string, tf.float64)
result.output_shapes == ([], [3], [2])

def i(...):
  return {"a": 37.0, "b": [42, 16]}, "foo"
result.output_classes == ({"a": tf.Tensor, "b": tf.Tensor}, tf.Tensor)
result.output_types == ({"a": tf.float32, "b": tf.int32}, tf.string)
result.output_shapes == ({"a": [], "b": [2]}, [])
""" .

"DESCRIPTION.The code defines functions to book a flight, book a hotel, and finalize a trip with a list of bookings. The trip consists of two flights and a hotel reservation. The code ultimately runs the trip and returns the result." <EXPLAINS> """CODE.@workflow.step
def book_flight(origin: str, dest: str) -> Flight:
    return Flight(...)

@workflow.step
def book_hotel(location: str) -> Reservation:
    return Reservation(...)

@workflow.step
def finalize_trip(bookings: List[Any]) -> Trip:
    return Trip(...)

flight1 = book_flight.step("OAK", "SAN")
flight2 = book_flight.step("SAN", "OAK")
hotel = book_hotel.step("SAN")
trip = finalize_trip.step([flight1, flight2, hotel])
result = trip.run()""" .

"DESCRIPTION.The code defines functions to remove the first element from a list of tokens and to remove a specific named result from a parsed string. It then parses strings containing numbers and labels, with the option to remove the named result \"LABEL\" from the parsed results." <EXPLAINS> """CODE.def remove_first(tokens):
    tokens.pop(0)
print(OneOrMore(Word(nums)).parseString("0 123 321")) # -> ['0', '123', '321']
print(OneOrMore(Word(nums)).addParseAction(remove_first).parseString("0 123 321")) # -> ['123', '321']

label = Word(alphas)
patt = label("LABEL") + OneOrMore(Word(nums))
print(patt.parseString("AAB 123 321").dump())

# Use pop() in a parse action to remove named result (note that corresponding value is not
# removed from list form of results)
def remove_LABEL(tokens):
    tokens.pop("LABEL")
    return tokens
patt.addParseAction(remove_LABEL)
print(patt.parseString("AAB 123 321").dump())""" .

"DESCRIPTION.The code defines input layers 'x2' and 'x1', applies normalization to 'x2', applies a lambda function to 'x1' to duplicate it, combines the inputs and outputs into a list, and then creates a FunctionalPreprocessingStage." <EXPLAINS> """CODE.inputs = {'x2': tf.keras.Input(shape=(5,)),
...           'x1': tf.keras.Input(shape=(1,))}
norm_layer = tf.keras.layers.experimental.preprocessing.Normalization()
y = norm_layer(inputs['x2'])
y, z = tf.keras.layers.Lambda(lambda x: (x, x))(inputs['x1'])
outputs = [inputs['x1'], [y, z]]
stage = FunctionalPreprocessingStage(inputs, outputs)""" .

"DESCRIPTION.The code defines lambda functions, saves the state of the main module, creates and saves a custom module with values and functions, and then loads the modules and uses the functions defined within them." <EXPLAINS> """CODE.import dill
squared = lambda x: x*x
dill.dump_module() # save state of __main__ to /tmp/session.pkl

import dill
import pox
pox.plus_one = lambda x: x+1
dill.dump_module('pox_session.pkl', module=pox)

import dill
from types import ModuleType
foo = ModuleType('foo')
foo.values = [1,2,3]
import math
foo.sin = math.sin
dill.dump_module('foo_session.pkl', module=foo, refimported=True)

import dill
dill.load_module()
squared(2)
pox = dill.load_module('pox_session.pkl')
pox.plus_one(1)
foo = dill.load_module('foo_session.pkl')
[foo.sin(x) for x in foo.values]
""" .

"DESCRIPTION.The code defines layers for multi-head attention with specified parameters and initializes variables. It then applies the multi-head attention layers to input data with different random keys and shapes. Additionally, it defines a module using the multi-head attention layer and applies the module to input data with dropout." <EXPLAINS> """CODE.import flax.linen as nn
import jax

layer = nn.MultiHeadAttention(num_heads=8, qkv_features=16)
key1, key2, key3, key4, key5, key6 = jax.random.split(jax.random.key(0), 6)
shape = (4, 3, 2, 5)
q, k, v = jax.random.uniform(key1, shape), jax.random.uniform(key2, shape), jax.random.uniform(key3, shape)
variables = layer.init(jax.random.key(0), q)

out = layer.apply(variables, q, k, v)
out = layer.apply(variables, q, k)
out = layer.apply(variables, q)

attention_kwargs = dict(
    num_heads=8,
    qkv_features=16,
    kernel_init=nn.initializers.ones,
    bias_init=nn.initializers.zeros,
    dropout_rate=0.5,
    deterministic=False,
    )
class Module(nn.Module):
  attention_kwargs: dict

  @nn.compact
  def __call__(self, x, dropout_rng=None):
    out1 = nn.MultiHeadAttention(**self.attention_kwargs)(x, dropout_rng=dropout_rng)
    out2 = nn.MultiHeadAttention(**self.attention_kwargs)(x, dropout_rng=dropout_rng)
    return out1, out2
module = Module(attention_kwargs)
variables = module.init({'params': key1, 'dropout': key2}, q)

out1, out2 = module.apply(variables, q, rngs={'dropout': key3})
out3, out4 = module.apply(variables, q, rngs={'dropout': key4})
out1, out2 = module.apply(variables, q, dropout_rng=key5)
out3, out4 = module.apply(variables, q, rngs={'dropout': key6}, dropout_rng=key5)""" .

"DESCRIPTION.The code defines linear operators and performs operations such as converting to a dense matrix, calculating shape, computing the log absolute determinant, matrix-vector multiplication, and concatenation. It also creates blocks of linear operators and applies matrix-vector multiplication on a random tensor." <EXPLAINS> """CODE.operator_0 = tf.linalg.LinearOperatorFullMatrix([[1., 2.], [3., 4.]])
operator_1 = tf.linalg.LinearOperatorFullMatrix([[1., 0.], [0., 1.]])
operator_2 = tf.linalg.LinearOperatorLowerTriangular([[5., 6.], [7., 8]])
operator = LinearOperatorBlockLowerTriangular(
  [[operator_0], [operator_1, operator_2]])

operator.to_dense()
operator.shape
operator.log_abs_determinant()
x0 = [[1., 6.], [-3., 4.]]
x1 = [[0., 2.], [4., 0.]]
x = tf.concat([x0, x1], 0)  # Shape [2, 4] Tensor
operator.matmul(x)
tf.concat([operator_0.matmul(x0),
  operator_1.matmul(x0) + operator_2.matmul(x1)], axis=0)
matrix_44 = tf.random.normal(shape=[2, 3, 4, 4])
operator_44 = tf.linalg.LinearOperatorFullMatrix(matrix_44)
matrix_54 = tf.random.normal(shape=[1, 3, 5, 4])
operator_54 = tf.linalg.LinearOperatorFullMatrix(matrix_54)
matrix_55 = tf.random.normal(shape=[1, 3, 5, 5])
operator_55 = tf.linalg.LinearOperatorFullMatrix(matrix_55)
operator_99 = LinearOperatorBlockLowerTriangular(
  [[operator_44], [operator_54, operator_55]])
operator_99.shape
x = tf.random.normal(shape=[2, 1, 9])
y = operator_99.matvec(x)
y.shape
""" .

"DESCRIPTION.The code defines packing and unpacking functions to print messages when tensors are packed and unpacked before and after a calculation involving two tensors (a and b) with requires_grad set to True. The tensors are multiplied together, summed, and then backpropagated to calculate gradients." <EXPLAINS> """CODE.def pack_hook(tensor: Tensor) -> Any
    print("Packing", tensor)
    return tensor

def unpack_hook(x)
    print("Unpacking", x)
    return x

a = torch.ones(5, requires_grad=True)
b = torch.ones(5, requires_grad=True) * 2
with torch.autograd.graph.saved_tensors_hooks(pack_hook, unpack_hook):
    y = a * b
    Packing tensor([1., 1., 1., 1., 1.])
    Packing tensor([2., 2., 2., 2., 2.])
y.sum().backward()
Unpacking tensor([1., 1., 1., 1., 1.])
Unpacking tensor([2., 2., 2., 2., 2.])""" .

"DESCRIPTION.The code defines placeholder tensors of different shapes and data types using TensorFlow's backend functions, and also creates variables with specific values and data types. It then checks and returns the data type of the created variables." <EXPLAINS> """CODE.tf.keras.backend.dtype(tf.keras.backend.placeholder(shape=(2,4,5)))
tf.keras.backend.dtype(tf.keras.backend.placeholder(shape=(2,4,5), dtype='float32'))
tf.keras.backend.dtype(tf.keras.backend.placeholder(shape=(2,4,5), dtype='float64'))
kvar = tf.keras.backend.variable(np.array([[1, 2], [3, 4]]))
tf.keras.backend.dtype(kvar)
kvar = tf.keras.backend.variable(np.array([[1, 2], [3, 4]]), dtype='float32')
tf.keras.backend.dtype(kvar)""" .

"DESCRIPTION.The code defines placeholders for two arrays, multiplies them using dot product operation, and then checks the shape of the result." <EXPLAINS> """CODE.x = tf.keras.backend.placeholder(shape=(2, 3))
y = tf.keras.backend.placeholder(shape=(3, 4))
xy = tf.keras.backend.dot(x, y)

x = tf.keras.backend.placeholder(shape=(32, 28, 3))
y = tf.keras.backend.placeholder(shape=(3, 4))
xy = tf.keras.backend.dot(x, y)

x = tf.keras.backend.random_uniform_variable(shape=(2, 3), low=0, high=1)
y = tf.keras.backend.ones((4, 3, 5))
xy = tf.keras.backend.dot(x, y)
tf.keras.backend.int_shape(xy)""" .

"DESCRIPTION.The code defines several classes that utilize a HyperparametersMixin to save hyperparameters. The first class, ManuallyArgsModel, manually assigns arguments arg1 and arg3. The second class, AutomaticArgsModel, automatically saves hyperparameters without specifying them. The third class, SingleArgModel, manually assigns a single argument. The fourth class, ManuallyArgsModel, passes arguments to ignore 'arg2'." <EXPLAINS> """CODE.class ManuallyArgsModel(HyperparametersMixin):
    def __init__(self, arg1, arg2, arg3):
        super().__init__()
        # manually assign arguments
        self.save_hyperparameters('arg1', 'arg3')
    def forward(self, *args, **kwargs):
        ...

class AutomaticArgsModel(HyperparametersMixin):
    def __init__(self, arg1, arg2, arg3):
        super().__init__()
        # equivalent automatic
        self.save_hyperparameters()
    def forward(self, *args, **kwargs):
        ...

class SingleArgModel(HyperparametersMixin):
    def __init__(self, params):
        super().__init__()
        # manually assign single argument
        self.save_hyperparameters(params)
    def forward(self, *args, **kwargs):
        ...

class ManuallyArgsModel(HyperparametersMixin):
    def __init__(self, arg1, arg2, arg3):
        super().__init__()
        # pass argument(s) to ignore as a string or in a list
        self.save_hyperparameters(ignore='arg2')
    def forward(self, *args, **kwargs):
        ...""" .

"DESCRIPTION.The code defines sparse columns using hash buckets, creates a crossed feature between the two sparse columns, and sets up a logistic regression classifier. It then provides input functions for training, evaluation, and testing the classifier, and uses the classifier to fit, evaluate, and predict classes and probabilities." <EXPLAINS> """CODE.sparse_column_a = sparse_column_with_hash_bucket(...)
sparse_column_b = sparse_column_with_hash_bucket(...)

sparse_feature_a_x_sparse_feature_b = crossed_column(...)

estimator = SDCALogisticClassifier(
    example_id_column='example_id',
    feature_columns=[sparse_column_a, sparse_feature_a_x_sparse_feature_b]),
    weight_column_name=...,
    l2_regularization=...,
    num_loss_partitions=...,
)

# Input builders
# returns x, y (where y is the label Tensor (with 0/1 values)
def input_fn_{train, eval}:

# returns x (features dict)
def input_fn_test:
  ...
estimator.fit(input_fn=input_fn_train)
estimator.evaluate(input_fn=input_fn_eval)
estimator.predict_classes(input_fn=input_fn_test) # returns predicted classes.
estimator.predict_proba(input_fn=input_fn_test) # returns predicted prob/ties.
""" .

"DESCRIPTION.The code defines sparse feature columns, creates a crossed feature column, initializes a SDCARegressor estimator with specific parameters, defines input functions for training, evaluation, and testing data, fits the estimator using the training data, evaluates the model using the evaluation data, and predicts scores for the test data." <EXPLAINS> """CODE.sparse_column_a = sparse_column_with_hash_bucket(...)
sparse_column_b = sparse_column_with_hash_bucket(...)

sparse_feature_a_x_sparse_feature_b = crossed_column(...)

estimator = SDCARegressor(
    example_id_column='example_id',
    feature_columns=[sparse_column_a, sparse_feature_a_x_sparse_feature_b]),
    weight_column_name=...,
    l2_regularization=...,
    num_loss_partitions=...,
)

# Input builders
# returns x, y (where y is the label Tensor (with 0/1 values)
def input_fn_{train, eval}:

# returns x (features dict)
def input_fn_test:
  ...
estimator.fit(input_fn=input_fn_train)
estimator.evaluate(input_fn=input_fn_eval)
estimator.predict_scores(input_fn=input_fn_test) # returns predicted scores.""" .

"DESCRIPTION.The code defines the functionality of merging two dimensions using the tf.Dimension class. The result of merging two dimensions is a new dimension object with the size of the merged dimensions. If one of the dimensions is None, the result will have the size of the other dimension. If the sizes of the two dimensions are not equal, a ValueError is raised." <EXPLAINS> """CODE.tf.Dimension(n)   .merge_with(tf.Dimension(n))    == tf.Dimension(n)
tf.Dimension(n)   .merge_with(tf.Dimension(None)) == tf.Dimension(n)
tf.Dimension(None).merge_with(tf.Dimension(n))    == tf.Dimension(n)
tf.Dimension(None).merge_with(tf.Dimension(None)) == tf.Dimension(None)
tf.Dimension(n)   .merge_with(tf.Dimension(m))  # raises ValueError for n != m
""" .

"""DESCRIPTION.The code defines three LambdaCallback functions: batch_print_callback, json_logging_callback, and cleanup_callback.
batch_print_callback prints the batch number at the beginning of each batch during model training.
json_logging_callback logs the epoch number and loss value in a JSON format to a file called 'loss_log.json' at the end of each epoch, and closes the file at the end of model training.
cleanup_callback terminates any processes that are still alive at the end of model training.
These LambdaCallback functions are then passed as callbacks to the model.fit function.""" <EXPLAINS> """CODE.batch_print_callback = LambdaCallback(
    on_batch_begin=lambda batch,logs: print(batch))

json_log = open('loss_log.json', mode='wt', buffering=1)
json_logging_callback = LambdaCallback(
    on_epoch_end=lambda epoch, logs: json_log.write(
        json.dumps({'epoch': epoch, 'loss': logs['loss']}) + '\\n'),
    on_train_end=lambda logs: json_log.close()
)

processes = ...
cleanup_callback = LambdaCallback(
    on_train_end=lambda logs: [
        p.terminate() for p in processes if p.is_alive()])

model.fit(...,
          callbacks=[batch_print_callback,
                     json_logging_callback,
                     cleanup_callback])
""" .

"DESCRIPTION.The code defines three functions that fill constant values in arrays of different shapes and data types. It then creates three predicates based on comparisons between constant values. Finally, it uses a case statement to call one of the predefined functions based on the result of the predicates. The output values are calculated using the selected functions and printed." <EXPLAINS> """CODE.import paddle.fluid as fluid
import paddle.fluid.layers as layers

def fn_1():
    return layers.fill_constant(shape=[1, 2], dtype='float32', value=1)

def fn_2():
    return layers.fill_constant(shape=[2, 2], dtype='int32', value=2)

def fn_3():
    return layers.fill_constant(shape=[3], dtype='int32', value=3)

main_program = fluid.default_startup_program()
startup_program = fluid.default_main_program()
with fluid.program_guard(main_program, startup_program):
    x = layers.fill_constant(shape=[1], dtype='float32', value=0.3)
    y = layers.fill_constant(shape=[1], dtype='float32', value=0.1)
    z = layers.fill_constant(shape=[1], dtype='float32', value=0.2)

    pred_1 = layers.less_than(z, x)  # true: 0.2 < 0.3
    pred_2 = layers.less_than(x, y)  # false: 0.3 < 0.1
    pred_3 = layers.equal(x, y)      # false: 0.3 == 0.1

    # Call fn_1 because pred_1 is True
    out_1 = layers.case(
        pred_fn_pairs=[(pred_1, fn_1), (pred_2, fn_2)], default=fn_3)

    # Argument default is None and no pred in pred_fn_pairs is True. fn_3 will be called.
    # because fn_3 is the last callable in pred_fn_pairs.
    out_2 = layers.case(pred_fn_pairs=[(pred_2, fn_2), (pred_3, fn_3)])

exe = fluid.Executor(fluid.CPUPlace())
res_1, res_2 = exe.run(main_program, fetch_list=[out_1, out_2])
print(res_1)  # [[1. 1.]]
print(res_2)  # [3 3 3]""" .

"DESCRIPTION.The code defines three functions, where function_1 and function_2 have a decorator log_latency applied to them, and function_3 contains a context manager log_latency." <EXPLAINS> """CODE.@log_latency
def function_1():
    pass

@log_latency("custom_label")
def function_2():
    pass

def function_3():
    with log_latency("region_within_function"):
        pass""" .

"DESCRIPTION.The code defines three learning rate schedulers: ConstantLR with a factor of 0.1 for 2 total iterations, ExponentialLR with a gamma of 0.9, and SequentialLR which sequentially applies the previously defined schedulers at specific milestones. The code then loops through 100 epochs, running training and validation functions, and updates the learning rate according to the scheduler at each step." <EXPLAINS> """CODE.scheduler1 = ConstantLR(self.opt, factor=0.1, total_iters=2)
scheduler2 = ExponentialLR(self.opt, gamma=0.9)
scheduler = SequentialLR(self.opt, schedulers=[scheduler1, scheduler2], milestones=[2])
for epoch in range(100):
    train(...)
    validate(...)
    scheduler.step()""" .

"DESCRIPTION.The code defines three models: linear model, deep neural network (DNN) model, and a combined model that combines the linear and DNN models. It then compiles and trains each model separately using different optimizers and loss functions. Finally, it combines the two models into a wide and deep model, compiles it with multiple optimizers, and fits the combined model using both linear and DNN inputs." <EXPLAINS> """CODE.linear_model = LinearModel()
dnn_model = keras.Sequential([keras.layers.Dense(units=64),
                             keras.layers.Dense(units=1)])
combined_model = WideDeepModel(linear_model, dnn_model)
combined_model.compile(optimizer=['sgd', 'adam'], 'mse', ['mse'])
combined_model.fit([linear_inputs, dnn_inputs], y, epochs)

linear_model = LinearModel()
linear_model.compile('adagrad', 'mse')
linear_model.fit(linear_inputs, y, epochs)
dnn_model = keras.Sequential([keras.layers.Dense(units=1)])
dnn_model.compile('rmsprop', 'mse')
dnn_model.fit(dnn_inputs, y, epochs)
combined_model = WideDeepModel(linear_model, dnn_model)
combined_model.compile(optimizer=['sgd', 'adam'], 'mse', ['mse'])
combined_model.fit([linear_inputs, dnn_inputs], y, epochs)""" .

"DESCRIPTION.The code defines three variables \"a\", \"b\", and \"c\" assigned to values \"x\", \"y\", and \"z\" respectively and retrieves the defined names within the module." <EXPLAINS> """CODE.from jedi._compatibility import u
from jedi.parser import Parser
parser = Parser(u('''
a = x
b = y
b.c = z
'''))
parser.module.get_defined_names()""" .

"DESCRIPTION.The code defines two Dense layers, layer_a and layer_b, with different constant initializers. The code then sets the weights of layer_b to be the same as the weights of layer_a." <EXPLAINS> """CODE.layer_a = tf.keras.layers.Dense(1,
  kernel_initializer=tf.constant_initializer(1.))
a_out = layer_a(tf.convert_to_tensor([[1., 2., 3.]]))
layer_a.get_weights()
[array([[1.],
       [1.],
       [1.]], dtype=float32), array([0.], dtype=float32)]
layer_b = tf.keras.layers.Dense(1,
  kernel_initializer=tf.constant_initializer(2.))
b_out = layer_b(tf.convert_to_tensor([[10., 20., 30.]]))
layer_b.get_weights()
[array([[2.],
       [2.],
       [2.]], dtype=float32), array([0.], dtype=float32)]
layer_b.set_weights(layer_a.get_weights())
layer_b.get_weights()
[array([[1.],
       [1.],
       [1.]], dtype=float32), array([0.], dtype=float32)]""" .

"DESCRIPTION.The code defines two PyLayer classes Tanh and Tanh2 which implement forward and backward propagation functions. Tanh returns the sum of input x three times as well as the input x doubled. Tanh2 also returns the sum of input x three times and the input x doubled, but it sets materialize_grads to False in the forward function. The backward functions of both classes assert conditions on the gradients and return the gradient values. The code then applies the Tanh and Tanh2 functions to input tensors x and x2 and performs backward propagation." <EXPLAINS> """CODE.import paddle
from paddle.autograd import PyLayer
import numpy as np

class Tanh(PyLayer):
    @staticmethod
    def forward(ctx, x):
        return x+x+x, x+x

    @staticmethod
    def backward(ctx, grad, grad2):
        assert np.equal(grad2.numpy(), paddle.zeros([1]).numpy())
        return grad

class Tanh2(PyLayer):
    @staticmethod
    def forward(ctx, x):
        ctx.set_materialize_grads(False)
        return x+x+x, x+x

    @staticmethod
    def backward(ctx, grad, grad2):
        assert grad2==None
        return grad

x = paddle.ones([1], dtype="float64")
x.stop_gradient = False
Tanh.apply(x)[0].backward()

x2 = paddle.ones([1], dtype="float64")
x2.stop_gradient = False
Tanh2.apply(x2)[0].backward()""" .

"DESCRIPTION.The code defines two PyLayer classes, Tanh and Tanh2, that implement the forward and backward propagation functions for a Tanh activation function. The Tanh class sets the materialize_grads attribute to True, while the Tanh2 class sets it to False. The forward function returns the input x and x+x, and the backward function performs some assertions and returns the gradients. Finally, the code applies the Tanh and Tanh2 layers to input tensors x and x2 respectively, and computes their gradients using backpropagation." <EXPLAINS> """CODE.import paddle
from paddle.autograd import PyLayer
import numpy as np

class Tanh(PyLayer):
    @staticmethod
    def forward(ctx, x):
        return x, x+x

    @staticmethod
    def backward(ctx, grad, grad2):
        assert np.equal(grad2.numpy(), paddle.zeros([1]).numpy())
        return grad

class Tanh2(PyLayer):
    @staticmethod
    def forward(ctx, x):
        ctx.set_materialize_grads(False)
        return x, x+x

    @staticmethod
    def backward(ctx, grad, grad2):
        assert grad2==None
        return grad

x = paddle.ones([1], dtype="float64")
x.stop_gradient = False
Tanh.apply(x)[0].backward()

x2 = paddle.ones([1], dtype="float64")
x2.stop_gradient = False
Tanh2.apply(x2)[0].backward()""" .

"DESCRIPTION.The code defines two RNN models, model_1 and model_2, which are guaranteed to create their own variables. It then calls the models with input and state to get output and next state. Finally, it asserts that both models have weights and that the weights of model_1 are not equal to the weights of model_2." <EXPLAINS> """CODE.set_keras_style()

model_1 = RNNModel(name="model_1")
model_2 = RNNModel(name="model_2")

# model_1 and model_2 are guaranteed to create their own variables.
output_1, next_state_1 = model_1(input, state)
output_2, next_state_2 = model_2(input, state)

assert len(model_1.weights) > 0
assert len(model_2.weights) > 0
assert(model_1.weights != model_2.weights)
""" .

"""DESCRIPTION.The code defines two classes Foo and Foo2, which are neural network modules using the Flax library.

- Foo takes an input vector, applies two dense layers, and returns the output vector.
- Foo2 takes an input vector, records two versions of the input vector in the 'intermediates' state dictionary, and returns the input vector multiplied by 2.""" <EXPLAINS> """CODE.  import jax
  import jax.numpy as jnp
  import flax.linen as nn

  class Foo(nn.Module):
    @nn.compact
    def __call__(self, x):
      h = nn.Dense(4)(x)
      self.sow('intermediates', 'h', h)
      return nn.Dense(2)(h)

  x = jnp.ones((16, 9))
  model = Foo()
  variables = model.init(jax.random.PRNGKey(0), x)
  y, state = model.apply(variables, x, mutable=['intermediates'])
  print(state['intermediates'])  # {'h': (...,)}

  class Foo2(nn.Module):
    @nn.compact
    def __call__(self, x):
      init_fn = lambda: 0
      reduce_fn = lambda a, b: a + b
      self.sow('intermediates', 'h', x,
               init_fn=init_fn, reduce_fn=reduce_fn)
      self.sow('intermediates', 'h', x * 2,
               init_fn=init_fn, reduce_fn=reduce_fn)
      return x

  model = Foo2()
  variables = model.init(jax.random.PRNGKey(0), x)
  y, state = model.apply(variables, jnp.ones((1, 1)), mutable=['intermediates'])
  print(state['intermediates'])  # ==> {'h': [[3.]]}
""" .

"DESCRIPTION.The code defines two classes, CounterWork and RootFlow, which are subclasses of LightningWork and LightningFlow respectively. CounterWork has a counter attribute that is incremented by 1 each time the run() method is called. RootFlow initializes a list with two CounterWork instances and runs the run() method of each work in the list. Finally, the code asserts that the counter attribute of the first CounterWork instance in the list is equal to 1 after running the flow." <EXPLAINS> """CODE.from lightning_app import LightningFlow, LightningWork
from lightning_app.core import List

class CounterWork(LightningWork):
    def __init__(self):
        super().__init__()
        self.counter = 0
    def run(self):
        self.counter += 1

class RootFlow(LightningFlow):
    def __init__(self):
        super().__init__()
        self.list = List(*[CounterWork(), CounterWork()])
    def run(self):
        for work in self.list:
            work.run()

flow = RootFlow()
flow.run()
assert flow.list[0].counter == 1
""" .

"DESCRIPTION.The code defines two classes, Downstream and Ingress, as deployments in Ray Serve. Downstream class has a method say_hi that returns a greeting message with the input message. Ingress class takes a handle to Downstream class in its constructor, and its call method sends a remote call to say_hi method of Downstream class and returns the response. The code then binds Ingress to Downstream, runs the application, sends a remote call with \"world\" as input, and asserts that the response is \"Hello world!\"." <EXPLAINS> """CODE.import ray
from ray import serve
from ray.serve.handle import DeploymentHandle, DeploymentResponse

@serve.deployment
class Downstream:
    def say_hi(self, message: str):
        return f"Hello {message}!"
        self._message = message

@serve.deployment
class Ingress:
    def __init__(self, handle: DeploymentHandle):
        self._downstream_handle = handle

    async def __call__(self, name: str) -> str:
        response = self._handle.say_hi.remote(name)
        return await response

app = Ingress.bind(Downstream.bind())
handle: DeploymentHandle = serve.run(app)
response = handle.remote("world")
assert response.result() == "Hello world!"
""" .

"DESCRIPTION.The code defines two classes, ExperimentalGraphsPlugin and ExperimentalDebuggerPluginLoader, which inherit functionalities from other classes (GraphsPlugin, ExperimentalPlugin, DebuggerPluginLoader) to add experimental features to graph and debugger plugins respectively." <EXPLAINS> """CODE.class ExperimentalGraphsPlugin(
    graphs_plugin.GraphsPlugin,
    experimental_plugin.ExperimentalPlugin,
):
    pass

class ExperimentalDebuggerPluginLoader(
    debugger_plugin_loader.DebuggerPluginLoader,
    experimental_plugin.ExperimentalPlugin
):
    pass""" .

"DESCRIPTION.The code defines two classes, Flow and SchedulerDAG, both inheriting from LightningFlow. The Flow class has a run method that prints a message every hour. The SchedulerDAG class initializes a list of DAG objects, and its run method schedules tasks to run hourly, adding new DAG objects to the list and running each DAG object's tasks." <EXPLAINS> """CODE.from lightning_app import LightningFlow

class Flow(LightningFlow):
    def run(self):
        if self.schedule("hourly"):
            # run some code once every hour.
            print("run this every hour")

from lightning_app import LightningFlow
from lightning_app.structures import List

class SchedulerDAG(LightningFlow):
    def __init__(self):
        super().__init__()
        self.dags = List()

    def run(self):
        if self.schedule("hourly"):
            self.dags.append(DAG(...))

        for dag in self.dags:
            payload = dag.run()
""" .

"""DESCRIPTION.The code defines two classes: `MyModel` and `FeatureExtractorFreezeUnfreeze`.

The `MyModel` class has a method `configure_optimizer` that configures an Adam optimizer by filtering the parameters based on whether they require gradients.

The `FeatureExtractorFreezeUnfreeze` class has a constructor that takes an argument `unfreeze_at_epoch` and has two methods. The `freeze_before_training` method freezes a specified module (`feature_extractor` in this case) before training. The `finetune_function` method unfreezes the `feature_extractor` module and adds parameter groups to the optimizer when the current epoch reaches the specified value in `unfreeze_at_epoch`.""" <EXPLAINS> """CODE.class MyModel(LightningModule):

    ...

    def configure_optimizer(self):
        # Make sure to filter the parameters based on `requires_grad`
        return Adam(filter(lambda p: p.requires_grad, self.parameters))

class FeatureExtractorFreezeUnfreeze(BaseFinetuning):

    def __init__(self, unfreeze_at_epoch=10)
        self._unfreeze_at_epoch = unfreeze_at_epoch

    def freeze_before_training(self, pl_module):
        # freeze any module you want
        # Here, we are freezing ``feature_extractor``
        self.freeze(pl_module.feature_extractor)

    def finetune_function(self, pl_module, current_epoch, optimizer, optimizer_idx):
        # When `current_epoch` is 10, feature_extractor will start training.
        if current_epoch == self._unfreeze_at_epoch:
            self.unfreeze_and_add_param_group(
                module=pl_module.feature_extractor,
                optimizer=optimizer,
                train_bn=True,
            )
""" .

"DESCRIPTION.The code defines two communication hooks for distributed data parallelism in PyTorch. The first hook, named \"noop\", simply returns the tensors from the input bucket. The second hook, named \"encode_and_decode\", divides the tensors in the bucket by the world size, encodes them, performs allreduce operation, and then decodes the result." <EXPLAINS> """CODE.def noop(state: object, bucket: dist._GradBucket) -> torch.futures.Future
    fut = torch.futures.Future()
    fut.set_result(bucket.get_tensors())
    return fut

ddp._register_comm_hook(state = None, hook = noop)

def encode_and_decode(state: object, bucket: dist._GradBucket) -> torch.futures.Future
    tensors = [t / process_group.world_size for t in bucket.get_tensors()]
    encoded_tensors = encode(tensors) # encode gradients
    fut = process_group.allreduce(encoded_tensors).get_future()
    # Define the then callback to decode.
    def decode(fut):
        decoded_tensors = decode(fut.value()) # decode gradients
        return decoded_tensors
    return fut.then(decode)

ddp._register_comm_hook(state = None, hook = encode_and_decode)""",
        """CODE.def noop(state: object, bucket: dist._GradBucket):
    fut = torch.futures.Future()
    fut.set_result(bucket.get_tensors())
    return fut

ddp.register_comm_hook(state = None, hook = noop)

def encode_and_decode(state: object, bucket: dist._GradBucket):
    tensors = [t / process_group.world_size for t in bucket.get_tensors()]
    encoded_tensors = encode(tensors) # encode gradients
    fut = process_group.allreduce(encoded_tensors).get_future()

    # Define the then callback to decode.
    def decode(fut):
        decoded_tensors = decode(fut.value()) # decode gradients
        return decoded_tensors

    return fut.then(decode)

ddp.register_comm_hook(state = None, hook = encode_and_decode)""" .

"DESCRIPTION.The code defines two constants, DistributedType.DDP and DistributedType.DDP2, and assigns them the values 'ddp' and 'DDP2' respectively." <EXPLAINS> """CODE.DistributedType.DDP == 'ddp'
DistributedType.DDP2 == 'DDP2'""" .

"DESCRIPTION.The code defines two custom neural network modules using PyTorch. The first module performs matrix multiplication and linear transformation on input data. The second module modifies a tensor value during forward pass. Both modules are then scripted and frozen to optimize for performance and memory usage." <EXPLAINS> """CODE.import torch
class MyModule(torch.nn.Module):
    def __init__(self, N, M):
        super(MyModule, self).__init__()
        self.weight = torch.nn.Parameter(torch.rand(N, M))
        self.linear = torch.nn.Linear(N, M)

    def forward(self, input):
        output = self.weight.mm(input)
        output = self.linear(output)
        return output

scripted_module = torch.jit.script(MyModule(2, 3).eval())
frozen_module = torch.jit.freeze(scripted_module)
# parameters have been removed and inlined into the Graph as constants
assert len(list(frozen_module.named_parameters())) == 0
# See the compiled graph as Python code
print(frozen_module.code)

import torch
class MyModule2(torch.nn.Module):
    def __init__(self):
        super(MyModule2, self).__init__()
        self.modified_tensor = torch.tensor(10.)
        self.version = 1

    def forward(self, input):
        self.modified_tensor += 1
        return input + self.modified_tensor

scripted_module = torch.jit.script(MyModule2().eval())
frozen_module = torch.jit.freeze(scripted_module, preserved_attrs=["version"])
# we've manually preserved `version`, so it still exists on the frozen module and can be modified
assert frozen_module.version == 1
frozen_module.version = 2
# `modified_tensor` is detected as being mutated in the forward, so freezing preserves
# it to retain model semantics
assert frozen_module(torch.tensor(1)) == torch.tensor(12)
# now that we've run it once, the next result will be incremented by one
assert frozen_module(torch.tensor(1)) == torch.tensor(13)""" .

"DESCRIPTION.The code defines two default functions for a class method named 'bar'. The first function returns an integer value of 11, while the second function returns a floating-point value of 3.0." <EXPLAINS> """CODE.@default('bar')
def get_bar_default(self):
    return 11

@default('bar')
def some_other_default(self):
    return 3.0""" .

"DESCRIPTION.The code defines two functions \"up\" and \"down\" that both apply a dense layer with 3 units to the input x." <EXPLAINS> """CODE.@nn.compact_name_scope
def up(self, x):
    return nn.Dense(3)(x)

@nn.compact_name_scope
def down(self, x):
    return nn.Dense(3)(x)""" .

"DESCRIPTION.The code defines two functions `f1` and `f2` that each return a constant value. It then executes a function on a specific device (CPU or GPU) based on the dictionary provided, with `f1` as the default function." <EXPLAINS> """CODE.def f1(): return tf.constant(1)
def f2(): return tf.constant(2)
r = tf.execute_fn_for_device({"CPU": f1, "GPU": f2}, default_fn=f1)
""" .

"DESCRIPTION.The code defines two functions named my_command, which gather metrics for a given URL. The first function uses the default name for the metric gathering, while the second function specifies a custom name for the metric gathering." <EXPLAINS> """CODE.@st.gather_metrics
def my_command(url):
    return url

@st.gather_metrics(name="custom_name")
def my_command(url):
    return url""" .

"DESCRIPTION.The code defines two functions that accept batches of inputs, ensuring that the inputs are lists before proceeding with the rest of the code." <EXPLAINS> """CODE.@serve.accept_batch
def serving_func(flask_request):
    assert isinstance(flask_request, list)
    ...

@serve.accept_batch
def __call__(self, *, python_arg=None):
    assert isinstance(python_arg, list)""" .

"DESCRIPTION.The code defines two functions, `pattern` and `target_graph`, to perform convolution and ReLU operations on input data `x` using given weights. The `pattern` function returns the ReLU output and a dictionary containing the convolution output and ReLU output. The `target_graph` function performs the same operations but multiplies the ReLU output by 2 before returning it. Lastly, the code uses a SubgraphMatcher to find matches between the pre-autograd graphs of the `pattern` and `target_graph` functions, and updates the annotation of the \"conv\" node in the match's name_node_map." <EXPLAINS> """CODE.def pattern(x, weight):
    conv = F.conv2d(x, weight)
    relu = F.relu(conv)
    return relu, {"conv": conv, "relu": relu}

def target_graph(x, weight):
    conv = F.conv2d(x, weight)
    relu = F.relu(conv)
    relu *= 2
    return relu

pattern_gm = capture_pre_autograd_graph(pattern, example_inputs)
target_gm = capture_pre_autograd_graph(target_graph, example_inputs)
matcher = SubgraphMatcherWithNameNodeMap(pattern_gm)
matches = matcher.match(target_gm)
for match in matches:
    match.name_node_map["conv"].meta["annotation"] = ...""" .

"DESCRIPTION.The code defines two functions, `train_input_fn` and `eval_input_fn`, which are used to provide the training and evaluation datasets. It then creates a DNNClassifier estimator using TensorFlow. An InMemoryEvaluatorHook is used to evaluate the model during training, with the evaluation dataset provided by `eval_input_fn`. The estimator is then trained using the training dataset provided by `train_input_fn` and the evaluation hook." <EXPLAINS> """CODE.def train_input_fn():
  ...
  return train_dataset

def eval_input_fn():
  ...
  return eval_dataset

estimator = tf.estimator.DNNClassifier(...)

evaluator = tf.contrib.estimator.InMemoryEvaluatorHook(
    estimator, eval_input_fn)
estimator.train(train_input_fn, hooks=[evaluator])
""" .

"""DESCRIPTION.The code defines two independent distributions:
1. The first independent distribution is a 2-dimensional normal distribution with loc [-1., 1] and scale [0.1, 0.5], where all batch dimensions have been absorbed into the event dimensions.
2. The second independent distribution is a 2-dimensional bivariate normal distribution with loc [[-1., 1], [1, -1]] and scale_identity_multiplier [1., 0.5], where all batch dimensions have been absorbed into the event dimensions.""" <EXPLAINS> """CODE.ds = tf.contrib.distributions

# Make independent distribution from a 2-batch Normal.
ind = ds.Independent(
    distribution=ds.Normal(loc=[-1., 1], scale=[0.1, 0.5]),
    reduce_batch_ndims=1)

# All batch dims have been "absorbed" into event dims.
ind.batch_shape  # ==> []
ind.event_shape  # ==> [2]

# Make independent distribution from a 2-batch bivariate Normal.
ind = ds.Independent(
    distribution=ds.MultivariateNormalDiag(
        loc=[[-1., 1], [1, -1]],
        scale_identity_multiplier=[1., 0.5]),
    reduce_batch_ndims=1)

# All batch dims have been "absorbed" into event dims.
ind.batch_shape  # ==> []
ind.event_shape  # ==> [2, 2]
""" .

"DESCRIPTION.The code defines two instances of a Version class, one with a post attribute and one without." <EXPLAINS> """CODE.Version("1.2.3").post
Version("1.2.3.post1").post""" .

"DESCRIPTION.The code defines two lambda functions to calculate values based on the epoch, creates a LambdaSL scheduler with these lambda functions, then iterates through 100 epochs to train, validate, and adjust the scheduler." <EXPLAINS> """CODE.lambda1 = lambda epoch: epoch // 30
lambda2 = lambda epoch: 0.95 ** epoch
scheduler = LambdaSL(sparsifier, sl_lambda=[lambda1, lambda2])
for epoch in range(100):
    train(...)
    validate(...)
    scheduler.step()""" .

"DESCRIPTION.The code defines two learning rate schedulers, one with a constant learning rate factor of 0.1 for a total of 2 iterations, and another with an exponential decay factor of 0.9. It then chains these two schedulers together into one scheduler and applies it in a training loop over 100 epochs by alternating between training and validating the model and updating the learning rate using the chained scheduler." <EXPLAINS> """CODE.scheduler1 = ConstantLR(self.opt, factor=0.1, total_iters=2)
scheduler2 = ExponentialLR(self.opt, gamma=0.9)
scheduler = ChainedScheduler([scheduler1, scheduler2])
for epoch in range(100):
    train(...)
    validate(...)
    scheduler.step()""" .

"DESCRIPTION.The code defines two neural network models with one hidden layer and 8 units, using mean squared error loss and RMSprop optimizer with a learning rate of 0.001. The models are trained on input data of size (10, 4) and target data of size (10, 8)." <EXPLAINS> """CODE.model = tf.keras.Sequential([
  tf.keras.layers.InputLayer(input_shape=(4,)),
  tf.keras.layers.Dense(8)])
model.compile(tf.optimizers.RMSprop(0.001), loss='mse')
model.fit(np.zeros((10, 4)),
          np.ones((10, 8)))

model = tf.keras.Sequential([
  tf.keras.layers.Dense(8, input_shape=(4,))])
model.compile(tf.optimizers.RMSprop(0.001), loss='mse')
model.fit(np.zeros((10, 4)),
          np.ones((10, 8)))
""" .

"DESCRIPTION.The code defines two operations, sim_op and replay_op, which are executed in parallel. The results of these two operations are combined using Concurrently to execute them concurrently." <EXPLAINS> """CODE.sim_op = ParallelRollouts(...).for_each(...)
replay_op = LocalReplay(...).for_each(...)
combined_op = Concurrently([sim_op, replay_op])""" .

"DESCRIPTION.The code defines two recurrent encoder configurations, one using a LSTM recurrent layer with 2 layers, 16 input dimensions, 128 hidden dimensions, and 256 output dimensions with linear activation function and bias, and the other using a GRU recurrent layer with 1 layer, 32 input dimensions, 64 hidden dimensions, and no specified output dimensions with no bias. It then builds the corresponding models using the specified configuration for the PyTorch framework." <EXPLAINS> """CODE.config = RecurrentEncoderConfig(
    recurrent_layer_type="lstm",
    input_dims=[16],  # must be 1D tensor
    hidden_dim=128,
    num_layers=2,
    output_dims=[256],  # maybe None or a 1D tensor
    output_activation="linear",
    use_bias=True,
)
model = config.build(framework="torch")

config = RecurrentEncoderConfig(
    recurrent_layer_type="gru",
    input_dims=[32],  # must be 1D tensor
    hidden_dim=64,
    num_layers=1,
    output_dims=None,  # maybe None or a 1D tensor
    use_bias=False,
)
model = config.build(framework="torch")
""" .

"DESCRIPTION.The code defines two sets of input shapes, generates random tensors with those input shapes, applies a 1D convolutional layer with specified parameters and activation function to the tensor, and finally prints the shape of the output tensor." <EXPLAINS> """CODE.input_shape = (4, 10, 128)
x = tf.random.normal(input_shape)
y = tf.keras.layers.Conv1D(
32, 3, activation='relu',input_shape=input_shape[1:])(x)
print(y.shape)

input_shape = (4, 7, 10, 128)
x = tf.random.normal(input_shape)
y = tf.keras.layers.Conv1D(
32, 3, activation='relu', input_shape=input_shape[2:])(x)
print(y.shape)
""" .

"DESCRIPTION.The code defines two sparse columns with hash bucket, creates embedding columns for the sparse features, initializes a DNNEstimator model with multiple hidden layers, uses multi_class_head for classification with 2 classes, provides input functions for training and evaluation, fits the model with training data, evaluates the model with evaluation data, and makes predictions with input data." <EXPLAINS> """CODE.sparse_feature_a = sparse_column_with_hash_bucket(...)
sparse_feature_b = sparse_column_with_hash_bucket(...)

sparse_feature_a_emb = embedding_column(sparse_id_column=sparse_feature_a,
                                        ...)
sparse_feature_b_emb = embedding_column(sparse_id_column=sparse_feature_b,
                                        ...)

estimator = DNNEstimator(
    feature_columns=[sparse_feature_a_emb, sparse_feature_b_emb],
    head=tf.contrib.learn.multi_class_head(n_classes=2),
    hidden_units=[1024, 512, 256])

head = tf.contrib.learn.multi_class_head(
    n_classes=2,
    label_name="x",
    weight_column_name="w",
    enable_centered_bias=True)
estimator = DNNEstimator(
    feature_columns=[sparse_feature_a_emb, sparse_feature_b_emb],
    head=head,
    hidden_units=[1024, 512, 256])

def input_fn_train: # returns x, y (where y represents label's class index).
  pass
estimator.fit(input_fn=input_fn_train)

def input_fn_eval: # returns x, y (where y represents label's class index).
  pass
estimator.evaluate(input_fn=input_fn_eval)
estimator.predict(x=x) # returns predicted labels (i.e. label's class index).
""" .

"DESCRIPTION.The code defines two sparse data types: one for integers with 0 as a default value, and another for integers with 1 as a default value. It then updates the second data type to use floats and set NaN as the default value." <EXPLAINS> """CODE.SparseDtype(int, 0).update_dtype(float)
SparseDtype(int, 1).update_dtype(SparseDtype(float, np.nan)""" .

"DESCRIPTION.The code defines two tensors (a and b) with shape (2, 2) and checks if they are sparse tensors or not." <EXPLAINS> """CODE.a = tf.keras.backend.placeholder((2, 2), sparse=False)
print(tf.keras.backend.is_sparse(a))
b = tf.keras.backend.placeholder((2, 2), sparse=True)
print(tf.keras.backend.is_sparse(b) )""" .

"DESCRIPTION.The code defines two test functions using a decorator that checks for internet connectivity before running the test. The first test function, test_something_with_yahoo, raises an IOError with a failure message. The second test function, test_something, prints \"I ran!\" and raises a ValueError to simulate a failure." <EXPLAINS> """CODE.@with_connectivity_check("http://www.yahoo.com")
def test_something_with_yahoo():
    raise IOError("Failure Message")

@with_connectivity_check("failing://url.blaher", check_before_test=True)
def test_something():
    print("I ran!")
    raise ValueError("Failure")""" .

"DESCRIPTION.The code defines two types of parameter traversal objects for kernels and biases, creates separate optimizers for kernels and biases using momentum optimization, creates a multi-optimizer object combining both optimizers, initializes the optimizer for a given model, retrieves hyperparameters from the optimizer, and applies gradient updates with modified hyperparameters for learning rates." <EXPLAINS> """CODE.kernels = traverse_util.ModelParamTraversal(lambda path, _: 'kernel' in path)
biases = traverse_util.ModelParamTraversal(lambda path, _: 'bias' in path)
kernel_opt = optim.Momentum(learning_rate=0.01)
bias_opt = optim.Momentum(learning_rate=0.1)
opt_def = MultiOptimizer((kernels, kernel_opt), (biases, bias_opt))
optimizer = opt_def.create(model)

hparams = optimizer.optimizer_def.hyper_params
new_optimizer = optimizer.apply_gradient(
    grads,
    hyper_params=[
        hparams[0].replace(learning_rate=0.2),
        hparams[1].replace(learning_rate=jnp.where(step < 1000, 0., lr)),
    ])
""" .

"DESCRIPTION.The code defines variables for comparison operators, variables, numbers, and terms, and creates a comparison expression pattern. The code searches for this pattern in the given string \"B = 12  AA=23 B<=AA AA>12\"." <EXPLAINS> """CODE.comp_oper = oneOf("< = > <= >= !=")
var = Word(alphas)
number = Word(nums)
term = var | number
comparison_expr = term + comp_oper + term
print(comparison_expr.searchString("B = 12  AA=23 B<=AA AA>12"))""" .

"DESCRIPTION.The code defines variables var1, var2, and var3 using TensorFlow get_variable function. It then defines a function fn that operates on var1, var2, and var3. Finally, it updates var1 using the function fn on every device var1 is present on, ensuring var2 and var3 are also on the same devices." <EXPLAINS> """CODE.
with strategy.scope():
  var1 = tf.get_variable(...)
  with strategy.extended.colocate_vars_with(var1):
    # var2 and var3 will be created on the same device(s) as var1
    var2 = tf.get_variable(...)
    var3 = tf.get_variable(...)

  def fn(v1, v2, v3):
    # operates on v1 from var1, v2 from var2, and v3 from var3

  # `fn` runs on every device `var1` is on, `var2` and `var3` will be there
  # too.
  strategy.extended.update(var1, fn, args=(var2, var3))
""" .

"DESCRIPTION.The code defines various types of variables and tensors using Keras, including numpy arrays, placeholders, variables, inputs, and layers. The code checks if each of these entities is a Keras tensor using the K.is_keras_tensor() function." <EXPLAINS> """CODE.from keras import backend as K
from keras.layers import Input, Dense
np_var = numpy.array([1, 2])
K.is_keras_tensor(np_var)
k_var = tf.placeholder('float32', shape=(1,1))
K.is_keras_tensor(k_var)
keras_var = K.variable(np_var)
K.is_keras_tensor(keras_var)
keras_placeholder = K.placeholder(shape=(2, 4, 5))
K.is_keras_tensor(keras_placeholder)
keras_input = Input([10])
K.is_keras_tensor(keras_input)
keras_layer_output = Dense(10)(keras_input)
K.is_keras_tensor(keras_layer_output)
""" .

"DESCRIPTION.The code defines version information objects and performs comparison operations between the version objects and tuples." <EXPLAINS> """CODE.attr.VersionInfo(19, 1, 0, "final")  <= (19, 2)
attr.VersionInfo(19, 1, 0, "final") < (19, 1, 1)
vi = attr.VersionInfo(19, 2, 0, "final")
vi < (19, 1, 1)
vi < (19,)
vi == (19, 2,)
vi == (19, 2, 1)""" .

"DESCRIPTION.The code deletes a collection (repository) named \"useless-collection-64f9a55bb3115b4f513ec026\" belonging to the user \"username\" from the Hugging Face Hub platform. If the collection does not exist, it will ignore the error and continue running." <EXPLAINS> """CODE.from huggingface_hub import delete_collection
collection = delete_collection("username/useless-collection-64f9a55bb3115b4f513ec026", missing_ok=True)
""" .

"DESCRIPTION.The code demonstrates creating a linear neural network layer using the paddle library in Python. It then showcases changing the data type of the layer's weight to float64, moving the layer to the CPU device, and moving the layer to a CUDA pinned place device while changing the layout of the weight tensor accordingly." <EXPLAINS> """CODE.import paddle

linear=paddle.nn.Linear(2, 2)
linear.weight
#Parameter containing:
#Tensor(shape=[2, 2], dtype=float32, place=CUDAPlace(0), stop_gradient=False,
#       [[-0.32770029,  0.38653070],
#        [ 0.46030545,  0.08158520]])

linear.to(dtype='float64')
linear.weight
#Tenor(shape=[2, 2], dtype=float64, place=CUDAPlace(0), stop_gradient=False,
#       [[-0.32770029,  0.38653070],
#        [ 0.46030545,  0.08158520]])

linear.to(device='cpu')
linear.weight
#Tensor(shape=[2, 2], dtype=float64, place=CPUPlace, stop_gradient=False,
#       [[-0.32770029,  0.38653070],
#        [ 0.46030545, 0.08158520]])
linear.to(device=paddle.CUDAPinnedPlace(), blocking=False)
linear.weight
#Tensor(shape=[2, 2], dtype=float64, place=CUDAPinnedPlace, stop_gradient=False,
#       [[-0.04989364, -0.56889004],
#        [ 0.33960250, 0.96878713]])""" .

"DESCRIPTION.The code demonstrates how to handle errors related to using a JAX Tracer in place of an integer when indexing a list or performing operations with a numpy array. The solution involves either converting the list to a JAX array or declaring the index as a static argument when using JAX's just-in-time (jit) compilation." <EXPLAINS> """CODE.Passing a tracer in place of an integer
    from jax import jit, partial
    import numpy as np

    @jit
    ... def func(x, axis):
    ...   return np.split(x, 2, axis)

    func(np.arange(4), 0)  # doctest: +IGNORE_EXCEPTION_DETAIL
    Traceback (most recent call last):
        ...
    TracerIntegerConversionError: The __index__() method was called on the JAX Tracer object

  When this happens, the solution is often to mark the problematic argument as static::

    @partial(jit, static_argnums=1)
    ... def func(x, axis):
    ...   return np.split(x, 2, axis)

    func(np.arange(10), 0)
    [DeviceArray([0, 1, 2, 3, 4], dtype=int32),
     DeviceArray([5, 6, 7, 8, 9], dtype=int32)]

Indexing a list with a Tracer
    import jax.numpy as jnp
    from jax import jit, partial

    L = [1, 2, 3]

    @jit
    ... def func(i):
    ...   return L[i]

    func(0)  # doctest: +IGNORE_EXCEPTION_DETAIL
    Traceback (most recent call last):
        ...
    TracerIntegerConversionError: The __index__() method was called on the JAX Tracer object

  Depending on the context, you can generally fix this either by converting the list
  to a JAX array::

    @jit
    ... def func(i):
    ...   return jnp.array(L)[i]

    func(0)
    DeviceArray(1, dtype=int32)

  or by declaring the index as a static argument::

    @partial(jit, static_argnums=0)
    ... def func(i):
    ...   return L[i]

    func(0)
    DeviceArray(1, dtype=int32)""" .

"DESCRIPTION.The code demonstrates registering datasets and loading a specific dataset using a PackageRegister object in Python." <EXPLAINS> """CODE.register = PackageRegister(path='/path/to/datasets-source-list.jsonl')

# List all registered datasets: ['kaggle:ds0', 'kaggle:ds1',...]
register.list_builders()

# Load a specific dataset
builder = register.builder('tensorflow_graphics:shapenet')
""" .

"DESCRIPTION.The code demonstrates the importance of using `clear_session()` in TensorFlow/Keras to manage the global state and memory consumption. The first loop without `clear_session()` increases the global state size over iterations, while the second loop with `clear_session()` ensures a blank state at each iteration, maintaining constant memory consumption. Additionally, setting the learning phase and clearing the session affects the naming convention of layers in the model." <EXPLAINS> """CODE.for _ in range(100):
  # Without `clear_session()`, each iteration of this loop will
  # slightly increase the size of the global state managed by Keras
  model = tf.keras.Sequential([tf.keras.layers.Dense(10) for _ in range(10)])

for _ in range(100):
  # With `clear_session()` called at the beginning,
  # Keras starts with a blank state at each iteration
  # and memory consumption is constant over time.
  tf.keras.backend.clear_session()
  model = tf.keras.Sequential([tf.keras.layers.Dense(10) for _ in range(10)])

import tensorflow as tf
layers = [tf.keras.layers.Dense(10) for _ in range(10)]
new_layer = tf.keras.layers.Dense(10)
print(new_layer.name)
dense_10
tf.keras.backend.set_learning_phase(1)
print(tf.keras.backend.learning_phase())
1
tf.keras.backend.clear_session()
new_layer = tf.keras.layers.Dense(10)
print(new_layer.name)
dense
""" .

"DESCRIPTION.The code demonstrates the usage of the Lightning framework in Python for auto-scaling server components out-of-the-box and customizing the scaling logic based on metrics such as pending requests and pending works. It includes examples of setting minimum and maximum replicas, autoscale intervals, maximum batch size, and timeout batching for auto scaling." <EXPLAINS> """CODE.import lightning as L

# Example 1: Auto-scaling serve component out-of-the-box
app = L.LightningApp(
    L.app.components.AutoScaler(
        MyPythonServer,
        min_replicas=1,
        max_replicas=8,
        autoscale_interval=10,
    )
)

# Example 2: Customizing the scaling logic
class MyAutoScaler(L.app.components.AutoScaler):
    def scale(self, replicas: int, metrics: dict) -> int:
        pending_requests_per_running_or_pending_work = metrics["pending_requests"] / (
            replicas + metrics["pending_works"]
        )

        # upscale
        max_requests_per_work = self.max_batch_size
        if pending_requests_per_running_or_pending_work >= max_requests_per_work:
            return replicas + 1

        # downscale
        min_requests_per_work = max_requests_per_work * 0.25
        if pending_requests_per_running_or_pending_work < min_requests_per_work:
            return replicas - 1

        return replicas


app = L.LightningApp(
    MyAutoScaler(
        MyPythonServer,
        min_replicas=1,
        max_replicas=8,
        autoscale_interval=10,
        max_batch_size=8,  # for auto batching
        timeout_batching=1,  # for auto batching
    )
)""" .

"DESCRIPTION.The code demonstrates the usage of various RNN (Recurrent Neural Network) models, such as LSTM (Long Short-Term Memory), ConvLSTM (Convolutional LSTM), with different configurations like cell size, time-majority, and return carry. It initializes the RNN models, processes input data of various shapes (batch, time, features), and applies the models to generate output of specified shapes. It also showcases the ability to extract intermediate states (carry) from the LSTM model." <EXPLAINS> """CODE.import jax.numpy as jnp
import jax
import flax.linen as nn

x = jnp.ones((10, 50, 32)) # (batch, time, features)
lstm = nn.RNN(nn.LSTMCell(), cell_size=64)
variables = lstm.init(jax.random.PRNGKey(0), x)
y = lstm.apply(variables, x)
y.shape # (batch, time, cell_size)

x = jnp.ones((10, 50, 32, 32, 3)) # (batch, time, height, width, features)
conv_lstm = nn.RNN(nn.ConvLSTMCell(64, kernel_size=(3, 3)), cell_size=(32, 32, 64))
y, variables = conv_lstm.init_with_output(jax.random.PRNGKey(0), x)
y.shape # (batch, time, height, width, features)

x = jnp.ones((50, 10, 32)) # (time, batch, features)
lstm = nn.RNN(nn.LSTMCell(), cell_size=64, time_major=True)
variables = lstm.init(jax.random.PRNGKey(0), x)
y = lstm.apply(variables, x)
y.shape # (time, batch, cell_size)

x = jnp.ones((10, 50, 32)) # (batch, time, features)
lstm = nn.RNN(nn.LSTMCell(), cell_size=64, return_carry=True)
variables = lstm.init(jax.random.PRNGKey(0), x)
carry, y = lstm.apply(variables, x)
jax.tree_map(jnp.shape, carry) # ((batch, cell_size), (batch, cell_size))
y.shape # (batch, time, cell_size)
""" .

"DESCRIPTION.The code deserializes activation functions in TensorFlow Keras for 'linear', 'sigmoid', and 'abcd'." <EXPLAINS> """CODE.tf.keras.activations.deserialize('linear')
tf.keras.activations.deserialize('sigmoid')
tf.keras.activations.deserialize('abcd')""" .

"DESCRIPTION.The code deserializes activation functions such as linear and sigmoid using tf.keras.activations.deserialize(). If the activation function 'abcd' is not recognized, an error may occur." <EXPLAINS> """CODE.tf.keras.activations.deserialize('linear')
tf.keras.activations.deserialize('sigmoid')
tf.keras.activations.deserialize('abcd')""" .

"DESCRIPTION.The code deserializes different activation functions using Tensorflow's Keras API." <EXPLAINS> """CODE.tf.keras.activations.deserialize('linear')
tf.keras.activations.deserialize('sigmoid')
tf.keras.activations.deserialize('abcd')""" .

"DESCRIPTION.The code deserializes the activation functions 'linear' and 'sigmoid' using tf.keras.activations module." <EXPLAINS> """CODE.tf.keras.activations.deserialize('linear')
tf.keras.activations.deserialize('sigmoid')
tf.keras.activations.deserialize('abcd')""" .

"DESCRIPTION.The code deserializes the activation functions 'linear' and 'sigmoid' using tf.keras.activations, and attempts to deserialize an unknown activation function 'abcd'." <EXPLAINS> """CODE.tf.keras.activations.deserialize('linear')
tf.keras.activations.deserialize('sigmoid')
tf.keras.activations.deserialize('abcd')""" .

"DESCRIPTION.The code deserializes various activation functions using the tf.keras.activations.deserialize function." <EXPLAINS> """CODE.tf.keras.activations.deserialize('linear')
tf.keras.activations.deserialize('sigmoid')
tf.keras.activations.deserialize('abcd')""" .

"DESCRIPTION.The code detects onsets in an audio signal using the onset detection function provided by the librosa library in Python. It generates onset frames and onset times based on the audio signal's amplitude and sampling rate. It also calculates the onset strength of the audio signal and detects onsets using the onset envelope generated from the onset strength." <EXPLAINS> """CODE.onset_frames    = librosa.onset.onset_detect(y=y, sr=sr, hop_length=64)
onset_times     = librosa.frames_to_time(onset_frames, sr, hop_length=64)

onsets          = librosa.onset.onset_strength(y, sr)
onset_frames    = librosa.onset.onset_detect(onset_envelope=onsets, sr=sr)
""" .

"DESCRIPTION.The code detects onsets in audio signals and backtracks them using the onset envelope and RMS energy. It then plots the onset strength, raw onsets, backtracked onsets, RMSE, and backtracked RMSE on separate subplots." <EXPLAINS> """CODE.y, sr = librosa.load(librosa.util.example_audio_file(),
...                      offset=30, duration=2.0)
oenv = librosa.onset.onset_strength(y=y, sr=sr)
# Detect events without backtracking
onset_raw = librosa.onset.onset_detect(onset_envelope=oenv,
...                                        backtrack=False)
# Backtrack the events using the onset envelope
onset_bt = librosa.onset.onset_backtrack(onset_raw, oenv)
# Backtrack the events using the RMS energy
rmse = librosa.feature.rmse(S=np.abs(librosa.stft(y=y)))
onset_bt_rmse = librosa.onset.onset_backtrack(onset_raw, rmse[0])

# Plot the results
import matplotlib.pyplot as plt
plt.figure()
plt.subplot(2,1,1)
plt.plot(oenv, label='Onset strength')
plt.vlines(onset_raw, 0, oenv.max(), label='Raw onsets')
plt.vlines(onset_bt, 0, oenv.max(), label='Backtracked', color='r')
plt.legend(frameon=True, framealpha=0.75)
plt.subplot(2,1,2)
plt.plot(rmse[0], label='RMSE')
plt.vlines(onset_bt_rmse, 0, rmse.max(), label='Backtracked (RMSE)', color='r')
plt.legend(frameon=True, framealpha=0.75)""" .

"DESCRIPTION.The code detects peaks in the input signal x using specified parameters." <EXPLAINS> "CODE.librosa.peak_pick(x, 3, 3, 5, 5, 0.5, 10)" .

"DESCRIPTION.The code detects the character encoding of the input bytes using the UniversalDetector in Python." <EXPLAINS> """CODE.        u = UniversalDetector()
        u.feed(some_bytes)
        u.close()
        detected = u.result""" .

"DESCRIPTION.The code detects the creation of processes on a Windows system using WMI. It watches for the creation of Win32_Process instances and prints out the names of the processes created. It also watches for the creation of Win32_NTLogEvent instances with Type \"error\" and \"warning\", and prints out the error and warning logs respectively." <EXPLAINS> """CODE.c = wmi.WMI ()
raw_wql = "SELECT * FROM __InstanceCreationEvent WITHIN 2 WHERE TargetInstance ISA 'Win32_Process'"
watcher = c.watch_for (raw_wql=raw_wql)
while 1:
  process_created = watcher ()
  print process_created.Name

watcher = c.watch_for (
  notification_type="Creation",
  wmi_class="Win32_Process",
  delay_secs=2,
  Name='calc.exe'
)
calc_created = watcher ()

import pythoncom
import wmi
c = wmi.WMI (privileges=["Security"])
watcher1 = c.watch_for (
  notification_type="Creation",
  wmi_class="Win32_NTLogEvent",
  Type="error"
)
watcher2 = c.watch_for (
  notification_type="Creation",
  wmi_class="Win32_NTLogEvent",
  Type="warning"
)

while 1:
  try:
    error_log = watcher1 (500)
  except wmi.x_wmi_timed_out:
    pythoncom.PumpWaitingMessages ()
  else:
    print error_log

  try:
    warning_log = watcher2 (500)
  except wmi.x_wmi_timed_out:
    pythoncom.PumpWaitingMessages ()
  else:
    print warning_log""" .

"DESCRIPTION.The code determines and returns the duplicated values in the given index." <EXPLAINS> """CODE.pd.Index([1, 2, 2, 3, 3, 3, 4]).get_duplicates()
pd.Index([1., 2., 2., 3., 3., 3., 4.]).get_duplicates()
pd.Index(['a', 'b', 'b', 'c', 'c', 'c', 'd']).get_duplicates()
dates = pd.to_datetime(['2018-01-01', '2018-01-02', '2018-01-03',
                        '2018-01-03', '2018-01-04', '2018-01-04'],
                       format='%Y-%m-%d')
pd.Index(dates).get_duplicates()
pd.Index([1, 2, 3, 2, 3, 4, 3]).get_duplicates()
pd.Index([1, 2, 3, 4]).get_duplicates()
dates = pd.to_datetime(['2018-01-01', '2018-01-02', '2018-01-03'],
                       format='%Y-%m-%d')
pd.Index(dates).get_duplicates()""" .

"DESCRIPTION.The code determines the number of CPU cores available for parallel processing tasks and adjusts the number of jobs accordingly. It ensures that at least one job is always available for processing." <EXPLAINS> """CODE.from sklearn.utils import _get_n_jobs
_get_n_jobs(4)
jobs = _get_n_jobs(-2)
assert jobs == max(cpu_count() - 1, 1)
_get_n_jobs(0)
""" .

"DESCRIPTION.The code determines whether the input is a number or not." <EXPLAINS> """CODE.is_number(1)
is_number("foo")""" .

"DESCRIPTION.The code determines whether the input is a regular expression object created using the re.compile function. If the input is a regular expression object, the function returns True. Otherwise, it returns False." <EXPLAINS> """CODE.is_re(re.compile(".*"))
True
is_re("foo")
False""" .

"DESCRIPTION.The code displays a chart with random numbers generated from actual dice rolls and provides an explanation." <EXPLAINS> """CODE.with st.beta_expander("See explanation"):
...     st.write(\"\"\"
...         The chart above shows some numbers I picked for you.
...         I rolled actual dice for these, so they're *guaranteed* to
...         be random.
...     \"\"\")
...     st.image("https://static.streamlit.io/examples/dice.jpg")""" .

"DESCRIPTION.The code displays a chart with some randomly generated numbers and an image of dice with an explanation, all within a Streamlit expander widget." <EXPLAINS> """CODE.with st.expander("See explanation"):
    st.write(\"\"\"
        The chart above shows some numbers I picked for you.
        I rolled actual dice for these, so they're *guaranteed* to
        be random.
    \"\"\")
    st.image("https://static.streamlit.io/examples/dice.jpg")""" .

"DESCRIPTION.The code displays a header \"This is a header\" on the screen." <EXPLAINS> "CODE.st.header('This is a header')" .

"DESCRIPTION.The code displays a header with the text \"This is a header\" on the screen." <EXPLAINS> "CODE.st.header('This is a header')" .

"DESCRIPTION.The code displays a radio button that allows the user to select their favorite movie genre between Comedy, Drama, and Documentary. If the user selects Comedy, it outputs \"You selected comedy.\", otherwise it outputs \"You didn't select comedy.\"." <EXPLAINS> """CODE.st.radio(
    "What's your favorite movie genre",
    ('Comedy', 'Drama', 'Documentary'))

if genre == 'Comedy':
    st.write('You selected comedy.')
else:
    st.write("You didn't select comedy.")""" .

"DESCRIPTION.The code displays a spectrogram plot using the input matrix S, with frequency ticks labeled on the y-axis in Hertz format." <EXPLAINS> """CODE.librosa.display.specshow(S)
librosa.display.frequency_ticks()
librosa.display.frequency_ticks(locations, frequencies)
librosa.display.frequency_ticks(frequencies, freq_fmt='Hz')
librosa.display.frequency_ticks(frequencies, axis='y')""" .

"DESCRIPTION.The code displays a success message using the Streamlit library." <EXPLAINS> "CODE.st.success('This is a success message!')" .

"DESCRIPTION.The code displays a title on the screen." <EXPLAINS> "CODE.st.title('This is a title')" .

"DESCRIPTION.The code displays a toast message in a streamlit application indicating that the edited image has been saved, with a heart emoji icon." <EXPLAINS> """CODE.import streamlit as st

st.toast('Your edited image was saved!', icon='ð')""" .

"DESCRIPTION.The code displays a warning message \"This is a warning\" using the Streamlit library." <EXPLAINS> "CODE.st.warning('This is a warning')" .

"DESCRIPTION.The code displays a warning message with the content \"This is a warning\"." <EXPLAINS> "CODE.st.warning('This is a warning')" .

"DESCRIPTION.The code displays an area chart using the data provided in the chart_data variable." <EXPLAINS> "CODE.st.area_chart(chart_data)" .

"DESCRIPTION.The code displays an error message 'This is an error'." <EXPLAINS> "CODE.st.error('This is an error')" .

"DESCRIPTION.The code displays an error message with the text \"This is an error\"." <EXPLAINS> "CODE.st.error('This is an error')" .

"DESCRIPTION.The code displays an informational message with the content \"This is a purely informational message\"." <EXPLAINS> "CODE.st.info('This is a purely informational message')" .

"DESCRIPTION.The code displays chat messages with different users (\"user\" and \"assistant\") and shows random data in line chart and bar chart formats." <EXPLAINS> """CODE.import streamlit as st
import numpy as np

with st.chat_message("user"):
    st.write("Hello ð")
    st.line_chart(np.random.randn(30, 3))

import streamlit as st
import numpy as np

message = st.chat_message("assistant")
message.write("Hello human")
message.bar_chart(np.random.randn(30, 3))
""" .

"DESCRIPTION.The code displays three columns where the first column shows images of a cat, a dog, and an owl, while the second column includes a line chart and the third column displays the actual data used in the chart." <EXPLAINS> """CODE.col1, col2, col3 = st.columns(3)

with col1:
    st.header("A cat")
    st.image("https://static.streamlit.io/examples/cat.jpg")

with col2:
    st.header("A dog")
    st.image("https://static.streamlit.io/examples/dog.jpg")

with col3:
    st.header("An owl")
    st.image("https://static.streamlit.io/examples/owl.jpg")

col1, col2 = st.columns([3, 1])
data = np.random.randn(10, 1)

col1.subheader("A wide column with a chart")
col1.line_chart(data)

col2.subheader("A narrow column with the data")
col2.write(data)
""" .

"DESCRIPTION.The code displays three images of a cat, a dog, and an owl in three separate columns. It then generates a random data array and visualizes it as a line chart in a wide column, while displaying the data in a narrow column." <EXPLAINS> """CODE.col1, col2, col3 = st.beta_columns(3)

with col1:
    st.header("A cat")
    st.image("https://static.streamlit.io/examples/cat.jpg")

with col2:
    st.header("A dog")
    st.image("https://static.streamlit.io/examples/dog.jpg")

with col3:
    st.header("An owl")
    st.image("https://static.streamlit.io/examples/owl.jpg")

col1, col2 = st.beta_columns([3, 1])
data = np.random.randn(10, 1)

col1.subheader("A wide column with a chart")
col1.line_chart(data)

col2.subheader("A narrow column with the data")
col2.write(data)
""" .

"DESCRIPTION.The code distributes a randomly generated tensor onto a specified mesh using a given placement strategy and then converts the tensor into a partition specification for XLA processing." <EXPLAINS> """CODE.t = torch.randn(4, 8, 8)
dt_mesh = DeviceMesh("xla", torch.arange(8).reshape(2,4))
placements = [Replicate(), Shard(0)]
my_dtensor = distribute_tensor(t, dt_mesh, placements)

partition_spec = convert_to_xla_partition_spec(t, placements)
""" .

"DESCRIPTION.The code distributes a specified number of shards evenly among a maximum number of jobs, ensuring that each job gets an equal number of shards." <EXPLAINS> """CODE._distribute_shards(2, max_num_jobs=4)
_distribute_shards(10, max_num_jobs=3)""" .

"DESCRIPTION.The code divides a range of numbers from 0 to 16 into chunks of size 8 and prints each chunk." <EXPLAINS> """CODE.from huggingface_hub.utils import chunk_iterable

for items in chunk_iterable(range(17), chunk_size=8):
    print(items)
""" .

"DESCRIPTION.The code divides the process into multiple nodes and sets the placement of each process on the nodes." <EXPLAINS> """CODE.import paddle
import paddle.distributed as dist

paddle.enable_static()

mesh = dist.ProcessMesh([[2, 4, 5], [0, 1, 3]])
mesh.set_placement([0, 1, 2, 3, 4, 5])""" .

"DESCRIPTION.The code downloads a badge image from a URL, checks if the image file exists, and then deletes the downloaded image file." <EXPLAINS> """CODE.path_img = _download_badge('https://img.shields.io/pypi/pyversions/pytorch-lightning',
                            'PyPI - Python Version', '.')
os.path.isfile(path_img)
path_img  # doctest: +ELLIPSIS
os.remove(path_img)""" .

"DESCRIPTION.The code downloads a badge image from a specific URL, checks if the image file exists, displays the path of the image, and then deletes the image file." <EXPLAINS> """CODE.path_img = _download_badge('https://img.shields.io/pypi/pyversions/pytorch-lightning',
                            'PyPI - Python Version', '.')
os.path.isfile(path_img)
path_img  # doctest: +ELLIPSIS
os.remove(path_img)""" .

"DESCRIPTION.The code downloads a compressed file containing flower photos from a specified URL using TensorFlow's utility function and saves it to a local directory." <EXPLAINS> """CODE.path_to_downloaded_file = tf.keras.utils.get_file(
    "flower_photos",
    "https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz",
    untar=True)
""" .

"DESCRIPTION.The code downloads a file from a specific URL using a download manager and stores the downloaded file in a variable called downloaded_files." <EXPLAINS> """CODE.downloaded_files = dl_manager.download('https://storage.googleapis.com/seldon-datasets/sentence_polarity_v1/rt-polaritydata.tar.gz')
""" .

"DESCRIPTION.The code downloads a file from a specified URL using urllib and shows a progress bar using tqdm." <EXPLAINS> """CODE.with tqdm(...) as t:
    reporthook = my_hook(t)
    urllib.urlretrieve(..., reporthook=reporthook)""" .

"DESCRIPTION.The code downloads a pre-trained ResNet18 model from a specified URL to a temporary file." <EXPLAINS> "CODE.torch.hub.download_url_to_file('https://s3.amazonaws.com/pytorch/models/resnet18-5c106cde.pth', '/tmp/temporary_file')" .

"DESCRIPTION.The code downloads a tar.gz file from a specific URL and then extracts the contents of the downloaded file." <EXPLAINS> """CODE.downloaded_files = dl_manager.download('https://storage.googleapis.com/seldon-datasets/sentence_polarity_v1/rt-polaritydata.tar.gz')
extracted_files = dl_manager.extract(downloaded_files)
""" .

"DESCRIPTION.The code downloads an image from a specified URL, processes the image using a pre-trained image processor, generates a textual description of the image using a pre-trained encoder-decoder model, and then decodes the generated text to describe the content of the image. Finally, it asserts that the generated description matches the expected description of the image." <EXPLAINS> """CODE.from transformers import TFVisionEncoderDecoderModel, AutoImageProcessor, AutoTokenizer
from PIL import Image
import requests

image_processor = AutoImageProcessor.from_pretrained("ydshieh/vit-gpt2-coco-en")
decoder_tokenizer = AutoTokenizer.from_pretrained("ydshieh/vit-gpt2-coco-en")
model = TFVisionEncoderDecoderModel.from_pretrained("ydshieh/vit-gpt2-coco-en")

url = "http://images.cocodataset.org/val2017/000000039769.jpg"
img = Image.open(requests.get(url, stream=True).raw)
pixel_values = image_processor(images=img, return_tensors="tf").pixel_values  # Batch size 1

output_ids = model.generate(
    pixel_values, max_length=16, num_beams=4, return_dict_in_generate=True
).sequences

preds = decoder_tokenizer.batch_decode(output_ids, skip_special_tokens=True)
preds = [pred.strip() for pred in preds]

assert preds == ["a cat laying on top of a couch next to another cat"]""" .

"DESCRIPTION.The code downloads files from a Kaggle competition by iterating through the competition files and using a downloader to download each file to a specified output path." <EXPLAINS> """CODE.
downloader = KaggleCompetitionDownloader(competition_name)
for fname in downloader.competition_files:
  downloader.download_file(fname, make_file_output_path(fname))
""" .

"DESCRIPTION.The code duplicates a Space from one account to another account on the Hugging Face Hub platform. It allows the user to specify a custom destination id and set the visibility flag to private if needed." <EXPLAINS> """CODE.from huggingface_hub import duplicate_space

# Duplicate a Space to your account
duplicate_space("multimodalart/dreambooth-training")
RepoUrl('https://huggingface.co/spaces/nateraw/dreambooth-training',...)

# Can set custom destination id and visibility flag.
duplicate_space("multimodalart/dreambooth-training", to_id="my-dreambooth", private=True)
RepoUrl('https://huggingface.co/spaces/nateraw/my-dreambooth',...)
""" .

"DESCRIPTION.The code duplicates the input tensor along the specified dimension and fills the new tensor with repeated copies of the original tensor." <EXPLAINS> """CODE.tile(ex, 2, 0)
torch.Tensor([[1,2],[1,2],[3,4],[3,4]])""" .

"DESCRIPTION.The code enables automatic mixed precision training for the LSTM model using bfloat16 data type." <EXPLAINS> """CODE... code-block:: python
    import paddle
    paddle.enable_static()
    with paddle.static.amp.bf16_guard():
        paddle.static.amp.AutoMixedPrecisionListsBF16(custom_fp32_list={'lstm'})""" .

"DESCRIPTION.The code enables tensor checking for NaN and Inf values, converts two lists into tensors, raises the elements of the first tensor to the power of the corresponding elements in the second tensor, and performs backpropagation on the result tensor. Finally, the tensor checking is disabled." <EXPLAINS> """CODE.import paddle

checker_config = paddle.amp.debugging.TensorCheckerConfig(enable=True, debug_mode=paddle.amp.debugging.DebugMode.CHECK_NAN_INF)
paddle.amp.debugging.enable_tensor_checker(checker_config)

x = paddle.to_tensor([1, 0, 3], place=paddle.CPUPlace(), dtype='float32', stop_gradient=False)
y = paddle.to_tensor([0.2, 0, 0.5], place=paddle.CPUPlace(), dtype='float32')
res = paddle.pow(x, y)
paddle.autograd.backward(res, retain_graph=True)
paddle.amp.debugging.disable_tensor_checker()""" .

"DESCRIPTION.The code enables the \"ViewSimplifyPattern\" optimization pattern for the IPU (Incremental Pointer Update) strategy in the PaddlePaddle framework." <EXPLAINS> """CODE.import paddle
import paddle.static as static

paddle.enable_static()

ipu_strategy = static.IpuStrategy()
ipu_strategy.enable_pattern("ViewSimplifyPattern")""" .

"DESCRIPTION.The code enables the packrat parsing optimization for the pyparsing library." <EXPLAINS> """CODE.import pyparsing
pyparsing.ParserElement.enablePackrat()""" .

"DESCRIPTION.The code enables the use of automatic differentiation in PaddlePaddle for gradient calculations. It demonstrates how to use automatic differentiation in two ways: as a generator and as a decorator." <EXPLAINS> """CODE.import paddle

# use as generator

x = paddle.to_tensor([1.], stop_gradient=False)
with paddle.no_grad():
    with paddle.enable_grad():
        y = x * 2
assert(y.stop_gradient == False)
y.backward()
assert(x.grad is not None)

# use as decorator

@paddle.enable_grad()
def double(x):
    return x * 2

with paddle.no_grad():
    z = double(x)

assert(z.stop_gradient == False)""" .

"DESCRIPTION.The code enables the user to perform a range test to determine good initial learning rates for a given model. It aims to reduce guesswork in selecting an appropriate starting learning rate by testing a range of learning rates. The user can specify the model, training data loader, minimum and maximum learning rates to investigate, number of learning rates to test, and search strategy (linear or exponential). After running the range test, the results can be inspected, a suggested learning rate can be obtained, and the model can be trained with the new learning rate." <EXPLAINS> """CODE.lr_find enables the user to do a range test of good initial learning rates,
to reduce the amount of guesswork in picking a good starting learning rate.

Args:
    model: Model to do range testing for

    train_dataloader: A PyTorch
        DataLoader with training samples. If the model has
        a predefined train_dataloader method this will be skipped.

    min_lr: minimum learning rate to investigate

    max_lr: maximum learning rate to investigate

    num_training: number of learning rates to test

    mode: search strategy, either 'linear' or 'exponential'. If set to
        'linear' the learning rate will be searched by linearly increasing
        after each batch. If set to 'exponential', will increase learning
        rate exponentially.

    num_accumulation_steps: number of batches to calculate loss over.

Example::

    # Setup model and trainer
    model = MyModelClass(hparams)
    trainer = pl.Trainer()

    # Run lr finder
    lr_finder = trainer.lr_find(model, ...)

    # Inspect results
    fig = lr_finder.plot(); fig.show()
    suggested_lr = lr_finder.suggest()

    # Overwrite lr and create new model
    hparams.lr = suggested_lr
    model = MyModelClass(hparams)

    # Ready to train with new learning rate
    trainer.fit(model)""" .

"DESCRIPTION.The code enables translation of input text from English to German or French using different pre-trained models." <EXPLAINS> """CODE.from huggingface_hub import InferenceClient
client = InferenceClient()
client.translation("My name is Wolfgang and I live in Berlin")
'Mein Name ist Wolfgang und ich lebe in Berlin.'
client.translation("My name is Wolfgang and I live in Berlin", model="Helsinki-NLP/opus-mt-en-fr")
"Je m'appelle Wolfgang et je vis Ã  Berlin."
""" .

"DESCRIPTION.The code encodes the input text using a pretrained FlaxBlenderbotSmall model and tokenizer from Facebook's Blenderbot_small-90M, without utilizing any specific dependencies or function calls." <EXPLAINS> """CODE.model = FlaxBlenderbotSmallForConditionalGeneration.from_pretrained('facebook/blenderbot_small-90M')
tokenizer = BlenderbotSmallTokenizer.from_pretrained('facebook/blenderbot_small-90M')
text = "My friends are cool but they eat too many carbs."
inputs = tokenizer(text, max_length=1024, return_tensors='np')
encoder_outputs = model.encode(**inputs)""" .

"DESCRIPTION.The code encodes the input text using the Blenderbot model to generate contextualized representations of the input." <EXPLAINS> """CODE.from transformers import BlenderbotTokenizer, FlaxBlenderbotForConditionalGeneration
model = FlaxBlenderbotForConditionalGeneration.from_pretrained('facebook/blenderbot-400M-distill')
tokenizer = BlenderbotTokenizer.from_pretrained('facebook/blenderbot-400M-distill')
text = "My friends are cool but they eat too many carbs."
inputs = tokenizer(text, max_length=1024, return_tensors='jax')
encoder_outputs = model.encode(**inputs)""" .

"DESCRIPTION.The code enforces a debug check to ensure that the input argument for function \"f\" is not equal to 0. If the input is 0, it raises an error with the message \"cannot be zero!\"" <EXPLAINS> """CODE.import jax
import jax.numpy as jnp
from jax.experimental import checkify
def f(x):
  checkify.debug_check(x!=0, "cannot be zero!")
  return x
_ = f(0)  # running without checkify means no debug_check is run.
checked_f = checkify.checkify(f)
err, out = jax.jit(checked_f)(0)  # running with checkify runs debug_check.
err.throw()  # doctest: +IGNORE_EXCEPTION_DETAIL
""" .

"DESCRIPTION.The code enforces that the input tensor \"x\" has a rank of either 2 or 4, and then calculates the sum of all elements in the tensor." <EXPLAINS> """CODE.with tf.control_dependencies([tf.assert_rank_in(x, (2, 4))]):
  output = tf.reduce_sum(x)
""" .

"DESCRIPTION.The code enqueues data for processing and allows for training, evaluating, and predicting using the inputs." <EXPLAINS> """CODE.    enqueuer = SequenceEnqueuer(...)
    enqueuer.start()
    datas = enqueuer.get()
    for data in datas:
        # Use the inputs; training, evaluating, predicting.
        # ... stop sometime.
    enqueuer.stop()
""" .

"DESCRIPTION.The code enqueues sequences of data for processing, allowing for training, evaluation, and prediction tasks to be performed on the data." <EXPLAINS> """CODE.    enqueuer = SequenceEnqueuer(...)
    enqueuer.start()
    datas = enqueuer.get()
    for data in datas:
        # Use the inputs; training, evaluating, predicting.
        # ... stop sometime.
    enqueuer.close()
""" .

"DESCRIPTION.The code ensures that the data type of the input numpy dtype is in nanoseconds precision." <EXPLAINS> """CODE._ensure_nanosecond_dtype(np.dtype("M8[s]"))
_ensure_nanosecond_dtype(np.dtype("m8[ps]"))""",
        """CODE.ensure_nanosecond_dtype(np.dtype("M8[s]"))
ensure_nanosecond_dtype(np.dtype("m8[ps]"))""" .

"DESCRIPTION.The code ensures that the shapes of the output arrays match the number of rows specified by the variable \"static_nrows\"." <EXPLAINS> """CODE.self.row_lengths().shape == [self.static_nrows]
self.row_starts().shape == [self.static_nrows]
self.row_limits().shape == [self.static_nrows]
self.row_splits().shape == [self.static_nrows + 1]
""" .

"DESCRIPTION.The code escapes special characters in a string to prevent HTML injection attacks." <EXPLAINS> """CODE.value = escape("<User 1>")
value
Markup('&lt;User 1&gt;')
escape(str(value))
Markup('&amp;lt;User 1&amp;gt;')
escape(soft_str(value))
Markup('&lt;User 1&gt;')""" .

"DESCRIPTION.The code establishes a connection named \"my_conn\" using Streamlit and checks if the connection is healthy. If the connection is not healthy, it resets the connection." <EXPLAINS> """CODE.import streamlit as st

conn = st.experimental_connection("my_conn")

if not conn.is_healthy():
    conn.reset()""" .

"DESCRIPTION.The code establishes a connection named \"my_conn\" using Streamlit. If the connection is not healthy, it is reset." <EXPLAINS> """CODE.import streamlit as st

conn = st.connection("my_conn")

if not conn.is_healthy():
    conn.reset()""" .

"DESCRIPTION.The code establishes a connection to a database named \"snowpark\", retrieves all data from the \"pet_owners\" table, and displays the data in a dataframe format using the Streamlit library." <EXPLAINS> """CODE.import streamlit as st
conn = st.experimental_connection("snowpark")
df = conn.query("select * from pet_owners")
st.dataframe(df)""" .

"DESCRIPTION.The code establishes a connection to a database using the provided DbConnectInfo." <EXPLAINS> "CODE.mock_dao.Connect(IsA(DbConnectInfo))" .

"DESCRIPTION.The code establishes a remote connection to a server named \"remote_machine\" using the provided username and password, and then creates an instance of WMI (Windows Management Instrumentation) using the remote connection for further interaction with the remote server." <EXPLAINS> """CODE.remote_connetion = wmi.connect_server (
    server="remote_machine", user="myname", password="mypassword"
)
c = wmi.WMI (wmi=remote_connection)""" .

"DESCRIPTION.The code establishes an HTTP connection to 'http://example.com' with different retry settings for each request. The first request has custom retry settings, the second request has increased retries, and the third request has retries set to False." <EXPLAINS> """CODE.retries = Retry(connect=5, read=2, redirect=5)
http = PoolManager(retries=retries)
response = http.request('GET', 'http://example.com/')

response = http.request('GET', 'http://example.com/', retries=Retry(10))

response = http.request('GET', 'http://example.com/', retries=False)
""" .

"DESCRIPTION.The code estimates static and dynamic tempos of audio signals using onset strength and autocorrelation techniques. The static tempo estimation provides a single tempo value, while the dynamic tempo estimation provides multiple tempo values. Additionally, the code visualizes the static and dynamic tempo estimates on tempo frequency axes using matplotlib." <EXPLAINS> """CODE.# Estimate a static tempo
y, sr = librosa.load(librosa.util.example_audio_file())
onset_env = librosa.onset.onset_strength(y, sr=sr)
tempo = librosa.beat.tempo(onset_envelope=onset_env, sr=sr)
tempo
array([129.199])

# Or a dynamic tempo
dtempo = librosa.beat.tempo(onset_envelope=onset_env, sr=sr,
                            aggregate=None)
dtempo
array([ 143.555,  143.555,  143.555, ...,  161.499,  161.499,
    172.266])

import matplotlib.pyplot as plt
# Convert to scalar
tempo = np.asscalar(tempo)
# Compute 2-second windowed autocorrelation
hop_length = 512
ac = librosa.autocorrelate(onset_env, 2 * sr // hop_length)
freqs = librosa.tempo_frequencies(len(ac), sr=sr,
                                hop_length=hop_length)
# Plot on a BPM axis.  We skip the first (0-lag) bin.
plt.figure(figsize=(8,4))
plt.semilogx(freqs[1:], librosa.util.normalize(ac)[1:],
            label='Onset autocorrelation', basex=2)
plt.axvline(tempo, 0, 1, color='r', alpha=0.75, linestyle='--',
            label='Tempo: {:.2f} BPM'.format(tempo))
plt.xlabel('Tempo (BPM)')
plt.grid()
plt.title('Static tempo estimation')
plt.legend(frameon=True)
plt.axis('tight')

plt.figure()
tg = librosa.feature.tempogram(onset_envelope=onset_env, sr=sr,
                            hop_length=hop_length)
librosa.display.specshow(tg, x_axis='time', y_axis='tempo')
plt.plot(librosa.frames_to_time(np.arange(len(dtempo))), dtempo,
        color='w', linewidth=1.5, label='Tempo estimate')
plt.title('Dynamic tempo estimation')
plt.legend(frameon=True, framealpha=0.75)
""" .

"DESCRIPTION.The code estimates tuning for either frequency values or pitch values." <EXPLAINS> """CODE.librosa.feature.estimate_tuning(freqs)
librosa.feature.estimate_tuning(pitches)""" .

"DESCRIPTION.The code evaluates question-answering models using specified datasets and computes their performance metrics for the SQuAD (Stanford Question Answering Dataset) tasks." <EXPLAINS> """CODE.from evaluate import evaluator
from datasets import load_dataset
task_evaluator = evaluator("question-answering")
data = load_dataset("squad", split="validation[:2]")
results = task_evaluator.compute(
    model_or_pipeline="sshleifer/tiny-distilbert-base-cased-distilled-squad",
    data=data,
    metric="squad",
)


from evaluate import evaluator
from datasets import load_dataset
task_evaluator = evaluator("question-answering")
data = load_dataset("squad_v2", split="validation[:2]")
results = task_evaluator.compute(
    model_or_pipeline="mrm8488/bert-tiny-finetuned-squadv2",
    data=data,
    metric="squad_v2",
    squad_v2_format=True,
)
""" .

"DESCRIPTION.The code evaluates the performance of the \"facebook/bart-large-cnn\" model on the summarization task using the dataset \"cnn_dailymail\" for its validation subset, utilizing the \"evaluate\" module and \"datasets\" module." <EXPLAINS> """CODE.from evaluate import evaluator
from datasets import load_dataset
task_evaluator = evaluator("summarization")
data = load_dataset("cnn_dailymail", "3.0.0", split="validation[:40]")
results = task_evaluator.compute(
    model_or_pipeline="facebook/bart-large-cnn",
    data=data,
    input_column="article",
    label_column="highlights",
)""" .

"DESCRIPTION.The code evaluates the performance of the Facebook BART model on the CNN/DailyMail dataset for text-to-text generation task using the ROUGE metric on the first 40 validation examples." <EXPLAINS> """CODE.from evaluate import evaluator
from datasets import load_dataset
task_evaluator = evaluator("text2text-generation")
data = load_dataset("cnn_dailymail", "3.0.0", split="validation[:40]")
results = task_evaluator.compute(
    model_or_pipeline="facebook/bart-large-cnn",
    data=data,
    input_column="article",
    label_column="highlights",
    metric="rouge",
)""" .

"DESCRIPTION.The code evaluates the translation task using the model \"Helsinki-NLP/opus-mt-de-fr\" on the WMT19 dataset with language pairs of French to German. It takes the validation data, maps the data to extract the German text and French labels, then computes the results using the evaluator." <EXPLAINS> """CODE.from evaluate import evaluator
from datasets import load_dataset
task_evaluator = evaluator("translation")
data = load_dataset("wmt19", "fr-de", split="validation[:40]")
data = data.map(lambda x: {"text": x["translation"]["de"], "label": x["translation"]["fr"]})
results = task_evaluator.compute(
    model_or_pipeline="Helsinki-NLP/opus-mt-de-fr",
    data=data,
)""" .

"DESCRIPTION.The code executes a single operation or function called \"operation1\"." <EXPLAINS> "CODE.operation1()" .

"DESCRIPTION.The code expands common English contractions in a list of sentences." <EXPLAINS> """CODE.import jiwer

sentences = ["she'll make sure you can't make it", "let's party!"]

print(jiwer.ExpandCommonEnglishContractions()(sentences))
# prints: ["she will make sure you can not make it", "let us party!"]
""" .

"DESCRIPTION.The code exports a TensorFlow model to a specified location, reloads the exported model from that location, and makes predictions on input data using the reloaded model." <EXPLAINS> """CODE.model.export("path/to/location")
reloaded_artifact = tf.saved_model.load("path/to/location")
predictions = reloaded_artifact.serve(input_data)""" .

"DESCRIPTION.The code exports a model to a specified path, reloads the model from the exported artifact, and then uses the reloaded model to make predictions on input data." <EXPLAINS> """CODE.model.export("path/to/artifact")
reloaded_layer = ReloadedLayer("path/to/artifact")
outputs = reloaded_layer(inputs)
""" .

"DESCRIPTION.The code exports a model, tracks its usage, adds an endpoint for serving the model, saves optimizer variables, and writes out the model to a specific location. The code then loads the saved model and retrieves the optimizer variables." <EXPLAINS> """CODE.export_archive = ExportArchive()
export_archive.track(model)
export_archive.add_endpoint(
    name="serve",
    fn=model.call,
    input_signature=[tf.TensorSpec(shape=(None, 3), dtype=tf.float32)],
)
export_archive.add_variable_collection(
    name="optimizer_variables", variables=model.optimizer.variables)
export_archive.write_out("path/to/location")

revived_object = tf.saved_model.load("path/to/location")
optimizer_variables = revived_object.optimizer_variables
""" .

"DESCRIPTION.The code extends a deque (double-ended queue) object with the elements [3, 4]." <EXPLAINS> "CODE.pdeque([1, 2]).extend([3, 4])" .

"DESCRIPTION.The code extracts Mel Spectrogram and Log Amplitude features from an audio file using librosa library and stores them in a pipeline using sklearn." <EXPLAINS> """CODE.import sklearn.pipeline
MS = librosa.util.FeatureExtractor(librosa.feature.melspectrogram,
                                   sr=22050, n_fft=2048,
                                   n_mels=128, fmax=8000)
LA = librosa.util.FeatureExtractor(librosa.logamplitude,
                                   ref_power=np.max)
Features = sklearn.pipeline.Pipeline([('MelSpectrogram', MS),
                                      ('LogAmplitude', LA)])
y, sr = librosa.load(librosa.util.example_audio_file())
F = Features.transform([y])""" .

"DESCRIPTION.The code extracts a real number in the format of 'x.y' from a given string input." <EXPLAINS> """CODE.real = Word(nums) + '.' + Word(nums)
print(real.parseString('3.1416'))
real = Combine(Word(nums) + '.' + Word(nums))
print(real.parseString('3.1416'))""" .

"DESCRIPTION.The code extracts an archive file from a specified path to a specified destination path." <EXPLAINS> "CODE.torchaudio.datasets.utils.extract_archive(from_path, to_path)" .

"DESCRIPTION.The code extracts and parses a date in the format of 'year/month/day' from a given input string." <EXPLAINS> """CODE.integer = Word(nums)
date_str = integer("year") + '/' + integer("month") + '/' + integer("day")

result = date_str.parseString('12/31/1999')
print(result.dump())""" .

"DESCRIPTION.The code extracts and returns the query parameters from the URL as a dictionary." <EXPLAINS> """CODE.st.experimental_get_query_params()
{"show_map": ["True"], "selected": ["asia", "america"]}""" .

"DESCRIPTION.The code extracts chroma features from two different audio signals using Short-Time Fourier Transform (STFT) and displays them as a chromagram." <EXPLAINS> """CODE.y, sr = librosa.load(librosa.util.example_audio_file())
librosa.feature.chroma_stft(y=y, sr=sr)
array([[ 0.974,  0.881, ...,  0.925,  1.   ],
       [ 1.   ,  0.841, ...,  0.882,  0.878],
       ...,
       [ 0.658,  0.985, ...,  0.878,  0.764],
       [ 0.969,  0.92 , ...,  0.974,  0.915]])

S = np.abs(librosa.stft(y, n_fft=4096))
chroma = librosa.feature.chroma_stft(S=S, sr=sr)
chroma
array([[ 0.685,  0.477, ...,  0.961,  0.986],
       [ 0.674,  0.452, ...,  0.952,  0.926],
       ...,
       [ 0.844,  0.575, ...,  0.934,  0.869],
       [ 0.793,  0.663, ...,  0.964,  0.972]])

import matplotlib.pyplot as plt
librosa.display.specshow(chroma, y_axis='chroma', x_axis='time')
plt.colorbar()
plt.title('Chromagram')
plt.tight_layout()""" .

"DESCRIPTION.The code extracts entities along with their patch indices from a given text." <EXPLAINS> """CODE.text = "<grounding> An image of<phrase> a snowman</phrase><object><patch_index_0044><patch_index_0863></object> warming himself by<phrase> a fire</phrase><object><patch_index_0005><patch_index_0911></object>."
entities = extract_entities_with_patch_indices(text)
entities
[(' a snowman', (31, 41), [(44, 863)]), (' a fire', (130, 137), [(5, 911)])]
""" .

"DESCRIPTION.The code extracts text features using a pretrained CLAP model for the given input text data of a cat and a dog." <EXPLAINS> """CODE.from transformers import AutoTokenizer, ClapModel
model = ClapModel.from_pretrained("laion/clap-htsat-unfused")
tokenizer = AutoTokenizer.from_pretrained("laion/clap-htsat-unfused")
inputs = tokenizer(["the sound of a cat", "the sound of a dog"], padding=True, return_tensors="pt")
text_features = model.get_text_features(**inputs)""" .

"DESCRIPTION.The code extracts text parts and floats from a given string, returning the text parts as a list and the floats as a numpy array." <EXPLAINS> """CODE.text_parts, floats = _FloatExtractor()("Text 1.0 Text")
text_parts
["Text ", " Text"]
floats
np.array([1.0])""" .

"DESCRIPTION.The code extracts the base name from a file path that includes a zip file protocol and an HTTP URL." <EXPLAINS> "CODE.xbasename(\"zip://folder1/file.txt::https://host.com/archive.zip\")" .

"DESCRIPTION.The code extracts the epoch number from the Version string." <EXPLAINS> """CODE.Version("2.0.0").epoch
Version("1!2.0.0").epoch""" .

"DESCRIPTION.The code extracts the text parts and floats (numbers with decimals) from the input text \"Text 1.0 Text\". The text parts are stored in a list while the floats are stored in an array." <EXPLAINS> """CODE.text_parts, floats = _FloatExtractor()("Text 1.0 Text")
text_parts
['Text ', ' Text']
floats
array([1.])""" .

"DESCRIPTION.The code features a client using the Hugging Face Hub to perform feature extraction on the text \"Hi, who are you?\"." <EXPLAINS> """CODE.from huggingface_hub import InferenceClient
client = InferenceClient()
client.feature_extraction("Hi, who are you?")
array([[ 2.424802  ,  2.93384   ,  1.1750331 , ...,  1.240499, -0.13776633, -0.7889173 ],
[-0.42943227, -0.6364878 , -1.693462  , ...,  0.41978157, -2.4336355 ,  0.6162071 ],
...,
[ 0.28552425, -0.928395  , -1.2077185 , ...,  0.76810825, -2.1069427 ,  0.6236161 ]], dtype=float32)
""" .

"DESCRIPTION.The code fetches data from a database table and executes SQL queries on the table using a Selectable object." <EXPLAINS> """CODE.from datasets import Dataset

# Fetch a database table
ds = Dataset.from_sql("test_data", "postgres:///db_name")
# Execute a SQL query on the table
ds = Dataset.from_sql("SELECT sentence FROM test_data", "postgres:///db_name")
# Use a Selectable object to specify the query
from sqlalchemy import select, text
stmt = select([text("sentence")]).select_from(text("test_data"))
ds = Dataset.from_sql(stmt, "postgres:///db_name")
""" .

"DESCRIPTION.The code fetches image data, labels, and bounding boxes from a Bigtable named \"my_table\" with keys prefixed by \"imagenet\". It then processes the image data by parsing and cropping it. Finally, the processed data is batched into training samples." <EXPLAINS> """CODE.table = bigtable_client.table("my_table")
key_dataset = table.get_keys_prefix("imagenet")
images = key_dataset.apply(table.lookup_columns(("cf1", "image"),
                                                ("cf2", "label"),
                                                ("cf2", "boundingbox")))
training_data = images.map(parse_and_crop, num_parallel_calls=64).batch(128)

table = bigtable_client.table("my_table")
key_dataset = table.get_keys_prefix("imagenet")
images = key_dataset.apply(table.lookup_columns(
    cf1="image", cf2=("label", "boundingbox")))
training_data = images.map(parse_and_crop, num_parallel_calls=64).batch(128)""" .

"DESCRIPTION.The code fetches options data for the stock AAPL from Yahoo Finance, including call and put data for the next expiry, call data within 3 strikes below and above the stock price, call and put data with expiry from now to 8 months out, and all call and put data available." <EXPLAINS> """CODE.# Instantiate object with ticker
aapl = Options('aapl', 'yahoo')

# Fetch next expiry call data
calls = aapl.get_call_data()

# Fetch next expiry put data
puts = aapl.get_put_data()

# cut down the call data to be 3 below and 3 above the stock price.
cut_calls = aapl.get_near_stock_price(call=True, above_below=3)

# Fetch call and put data with expiry from now to 8 months out
forward_data = aapl.get_forward_data(8, call=True, put=True)

# Fetch all call and put data
all_data = aapl.get_all_data()
""" .

"DESCRIPTION.The code fills any null values in the array \"arr\" with the scalar value of 5." <EXPLAINS> """CODE.arr = pa.array([1, 2, None, 3], type=pa.int8())
fill_value = pa.scalar(5, type=pa.int8())
arr.fill_null(fill_value)""" .

"DESCRIPTION.The code filters a TensorFlow dataset to keep only elements less than 3, then further filters the dataset to keep only elements equal to 1." <EXPLAINS> """CODE.d = tf.data.Dataset.from_tensor_slices([1, 2, 3])

d = d.filter(lambda x: x < 3) # [1, 2]

# `tf.math.equal(x, y)` is required for equality comparison
def filter_fn(x):
  return tf.math.equal(x, 1)

d = d.filter(filter_fn) # [1]
""" .

"DESCRIPTION.The code filters a list of version numbers based on a specified version specifier. It returns only the version numbers that meet the criteria defined by the specifier, including handling of prerelease versions if specified." <EXPLAINS> """CODE.list(Specifier(">=1.2.3").filter(["1.2", "1.3", "1.5a1"]))
['1.3']
list(Specifier(">=1.2.3").filter(["1.2", "1.2.3", "1.3", Version("1.4")]))
['1.2.3', '1.3', <Version('1.4')>]
list(Specifier(">=1.2.3").filter(["1.2", "1.5a1"]))
['1.5a1']
list(Specifier(">=1.2.3").filter(["1.3", "1.5a1"], prereleases=True))
['1.3', '1.5a1']
list(Specifier(">=1.2.3", prereleases=True).filter(["1.3", "1.5a1"]))
['1.3', '1.5a1']
""" .

"DESCRIPTION.The code filters the devices in the TensorFlow cluster configuration based on the worker and parameter server roles specified in the code, and connects to the cluster using the updated device filters." <EXPLAINS> """CODE.cdf = tf.config.experimental.ClusterDeviceFilters()
for i in range(num_workers):
  cdf.set_device_filters('worker', i, ['/job:ps'])
for i in range(num_ps):
  cdf.set_device_filters('ps', i, ['/job:worker'])

tf.config.experimental_connect_to_cluster(cluster_def,
                                          cluster_device_filters=cdf)
""" .

"DESCRIPTION.The code finds the local minima in a given numpy array along the specified axis." <EXPLAINS> """CODE.x = np.array([1, 0, 1, 2, -1, 0, -2, 1])
librosa.util.localmin(x)
array([False,  True, False, False,  True, False,  True, False])

x = np.array([[1,0,1], [2, -1, 0], [2, 1, 3]])
librosa.util.localmin(x, axis=0)
array([[False, False, False],
       [False,  True,  True],
       [False, False, False]])

librosa.util.localmin(x, axis=1)
array([[False,  True, False],
       [False,  True, False],
       [False,  True, False]])
""" .

"DESCRIPTION.The code finds the minimum value of the elements in the index object." <EXPLAINS> """CODE.idx = pd.Index([3, 2, 1])
idx.min()

idx = pd.Index(['c', 'b', 'a'])
idx.min()

idx = pd.MultiIndex.from_product([('a', 'b'), (2, 1)])
idx.min()""" .

"DESCRIPTION.The code fine-tunes a BERT model and then pushes it to a Hugging Face repository with various specified configurations." <EXPLAINS> """CODE.from transformers import AutoModel

model = AutoModel.from_pretrained("bert-base-cased")

model.push_to_hub("my-finetuned-bert")

model.push_to_hub("my-finetuned-bert", use_temp_dir=True)

model.push_to_hub("my-finetuned-bert", organization="huggingface")

model.push_to_hub("my-finetuned-bert", repo_url="https://huggingface.co/sgugger/my-finetuned-bert")
""" .

"DESCRIPTION.The code first calls the help function on a pandas DataFrame object to retrieve information about its methods and attributes. It then calls a poorly documented function and uses the help function again to retrieve information about this function." <EXPLAINS> """CODE.st.help(pandas.DataFrame)
x = my_poorly_documented_function()
st.help(x)""" .

"DESCRIPTION.The code fits a Linear Discriminant Analysis model to the data specified by X and y, and then predicts the class label for a new data point [-0.8, -1]." <EXPLAINS> """CODE.clf = LinearDiscriminantAnalysis()
clf.fit(X, y)
LinearDiscriminantAnalysis(n_components=None, priors=None, shrinkage=None,
              solver='svd', store_covariance=False, tol=0.0001)
print(clf.predict([[-0.8, -1]]))""" .

"DESCRIPTION.The code fits a Voting Regressor model using a Linear Regression and a Random Forest Regressor with specified parameters on the given dataset X (features) and y (target values), and then predicts the target values for the same dataset X." <EXPLAINS> """CODE.import numpy as np
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor
from sklearn.ensemble import VotingRegressor

r1 = LinearRegression()
r2 = RandomForestRegressor(n_estimators=10, random_state=1)
X = np.array([[1, 1], [2, 4], [3, 9], [4, 16], [5, 25], [6, 36]])
y = np.array([2, 6, 12, 20, 30, 42])
er = VotingRegressor([('lr', r1), ('rf', r2)])
print(er.fit(X, y).predict(X))""" .

"DESCRIPTION.The code fits a multi-task Lasso regression model to the input data and target values, then prints the coefficients and intercept of the model." <EXPLAINS> """CODE.from sklearn import linear_model
clf = linear_model.MultiTaskLasso(alpha=0.1)
clf.fit([[0,0], [1, 1], [2, 2]], [[0, 0], [1, 1], [2, 2]])
print(clf.coef_)
print(clf.intercept_)
""" .

"DESCRIPTION.The code flattens a batch of 4D tensors into a 2D tensor by reshaping it, resulting in a tensor with shape (2, 60)." <EXPLAINS> """CODE.x_batch = tf.keras.backend.ones(shape=(2, 3, 4, 5))
x_batch_flatten = batch_flatten(x_batch)
tf.keras.backend.int_shape(x_batch_flatten)
(2, 60)""" .

"DESCRIPTION.The code flattens a multi-dimensional NumPy array into a 2D tensor starting from axis 1 and stopping at axis 2." <EXPLAINS> """CODE.import paddle
import numpy as np

inp_np = np.ones([5, 2, 3, 4]).astype('float32')
inp_np = paddle.to_tensor(inp_np)
flatten = paddle.nn.Flatten(start_axis=1, stop_axis=2)
flatten_res = flatten(inp_np)""" .

"DESCRIPTION.The code flattens a tensor 'b' into a one-dimensional array." <EXPLAINS> "CODE.tf.keras.backend.flatten(b)" .

"DESCRIPTION.The code flattens the input tensor `b` into a 1D tensor." <EXPLAINS> "CODE.tf.keras.backend.flatten(b)" .

"DESCRIPTION.The code formats the DataFrame 'df' by displaying the values in percentage format with two decimal places and capitalizing the values in the 'C' column." <EXPLAINS> """CODE.df.style.format("{:.2%}")
df.style.format({'C': str.upper})""" .

"DESCRIPTION.The code forward_chunk function processes hidden states using the decoder model, and the forward function applies chunking to the forward_chunk function based on specified chunk size and sequence length dimension parameters." <EXPLAINS> """CODE.def forward_chunk(self, hidden_states):
    hidden_states = self.decoder(hidden_states)
    return hidden_states

def forward(self, hidden_states):
    return apply_chunking_to_forward(self.chunk_size_lm_head, self.seq_len_dim, self.forward_chunk, hidden_states)""" .

"DESCRIPTION.The code freezes the input data structures (sets, lists, tuples) to prevent any modification or mutation of their elements." <EXPLAINS> """CODE.freeze(set([1, 2]))
freeze([1, {'a': 3}])
freeze((1, []))""" .

"DESCRIPTION.The code generate a Series of numbers in ascending order and uses the searchsorted function to find the indices where specified values can be inserted to maintain the order. It also demonstrates different options for the side and sorter parameters of the searchsorted function." <EXPLAINS> """CODE.x = pd.Series([1, 2, 3])
x
0    1
1    2
2    3
dtype: int64
x.searchsorted(4)
array([3])
x.searchsorted([0, 4])
array([0, 3])
x.searchsorted([1, 3], side='left')
array([0, 2])
x.searchsorted([1, 3], side='right')
array([1, 3])
x.searchsorted([1, 2], side='right', sorter=[0, 2, 1])
array([1, 3])""" .

"DESCRIPTION.The code generates 3 new random seeds based on the input seed list [1, 2], and then uses the first new seed to generate a random normal distribution with a shape of [3]." <EXPLAINS> """CODE.seed = [1, 2]
new_seeds = tf.random.experimental.stateless_split(seed, num=3)
print(new_seeds)
tf.random.stateless_normal(shape=[3], seed=new_seeds[0, :])
""" .

"DESCRIPTION.The code generates 3x3 and 2x3 identity matrices using the Keras backend." <EXPLAINS> """CODE.from keras import backend as K
K.eval(K.eye(3))
K.eval(K.eye((2, 3)))
""" .

"DESCRIPTION.The code generates DatasetCard objects for datasets with specified metadata such as language, license, annotations creators, task categories, task ids, multilinguality, and pretty name. It can use both default and custom templates to create DatasetCard objects." <EXPLAINS> """CODE.from huggingface_hub import DatasetCard, DatasetCardData

# Using the Default Template
card_data = DatasetCardData(
    language='en',
    license='mit',
    annotations_creators='crowdsourced',
    task_categories=['text-classification'],
    task_ids=['sentiment-classification', 'text-scoring'],
    multilinguality='monolingual',
    pretty_name='My Text Classification Dataset',
)
card = DatasetCard.from_template(
    card_data,
    pretty_name=card_data.pretty_name,
)

# Using a Custom Template
card_data = DatasetCardData(
    language='en',
    license='mit',
)
card = DatasetCard.from_template(
    card_data=card_data,
    template_path='./src/huggingface_hub/templates/datasetcard_template.md',
    custom_template_var='custom value',  # will be replaced in template if it exists
)
""" .

"DESCRIPTION.The code generates RGB values from the given data using different bit lengths for each color channel." <EXPLAINS> """CODE.data = struct.pack('BBBB', 0x21, 0x08, 0xff, 0xff)
print(unpack_rgb(data, '<B', (5, 6, 5), False))
print(unpack_rgb(data, '<B', (5, 6, 5)))
print(unpack_rgb(data, '<B', (5, 5, 5)))
""",
        """CODE.data = struct.pack('BBBB', 0x21, 0x08, 0xff, 0xff)
print(unpackrgb(data, '<B', (5, 6, 5), False))
print(unpackrgb(data, '<B', (5, 6, 5)))
print(unpackrgb(data, '<B', (5, 5, 5)))
""" .

"DESCRIPTION.The code generates a 2D array of time values corresponding to the spectrogram computed from an audio file." <EXPLAINS> """CODE.y, sr = librosa.load(librosa.util.example_audio_file())
times, S = librosa.core.spectrum.__reassign_times(y, sr=sr)
times
array([[ 0.077,  0.079,  ..., 61.362, 61.388],
       [ 0.078,  0.077,  ..., 61.366, 61.538],
       [ 0.088,  0.08 ,  ..., 61.358, 61.399],
       ...,
       [ 0.078,  0.077,  ..., 61.378, 61.372],
       [ 0.082,  0.077,  ..., 61.371, 61.38 ],
       [ 0.075,  0.076,  ..., 61.374, 61.385]])
""" .

"DESCRIPTION.The code generates a 2x3 matrix of random numbers sampled from a normal distribution with mean 0 and standard deviation 1." <EXPLAINS> """CODE.kvar = K.random_normal_variable((2,3), 0, 1)
kvar
K.eval(kvar)
""" .

"DESCRIPTION.The code generates a 2x3 tensor with random values sampled from a normal distribution with mean 0.0 and standard deviation 1.0." <EXPLAINS> """CODE.random_normal_tensor = tf.keras.backend.random_normal(shape=(2,3),
mean=0.0, stddev=1.0)""" .

"DESCRIPTION.The code generates a 3x3 identity matrix using TensorFlow's backend and evaluates it." <EXPLAINS> """CODE.kvar = tf.keras.backend.eye(3)
tf.keras.backend.eval(kvar)
array([[1.,  0.,  0.],
       [0.,  1.,  0.],
       [0.,  0.,  1.]], dtype=float32)""" .

"DESCRIPTION.The code generates a Bernoulli distribution with a probability of success as 0.3 and calculates the log probability of the value 1.0 in the distribution." <EXPLAINS> """CODE.import paddle
from paddle.distribution import Bernoulli

rv = Bernoulli(0.3)
print(rv.log_prob(paddle.to_tensor([1.0]))""" .

"DESCRIPTION.The code generates a Bernoulli distribution with a success probability of 0.3 and calculates the cumulative distribution function (CDF) value for the input tensor [1.0]. The CDF represents the probability that a random variable in the distribution is less than or equal to the given input value." <EXPLAINS> """CODE.import paddle
from paddle.distribution import Bernoulli

rv = Bernoulli(0.3)
print(rv.cdf(paddle.to_tensor([1.0])))""" .

"DESCRIPTION.The code generates a DataFrame with 10 rows and 2 columns filled with random numbers, then filters the DataFrame rows where the values in column 'a' are greater than the values in column 'b'." <EXPLAINS> """CODE.from numpy.random import randn
from pandas import DataFrame
df = DataFrame(randn(10, 2), columns=list('ab'))
df.query('a > b')
df[df.a > df.b]  # same result as the previous expression
""" .

"DESCRIPTION.The code generates a DataFrame with 10 rows and 5 columns filled with random numbers, and then displays the DataFrame in a table format." <EXPLAINS> """CODE.df = pd.DataFrame(
...    np.random.randn(10, 5),
...    columns=('col %d' % i for i in range(5)))
...
st.table(df)""" .

"DESCRIPTION.The code generates a DataFrame with 20 rows and 3 columns filled with random data, then plots a line chart using the data in the DataFrame." <EXPLAINS> """CODE.chart_data = pd.DataFrame(
    np.random.randn(20, 3),
    columns=['a', 'b', 'c'])
st.line_chart(chart_data)
""" .

"DESCRIPTION.The code generates a DataFrame with 20 rows and 3 columns filled with random numbers, labeled as 'a', 'b', and 'c'. It then displays an area chart based on the data in the DataFrame." <EXPLAINS> """CODE.chart_data = pd.DataFrame(
...     np.random.randn(20, 3),
...     columns=['a', 'b', 'c'])
...
st.area_chart(chart_data)""" .

"DESCRIPTION.The code generates a DataFrame with a shape of 25 rows and 4 columns filled with randomly generated data, and then creates a box plot based on the data in the DataFrame." <EXPLAINS> """CODE.data = np.random.randn(25, 4)
df = pd.DataFrame(data, columns=list('ABCD'))
ax = df.plot.box()
""" .

"DESCRIPTION.The code generates a DataFrame with columns for different commands, their ratings, and if they are widgets. It then allows the user to edit the DataFrame interactively using a data editor function provided by the streamlit library. The code then identifies the row with the highest rating and displays the corresponding command as the user's favorite command." <EXPLAINS> """CODE.import streamlit as st
import pandas as pd

df = pd.DataFrame(
    [
       {"command": "st.selectbox", "rating": 4, "is_widget": True},
       {"command": "st.balloons", "rating": 5, "is_widget": False},
       {"command": "st.time_input", "rating": 3, "is_widget": True},
   ]
)
edited_df = st.experimental_data_editor(df)

favorite_command = edited_df.loc[edited_df["rating"].idxmax()]["command"]
st.markdown(f"Your favorite command is **{favorite_command}** ð")

You can also allow the user to add and delete rows by setting `num_rows` to "dynamic":

import streamlit as st
import pandas as pd

df = pd.DataFrame(
    [
       {"command": "st.selectbox", "rating": 4, "is_widget": True},
       {"command": "st.balloons", "rating": 5, "is_widget": False},
       {"command": "st.time_input", "rating": 3, "is_widget": True},
   ]
)
edited_df = st.experimental_data_editor(df, num_rows="dynamic")

favorite_command = edited_df.loc[edited_df["rating"].idxmax()]["command"]
st.markdown(f"Your favorite command is **{favorite_command}** ð")""" .

"DESCRIPTION.The code generates a DataFrame with random values and displays it using the Streamlit library. It also allows for customization of the DataFrame's style using a Pandas Styler object." <EXPLAINS> """CODE.import streamlit as st
import pandas as pd
import numpy as np

df = pd.DataFrame(
...    np.random.randn(50, 20),
...    columns=('col %d' % i for i in range(20)))
...
st._arrow_dataframe(df)

st._arrow_dataframe(df, 200, 100)

You can also pass a Pandas Styler object to change the style of
the rendered DataFrame:

df = pd.DataFrame(
...    np.random.randn(10, 20),
...    columns=('col %d' % i for i in range(20)))
...
st._arrow_dataframe(df.style.highlight_max(axis=0))""",
        """CODE.import streamlit as st
import pandas as pd
import numpy as np

df = pd.DataFrame(np.random.randn(50, 20), columns=("col %d" % i for i in range(20)))

st.dataframe(df)  # Same as st.write(df)

df = pd.DataFrame(np.random.randn(10, 20), columns=("col %d" % i for i in range(20)))

st.dataframe(df.style.highlight_max(axis=0))

df = pd.DataFrame(
    {
        "name": ["Roadmap", "Extras", "Issues"],
        "url": ["https://roadmap.streamlit.app", "https://extras.streamlit.app", "https://issues.streamlit.app"],
        "stars": [random.randint(0, 1000) for _ in range(3)],
        "views_history": [[random.randint(0, 5000) for _ in range(30)] for _ in range(3)],
    }
)
st.dataframe(
    df,
    column_config={
        "name": "App name",
        "stars": st.column_config.NumberColumn(
            "Github Stars",
            help="Number of stars on GitHub",
            format="%d â­",
        ),
        "url": st.column_config.LinkColumn("App URL"),
        "views_history": st.column_config.LineChartColumn(
            "Views (past 30 days)", y_min=0, y_max=5000
        ),
    },
    hide_index=True,
)""" .

"DESCRIPTION.The code generates a DatetimeIndex object containing dates from January 2018 to March 2018, and then converts the month names of the dates in the index to a new Index object." <EXPLAINS> """CODE.idx = pd.date_range(start='2018-01', freq='M', periods=3)
idx
DatetimeIndex(['2018-01-31', '2018-02-28', '2018-03-31'],
              dtype='datetime64[ns]', freq='M')
idx.month_name()
Index(['January', 'February', 'March'], dtype='object')""" .

"DESCRIPTION.The code generates a DatetimeIndex with hourly frequency starting from '2014-08-01 10:00' in the timezone 'Asia/Calcutta', for a period of 3 hours. It then normalizes the DatetimeIndex to midnight of the same day in the timezone 'Asia/Calcutta'." <EXPLAINS> """CODE.idx = pd.date_range(start='2014-08-01 10:00', freq='H',
...                     periods=3, tz='Asia/Calcutta')
idx
DatetimeIndex(['2014-08-01 10:00:00+05:30',
               '2014-08-01 11:00:00+05:30',
               '2014-08-01 12:00:00+05:30'],
                dtype='datetime64[ns, Asia/Calcutta]', freq='H')
idx.normalize()
DatetimeIndex(['2014-08-01 00:00:00+05:30',
               '2014-08-01 00:00:00+05:30',
               '2014-08-01 00:00:00+05:30'],
               dtype='datetime64[ns, Asia/Calcutta]', freq=None)""" .

"DESCRIPTION.The code generates a JSON string representing an object with keys \"shape\" and \"axes\" and values [256, 256, 3] and \"YXS\" respectively." <EXPLAINS> """CODE.json_description((256, 256, 3), axes='YXS')  # doctest: +SKIP
b'{"shape": [256, 256, 3], "axes": "YXS"}'""" .

"DESCRIPTION.The code generates a MultiIndex object that represents combinations of years and panels. The first block of code uses the range function to create a range of years from 1960 to 1962 and creates a list of panels ['A', 'B', 'C']. Then it calls a function panel_index with the years and panels as arguments to create the MultiIndex object. The second block of code achieves the same result using NumPy to repeat and tile the years and panels arrays before calling the panel_index function." <EXPLAINS> """CODE.years = range(1960,1963)
panels = ['A', 'B', 'C']
panel_idx = panel_index(years, panels)
panel_idx
MultiIndex([(1960, 'A'), (1961, 'A'), (1962, 'A'), (1960, 'B'), (1961, 'B'),
   (1962, 'B'), (1960, 'C'), (1961, 'C'), (1962, 'C')], dtype=object)

import numpy as np
years = np.repeat(range(1960,1963), 3)
panels = np.tile(['A', 'B', 'C'], 3)
panel_idx = panel_index(years, panels)
panel_idx
MultiIndex([(1960, 'A'), (1960, 'B'), (1960, 'C'), (1961, 'A'), (1961, 'B'),
   (1961, 'C'), (1962, 'A'), (1962, 'B'), (1962, 'C')], dtype=object)""" .

"DESCRIPTION.The code generates a RelaxedOneHotCategorical distribution with specified temperature and either probabilities or logits values." <EXPLAINS> """CODE.temperature = 0.5
p = [0.1, 0.5, 0.4]
dist = RelaxedOneHotCategorical(temperature, probs=p)


temperature = 0.5
logits = [-2, 2, 0]
dist = RelaxedOneHotCategorical(temperature, logits=logits)


temperature = 1e-5
logits = [-2, 2, 0]
dist = RelaxedOneHotCategorical(temperature, logits=logits)


temperature = 10
logits = [-2, 2, 0]
dist = RelaxedOneHotCategorical(temperature, logits=logits)
""" .

"DESCRIPTION.The code generates a Vandermonde matrix using numpy, and then checks for uniqueness along the rows and columns of the matrix." <EXPLAINS> """CODE.x = np.vander(np.arange(5))
x
array([[  0,   0,   0,   0,   1],
   [  1,   1,   1,   1,   1],
   [ 16,   8,   4,   2,   1],
   [ 81,  27,   9,   3,   1],
   [256,  64,  16,   4,   1]])
# Check uniqueness along rows
librosa.util.is_unique(x, axis=0)
array([ True,  True,  True,  True, False])
# Check uniqueness along columns
librosa.util.is_unique(x, axis=-1)
array([False, False,  True,  True,  True])""" .

"DESCRIPTION.The code generates a Vega-Lite chart using a pandas DataFrame with 200 rows and 3 columns filled with random numbers. The chart is a scatter plot where 'a' values are plotted on the x-axis, 'b' values are plotted on the y-axis, and the size and color of the circles are determined by the 'c' column values." <EXPLAINS> """CODE.import pandas as pd
import numpy as np

df = pd.DataFrame(
    np.random.randn(200, 3),
    columns=['a', 'b', 'c'])

st.vega_lite_chart(df, {
    'mark': {'type': 'circle', 'tooltip': True},
    'encoding': {
        'x': {'field': 'a', 'type': 'quantitative'},
        'y': {'field': 'b', 'type': 'quantitative'},
        'size': {'field': 'c', 'type': 'quantitative'},
        'color': {'field': 'c', 'type': 'quantitative'},
    },
})""" .

"DESCRIPTION.The code generates a bar chart using the data provided by the variable chart_data." <EXPLAINS> "CODE.st.bar_chart(chart_data)" .

"DESCRIPTION.The code generates a batch of random normal inputs with shape (32, 10), applies the softmax function from the tf.keras.activations module to the inputs, calculates the sum of the softmax values for the first sample in the batch (which should be close to 1), and creates a Dense layer with 32 units and softmax activation function." <EXPLAINS> """CODE.inputs = tf.random.normal(shape=(32, 10))
outputs = tf.keras.activations.softmax(inputs)
tf.reduce_sum(outputs[0, :])  # Each sample in the batch now sums to 1
<tf.Tensor: shape=(), dtype=float32, numpy=1.0000001>

layer = tf.keras.layers.Dense(32, activation=tf.keras.activations.softmax)
""" .

"DESCRIPTION.The code generates a binary array indicating missing values in the input dataset X2 by comparing it with X1." <EXPLAINS> """CODE.indicator = MissingIndicator()
indicator.fit(X1)
X2_tr = indicator.transform(X2)
X2_tr
array([[False,  True],
       [ True, False],
       [False, False]])""" .

"DESCRIPTION.The code generates a binary classification dataset, splits it into training and testing sets, trains a logistic regression model on the training data, makes predictions on the testing data, and displays a calibration plot showing the relationship between predicted probabilities and true outcomes." <EXPLAINS> """CODE.import matplotlib.pyplot as plt
from sklearn.datasets import make_classification
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.calibration import CalibrationDisplay

X, y = make_classification(random_state=0)
X_train, X_test, y_train, y_test = train_test_split(
    X, y, random_state=0)
clf = LogisticRegression(random_state=0)
clf.fit(X_train, y_train)
LogisticRegression(random_state=0)
y_prob = clf.predict_proba(X_test)[:, 1]
disp = CalibrationDisplay.from_predictions(y_test, y_prob)
plt.show()""" .

"DESCRIPTION.The code generates a boolean mask with length 3, where the values at indices 1 and 2 are set to True in the first sublist, and the values at indices 1 and 2 are set to True in the second sublist." <EXPLAINS> """CODE.
sequence_mask([1, 2], 3)
[[True, False, False],
 [True, True, False]]
""" .

"DESCRIPTION.The code generates a calibration plot for evaluating the calibration of a classifier's predicted probabilities against the true probabilities. This plot helps to assess how reliable the predicted probabilities are." <EXPLAINS> """CODE.prob_true, prob_pred = calibration_curve(y_test, y_prob, n_bins=10)
disp = CalibrationDisplay(prob_true, prob_pred, y_prob)
disp.plot()
""" .

"DESCRIPTION.The code generates a chromagram from a given waveform signal using a ChromaSpectrogram transformation with a specified sample rate and n_fft value." <EXPLAINS> """CODE.transform = transforms.ChromaSpectrogram(sample_rate=sample_rate, n_fft=400)
chromagram = transform(waveform)  # (channel, n_chroma, time)
""" .

"DESCRIPTION.The code generates a colormap by stacking the input image three times, converting it to type 'uint16', multiplying by 256, and then applying the colormap to the input image, resulting in an array with three elements of type 'uint16'." <EXPLAINS> """CODE.image = numpy.arange(256, dtype='uint8')
colormap = numpy.vstack([image, image, image]).astype('uint16') * 256
apply_colormap(image, colormap)[-1]
array([65280, 65280, 65280], dtype=uint16)
""" .

"DESCRIPTION.The code generates a custom distribution plot with three groups of data (Group 1, Group 2, Group 3), each represented by a histogram. The bins sizes for the histograms are set to [.1, .25, .5]. The plot is then displayed using Streamlit." <EXPLAINS> """CODE.import streamlit as st
import plotly.figure_factory as ff
import numpy as np

# Add histogram data
x1 = np.random.randn(200) - 2
x2 = np.random.randn(200)
x3 = np.random.randn(200) + 2

# Group data together
hist_data = [x1, x2, x3]

group_labels = ['Group 1', 'Group 2', 'Group 3']

# Create distplot with custom bin_size
fig = ff.create_distplot(
        hist_data, group_labels, bin_size=[.1, .25, .5])

# Plot!
st.plotly_chart(fig, use_container_width=True)""" .

"DESCRIPTION.The code generates a customized distribution plot using Plotly Figure Factory in a Streamlit application. It randomly generates three sets of data, groups them together, and creates a distribution plot with custom bin sizes for each group label. Finally, it displays the plot in the Streamlit application." <EXPLAINS> """CODE.import streamlit as st
import plotly.figure_factory as ff
import numpy as np

# Add histogram data
x1 = np.random.randn(200) - 2
x2 = np.random.randn(200)
x3 = np.random.randn(200) + 2

# Group data together
hist_data = [x1, x2, x3]

group_labels = ['Group 1', 'Group 2', 'Group 3']

# Create distplot with custom bin_size
fig = ff.create_distplot(
        hist_data, group_labels, bin_size=[.1, .25, .5])

# Plot!
st.plotly_chart(fig, use_container_width=True)""" .

"DESCRIPTION.The code generates a data set with two columns, \"A\" and \"B\", where \"A\" is the remainder of x divided by 3, and \"B\" is the value of x. It then groups the data by the \"A\" column and counts the number of occurrences of each group." <EXPLAINS> """CODE.import ray
ray.data.from_items([
    {"A": x % 3, "B": x} for x in range(100)]).groupby(
    "A").count()""" .

"DESCRIPTION.The code generates a dataset with 1000 elements and then maps each element to a new dictionary with a key \"v2\" whose value is twice the original element's value. Finally, it displays the resulting dataset." <EXPLAINS> """CODE.ds = ray.data.range_arrow(1000)
ds.map(lambda r: {"v2": r["value"] * 2}).show()
""",
        """CODE.import ray
ds = ray.data.range_table(1000)
ds.map(lambda r: {"v2": r["value"] * 2}).show()
""" .

"DESCRIPTION.The code generates a dataset with 1000 elements, each element being a 3x10 tensor. Then it squares each element in the dataset and displays the resulting dataset." <EXPLAINS> """CODE.ds = ray.data.range_tensor(1000, shape=(3, 10))
ds.map_batches(lambda arr: arr ** 2).show()
""" .

"DESCRIPTION.The code generates a dataset with two examples, where each example consists of a text and a corresponding label. The first example has the text \"Good\" and a label of 0, while the second example has the text \"Bad\" and a label of 1." <EXPLAINS> """CODE.def gen():
    yield {"text": "Good", "label": 0}
    yield {"text": "Bad", "label": 1}

ds = Dataset.from_generator(gen)""" .

"DESCRIPTION.The code generates a file pattern, creates file paths based on the pattern, and then checks if the most recently modified file that matches the pattern is the same as the last file path in the list." <EXPLAINS> """CODE.file_pattern = 'f.batch{batch:02d}epoch{epoch:02d}.h5'
test_dir = self.get_temp_dir()
path_pattern = os.path.join(test_dir, file_pattern)
file_paths = [
    os.path.join(test_dir, file_name) for file_name in
    ['f.batch03epoch02.h5', 'f.batch02epoch02.h5', 'f.batch01epoch01.h5']
]
for file_path in file_paths:
  # Write something to each of the files
self.assertEqual(
    _get_most_recently_modified_file_matching_pattern(path_pattern),
    file_paths[-1])
""" .

"DESCRIPTION.The code generates a function that zips together multiple sets of elements into tuples based on the corresponding index positions. It can zip two sets, three sets, or combinations of sets containing integers and tuples." <EXPLAINS> """CODE.a = { 1, 2, 3 }
b = { 4, 5, 6 }
c = { (7, 8), (9, 10), (11, 12) }
d = { 13, 14 }

Dataset.zip((a, b)) == { (1, 4), (2, 5), (3, 6) }
Dataset.zip((b, a)) == { (4, 1), (5, 2), (6, 3) }

Dataset.zip((a, b, c)) == { (1, 4, (7, 8)),
                            (2, 5, (9, 10)),
                            (3, 6, (11, 12)) }

Dataset.zip((a, d)) == { (1, 13), (2, 14) }
""" .

"DESCRIPTION.The code generates a histogram plot of 100 random numbers sampled from a normal distribution with mean 1 and standard deviation 1, using 20 bins." <EXPLAINS> """CODE.import matplotlib.pyplot as plt
import numpy as np

arr = np.random.normal(1, 1, size=100)
plt.hist(arr, bins=20)

st.pyplot()
""" .

"DESCRIPTION.The code generates a legacy area chart using the provided chart data." <EXPLAINS> "CODE.st._legacy_area_chart(chart_data)" .

"DESCRIPTION.The code generates a line chart displaying data from a random 20x3 matrix." <EXPLAINS> """CODE.st.line_chart(chart_data)
chart_data = pd.DataFrame(
    np.random.randn(20, 3),
    columns=['a', 'b', 'c'])""" .

"DESCRIPTION.The code generates a line chart using randomly generated data with 20 rows and 3 columns labeled 'a', 'b', and 'c'." <EXPLAINS> """CODE.st.line_chart(chart_data)
chart_data = pd.DataFrame(
    np.random.randn(20, 3),
    columns=['a', 'b', 'c'])""" .

"DESCRIPTION.The code generates a list of dictionaries with randomized parameters from a given parameter grid, rounding the values to 6 decimal places. It then checks if the rounded list matches a predefined list of dictionaries with specific values for 'a' and 'b'." <EXPLAINS> """CODE.from sklearn.grid_search import ParameterSampler
from scipy.stats.distributions import expon
import numpy as np
np.random.seed(0)
param_grid = {'a':[1, 2], 'b': expon()}
param_list = list(ParameterSampler(param_grid, n_iter=4))
rounded_list = [dict((k, round(v, 6)) for (k, v) in d.items())
                for d in param_list]
rounded_list == [{'b': 0.89856, 'a': 1},
                 {'b': 0.923223, 'a': 1},
                 {'b': 1.878964, 'a': 2},
                 {'b': 1.038159, 'a': 2}]""" .

"DESCRIPTION.The code generates a magnitude spectrogram image from an input audio signal." <EXPLAINS> """CODE.pcm = tf.placeholder(tf.float32, [None, 9152])
frames = tf.contrib.signal.frames(pcm, 512, 180)
magspec = tf.abs(tf.spectral.rfft(frames, [512]))
image = tf.expand_dims(magspec, 3)
""" .

"DESCRIPTION.The code generates a map visualization using Pydeck library, displaying hexagon and scatterplot layers based on latitude and longitude data in a DataFrame. The map is centered at coordinates (37.76, -122.4) with specified zoom level, pitch, and map style. The hexagon layer shows data distribution with elevation scaling, while the scatterplot layer displays individual data points with specified color and radius." <EXPLAINS> """CODE.df = pd.DataFrame(
...    np.random.randn(1000, 2) / [50, 50] + [37.76, -122.4],
...    columns=['lat', 'lon'])

st.pydeck_chart(pdk.Deck(
...     map_style='mapbox://styles/mapbox/light-v9',
...     initial_view_state=pdk.ViewState(
...         latitude=37.76,
...         longitude=-122.4,
...         zoom=11,
...         pitch=50,
...     ),
...     layers=[
...         pdk.Layer(
...            'HexagonLayer',
...            data=df,
...            get_position='[lon, lat]',
...            radius=200,
...            elevation_scale=4,
...            elevation_range=[0, 1000],
...            pickable=True,
...            extruded=True,
...         ),
...         pdk.Layer(
...             'ScatterplotLayer',
...             data=df,
...             get_position='[lon, lat]',
...             get_color='[200, 30, 0, 160]',
...             get_radius=200,
...         ),
...     ],
... ))""" .

"DESCRIPTION.The code generates a map visualization using a Hexagon Layer and Scatterplot Layer on a map centered at latitude 37.76 and longitude -122.4. The Hexagon Layer displays data points in a hexagonal grid pattern with elevation scaling, while the Scatterplot Layer displays individual data points with custom color and radius." <EXPLAINS> """CODE.df = pd.DataFrame(
...    np.random.randn(1000, 2) / [50, 50] + [37.76, -122.4],
...    columns=['lat', 'lon'])

st.pydeck_chart(pdk.Deck(
...     map_style='mapbox://styles/mapbox/light-v9',
...     initial_view_state=pdk.ViewState(
...         latitude=37.76,
...         longitude=-122.4,
...         zoom=11,
...         pitch=50,
...     ),
...     layers=[
...         pdk.Layer(
...            'HexagonLayer',
...            data=df,
...            get_position='[lon, lat]',
...            radius=200,
...            elevation_scale=4,
...            elevation_range=[0, 1000],
...            pickable=True,
...            extruded=True,
...         ),
...         pdk.Layer(
...             'ScatterplotLayer',
...             data=df,
...             get_position='[lon, lat]',
...             get_color='[200, 30, 0, 160]',
...             get_radius=200,
...         ),
...     ],
... ))""" .

"DESCRIPTION.The code generates a multivariate normal distribution with a diagonal covariance matrix of the specified scale. It then reshapes the batch shape of the distribution to a new shape while validating arguments. Finally, it samples from the reshaped distribution and calculates the log probability of the samples." <EXPLAINS> """CODE.tfd = tf.contrib.distributions

dtype = np.float32
dims = 2
new_batch_shape = [1, 2, 3]
old_batch_shape = [6]

scale = np.ones(old_batch_shape + [dims], dtype)
mvn = tfd.MultivariateNormalDiag(scale_diag=scale)
reshape_mvn = tfd.BatchReshape(
    distribution=mvn,
    batch_shape=new_batch_shape,
    validate_args=True)

reshape_mvn.batch_shape

x = reshape_mvn.sample(sample_shape=[4, 5])
x.shape

reshape_mvn.log_prob(x).shape
""" .

"DESCRIPTION.The code generates a new index, gets the indexers for the new index, takes values based on the indexers from the current values, and sets values at the specified index positions to NaN." <EXPLAINS> """CODE.indexer, mask = index.get_indexer(new_index)
new_values = cur_values.take(indexer)
new_values[-mask] = np.nan""" .

"DESCRIPTION.The code generates a pandas DataFrame with columns 'A', 'B', and 'C', and an index named 'idx', and then builds a table schema based on the DataFrame." <EXPLAINS> """CODE.df = pd.DataFrame(
    {'A': [1, 2, 3],
     'B': ['a', 'b', 'c'],
     'C': pd.date_range('2016-01-01', freq='d', periods=3),
    }, index=pd.Index(range(3), name='idx'))
build_table_schema(df)
""" .

"DESCRIPTION.The code generates a pandas Series named 'animal' with duplicate values and then uses the 'drop_duplicates' method to remove duplicate values from the Series. The method can be used with different parameters such as 'keep' to control which duplicates to keep, and 'inplace' to modify the original Series." <EXPLAINS> """CODE.s = pd.Series(['lama', 'cow', 'lama', 'beetle', 'lama', 'hippo'],
              name='animal')
s
0      lama
1       cow
2      lama
3    beetle
4      lama
5     hippo
Name: animal, dtype: object

s.drop_duplicates()
0      lama
1       cow
3    beetle
5     hippo
Name: animal, dtype: object

s.drop_duplicates(keep='last')
1       cow
3    beetle
4      lama
5     hippo
Name: animal, dtype: object

s.drop_duplicates(keep=False, inplace=True)
s
1       cow
3    beetle
5     hippo
Name: animal, dtype: object
""" .

"DESCRIPTION.The code generates a parallel coordinates plot using the data in the DataFrame 'df', with lines colored based on the values in the 'Name' column using the specified colors." <EXPLAINS> """CODE.parallel_coordinates(df, 'Name', colors=('#556270', '#4ECDC4', '#C7F464'))
plt.show()""" .

"DESCRIPTION.The code generates a pivot table from a DataFrame 'df' with values from column 'D', using columns 'A' and 'B' as index and column 'C' as columns, summing up the values." <EXPLAINS> """CODE.table = pivot_table(df, values='D', index=['A', 'B'],
                     columns=['C'], aggfunc=np.sum)""" .

"DESCRIPTION.The code generates a plot displaying the A-weighting of constant-Q (CQT) frequencies in the frequency range of C1." <EXPLAINS> """CODE.import matplotlib.pyplot as plt
freqs = librosa.cqt_frequencies(108, librosa.note_to_hz('C1'))
weights = librosa.frequency_weighting(freqs, 'A')
fig, ax = plt.subplots()
ax.plot(freqs, weights)
ax.set(xlabel='Frequency (Hz)', ylabel='Weighting (log10)',
       title='A-Weighting of CQT frequencies')""" .

"DESCRIPTION.The code generates a plot of the values from 0 to 11 on the y-axis, with the y-axis labels formatted using a custom ChromaFJSFormatter from the librosa.display module, with intervals set to \"ji5\" and bins_per_octave set to 12. The ylabel of the plot is set to 'Pitch class'." <EXPLAINS> """CODE.import matplotlib.pyplot as plt
import numpy as np
import librosa.display.ChromaFJSFormatter

values = np.arange(12)
fig, ax = plt.subplots()
ax.plot(values)
ax.yaxis.set_major_formatter(librosa.display.ChromaFJSFormatter(intervals="ji5", bins_per_octave=12))
ax.set(ylabel='Pitch class')""" .

"DESCRIPTION.The code generates a precision-recall plot for the given true labels and predicted probabilities." <EXPLAINS> "CODE.wandb.sklearn.plot_precision_recall(y_true, y_probas, labels)" .

"DESCRIPTION.The code generates a random 100x100x3 array representing an image and converts it to a PIL image object." <EXPLAINS> """CODE.from PIL import Image
img = np.random.random(size=(100, 100, 3))
pil_img = tf.keras.preprocessing.image.array_to_img(img)
""" .

"DESCRIPTION.The code generates a random 10x5 DataFrame and displays it in a table format using the Streamlit library." <EXPLAINS> """CODE.df = pd.DataFrame(
...    np.random.randn(10, 5),
...    columns=('col %d' % i for i in range(5)))
...
st.table(df)""" .

"DESCRIPTION.The code generates a random 10x5 DataFrame and displays it in a table using Streamlit." <EXPLAINS> """CODE.df = pd.DataFrame(
...    np.random.randn(10, 5),
...    columns=('col %d' % i for i in range(5)))
...
st.table(df)""" .

"DESCRIPTION.The code generates a random DataFrame with 200 rows and 3 columns ('a', 'b', 'c'), and then creates a Vega-Lite chart using the DataFrame with circles as marks. The x-axis represents column 'a' as a quantitative value, the y-axis represents column 'b' as a quantitative value, the size of the circles represents column 'c' as a quantitative value, and the color of the circles also represents column 'c' as a quantitative value." <EXPLAINS> """CODE.import pandas as pd
import numpy as np

df = pd.DataFrame(
    np.random.randn(200, 3),
    columns=['a', 'b', 'c'])

st.vega_lite_chart(df, {
    'mark': {'type': 'circle', 'tooltip': True},
    'encoding': {
        'x': {'field': 'a', 'type': 'quantitative'},
        'y': {'field': 'b', 'type': 'quantitative'},
        'size': {'field': 'c', 'type': 'quantitative'},
        'color': {'field': 'c', 'type': 'quantitative'},
    },
})""" .

"DESCRIPTION.The code generates a random RGB image with dimensions 100x100 and converts it first to a PIL image object and then to a NumPy array." <EXPLAINS> """CODE.from PIL import Image
import numpy as np
import tensorflow as tf

img_data = np.random.random(size=(100, 100, 3))
img = tf.keras.preprocessing.image.array_to_img(img_data)
array = tf.keras.preprocessing.image.img_to_array(img)
""" .

"DESCRIPTION.The code generates a random array of numbers between 0 and 10 with a shape of [batch_size, 13] and data type float32, then uses an inferencer to make predictions based on the input data tensor_x." <EXPLAINS> """CODE.tensor_x = numpy.random.uniform(0, 10, [batch_size, 13]).astype("float32")
results = inferencer.infer({'x': tensor_x})""" .

"DESCRIPTION.The code generates a random dataset with 20 rows and 3 columns ('a', 'b', 'c') and then creates an area chart visualization using the generated data." <EXPLAINS> """CODE.chart_data = pd.DataFrame(
...     np.random.randn(20, 3),
...     columns=['a', 'b', 'c'])
...
st.area_chart(chart_data)""" .

"DESCRIPTION.The code generates a random dot stereogram image from the input image array using TensorFlow." <EXPLAINS> """CODE.img=[[1,2,3,3,2,1],
     [1,2,3,4,5,2],
     [1,2,3,4,5,3],
     [1,2,3,4,5,4],
     [6,5,4,4,5,5]]
session = tf.InteractiveSession()
sirds = single_image_random_dot_stereograms(
    img,
    convergence_dots_size=8,
    number_colors=256,normalize=True)

out = sirds.eval()
png = tf.image.encode_png(out).eval()
with open('picture_out.png', 'wb') as f:
  f.write(png)
""" .

"DESCRIPTION.The code generates a random image with dimensions 100x100 and 3 channels, and converts the numpy array representation of the image to a PIL Image object." <EXPLAINS> """CODE.from PIL import Image
img = np.random.random(size=(100, 100, 3))
pil_img = tf.keras.preprocessing.image.array_to_img(img)
""" .

"DESCRIPTION.The code generates a random image with dimensions 400x300x3, saves it as 'fake.jpg', reads the image file, decodes it using jpeg format, and prints the shape of the decoded image." <EXPLAINS> """CODE.import cv2
import paddle

fake_img = (np.random.random(
            (400, 300, 3)) * 255).astype('uint8')

cv2.imwrite('fake.jpg', fake_img)

img_bytes = paddle.vision.ops.read_file('fake.jpg')
img = paddle.vision.ops.decode_jpeg(img_bytes)

print(img.shape)""" .

"DESCRIPTION.The code generates a random image with size 500x500x3, and applies a contrast transformation with a factor of 0.4 to the image. It then prints the shape of the transformed image." <EXPLAINS> """CODE.import numpy as np
from paddle.incubate.hapi.vision.transforms import ContrastTransform
transform = ContrastTransform(0.4)
fake_img = np.random.rand(500, 500, 3).astype('float32')
fake_img = transform(fake_img)
print(fake_img.shape)""" .

"DESCRIPTION.The code generates a random key using 'stats' as a seed and then assigns the mean of a 2x2 matrix with weights initialized using LeCun Normal initialization and the generated key." <EXPLAINS> """CODE.key = self.make_rng('stats')
mean = self.variable('stats', 'mean', lecun_normal(), key, (2, 2))
""" .

"DESCRIPTION.The code generates a random key using JAX library and checks if the data type of the key is a pseudo-random number generator key." <EXPLAINS> """CODE.from jax import random
from jax import dtypes
key = random.key(0)
jnp.issubdtype(key.dtype, dtypes.prng_key)""" .

"DESCRIPTION.The code generates a random key using JAX library and checks if the key's data type is a subtype of the extended data types supported by JAX." <EXPLAINS> """CODE.from jax import random
from jax._src import dtypes
key = random.key(0)
jnp.issubdtype(key.dtype, dtypes.extended)""" .

"DESCRIPTION.The code generates a random number generator based on 'stats' and creates a variable named 'mean' using the LeCun normal distribution with a shape of (2, 2) initialized with the generated random number generator." <EXPLAINS> """CODE.key = self.make_rng('stats')
mean = self.variable('stats', 'mean', lecun_normal(), key, (2, 2))
""" .

"DESCRIPTION.The code generates a random sparse binary matrix and performs a rolling operation on the matrix by shifting its elements by 2 positions along the first axis. It then converts the sparse matrix to a dense matrix and performs the same rolling operation. Finally, it checks if the two rolled matrices are equivalent." <EXPLAINS> """CODE.# Generate a random sparse binary matrix
X = scipy.sparse.lil_matrix(np.random.randint(0, 2, size=(5,5)))
X_roll = roll_sparse(X, 2, axis=0)  # Roll by 2 on the first axis
X_dense_r = roll_sparse(X.toarray(), 2, axis=0)  # Equivalent dense roll
np.allclose(X_roll, X_dense_r.toarray())
""" .

"DESCRIPTION.The code generates a random tensor of shape (100, 3, 224, 224) using PaddlePaddle and then applies the Unfold operation with kernel size=[3, 3]. It returns the unfolded tensor." <EXPLAINS> """CODE.import paddle
import paddle.nn as nn

x = paddle.randn((100,3,224,224))
unfold = nn.Unfold(kernel_sizes=[3, 3])
result = unfold(x)
print(result)""" .

"DESCRIPTION.The code generates a random text string with a length of 5 characters using digits 0-9 as the charset." <EXPLAINS> """CODE.import string
Text(5)
Text(min_length = 1,
     max_length = 10,
     charset = string.digits)""" .

"DESCRIPTION.The code generates a random uniform variable with shape (2,3) and values between 0 and 1, and then evaluates and returns the value of the variable." <EXPLAINS> """CODE.kvar = K.random_uniform_variable((2,3), 0, 1)
K.eval(kvar)
""" .

"DESCRIPTION.The code generates a random uniformly distributed tensor of shape (2, 3) with values between 0 and 1." <EXPLAINS> """CODE.kvar = K.random_uniform_variable((2,3), 0, 1)
K.eval(kvar)
""" .

"DESCRIPTION.The code generates a random variable with a shape of 2x3, where the values are uniformly distributed between 0.0 and 1.0." <EXPLAINS> "CODE.kvar = tf.keras.backend.random_uniform_variable(shape=(2,3), low=0.0, high=1.0)" .

"DESCRIPTION.The code generates a range of numbers based on the provided parameters. The range starts from the first parameter (inclusive), ends at the second parameter (exclusive), and increments by the third parameter if provided. If the increment is negative, the range counts down instead of up. If the parameters are not valid to generate a range, an empty list is returned." <EXPLAINS> """CODE.Dataset.range(5) == [0, 1, 2, 3, 4]
Dataset.range(2, 5) == [2, 3, 4]
Dataset.range(1, 5, 2) == [1, 3]
Dataset.range(1, 5, -2) == []
Dataset.range(5, 1) == []
Dataset.range(5, 1, -2) == [5, 3]
""" .

"DESCRIPTION.The code generates a range of numbers from 0 to 4, repeats the sequence, and then takes the output. Additionally, it maps each number in the repeated sequence to its negative value and then takes the output. Lastly, it repeats the sequence, shuffles the order randomly, and takes the output." <EXPLAINS> """CODE.ray.data.range(5).repeat().take()
ray.data.range(5).repeat().map(lambda x: -x).take()
ray.data.range(5).repeat().random_shuffle().take()""" .

"DESCRIPTION.The code generates a result collection object and logs training step accuracy and validation step recall using torch tensors. It includes logging for both individual steps and entire epochs." <EXPLAINS> """CODE.result = _ResultCollection(training=True, torch.device("cpu"))
result.log('training_step', 'acc', torch.tensor(...), on_step=True, on_epoch=True)
result.log('validation_step', 'recall', torch.tensor(...), on_step=True, on_epoch=True)""" .

"DESCRIPTION.The code generates a scatter matrix from the provided DataFrame, with a specified transparency level for the data points." <EXPLAINS> "CODE.scatter_matrix(df, alpha=0.2)" .

"DESCRIPTION.The code generates a scatter plot chart using Streamlit and displays it using the Arrow Vega-Lite chart. The chart visualizes random data points with three variables 'a', 'b', and 'c', where 'a' and 'b' are plotted on the x and y axes as quantitative values, and 'c' is used for both the size and color of the data points." <EXPLAINS> """CODE.import streamlit as st
import pandas as pd
import numpy as np

chart_data = pd.DataFrame(np.random.randn(200, 3), columns=["a", "b", "c"])

st.vega_lite_chart(
    chart_data,
    {
        "mark": {"type": "circle", "tooltip": True},
        "encoding": {
            "x": {"field": "a", "type": "quantitative"},
            "y": {"field": "b", "type": "quantitative"},
            "size": {"field": "c", "type": "quantitative"},
            "color": {"field": "c", "type": "quantitative"},
        },
    },
)""",
        """CODE.import streamlit as st
import pandas as pd
import numpy as np

df = pd.DataFrame(
    np.random.randn(200, 3),
    columns=['a', 'b', 'c'])

st._arrow_vega_lite_chart(df, {
    'mark': {'type': 'circle', 'tooltip': True},
    'encoding': {
        'x': {'field': 'a', 'type': 'quantitative'},
        'y': {'field': 'b', 'type': 'quantitative'},
        'size': {'field': 'c', 'type': 'quantitative'},
        'color': {'field': 'c', 'type': 'quantitative'},
    },
})""" .

"DESCRIPTION.The code generates a scatter plot using Altair library based on a randomly generated DataFrame with 3 columns. The scatter plot displays the values of columns 'a' and 'b' as x and y coordinates, the size and color of the circles represent the values in column 'c'. The plot is displayed using Streamlit library." <EXPLAINS> """CODE.import streamlit as st
import pandas as pd
import numpy as np
import altair as alt

chart_data = pd.DataFrame(np.random.randn(20, 3), columns=["a", "b", "c"])

c = (
   alt.Chart(chart_data)
   .mark_circle()
   .encode(x="a", y="b", size="c", color="c", tooltip=["a", "b", "c"])
)

st.altair_chart(c, use_container_width=True)""",
        """CODE.import streamlit as st
import pandas as pd
import numpy as np
import altair as alt

df = pd.DataFrame(
    np.random.randn(200, 3),
    columns=['a', 'b', 'c'])

c = alt.Chart(df).mark_circle().encode(
    x='a', y='b', size='c', color='c', tooltip=['a', 'b', 'c'])

st._arrow_altair_chart(c, use_container_width=True)""" .

"DESCRIPTION.The code generates a scatter plot using Altair library to visualize a pandas DataFrame with random data. The scatter plot displays columns 'a' and 'b' as x and y coordinates, column 'c' as size and color of circles, and shows tooltips with values of 'a', 'b', and 'c' when hovering over the circles." <EXPLAINS> """CODE.import pandas as pd
import numpy as np
import altair as alt

df = pd.DataFrame(
    np.random.randn(200, 3),
    columns=['a', 'b', 'c'])

c = alt.Chart(df).mark_circle().encode(
    x='a', y='b', size='c', color='c', tooltip=['a', 'b', 'c'])

st._legacy_altair_chart(c, use_container_width=True)""" .

"DESCRIPTION.The code generates a scatter plot using Altair library with random data from a Pandas DataFrame. The plot displays 'a' on the x-axis, 'b' on the y-axis, size and color specified by 'c', and tooltip showing values of 'a', 'b', and 'c'." <EXPLAINS> """CODE.import pandas as pd
import numpy as np
import altair as alt

df = pd.DataFrame(
    np.random.randn(200, 3),
    columns=['a', 'b', 'c'])

c = alt.Chart(df).mark_circle().encode(
    x='a', y='b', size='c', color='c', tooltip=['a', 'b', 'c'])

st.altair_chart(c, use_container_width=True)""" .

"DESCRIPTION.The code generates a scatter plot using Altair library with random data from a Pandas DataFrame. The plot displays column 'a' on the x-axis, column 'b' on the y-axis, column 'c' as the size of the circles, and column 'c' as the color of the circles." <EXPLAINS> """CODE.import pandas as pd
import numpy as np
import altair as alt

df = pd.DataFrame(
    np.random.randn(200, 3),
    columns=['a', 'b', 'c'])

c = alt.Chart(df).mark_circle().encode(
    x='a', y='b', size='c', color='c')

st.altair_chart(c)""" .

"DESCRIPTION.The code generates a scatter plot using Altair library, where the x-axis represents column 'a', y-axis represents column 'b', the size of the circles represents column 'c', the color of the circles also represents column 'c', and the tooltip shows values of columns 'a', 'b', and 'c'." <EXPLAINS> """CODE.import pandas as pd
import numpy as np
import altair as alt

df = pd.DataFrame(
    np.random.randn(200, 3),
    columns=['a', 'b', 'c'])

c = alt.Chart(df).mark_circle().encode(
    x='a', y='b', size='c', color='c', tooltip=['a', 'b', 'c'])

st.altair_chart(c, use_container_width=True)""" .

"DESCRIPTION.The code generates a scatter plot using Vega-Lite library with random data in a Pandas DataFrame. It specifies the x, y, size, and color encoding for the scatter plot." <EXPLAINS> """CODE.import pandas as pd
import numpy as np

df = pd.DataFrame(
    np.random.randn(200, 3),
    columns=['a', 'b', 'c'])

st.vega_lite_chart(df, {
    'mark': {'type': 'circle', 'tooltip': True},
    'encoding': {
        'x': {'field': 'a', 'type': 'quantitative'},
        'y': {'field': 'b', 'type': 'quantitative'},
        'size': {'field': 'c', 'type': 'quantitative'},
        'color': {'field': 'c', 'type': 'quantitative'},
    },
})""" .

"DESCRIPTION.The code generates a scatter plot using a random dataset with 200 rows and 3 columns, where columns are named 'a', 'b', and 'c'. The scatter plot contains circles with tooltips, where 'a' is plotted on the x-axis, 'b' on the y-axis, 'c' is used for both size and color." <EXPLAINS> """CODE.import pandas as pd
import numpy as np

df = pd.DataFrame(
    np.random.randn(200, 3),
    columns=['a', 'b', 'c'])

st._arrow_vega_lite_chart(df, {
    'mark': {'type': 'circle', 'tooltip': True},
    'encoding': {
        'x': {'field': 'a', 'type': 'quantitative'},
        'y': {'field': 'b', 'type': 'quantitative'},
        'size': {'field': 'c', 'type': 'quantitative'},
        'color': {'field': 'c', 'type': 'quantitative'},
    },
})""" .

"DESCRIPTION.The code generates a scatter plot using random data with 3 variables (a, b, c) and displays it using Altair library." <EXPLAINS> """CODE.import pandas as pd
import numpy as np
import altair as alt

df = pd.DataFrame(
    np.random.randn(200, 3),
    columns=['a', 'b', 'c'])

c = alt.Chart(df).mark_circle().encode(
    x='a', y='b', size='c', color='c', tooltip=['a', 'b', 'c'])

st.altair_chart(c, use_container_width=True)""" .

"DESCRIPTION.The code generates a scatter plot with circles where the x-axis represents values from column 'a', the y-axis represents values from column 'b', and the size and color of the circles are based on values from column 'c' in a randomly generated DataFrame of 200 rows and 3 columns." <EXPLAINS> """CODE.import pandas as pd
import numpy as np

df = pd.DataFrame(
    np.random.randn(200, 3),
    columns=['a', 'b', 'c'])

st._legacy_vega_lite_chart(df, {
    'mark': {'type': 'circle', 'tooltip': True},
    'encoding': {
        'x': {'field': 'a', 'type': 'quantitative'},
        'y': {'field': 'b', 'type': 'quantitative'},
        'size': {'field': 'c', 'type': 'quantitative'},
        'color': {'field': 'c', 'type': 'quantitative'},
    },
})""" .

"DESCRIPTION.The code generates a sequence input layer using TensorFlow's feature columns such as sequence_numeric_column and sequence_categorical_column_with_identity. It then applies an embedding layer to one of the feature columns. The input features are provided in a sparse format and the code utilizes a SimpleRNNCell to process the sequence input and generate outputs based on the provided features." <EXPLAINS> """CODE.import tensorflow as tf

# Behavior of some cells or feature columns may depend on whether we are in
# training or inference mode, e.g. applying dropout.
training = True
rating = tf.feature_column.sequence_numeric_column('rating')
watches = tf.feature_column.sequence_categorical_column_with_identity(
    'watches', num_buckets=1000)
watches_embedding = tf.feature_column.embedding_column(watches,
                                            dimension=10)
columns = [rating, watches_embedding]

features = {
 'rating': tf.sparse.from_dense([[1.0,1.1, 0, 0, 0],
                                             [2.0,2.1,2.2, 2.3, 2.5]]),
 'watches': tf.sparse.from_dense([[2, 85, 0, 0, 0],[33,78, 2, 73, 1]])
}

sequence_input_layer = tf.keras.experimental.SequenceFeatures(columns)
sequence_input, sequence_length = sequence_input_layer(
   features, training=training)
sequence_length_mask = tf.sequence_mask(sequence_length)
hidden_size = 32
rnn_cell = tf.keras.layers.SimpleRNNCell(hidden_size)
rnn_layer = tf.keras.layers.RNN(rnn_cell)
outputs, state = rnn_layer(sequence_input, mask=sequence_length_mask)
""" .

"DESCRIPTION.The code generates a sequence of frames, then fixes the frames based on a maximum value. It provides an array of frame indices that are within the specified range, optionally padding the sequence with additional values." <EXPLAINS> """CODE.frames = np.arange(0, 1000.0, 50)
frames
array([   0.,   50.,  100.,  150.,  200.,  250.,  300.,  350.,
        400.,  450.,  500.,  550.,  600.,  650.,  700.,  750.,
        800.,  850.,  900.,  950.])
librosa.util.fix_frames(frames, x_max=250)
array([  0,  50, 100, 150, 200, 250])
librosa.util.fix_frames(frames, x_max=2500)
array([   0,   50,  100,  150,  200,  250,  300,  350,  400,
        450,  500,  550,  600,  650,  700,  750,  800,  850,
        900,  950, 2500])
librosa.util.fix_frames(frames, x_max=2500, pad=False)
array([  0,  50, 100, 150, 200, 250, 300, 350, 400, 450, 500,
       550, 600, 650, 700, 750, 800, 850,900,950])
frames = np.arange(200,500,33)
frames
array([200,233,266,299,332,365,398,431,464,497])
librosa.util.fix_frames(frames)
array([0,200,233,266,299,332,365,398,431,464,497])
librosa.util.fix_frames(frames,x_max=500)
array([0,200,233,266,299,332,365,398,431,464,497,
       500])
""" .

"DESCRIPTION.The code generates a sequence of integers from 0 to 4, repetitively applies a mapping function that negates each integer, shuffles the sequence randomly, and retrieves a subset of the sequence." <EXPLAINS> """CODE.ray.data.range(5).repeat().take()
ray.data.range(5).repeat().map(lambda x: -x).take()
ray.data.range(5).repeat().random_shuffle().take()""" .

"DESCRIPTION.The code generates a sequence of numbers based on the given parameters." <EXPLAINS> """CODE.Dataset.range(5)
Dataset.range(2, 5)
Dataset.range(1, 5, 2)
Dataset.range(1, 5, -2)
Dataset.range(5, 1)
Dataset.range(5, 1, -2)""" .

"DESCRIPTION.The code generates a series of non-zero elements' coordinates in the given numpy arrays." <EXPLAINS> """CODE.import paddle
import paddle.fluid as fluid
import numpy as np

data1 = np.array([[1.0, 0.0, 0.0],
                  [0.0, 2.0, 0.0],
                  [0.0, 0.0, 3.0]])
data2 = np.array([0.0, 1.0, 0.0, 3.0])
data3 = np.array([0.0, 0.0, 0.0])
with fluid.dygraph.guard():
    x1 = fluid.dygraph.to_variable(data1)
    x2 = fluid.dygraph.to_variable(data2)
    x3 = fluid.dygraph.to_variable(data3)
    out_z1 = fluid.layers.nonzero(x1)
    print(out_z1.numpy())
    #[[0 0]
    # [1 1]
    # [2 2]]
    out_z1_tuple = fluid.layers.nonzero(x1, as_tuple=True)
    for out in out_z1_tuple:
        print(out.numpy())
    #[[0]
    # [1]
    # [2]]
    #[[0]
    # [1]
    # [2]]
    out_z2 = fluid.layers.nonzero(x2)
    print(out_z2.numpy())
    #[[1]
    # [3]]
    out_z2_tuple = fluid.layers.nonzero(x2, as_tuple=True)
    for out in out_z2_tuple:
        print(out.numpy())
    #[[1]
    # [3]]
    out_z3 = fluid.layers.nonzero(x3)
    print(out_z3.numpy())
    #[]
    out_z3_tuple = fluid.layers.nonzero(x3, as_tuple=True)
    for out in out_z3_tuple:
        print(out.numpy())
    #[]""" .

"DESCRIPTION.The code generates a simple line plot using the Bokeh library and displays it using the Streamlit framework." <EXPLAINS> """CODE.import streamlit as st
from bokeh.plotting import figure

x = [1, 2, 3, 4, 5]
y = [6, 7, 2, 4, 5]

p = figure(
    title='simple line example',
    x_axis_label='x',
    y_axis_label='y')

p.line(x, y, legend='Trend', line_width=2)

st.bokeh_chart(p, use_container_width=True)""" .

"DESCRIPTION.The code generates a sparse matrix using numpy and scipy, then uses cs_graph_components function to find the connected components in the graph represented by the matrix." <EXPLAINS> """CODE.from scipy.sparse import cs_graph_components
import numpy as np
D = np.eye(4)
D[0,1] = D[1,0] = 1
cs_graph_components(D)
from scipy.sparse import dok_matrix
cs_graph_components(dok_matrix(D))""" .

"DESCRIPTION.The code generates a sparse representation of a Hann window of size 32 with different sparsity levels (quantile=0.01 and quantile=0.1) using the librosa library." <EXPLAINS> """CODE.x = scipy.signal.hann(32)
x_sparse = librosa.util.sparsify_rows(x, quantile=0.01)
x_sparse.todense()
x_sparse = librosa.util.sparsify_rows(x, quantile=0.1)
x_sparse.todense()
""" .

"DESCRIPTION.The code generates a summary of a given text using the Pegasus model." <EXPLAINS> """CODE.from transformers import PegasusTokenizer, PegasusForConditionalGeneration
from typing import List
PGE_ARTICLE = "PG&E stated it scheduled the blackouts in response to forecasts for high winds amid dry conditions. The aim is to reduce the risk of wildfires. Nearly 800 thousand customers were scheduled to be affected by the shutoffs which were expected to last through at least midday tomorrow."
mname = "google/pegasus-xsum"

model = PegasusForConditionalGeneration.from_pretrained(mname)
tok = PegasusTokenizer.from_pretrained(mname)
batch = tok.prepare_seq2seq_batch(src_texts=[PGE_ARTICLE])  # don't need tgt_text for inference
gen = model.generate(**batch)  # for forward pass: model(**batch)
summary: List[str] = tok.batch_decode(gen, skip_special_tokens=True)
assert summary == "California's largest electricity provider has turned off power to tens of thousands of customers.\"""" .

"DESCRIPTION.The code generates a symmetric positive definite tensor 'a', performs a partial Cholesky decomposition on 'a' to obtain 'u' and 'piv', constructs a permutation matrix 'p' based on 'piv', and reconstructs the original tensor 'a' using 'u' and 'p'." <EXPLAINS> """CODE.a = torch.randn(3, 3)
a = torch.mm(a, a.t()) # make symmetric positive definite
a
tensor([[ 3.5405, -0.4577,  0.8342],
        [-0.4577,  1.8244, -0.1996],
        [ 0.8342, -0.1996,  3.7493]])
u,piv = torch.pstrf(a)
u
tensor([[ 1.9363,  0.4308, -0.1031],
        [ 0.0000,  1.8316, -0.2256],
        [ 0.0000,  0.0000,  1.3277]])
piv
tensor([ 2,  0,  1], dtype=torch.int32)
p = torch.eye(3).index_select(0,piv.long()).index_select(0,piv.long()).t() # make pivot permutation
torch.mm(torch.mm(p.t(),torch.mm(u.t(),u)),p) # reconstruct
tensor([[ 3.5405, -0.4577,  0.8342],
        [-0.4577,  1.8244, -0.1996],
        [ 0.8342, -0.1996,  3.7493]])""" .

"DESCRIPTION.The code generates a synthetic Kitti dataset with specified image dimensions and creates a segmentation model using a UNet architecture with specific layers and convolutional operations." <EXPLAINS> """CODE.dataset_path = os.path.join(".", "Kitti")
_create_synth_kitti_dataset(dataset_path, image_dims=(1024, 512))
SegModel(dataset_path)  # doctest: +ELLIPSIS +NORMALIZE_WHITESPACE
SegModel(
  (net): UNet(
    (layers): ModuleList(
      (0): DoubleConv(...)
      (1): Down(...)
      (2): Down(...)
      (3): Up(...)
      (4): Up(...)
      (5): Conv2d(64, 19, kernel_size=(1, 1), stride=(1, 1))
    )
  )
)""" .

"DESCRIPTION.The code generates a tensor filled with ones of shape (2,3), then creates a SoftmaxTransform distribution. It uses the distribution to apply the forward and inverse transformations on the tensor x." <EXPLAINS> """CODE.import paddle

x = paddle.ones((2,3))
t = paddle.distribution.SoftmaxTransform()
print(t.forward(x))
print(t.inverse(t.forward(x)))""" .

"DESCRIPTION.The code generates a text file named \"foobar\" using either a default or custom file name." <EXPLAINS> """CODE.pytester.maketxtfile("foobar")
pytester.maketxtfile(custom="foobar")""" .

"DESCRIPTION.The code generates a time series data with integer values starting from '20180101' with hourly frequency. It then resamples the data to fill missing values using backward fill (bfill) method with different time intervals such as 30 minutes and 15 minutes, limiting the number of backfills for the latter." <EXPLAINS> """CODE.s = pd.Series([1, 2, 3],
...               index=pd.date_range('20180101', periods=3, freq='h'))
s
2018-01-01 00:00:00    1
2018-01-01 01:00:00    2
2018-01-01 02:00:00    3
Freq: H, dtype: int64

s.resample('30min').bfill()
2018-01-01 00:00:00    1
2018-01-01 00:30:00    2
2018-01-01 01:00:00    2
2018-01-01 01:30:00    3
2018-01-01 02:00:00    3
Freq: 30T, dtype: int64

s.resample('15min').bfill(limit=2)
2018-01-01 00:00:00    1.0
2018-01-01 00:15:00    NaN
2018-01-01 00:30:00    2.0
2018-01-01 00:45:00    2.0
2018-01-01 01:00:00    2.0
2018-01-01 01:15:00    NaN
2018-01-01 01:30:00    3.0
2018-01-01 01:45:00    3.0
2018-01-01 02:00:00    3.0""" .

"DESCRIPTION.The code generates a visual representation of a TensorFlow checkpoint file in SVG format." <EXPLAINS> """CODE.import tensorflow as tf
import pydot

dot_string = tf.contrib.checkpoint.dot_graph_from_checkpoint('/path/to/ckpt')
parsed, = pydot.graph_from_dot_data(dot_string)
parsed.write_svg('/tmp/tensorflow/visualized_checkpoint.svg')
""" .

"DESCRIPTION.The code generates a visualization of the clustering results by applying the KMeans algorithm to the training data X_train and assigning cluster labels to each data point." <EXPLAINS> "CODE.wandb.sklearn.plot_clusterer(kmeans, X_train, cluster_labels, labels, 'KMeans')" .

"DESCRIPTION.The code generates all possible combinations of elements from the input lists provided, creating a Cartesian product." <EXPLAINS> """CODE.cartesian(([1, 2, 3], [4, 5], [6, 7]))
array([[1, 4, 6],
       [1, 4, 7],
       [1, 5, 6],
       [1, 5, 7],
       [2, 4, 6],
       [2, 4, 7],
       [2, 5, 6],
       [2, 5, 7],
       [3, 4, 6],
       [3, 4, 7],
       [3, 5, 6],
       [3, 5, 7]])""" .

"DESCRIPTION.The code generates all possible combinations of parameters specified in a grid for parameter tuning in machine learning models." <EXPLAINS> """CODE.from sklearn.grid_search import ParameterGrid
param_grid = {'a': [1, 2], 'b': [True, False]}
list(ParameterGrid(param_grid)) == (
   [{'a': 1, 'b': True}, {'a': 1, 'b': False},
    {'a': 2, 'b': True}, {'a': 2, 'b': False}])

grid = [{'kernel': ['linear']}, {'kernel': ['rbf'], 'gamma': [1, 10]}]
list(ParameterGrid(grid)) == [{'kernel': 'linear'},
                              {'kernel': 'rbf', 'gamma': 1},
                              {'kernel': 'rbf', 'gamma': 10}]""" .

"DESCRIPTION.The code generates all possible combinations of values for the keys 'foo' and 'spam' from the given input tuple." <EXPLAINS> """CODE.combinations_grid(foo=("bar", "baz"), spam=("eggs", "ham"))
[
    {'foo': 'bar', 'spam': 'eggs'},
    {'foo': 'bar', 'spam': 'ham'},
    {'foo': 'baz', 'spam': 'eggs'},
    {'foo': 'baz', 'spam': 'ham'}
]""" .

"DESCRIPTION.The code generates an HTML file that contains the message \"Hello foo!\"." <EXPLAINS> "CODE.Template('Hello {{ name }}!').stream(name='foo').dump('hello.html')" .

"DESCRIPTION.The code generates an IntervalIndex object from the given breakpoints [0, 1, 2, 3]." <EXPLAINS> "CODE.IntervalIndex.from_breaks([0, 1, 2, 3])" .

"DESCRIPTION.The code generates an area chart using the data provided in the variable \"chart_data\"." <EXPLAINS> "CODE.st.area_chart(chart_data)" .

"DESCRIPTION.The code generates an area chart using the data provided in the variable chart_data." <EXPLAINS> "CODE.st._arrow_area_chart(chart_data)" .

"DESCRIPTION.The code generates an array of values using TensorFlow with dtype float32, applies the hard sigmoid activation function to each element of the array, and then returns the resulting array as a numpy array." <EXPLAINS> """CODE.a = tf.constant([-3.0,-1.0, 0.0,1.0,3.0], dtype = tf.float32)
b = tf.keras.activations.hard_sigmoid(a)
b.numpy()""" .

"DESCRIPTION.The code generates an iterator that iterates over a range of numbers from 10 to 1, shuffles the data locally with a buffer size of 2, and then gathers the shuffled data synchronously. It then iterates over the data four times using the `next()` function." <EXPLAINS> """CODE.it = from_range(10, 1).local_shuffle(shuffle_buffer_size=2)
it = it.gather_sync()
next(it)
next(it)
next(it)
next(it)
""" .

"DESCRIPTION.The code generates an iterator that loops through a range of 1,000,000 and splits it into batches, then prints each batch." <EXPLAINS> """CODE.import ray
for batch in ray.data.range(1000000).iterator().iter_batches():
    print(batch)
""" .

"DESCRIPTION.The code generates and displays a Discrete Cosine Transform (DCT) filter bank with 13 filters using the specified number of points (n_fft) for analysis. Each filter in the DCT filter bank is represented as a matrix of coefficients, and the visualization includes a plot of the DCT filters with the x-axis displayed in a linear scale. The plot is labeled with 'DCT function' on the y-axis and 'DCT filter bank' as the title, with a colorbar indicating the magnitude of the coefficients. Finally, the plot is adjusted for better layout using tight_layout()." <EXPLAINS> """CODE.n_fft = 2048
dct_filters = librosa.filters.dct(13, 1 + n_fft // 2)
dct_filters
array([[ 0.031,  0.031, ...,  0.031,  0.031],
       [ 0.044,  0.044, ..., -0.044, -0.044],
       ...,
       [ 0.044,  0.044, ..., -0.044, -0.044],
       [ 0.044,  0.044, ...,  0.044,  0.044]])

import matplotlib.pyplot as plt
plt.figure()
librosa.display.specshow(dct_filters, x_axis='linear')
plt.ylabel('DCT function')
plt.title('DCT filter bank')
plt.colorbar()
plt.tight_layout()""" .

"DESCRIPTION.The code generates and displays a detection error tradeoff (DET) curve for a classifier using the test dataset." <EXPLAINS> """CODE.metrics.plot_det_curve(clf, X_test, y_test)
plt.show()""" .

"DESCRIPTION.The code generates and displays the original, mel, and reconstructed spectrograms of an audio signal using librosa." <EXPLAINS> """CODE.y, sr = librosa.load(librosa.util.example_audio_file(), offset=30, duration=10)
S = np.abs(librosa.stft(y, n_fft=2048))
M = librosa.feature.melspectrogram(S=S, sr=sr, power=1)
mel_basis = librosa.filters.mel(sr, n_fft=2048, n_mels=M.shape[0])
S_recover = librosa.util.nnls(mel_basis, M)
import matplotlib.pyplot as plt
plt.figure()
plt.subplot(3,1,1)
librosa.display.specshow(librosa.amplitude_to_db(S, ref=np.max), y_axis='log')
plt.colorbar()
plt.title('Original spectrogram (1025 bins)')
plt.subplot(3,1,2)
librosa.display.specshow(librosa.amplitude_to_db(M, ref=np.max), y_axis='mel')
plt.title('Mel spectrogram (128 bins)')
plt.colorbar()
plt.subplot(3,1,3)
librosa.display.specshow(librosa.amplitude_to_db(S_recover, ref=np.max), y_axis='log')
plt.colorbar()
plt.title('Reconstructed spectrogram (1025 bins)')
plt.tight_layout()
plt.show()""" .

"DESCRIPTION.The code generates arrow scatter charts using the data provided in the DataFrame. The charts can display different combinations of columns as x, y, color, and size parameters." <EXPLAINS> """CODE.import streamlit as st
import pandas as pd
import numpy as np

chart_data = pd.DataFrame(
...     np.random.randn(20, 3),
...     columns=['a', 'b', 'c'])
...
st._arrow_scatter_chart(chart_data)

chart_data = pd.DataFrame(
...     np.random.randn(20, 4),
...     columns=['col1', 'col2', 'col3', 'col4'])
...
st._arrow_scatter_chart(
...     chart_data,
...     x='col1',
...     y='col2',
...     color='col3',
...     size='col4',
... )

st._arrow_scatter_chart(
...     chart_data,
...     x='col1',
...     y=['col2', 'col3'],
...     size='col4',
...     color=['#FF0000', '#0000FF'],  # Optional
... )""",
        """CODE.import streamlit as st
import pandas as pd
import numpy as np

chart_data = pd.DataFrame(np.random.randn(20, 3), columns=["a", "b", "c"])

st.scatter_chart(chart_data)

import streamlit as st
import pandas as pd
import numpy as np

chart_data = pd.DataFrame(np.random.randn(20, 3), columns=["col1", "col2", "col3"])
chart_data['col4'] = np.random.choice(['A','B','C'], 20)

st.scatter_chart(
...     chart_data,
...     x='col1',
...     y='col2',
...     color='col4',
...     size='col3',
... )

import streamlit as st
import pandas as pd
import numpy as np

chart_data = pd.DataFrame(np.random.randn(20, 4), columns=["col1", "col2", "col3", "col4"])

st.scatter_chart(
...     chart_data,
...     x='col1',
...     y=['col2', 'col3'],
...     size='col4',
...     color=['#FF0000', '#0000FF'],  # Optional
... )""" .

"DESCRIPTION.The code generates arrow scatter charts using the provided data with specified x, y, color, and size parameters." <EXPLAINS> """CODE.import streamlit as st
import pandas as pd
import numpy as np

chart_data = pd.DataFrame(
...     np.random.randn(20, 3),
...     columns=['a', 'b', 'c'])
...
st._arrow_scatter_chart(chart_data)

chart_data = pd.DataFrame(
...     np.random.randn(20, 4),
...     columns=['col1', 'col2', 'col3', 'col4'])
...
st._arrow_scatter_chart(
...     chart_data,
...     x='col1',
...     y='col2',
...     color='col3',
...     size='col4',
... )

st._arrow_scatter_chart(
...     chart_data,
...     x='col1',
...     y=['col2', 'col3'],
...     size='col4',
...     color=['#FF0000', '#0000FF'],  # Optional
... )""",
        """CODE.import streamlit as st
import pandas as pd
import numpy as np

chart_data = pd.DataFrame(np.random.randn(20, 3), columns=["a", "b", "c"])

st.scatter_chart(chart_data)

import streamlit as st
import pandas as pd
import numpy as np

chart_data = pd.DataFrame(np.random.randn(20, 3), columns=["col1", "col2", "col3"])
chart_data['col4'] = np.random.choice(['A','B','C'], 20)

st.scatter_chart(
...     chart_data,
...     x='col1',
...     y='col2',
...     color='col4',
...     size='col3',
... )

import streamlit as st
import pandas as pd
import numpy as np

chart_data = pd.DataFrame(np.random.randn(20, 4), columns=["col1", "col2", "col3", "col4"])

st.scatter_chart(
...     chart_data,
...     x='col1',
...     y=['col2', 'col3'],
...     size='col4',
...     color=['#FF0000', '#0000FF'],  # Optional
... )""",
        """CODE.import streamlit as st
import pandas as pd
import numpy as np

chart_data = pd.DataFrame(np.random.randn(20, 3), columns=["a", "b", "c"])

st.scatter_chart(chart_data)

import streamlit as st
import pandas as pd
import numpy as np

chart_data = pd.DataFrame(np.random.randn(20, 3), columns=["col1", "col2", "col3"])
chart_data['col4'] = np.random.choice(['A','B','C'], 20)

st.scatter_chart(
...     chart_data,
...     x='col1',
...     y='col2',
...     color='col4',
...     size='col3',
... )

import streamlit as st
import pandas as pd
import numpy as np

chart_data = pd.DataFrame(np.random.randn(20, 4), columns=["col1", "col2", "col3", "col4"])

st.scatter_chart(
...     chart_data,
...     x='col1',
...     y=['col2', 'col3'],
...     size='col4',
...     color=['#FF0000', '#0000FF'],  # Optional
... )
""" .

"DESCRIPTION.The code generates batches of data by creating a tensor dictionary of features and using a Python generator to yield values. The `tf.train.batch` function is then used to batch the data with a specific batch size. The code then uses a TensorFlow session to run the batched dictionary and retrieve the batches of data." <EXPLAINS> """CODE.def generator():
  for i in range(3):
    yield {"value": i}

features = {
  "value": tf.FixedLenFeature(shape=[], dtype=dtypes.int32)
}

tensor_dict = tf.contrib.training.python_input(generator, features)
batched_dict = tf.train.batch(
  tensor_dict, batch_size=2, allow_smaller_final_batch=True)

s = tf.Session()
tf.train.start_queue_runners()

batch1 = s.run(batched_dict)  # returns {"value": np.array([0, 1])}
batch2 = s.run(batched_dict)  # returns {"value": np.array([2])}
s.run(batched_dict)  # error: Queue is closed (generator finished at i==3)
""" .

"DESCRIPTION.The code generates box plots for each group in a dataframe after grouping the data by specific level(s) along the columns." <EXPLAINS> """CODE.boxplot_frame_groupby(grouped)

grouped = df.unstack(level='lvl1').groupby(level=0, axis=1)
boxplot_frame_groupby(grouped, subplots=False)""" .

"DESCRIPTION.The code generates cached paths for different assets related to specified libraries, namespaces, and subfolders." <EXPLAINS> """CODE.from huggingface_hub import cached_assets_path

cached_assets_path(library_name="datasets", namespace="SQuAD", subfolder="download")
PosixPath('/home/wauplin/.cache/huggingface/extra/datasets/SQuAD/download')

cached_assets_path(library_name="datasets", namespace="SQuAD", subfolder="extracted")
PosixPath('/home/wauplin/.cache/huggingface/extra/datasets/SQuAD/extracted')

cached_assets_path(library_name="datasets", namespace="Helsinki-NLP/tatoeba_mt")
PosixPath('/home/wauplin/.cache/huggingface/extra/datasets/Helsinki-NLP--tatoeba_mt/default')

cached_assets_path(library_name="datasets", assets_dir="/tmp/tmp123456")
PosixPath('/tmp/tmp123456/datasets/default/default')
""" .

"DESCRIPTION.The code generates chroma features using Constant Q Transform (chroma\\_cq) and Variable Q Transform (chroma\\_vq) for a given audio signal (y) sampled at a certain rate (sr). These features are then displayed as spectrograms using matplotlib to visualize the chroma features in two different subplots." <EXPLAINS> """CODE.import librosa
y, sr = librosa.load(librosa.ex('trumpet'))
n_bins = 36
chroma_cq = librosa.feature.chroma_cqt(y=y, sr=sr, n_chroma=n_bins)
chroma_vq = librosa.feature.chroma_vqt(y=y, sr=sr, intervals='ji5', bins_per_octave=n_bins)
import matplotlib.pyplot as plt
fig, ax = plt.subplots(nrows=2, sharex=True)
librosa.display.specshow(chroma_cq, y_axis='chroma', x_axis='time', ax=ax[0], bins_per_octave=n_bins)
ax[0].set(ylabel='chroma_cqt')
ax[0].label_outer()
img = librosa.display.specshow(chroma_vq, y_axis='chroma_fjs', x_axis='time', ax=ax[1], bins_per_octave=n_bins, intervals='ji5')
ax[1].set(ylabel='chroma_vqt')
fig.colorbar(img, ax=ax)
""" .

"DESCRIPTION.The code generates chroma filters for audio signal analysis with different parameters such as number of chroma bins, center octave, and octave width." <EXPLAINS> """CODE.chroma_fb   = librosa.filters.chroma(22050, 4096)
chroma_fbq  = librosa.filters.chroma(22050, 4096, n_chroma=24)
chroma_fb   = librosa.filters.chroma(22050, 4096, ctroct=5, octwidth=2)""" .

"DESCRIPTION.The code generates click sounds at specified beat frames or time intervals in a audio signal." <EXPLAINS> """CODE.y_beats = librosa.clicks(frames=beats, sr=sr)
y_beats = librosa.clicks(frames=beats, sr=sr, length=len(y))
times = librosa.frames_to_time(beats, sr=sr)
y_beat_times = librosa.clicks(times=times, sr=sr)
y_beat_times880 = librosa.clicks(times=times, sr=sr, click_freq=880, click_duration=0.5)""" .

"DESCRIPTION.The code generates completions for the method 'up' of the 'join' function in Python." <EXPLAINS> """CODE.from os.path import join
namespace = locals()
script = Interpreter('join().up', [namespace])
print(script.completions()[0].name)""" .

"DESCRIPTION.The code generates diagonal tensors from the given input array with specified offsets and dimensions." <EXPLAINS> """CODE.import paddle.fluid as fluid
import paddle.fluid.dygraph as dg
import numpy as np

diag_embed = np.random.randn(2, 3).astype('float32')
with dg.guard():
    data1 = fluid.layers.diag_embed(diag_embed)
    data1.numpy()

    data2 = fluid.layers.diag_embed(diag_embed, offset=-1, dim1=0, dim2=2)
    data2.numpy()

    data3 = fluid.layers.diag_embed(diag_embed, offset=1, dim1=0, dim2=2)
    data3.numpy()""" .

"DESCRIPTION.The code generates different chroma filterbanks for audio signals with specified sampling rate, frame size, and optional parameters like number of chroma bins, center octave, and octave width." <EXPLAINS> """CODE.chroma_fb   = librosa.filters.chroma(22050, 4096)
chroma_fbq  = librosa.filters.chroma(22050, 4096, n_chroma=24)
chroma_fb   = librosa.filters.chroma(22050, 4096, ctroct=5, octwidth=2)""" .

"DESCRIPTION.The code generates download buttons for various types of data including a dataframe converted to CSV format, text contents, binary contents, and an image file. The download buttons allow users to download the specified data in the specified format with the given filename." <EXPLAINS> """CODE.@st.cache
... def convert_df(df):
...   # Cache the conversion to prevent computation on every rerun
...       return df.to_csv().encode('utf-8')
csv = convert_df(my_large_df)
st.download_button(
...     label="Press to Download",
...     data=csv,
...     file_name='large_df.csv',
...     mime='text/csv',
... )

text_contents = '''
... Col1, Col2
... 123, 456
... 789, 000
... '''
st.download_button(
...     label='Download CSV', data=text_contents,
...     file_name='file.csv', mime='text/csv'
... )

binary_contents = b'example content'
... # Defaults to 'application/octet-stream'
st.download_button('Download binary file', binary_contents)

with open("flower.png", "rb") as file:
...     btn = st.download_button(
...             label="Download Image",
...             data=file,
...             file_name="flower.png",
...             mime="image/png"
...           )""" .

"DESCRIPTION.The code generates entity annotations based on IOB2 tagging scheme for a list of tokens and their corresponding entity types." <EXPLAINS> """CODE.tokens = Tokens(['B-PER', 'I-PER', 'O', 'B-LOC'], IOB2)
tokens.entities
[('PER', 0, 2), ('LOC', 3, 4)]""" .

"DESCRIPTION.The code generates multiple plots using matplotlib in Python, including a simple plot, two subplots sharing the y-axis, and four polar axes." <EXPLAINS> """CODE.x = np.linspace(0, 2*np.pi, 400)
y = np.sin(x**2)

# Just a figure and one subplot
f, ax = plt.subplots()
ax.plot(x, y)
ax.set_title('Simple plot')

# Two subplots, unpack the output array immediately
f, (ax1, ax2) = plt.subplots(1, 2, sharey=True)
ax1.plot(x, y)
ax1.set_title('Sharing Y axis')
ax2.scatter(x, y)

# Four polar axes
plt.subplots(2, 2, subplot_kw=dict(polar=True)""" .

"DESCRIPTION.The code generates multiple subplots with different types of plots and shares the y-axis for the first two subplots." <EXPLAINS> """CODE.x = np.linspace(0, 2*np.pi, 400)
y = np.sin(x**2)

f, ax = plt.subplots()
ax.plot(x, y)
ax.set_title('Simple plot')

f, (ax1, ax2) = plt.subplots(1, 2, sharey=True)
ax1.plot(x, y)
ax1.set_title('Sharing Y axis')
ax2.scatter(x, y)

plt.subplots(2, 2, subplot_kw=dict(polar=True) )""" .

"DESCRIPTION.The code generates pie charts representing the mass of planets (Mercury, Venus, Earth) in the solar system." <EXPLAINS> """CODE.df = pd.DataFrame({'mass': [0.330, 4.87 , 5.97],
                   'radius': [2439.7, 6051.8, 6378.1]},
                  index=['Mercury', 'Venus', 'Earth'])
plot = df.plot.pie(y='mass', figsize=(5, 5))
plot = df.plot.pie(subplots=True, figsize=(6, 3))
""" .

"DESCRIPTION.The code generates random data with 50 rows and 3 columns and creates a bar chart using the generated data." <EXPLAINS> """CODE.chart_data = pd.DataFrame(
...     np.random.randn(50, 3),
...     columns=["a", "b", "c"])
...
st.bar_chart(chart_data)""" .

"DESCRIPTION.The code generates random integers within a specified range and shape." <EXPLAINS> """CODE.import paddle.fluid as fluid

# example 1:
# attr shape is a list which doesn't contain tensor Variable.
result_1 = fluid.layers.randint(low=-5, high=5, shape=[3, 4], dtype="int64")

# example 2:
# attr shape is a list which contains tensor Variable.
dim_1 = fluid.layers.fill_constant([1],"int64",3)
dim_2 = fluid.layers.fill_constant([1],"int32",5)
result_2 = fluid.layers.randint(low=-5, high=5, shape=[dim_1, dim_2], dtype="int32")

# example 3:
# attr shape is a Variable, the data type must be int64 or int32.
var_shape = fluid.data(name='var_shape', shape=[2], dtype="int64")
result_3 = fluid.layers.randint(low=-5, high=5, shape=var_shape, dtype="int32")
var_shape_int32 = fluid.data(name='var_shape_int32', shape=[2], dtype="int32")
result_4 = fluid.layers.randint(low=-5, high=5, shape=var_shape_int32, dtype="int64")

# example 4:
# Input only one parameter
# low=0, high=10, shape=[1], dtype='int64'
result_4 = fluid.layers.randint(10)""" .

"DESCRIPTION.The code generates random metadata strings and associates them with corresponding indices. It also creates a random label image tensor and adds embeddings with metadata, label images, and random tensors to a writer." <EXPLAINS> """CODE.import keyword
import torch
meta = []
while len(meta)<100:
    meta = meta+keyword.kwlist # get some strings
meta = meta[:100]

for i, v in enumerate(meta):
    meta[i] = v+str(i)

label_img = torch.rand(100, 3, 10, 32)
for i in range(100):
    label_img[i]*=i/100.0

writer.add_embedding(torch.randn(100, 5), metadata=meta, label_img=label_img)
writer.add_embedding(torch.randn(100, 5), label_img=label_img)
writer.add_embedding(torch.randn(100, 5), metadata=meta)""" .

"DESCRIPTION.The code generates random permutations of numbers from 0 to 5 using PaddlePaddle's fluid library, and prints the generated permutations. It can be configured to run on either the CPU or GPU based on the value of the is_use_gpu variable." <EXPLAINS> """CODE.import paddle.fluid as fluid

num = 6
is_use_gpu = False

data_1 = fluid.layers.randperm(num)
fluid.layers.Print(data_1)

data_2 = fluid.layers.randperm(num, dtype="int32", seed=1)
fluid.layers.Print(data_2)

data_3 = fluid.layers.randperm(num, stop_gradient=False, device="cpu")
fluid.layers.Print(data_3)

fluid.layers.randperm(num, out=data_3)
fluid.layers.Print(data_3)

place = fluid.CUDAPlace(0) if is_use_gpu else fluid.CPUPlace()
exe = fluid.Executor(place)
exe.run(fluid.default_startup_program())
exe.run()""" .

"DESCRIPTION.The code generates random samples from Bernoulli distributions with different probabilities and shapes." <EXPLAINS> """CODE.import paddle
from paddle.distribution import Bernoulli

rv = Bernoulli(paddle.full((), 0.3))
print(rv.sample([100]).shape)

rv = Bernoulli(paddle.to_tensor(0.3))
print(rv.sample([100]).shape)

rv = Bernoulli(paddle.to_tensor([0.3, 0.5]))
print(rv.sample([100]).shape)

rv = Bernoulli(paddle.to_tensor([0.3, 0.5]))
print(rv.sample([100, 2]).shape)""" .

"DESCRIPTION.The code generates random tensor data of shape (2, 4) using PaddlePaddle's fluid framework and prints the generated data." <EXPLAINS> """CODE.import paddle.fluid as fluid

data = fluid.layers.randn([2, 4])
place = fluid.CPUPlace()
exe = fluid.Executor(place)
res, = exe.run(fluid.default_main_program(), feed={}, fetch_list=[data])
print(res)

import paddle.fluid as fluid
import paddle.fluid.dygraph as dg

place = fluid.CPUPlace()
with dg.guard(place) as g:
    x = fluid.layers.randn([2, 4])
    x_np = x.numpy()
    print(x_np)""" .

"DESCRIPTION.The code generates random x and index values, then uses the index values to sample elements from the x values, the output is a sampled subset of x based on the index." <EXPLAINS> """CODE.import paddle.fluid as fluid
import numpy as np
# create x value
x_shape = (2, 5)
x_type = "float64"
x_np = np.random.random(x_shape).astype(x_type)
# create index value
index_shape = (2, 3)
index_type = "int32"
index_np = np.random.randint(low=0,
                             high=x_shape[1],
                             size=index_shape).astype(index_type)
x = fluid.data(name='x', shape=[-1, 5], dtype='float64')
index = fluid.data(name='index', shape=[-1, 3], dtype='int32')
output = fluid.contrib.layers.index_sample(x=x, index=index)""" .

"DESCRIPTION.The code generates samples from a Cauchy distribution with different parameter initializations and dimensions." <EXPLAINS> """CODE.import paddle
from paddle.distribution import Cauchy

# init Cauchy with float
rv = Cauchy(loc=0.1, scale=1.2)
print(rv.sample([10]).shape)

# init Cauchy with 0-Dim tensor
rv = Cauchy(loc=paddle.full((), 0.1), scale=paddle.full((), 1.2))
print(rv.sample([10]).shape)

# init Cauchy with N-Dim tensor
rv = Cauchy(loc=paddle.to_tensor(0.1), scale=paddle.to_tensor([1.0, 2.0]))
print(rv.sample([10]).shape)

# sample 2-Dim data
rv = Cauchy(loc=0.1, scale=1.2)
print(rv.sample([10, 2]).shape)

rv = Cauchy(loc=paddle.to_tensor(0.1), scale=paddle.to_tensor([1.0, 2.0]))
print(rv.sample([10, 2]).shape)""" .

"DESCRIPTION.The code generates samples from a Geometric distribution with a probability of success 0.5. The function sample() generates 2 samples of size 2 each." <EXPLAINS> """CODE.import paddle
from paddle.distribution import Geometric

geom = Geometric(0.5)
geom.sample((2,2))""" .

"DESCRIPTION.The code generates samples from a RelaxedOneHotCategorical distribution with the specified temperature and logits/probabilities, and then calculates the exponent for each sample. The resulting exp_samples have the same distribution as the samples from RelaxedOneHotCategorical distribution." <EXPLAINS> """CODE.temperature = 0.5
p = [0.1, 0.5, 0.4]
dist = ExpRelaxedOneHotCategorical(temperature, probs=p)
samples = dist.sample()
exp_samples = tf.exp(samples)
# exp_samples has the same distribution as samples from
# RelaxedOneHotCategorical(temperature, probs=p)


temperature = 0.5
logits = [-2, 2, 0]
dist = ExpRelaxedOneHotCategorical(temperature, logits=logits)
samples = dist.sample()
exp_samples = tf.exp(samples)
# exp_samples has the same distribution as samples from
# RelaxedOneHotCategorical(temperature, probs=p)


temperature = 1e-5
logits = [-2, 2, 0]
dist = ExpRelaxedOneHotCategorical(temperature, logits=logits)
samples = dist.sample()
exp_samples = tf.exp(samples)
# exp_samples has the same distribution as samples from
# RelaxedOneHotCategorical(temperature, probs=p)


temperature = 10
logits = [-2, 2, 0]
dist = ExpRelaxedOneHotCategorical(temperature, logits=logits)
samples = dist.sample()
exp_samples = tf.exp(samples)
# exp_samples has the same distribution as samples from
# RelaxedOneHotCategorical(temperature, probs=p)
""" .

"DESCRIPTION.The code generates suggestions for filling in the blank \"<mask>\" in the sentence \"The goal of life is <mask>.\" with potential words such as \"happiness\" and \"immortality\", along with their corresponding likelihood scores." <EXPLAINS> """CODE.from huggingface_hub import InferenceClient
client = InferenceClient()
client.fill_mask("The goal of life is <mask>.")
[{'score': 0.06897063553333282,
'token': 11098,
'token_str': ' happiness',
'sequence': 'The goal of life is happiness.'},
{'score': 0.06554922461509705,
'token': 45075,
'token_str': ' immortality',
'sequence': 'The goal of life is immortality.'}]
""" .

"DESCRIPTION.The code generates text based on the input prompt using the GPT-2 model by tokenizing the input, processing the logits with specific constraints, and then sampling from the model to produce a text output." <EXPLAINS> """CODE.from transformers import (
...     AutoTokenizer,
...     TFAutoModelForCausalLM,
...     TFLogitsProcessorList,
...     TFMinLengthLogitsProcessor,
...     TFTopKLogitsWarper,
...     TFTemperatureLogitsWarper,
... )

tokenizer = AutoTokenizer.from_pretrained("gpt2")
model = TFAutoModelForCausalLM.from_pretrained("gpt2")

# set pad_token_id to eos_token_id because GPT2 does not have a EOS token
model.config.pad_token_id = model.config.eos_token_id

input_prompt = "Today is a beautiful day, and"
input_ids = tokenizer(input_prompt, return_tensors="tf").input_ids

# instantiate logits processors
logits_processor = TFLogitsProcessorList(
...     [
...         TFMinLengthLogitsProcessor(15, eos_token_id=model.config.eos_token_id),
...     ]
... )
# instantiate logits processors
logits_warper = TFLogitsProcessorList(
...     [
...         TFTopKLogitsWarper(50),
...         TFTemperatureLogitsWarper(0.7),
...     ]
... )

outputs = model.sample(input_ids, logits_processor=logits_processor, logits_warper=logits_warper)

print("Generated:", tokenizer.batch_decode(outputs, skip_special_tokens=True))
""" .

"DESCRIPTION.The code generates text completion predictions using the GPT-2 language model through the Hugging Face library. It uses a pre-trained tokenizer and model to generate text based on given prompts." <EXPLAINS> """CODE.import pandas as pd
from transformers import AutoConfig, AutoModelForCausalLM, AutoTokenizer
from transformers.pipelines import pipeline
from ray.ml.predictors.integrations.huggingface import HuggingFacePredictor

model_checkpoint = "gpt2"
tokenizer_checkpoint = "sgugger/gpt2-like-tokenizer"
tokenizer = AutoTokenizer.from_pretrained(tokenizer_checkpoint)

model_config = AutoConfig.from_pretrained(model_checkpoint)
model = AutoModelForCausalLM.from_config(model_config)
predictor = HuggingFacePredictor(
    pipeline=pipeline(
        task="text-generation", model=model, tokenizer=tokenizer
    )
)

prompts = pd.DataFrame(
    ["Complete me", "And me", "Please complete"], columns=["sentences"]
)
predictions = predictor.predict(prompts)""" .

"DESCRIPTION.The code generates text completions for the given prompts using a pre-trained GPT-2 model from Hugging Face." <EXPLAINS> """CODE.import pandas as pd
    from transformers import AutoConfig, AutoModelForCausalLM, AutoTokenizer
    from transformers.pipelines import pipeline
    from ray.ml.predictors.integrations.huggingface import HuggingFacePredictor

    model_checkpoint = "gpt2"
    tokenizer_checkpoint = "sgugger/gpt2-like-tokenizer"
    tokenizer = AutoTokenizer.from_pretrained(tokenizer_checkpoint)

    model_config = AutoConfig.from_pretrained(model_checkpoint)
    model = AutoModelForCausalLM.from_config(model_config)
    predictor = HuggingFacePredictor(
        pipeline=pipeline(
            task="text-generation", model=model, tokenizer=tokenizer
        )
    )

    prompts = pd.DataFrame(
        ["Complete me", "And me", "Please complete"], columns=["sentences"]
    )
    predictions = predictor.predict(prompts)""" .

"DESCRIPTION.The code generates text completions for the provided prompts using a pre-trained GPT-2 model." <EXPLAINS> """CODE.import pandas as pd
from transformers import AutoConfig, AutoModelForCausalLM, AutoTokenizer
from transformers.pipelines import pipeline
from ray.train.huggingface import TransformersPredictor

model_checkpoint = "gpt2"
tokenizer_checkpoint = "sgugger/gpt2-like-tokenizer"
tokenizer = AutoTokenizer.from_pretrained(tokenizer_checkpoint)

model_config = AutoConfig.from_pretrained(model_checkpoint)
model = AutoModelForCausalLM.from_config(model_config)
predictor = TransformersPredictor(
    pipeline=pipeline(
        task="text-generation", model=model, tokenizer=tokenizer
    )
)

prompts = pd.DataFrame(
    ["Complete me", "And me", "Please complete"], columns=["sentences"]
)
predictions = predictor.predict(prompts)""" .

"DESCRIPTION.The code generates text continuation based on the input prompt using the GPT-2 model. It sets up pre-trained tokenizer and model, processes the generated logits, and performs greedy search to generate text based on the input prompt. Finally, it decodes and prints the generated text." <EXPLAINS> """CODE.from transformers import (
...     AutoTokenizer,
...     TFAutoModelForCausalLM,
...     TFLogitsProcessorList,
...     TFMinLengthLogitsProcessor,
... )

tokenizer = AutoTokenizer.from_pretrained("gpt2")
model = TFAutoModelForCausalLM.from_pretrained("gpt2")

# set pad_token_id to eos_token_id because GPT2 does not have a EOS token
model.config.pad_token_id = model.config.eos_token_id

input_prompt = "Today is a beautiful day, and"
input_ids = tokenizer(input_prompt, return_tensors="tf").input_ids

# instantiate logits processors
logits_processor = TFLogitsProcessorList(
...     [
...         TFMinLengthLogitsProcessor(15, eos_token_id=model.config.eos_token_id),
...     ]
... )

outputs = model.greedy_search(input_ids, logits_processor=logits_processor)

print("Generated:", tokenizer.batch_decode(outputs, skip_special_tokens=True))
""" .

"DESCRIPTION.The code generates the top 3 values from the array \"arr\" in an unstable manner." <EXPLAINS> """CODE.import pyarrow as pa
import pyarrow.compute as pc
arr = pa.array(["a", "b", "c", None, "e", "f"])
pc.top_k_unstable(arr, k=3)
""" .

"DESCRIPTION.The code generates three sets of random data, creates a distribution plot with custom bin sizes for each set, and displays the plot using Streamlit." <EXPLAINS> """CODE.import streamlit as st
import plotly.plotly as py
import plotly.figure_factory as ff
import numpy as np

# Add histogram data
x1 = np.random.randn(200) - 2
x2 = np.random.randn(200)
x3 = np.random.randn(200) + 2

# Group data together
hist_data = [x1, x2, x3]

group_labels = ['Group 1', 'Group 2', 'Group 3']

# Create distplot with custom bin_size
fig = ff.create_distplot(
        hist_data, group_labels, bin_size=[.1, .25, .5])

# Plot!
st.plotly_chart(fig)""" .

"DESCRIPTION.The code generates time series data and its corresponding target data with specific length, sampling rate, and batch size using the TimeseriesGenerator." <EXPLAINS> """CODE.from keras.preprocessing.sequence import TimeseriesGenerator
import numpy as np
data = np.array([[i] for i in range(50)])
targets = np.array([[i] for i in range(50)])
data_gen = TimeseriesGenerator(data, targets,
                               length=10, sampling_rate=2,
                               batch_size=2)
assert len(data_gen) == 20
batch_0 = data_gen[0]
x, y = batch_0
assert np.array_equal(x,
                      np.array([[[0], [2], [4], [6], [8]],
                                [[1], [3], [5], [7], [9]]]))
assert np.array_equal(y,
                      np.array([[10], [11]]))
""" .

"DESCRIPTION.The code generates two dataframes with random values and creates a table using one dataframe. It then adds rows from the second dataframe to the table. Next, it creates a line chart using the first dataframe and adds rows from the second dataframe to the chart. Lastly, it creates another line chart using vega-lite, assigns the first dataframe to a named dataset, and then adds rows from the second dataframe to the chart using the named dataset." <EXPLAINS> """CODE.df1 = pd.DataFrame(
    np.random.randn(50, 20),
    columns=('col %d' % i for i in range(20)))

my_table = st.table(df1)

df2 = pd.DataFrame(
    np.random.randn(50, 20),
    columns=('col %d' % i for i in range(20)))

my_table.add_rows(df2)

my_chart = st.line_chart(df1)
my_chart.add_rows(df2)

my_chart = st.vega_lite_chart({
    'mark': 'line',
    'encoding': {'x': 'a', 'y': 'b'},
    'datasets': {
        'some_fancy_name': df1,  # <-- named dataset
    },
    'data': {'name': 'some_fancy_name'},
}),
my_chart.add_rows(some_fancy_name=df2)  # <-- name used as keyword
""" .

"DESCRIPTION.The code generates two images: one is a grayscale gradient image from black to white, and the other is an RGB gradient image with a varying red channel and an inverse blue channel. Both images are saved as .png files." <EXPLAINS> """CODE.from scipy.misc import imsave
x = np.zeros((255, 255), dtype=np.uint8)
x[:] = np.arange(255)
imsave('gradient.png', x)

rgb = np.zeros((255, 255, 3), dtype=np.uint8)
rgb[..., 0] = np.arange(255)
rgb[..., 1] = 55
rgb[..., 2] = 1 - np.arange(255)
imsave('rgb_gradient.png', rgb)""" .

"DESCRIPTION.The code generates two random DataFrames, adds rows from the second DataFrame to a table, creates a line chart with the first DataFrame, adds rows from the second DataFrame to the line chart, and finally creates a Vega-Lite line chart using both DataFrames as datasets." <EXPLAINS> """CODE.df1 = pd.DataFrame(
   np.random.randn(50, 20),
   columns=('col %d' % i for i in range(20)))

df2 = pd.DataFrame(
   np.random.randn(50, 20),
   columns=('col %d' % i for i in range(20)))

my_table._arrow_add_rows(df2)

my_chart = st._arrow_line_chart(df1)
my_chart._arrow_add_rows(df2)

my_chart = st._arrow_vega_lite_chart({
    'mark': 'line',
    'encoding': {'x': 'a', 'y': 'b'},
    'datasets': {
      'some_fancy_name': df1,  # <-- named dataset
     },
    'data': {'name': 'some_fancy_name'},
}),
my_chart._arrow_add_rows(some_fancy_name=df2)  # <-- name used as keyword""" .

"DESCRIPTION.The code generates two samples of size 3 from a multinomial distribution with 10 trials and probabilities [0.2, 0.3, 0.5]." <EXPLAINS> """CODE.import paddle

multinomial = paddle.distribution.Multinomial(10, paddle.to_tensor([0.2, 0.3, 0.5]))
print(multinomial.sample((2, 3)))



""" .

"DESCRIPTION.The code generates unique consecutive elements from a given tensor 'x'. It can return the unique elements, their inverse indices, and the counts of each unique element in the tensor." <EXPLAINS> """CODE.x = torch.tensor([1, 1, 2, 2, 3, 1, 1, 2])
    output = torch.unique_consecutive(x)
    output
    tensor([1, 2, 3, 1, 2])

    output, inverse_indices = torch.unique_consecutive(x, return_inverse=True)
    output
    tensor([1, 2, 3, 1, 2])
    inverse_indices
    tensor([0, 0, 1, 1, 2, 3, 3, 4])

    output, counts = torch.unique_consecutive(x, return_counts=True)
    output
    tensor([1, 2, 3, 1, 2])
    counts
    tensor([2, 2, 1, 2, 1])""" .

"DESCRIPTION.The code generates unique ids for the string 'dense' in the keras backend." <EXPLAINS> """CODE.keras.backend.get_uid('dense')
1
keras.backend.get_uid('dense')
2
""" .

"DESCRIPTION.The code generates unique layer names by appending a number to the base layer name 'dense'." <EXPLAINS> """CODE._unique_layer_name('dense')  # dense_1
_unique_layer_name('dense')  # dense_2
""" .

"DESCRIPTION.The code generates unique values from different types of Series data including integers, timestamps, and categorical data." <EXPLAINS> """CODE.pd.Series([2, 1, 3, 3], name='A').unique()
pd.Series([pd.Timestamp('2016-01-01') for _ in range(3)]).unique()
pd.Series([pd.Timestamp('2016-01-01', tz='US/Eastern') for _ in range(3)]).unique()
pd.Series(pd.Categorical(list('baabc'))).unique()
pd.Series(pd.Categorical(list('baabc'), categories=list('abc'), ordered=True)).unique()""" .

"DESCRIPTION.The code generates visualizations of the performance of a Ridge regression model trained on the provided training data and tested on the provided testing data." <EXPLAINS> "CODE.wandb.sklearn.plot_regressor(reg, X_train, X_test, y_train, y_test, 'Ridge')" .

"DESCRIPTION.The code generates zipped pairs of elements from different sets." <EXPLAINS> """CODE.a = { 1, 2, 3 }
b = { 4, 5, 6 }
c = { (7, 8), (9, 10), (11, 12) }
d = { 13, 14 }

Dataset.zip((a, b)) == { (1, 4), (2, 5), (3, 6) }
Dataset.zip((b, a)) == { (4, 1), (5, 2), (6, 3) }

Dataset.zip((a, b, c) == { (1, 4, (7, 8)),
                           (2, 5, (9, 10)),
                           (3, 6, (11, 12)) }

Dataset.zip((a, d)) == { (1, 13), (2, 14) }
""" .

"DESCRIPTION.The code gets a model from a plugin using the function `get_model_from_plugin()` and then calls the `training_step()` method on the obtained model." <EXPLAINS> """CODE.ref_model = ddp_plugin.get_model_from_plugin(model)
ref_model.training_step(...)""" .

"DESCRIPTION.The code gets the index and mask arrays using the 'get_indexer' method. It then uses the index to reorder the 'cur_values' array and sets some of the values to NaN based on the mask." <EXPLAINS> """CODE.indexer, mask = index.get_indexer(new_index)
new_values = cur_values.take(indexer)
new_values[-mask] = np.nan""" .

"DESCRIPTION.The code groups a range of numbers from 0 to 99 by the remainder of division by 3 and counts the frequency of each group." <EXPLAINS> """CODE.ray.data.range(100).groupby(lambda x: x % 3).count()
ray.data.from_items([
    {"A": x % 3, "B": x} for x in range(100)]).groupby(
    "A").count()""" .

"DESCRIPTION.The code groups the DataFrame 'df' by the column 'animal' and returns a list of unique values in the 'breed' column corresponding to each unique 'animal' value." <EXPLAINS> "CODE.ser = df.groupby('animal')['breed'].unique()" .

"DESCRIPTION.The code groups the data in a DataFrame by a specified level and then creates box plots for each group." <EXPLAINS> """CODE.grouped = df.groupby(level='lvl1')
boxplot_frame_groupby(grouped)

grouped = df.unstack(level='lvl1').groupby(level=0, axis=1)
boxplot_frame_groupby(grouped, subplots=False)""" .

"DESCRIPTION.The code implements a dynamic learning rate schedule using lambda decay in PaddlePaddle's fluid library. It starts with a learning rate of 0.5 and reduces it by a factor of 0.95 at each epoch. The code also includes a linear layer, optimizer, and training loop that updates the learning rate during each epoch according to the lambda decay schedule." <EXPLAINS> """CODE.learning_rate = 0.5
lr_lambda = lambda epoch: 0.95 ** epoch

learning_rate = 0.5
learning_rate = 0.475
learning_rate = 0.45125

import paddle.fluid as fluid
import numpy as np
with fluid.dygraph.guard():
    x = np.random.uniform(-1, 1, [10, 10]).astype("float32")
    linear = fluid.dygraph.Linear(10, 10)
    input = fluid.dygraph.to_variable(x)
    scheduler = fluid.dygraph.LambdaDecay(0.5, lr_lambda=lambda x: 0.95**x)
    adam = fluid.optimizer.Adam(learning_rate = scheduler, parameter_list = linear.parameters())

    for epoch in range(6):
        for batch_id in range(5):
            out = linear(input)
            loss = fluid.layers.reduce_mean(out)
            adam.minimize(loss)
        scheduler.epoch()

        print("epoch:%d, current lr is %f" .format(epoch, adam.current_step_lr()))
        # epoch:0, current lr is 0.5
        # epoch:1, current lr is 0.475
        # epoch:2, current lr is 0.45125""" .

"DESCRIPTION.The code implements a linear learning rate scheduler for an optimizer with a starting factor of 0.5 and a total of 4 iterations. In each epoch (100 in total), it trains and validates the model using the specified functions and updates the learning rate using the scheduler." <EXPLAINS> """CODE.scheduler = LinearLR(optimizer, start_factor=0.5, total_iters=4)
for epoch in range(100):
    train(...)
    validate(...)
    scheduler.step()""" .

"DESCRIPTION.The code implements a model that performs token embeddings on query and value inputs, applies a 1D convolutional layer with specified filters and kernel size, calculates attention scores between query and value sequences, and finally concatenates the global average pooling of query encoding and attention scores." <EXPLAINS> """CODE.query_input = tf.keras.Input(shape=(None,), dtype='int32')
value_input = tf.keras.Input(shape=(None,), dtype='int32')

token_embedding = tf.keras.layers.Embedding(max_tokens, dimension)
query_embeddings = token_embedding(query_input)
value_embeddings = token_embedding(value_input)

cnn_layer = tf.keras.layers.Conv1D(
    filters=100,
    kernel_size=4,
    padding='same')
query_seq_encoding = cnn_layer(query_embeddings)
value_seq_encoding = cnn_layer(value_embeddings)

query_value_attention_seq = tf.keras.layers.AdditiveAttention()(
    [query_seq_encoding, value_seq_encoding])

query_encoding = tf.keras.layers.GlobalAveragePooling1D()(
    query_seq_encoding)
query_value_attention = tf.keras.layers.GlobalAveragePooling1D()(
    query_value_attention_seq)

input_layer = tf.keras.layers.Concatenate()(
    [query_encoding, query_value_attention])
""" .

"DESCRIPTION.The code implements a neural network layer using dropout regularization with a dropout rate of 20% on a 2-dimensional input data of shape (5, 2). The input data is reshaped from a 1-dimensional array of values from 0 to 9 into a 2-dimensional array. The dropout layer randomly sets a fraction of input units to 0 during training to prevent overfitting. The output of the layer after applying dropout to the input data is displayed." <EXPLAINS> """CODE.tf.random.set_seed(0)
layer = tf.keras.layers.Dropout(.2, input_shape=(2,))
data = np.arange(10).reshape(5, 2).astype(np.float32)
print(data)
[[0. 1.]
 [2. 3.]
 [4. 5.]
 [6. 7.]
 [8. 9.]]
outputs = layer(data, training=True)
print(outputs)
tf.Tensor(
[[ 0.    1.25]
 [ 2.5   3.75]
 [ 5.    6.25]
 [ 7.5   8.75]
 [10.    0.  ]], shape=(5, 2), dtype=float32)""" .

"DESCRIPTION.The code implements a recurrent neural network (RNN) using a SimpleRNNCell with 4 units. The first usage of RNN is for generating an output with shape `[32, 4]`. The second usage of RNN is for generating both the whole sequence output with shape `[32, 10, 4]` and the final state with shape `[32, 4]`." <EXPLAINS> """CODE.rnn = tf.keras.layers.RNN(tf.keras.layers.SimpleRNNCell(4))

output = rnn(inputs)  # The output has shape `[32, 4]`.

rnn = tf.keras.layers.RNN(
    tf.keras.layers.SimpleRNNCell(4),
    return_sequences=True,
    return_state=True)

# whole_sequence_output has shape `[32, 10, 4]`.
# final_state has shape `[32, 4]`.
whole_sequence_output, final_state = rnn(inputs)
""" .

"DESCRIPTION.The code implements the functionality of a Trie data structure." <EXPLAINS> """CODE.trie = Trie()
trie.add("Hello åé")
trie.data
{"H": {"e": {"l": {"l": {"o": {" ": {"å": {"é": {"": 1}}}}}}}}

trie.add("Hello")
trie.data
{"H": {"e": {"l": {"l": {"o": {"": 1, " ": {"å": {"é": {"": 1}}}}}}}}
""" .

"DESCRIPTION.The code imports the \"Class\" class from the \"module\" module." <EXPLAINS> "CODE.ImportedBackend(\"module.Class\")" .

"DESCRIPTION.The code imports the MyClass class from the module.submodule package in Python." <EXPLAINS> """CODE.MyClass = import_class("module.submodule.MyClass")
from module.submodule import MyClass""" .

"DESCRIPTION.The code imports the `json` module and infers the module name using the Jedi library, then prints the module name." <EXPLAINS> """CODE.from jedi import Script
source = 'import json'
script = Script(source, path='example.py')
d = script.infer()[0]
print(d.module_name)  # doctest: +ELLIPSIS""" .

"DESCRIPTION.The code imports the paddle.fluid module and loads a custom op library file named custom_op.so." <EXPLAINS> """CODE.import paddle.fluid as fluid
fluid.load_op_library('custom_op.so')""" .

"DESCRIPTION.The code increases the value of the counter in the app state by 1 and prints a message indicating the updated value of the counter." <EXPLAINS> """CODE.import param
app = AppStateWatcher()
app.state.counter = 1

@param.depends(app.param.state, watch=True)
def update(state):
    print(f"The counter was updated to {state.counter}")

app.state.counter += 1""" .

"DESCRIPTION.The code infers the module name of the import statement \"import json\" in the provided source code file." <EXPLAINS> """CODE.from jedi import Script
source = 'import json'
script = Script(source, path='example.py')
d = script.infer()[0]
print(d.module_name)  # doctest: +ELLIPSIS""" .

"DESCRIPTION.The code initializes Cauchy distributions with different parameters and calculates the log probability of the given input values. It demonstrates how to work with Cauchy distributions in PaddlePaddle." <EXPLAINS> """CODE.import paddle
from paddle.distribution import Cauchy

# init Cauchy with float
rv = Cauchy(loc=0.1, scale=1.2)
print(rv.log_prob(paddle.to_tensor(1.5))

# broadcast to value
rv = Cauchy(loc=0.1, scale=1.2)
print(rv.log_prob(paddle.to_tensor([1.5, 5.1]))

# init Cauchy with N-Dim tensor
rv = Cauchy(loc=paddle.to_tensor([0.1, 0.1]), scale=paddle.to_tensor([1.0, 2.0]))
print(rv.log_prob(paddle.to_tensor([1.5, 5.1]))

# init Cauchy with N-Dim tensor with broadcast
rv = Cauchy(loc=paddle.to_tensor(0.1), scale=paddle.to_tensor([1.0, 2.0]))
print(rv.log_prob(paddle.to_tensor([1.5, 5.1]))""" .

"DESCRIPTION.The code initializes Cauchy distributions with specified location and scale parameters, and then calculates the entropy of the distributions." <EXPLAINS> """CODE.import paddle
from paddle.distribution import Cauchy

# init Cauchy with float
rv = Cauchy(loc=0.1, scale=1.2)
print(rv.entropy())

# init Cauchy with N-Dim tensor
rv = Cauchy(loc=paddle.to_tensor(0.1), scale=paddle.to_tensor([1.0, 2.0]))
print(rv.entropy())""" .

"DESCRIPTION.The code initializes JAX configuration using absl flags and runs the main function when the script is executed." <EXPLAINS> """CODE.from absl import app
import jax
...

if __name__ == '__main__':
  jax.config.config_with_absl()
  app.run(main)
""" .

"DESCRIPTION.The code initializes VectorExponential distributions with different parameters and computes the probability density function (pdf) of observations based on the specified distribution parameters." <EXPLAINS> """CODE.ds = tf.contrib.distributions
la = tf.contrib.linalg

# Initialize a single 2-variate VectorExponential, supported on
# {(x, y) in R^2 : x > 0, y > 0}.
mat = [[1.0, 0.1],
       [0.1, 1.0]]

vex = ds.VectorExponentialLinearOperator(
    scale=la.LinearOperatorFullMatrix(mat))

# Compute the pdf of an`R^2` observation; return a scalar.
vex.prob([1., 2.]).eval()  # shape: []

# Initialize a 2-batch of 3-variate Vector Exponential's.
mu = [[1., 2, 3],
      [1., 0, 0]]              # shape: [2, 3]
scale_diag = [[1., 2, 3],
              [0.5, 1, 1.5]]     # shape: [2, 3]

vex = ds.VectorExponentialLinearOperator(
    loc=mu,
    scale=la.LinearOperatorDiag(scale_diag))

# Compute the pdf of two `R^3` observations; return a length-2 vector.
x = [[1.9, 2.2, 3.1],
     [10., 1.0, 9.0]]     # shape: [2, 3]
vex.prob(x).eval()    # shape: [2]
""" .

"DESCRIPTION.The code initializes a 2x3 matrix with LeCun normal distribution using JAX." <EXPLAINS> """CODE.import jax, jax.numpy as jnp
initializer = jax.nn.initializers.lecun_normal()
initializer(jax.random.PRNGKey(42), (2, 3), jnp.float32)  # doctest: +SKIP
""" .

"DESCRIPTION.The code initializes a 2x3 tensor with zeros, counts the number of parameters in the tensor, and evaluates the tensor." <EXPLAINS> """CODE.    kvar = K.zeros((2,3))
    K.count_params(kvar)
    K.eval(kvar)
""" .

"DESCRIPTION.The code initializes a 3x4 tensor filled with ones using Keras backend and evaluates the tensor values." <EXPLAINS> """CODE.from keras import backend as K
kvar = K.ones((3,4))
K.eval(kvar)
array([[ 1.,  1.,  1.,  1.],
       [ 1.,  1.,  1.,  1.],
       [ 1.,  1.,  1.,  1.]], dtype=float32)
""" .

"DESCRIPTION.The code initializes a 3x4 tensor filled with ones using the Keras backend functionality." <EXPLAINS> """CODE.from keras import backend as K
kvar = K.ones((3,4))
K.eval(kvar)
array([[ 1.,  1.,  1.,  1.],
       [ 1.,  1.,  1.,  1.],
       [ 1.,  1.,  1.,  1.]], dtype=float32)
""" .

"DESCRIPTION.The code initializes a 3x4 tensor filled with zeros using the Keras backend." <EXPLAINS> """CODE.from keras import backend as K
kvar = K.zeros((3,4))
K.eval(kvar)
array([[ 0.,  0.,  0.,  0.],
       [ 0.,  0.,  0.,  0.],
       [ 0.,  0.,  0.,  0.]], dtype=float32)
""" .

"DESCRIPTION.The code initializes a 5-dimensional tensor with shape (2, 1, 2, 1, 3), then applies 3D upsampling with a factor of 2 along each dimension, resulting in a tensor with shape (2, 2, 4, 2, 3)." <EXPLAINS> """CODE.input_shape = (2, 1, 2, 1, 3)
x = tf.constant(1, shape=input_shape)
y = tf.keras.layers.UpSampling3D(size=2)(x)
print(y.shape)
(2, 2, 4, 2, 3)""" .

"DESCRIPTION.The code initializes a BERT model configuration using the 'bert-base-uncased' pretrained model, and then creates a multiple choice model using that configuration." <EXPLAINS> """CODE.config = BertConfig.from_pretrained('bert-base-uncased')
model = AutoModelForMulitpleChoice.from_config(config)""" .

"DESCRIPTION.The code initializes a BERT tokenizer object using the \"bert-base-uncased\" pre-trained model." <EXPLAINS> """CODE.from transformers import TFBertTokenizer

tf_tokenizer = TFBertTokenizer.from_pretrained("bert-base-uncased")
""" .

"DESCRIPTION.The code initializes a BERT tokenizer using the \"bert-base-uncased\" pre-trained model." <EXPLAINS> """CODE.from transformers import AutoTokenizer, TFBertTokenizer

tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")
tf_tokenizer = TFBertTokenizer.from_tokenizer(tokenizer)
""" .

"DESCRIPTION.The code initializes a BERT-based encoder-decoder model with separate configurations for the encoder and decoder parts." <EXPLAINS> """CODE.from transformers import BertConfig, EncoderDecoderConfig, EncoderDecoderModel

# Initializing a BERT bert-base-uncased style configuration
config_encoder = BertConfig()
config_decoder = BertConfig()

config = EncoderDecoderConfig.from_encoder_decoder_configs(config_encoder, config_decoder)

# Initializing a Bert2Bert model from the bert-base-uncased style configurations
model = EncoderDecoderModel(config=config)

# Accessing the model configuration
config_encoder = model.config.encoder
config_decoder  = model.config.decoder""" .

"DESCRIPTION.The code initializes a BLIP-2 (Bidirectional Long Input Passage for Question Answering) model with random weights using a specific configuration." <EXPLAINS> """CODE.from transformers import Blip2QFormerConfig, Blip2QFormerModel

# Initializing a BLIP-2 Salesforce/blip2-opt-2.7b style configuration
configuration = Blip2QFormerConfig()

# Initializing a model (with random weights) from the Salesforce/blip2-opt-2.7b style configuration
model = Blip2QFormerModel(configuration)
# Accessing the model configuration
configuration = model.config
""" .

"DESCRIPTION.The code initializes a BROS model with a specific configuration and then accesses the model's configuration." <EXPLAINS> """CODE.from transformers import BrosConfig, BrosModel

# Initializing a BROS jinho8345/bros-base-uncased style configuration
configuration = BrosConfig()

# Initializing a model from the jinho8345/bros-base-uncased style configuration
model = BrosModel(configuration)

# Accessing the model configuration
configuration = model.config
""" .

"DESCRIPTION.The code initializes a BigModel object with an empty init function and then loads the model's state from a saved checkpoint file \"checkpoint.pt\"." <EXPLAINS> """CODE.with _EmptyInit():
    model = BigModel()
model.load_state_dict(torch.load("checkpoint.pt"))""" .

"DESCRIPTION.The code initializes a BioGPT model and configuration, accessing the model's configuration." <EXPLAINS> """CODE.from transformers import BioGptModel, BioGptConfig

# Initializing a BioGPT microsoft/biogpt style configuration
configuration = BioGptConfig()

# Initializing a model from the microsoft/biogpt style configuration
model = BioGptModel(configuration)

# Accessing the model configuration
configuration = model.config
""" .

"DESCRIPTION.The code initializes a Blip2Vision model with random weights using a specific configuration and accesses the model's configuration." <EXPLAINS> """CODE.from transformers import Blip2VisionConfig, Blip2VisionModel

# Initializing a Blip2VisionConfig with Salesforce/blip2-opt-2.7b style configuration
configuration = Blip2VisionConfig()

# Initializing a Blip2VisionModel (with random weights) from the Salesforce/blip2-opt-2.7b style configuration
model = Blip2VisionModel(configuration)

# Accessing the model configuration
configuration = model.config
""" .

"DESCRIPTION.The code initializes a BlipText model with a configuration based on the Salesforce/blip-vqa-base style, then accesses the configuration of the model." <EXPLAINS> """CODE.from transformers import BlipTextConfig, BlipTextModel

# Initializing a BlipTextConfig with Salesforce/blip-vqa-base style configuration
configuration = BlipTextConfig()

# Initializing a BlipTextModel (with random weights) from the Salesforce/blip-vqa-base style configuration
model = BlipTextModel(configuration)

# Accessing the model configuration
configuration = model.config
""" .

"DESCRIPTION.The code initializes a Bloom model by first creating a Bloom configuration, then creating a Bloom model using that configuration. Finally, the code accesses the configuration of the model." <EXPLAINS> """CODE.from transformers import BloomModel, BloomConfig

# Initializing a Bloom configuration
configuration = BloomConfig()

# Initializing a model from the configuration
model = BloomModel(configuration)

# Accessing the model configuration
configuration = model.config
""" .

"DESCRIPTION.The code initializes a BridgeTower model using a BridgeTower/bridgetower-base style configuration, then accesses the configuration of the model." <EXPLAINS> """CODE.from transformers import BridgeTowerModel, BridgeTowerConfig

# Initializing a BridgeTower BridgeTower/bridgetower-base style configuration
configuration = BridgeTowerConfig()

# Initializing a model from the BridgeTower/bridgetower-base style configuration
model = BridgeTowerModel(configuration)

# Accessing the model configuration
configuration = model.config
""" .

"DESCRIPTION.The code initializes a CLAP text model and accesses its configuration." <EXPLAINS> """CODE.from transformers import ClapTextConfig, ClapTextModel

# Initializing a CLAP text configuration
configuration = ClapTextConfig()

# Initializing a model (with random weights) from the configuration
model = ClapTextModel(configuration)

# Accessing the model configuration
configuration = model.config
""" .

"DESCRIPTION.The code initializes a CLIPImageProcessor object with different configurations and options, such as loading a pretrained model, saving a model, loading from a specific file path, and setting normalization options. It also includes assertions to check the correctness of the settings." <EXPLAINS> """CODE.image_processor = CLIPImageProcessor.from_pretrained(
    "openai/clip-vit-base-patch32"
)  # Download image_processing_config from huggingface.co and cache.
image_processor = CLIPImageProcessor.from_pretrained(
    "./test/saved_model/"
)  # E.g. image processor (or model) was saved using *save_pretrained('./test/saved_model/')*
image_processor = CLIPImageProcessor.from_pretrained("./test/saved_model/preprocessor_config.json")
image_processor = CLIPImageProcessor.from_pretrained(
    "openai/clip-vit-base-patch32", do_normalize=False, foo=False
)
assert image_processor.do_normalize is False
image_processor, unused_kwargs = CLIPImageProcessor.from_pretrained(
    "openai/clip-vit-base-patch32", do_normalize=False, foo=False, return_unused_kwargs=True
)
assert image_processor.do_normalize is False
assert unused_kwargs == {"foo": False}
""" .

"DESCRIPTION.The code initializes a CUDA CTC decoder with specified parameters, such as a vocabulary file and a blank skip threshold. It then uses the decoder to decode the log probabilities and encoder output lengths to produce a list of hypotheses for each input batch." <EXPLAINS> """CODE.decoder = cuda_ctc_decoder(
    vocab_file="tokens.txt",
    blank_skip_threshold=0.95,
)
results = decoder(log_probs, encoder_out_lens) # List of shape (B, nbest) of Hypotheses""" .

"DESCRIPTION.The code initializes a CartPole environment and sets a maximum episode steps limit of 1000." <EXPLAINS> """CODE.from gym.envs.classic_control import CartPoleEnv
from gym.wrappers import TimeLimit
env = CartPoleEnv()
env = TimeLimit(env, max_episode_steps=1000)""" .

"DESCRIPTION.The code initializes a CartPole-v1 environment and applies a reward transformation function to scale the rewards by a factor of 0.01. It then resets the environment, takes a random action, and retrieves the modified reward value, which is then returned as 0.01." <EXPLAINS> """CODE.import gym
env = gym.make('CartPole-v1')
env = TransformReward(env, lambda r: 0.01*r)
env.reset()
observation, reward, done, info = env.step(env.action_space.sample())
reward
0.01""" .

"DESCRIPTION.The code initializes a Cauchy distribution with a specific location and scale parameter. It then computes the entropy of the distribution and prints the result." <EXPLAINS> """CODE.import paddle
from paddle.distribution import Cauchy

# init Cauchy with float
rv = Cauchy(loc=0.1, scale=1.2)
print(rv.entropy())
# Tensor(shape=[], dtype=float32, place=Place(cpu), stop_gradient=True,
#        2.71334577)

# init Cauchy with N-Dim tensor
rv = Cauchy(loc=paddle.to_tensor(0.1), scale=paddle.to_tensor([1.0, 2.0]))
print(rv.entropy())
# Tensor(shape=[2], dtype=float32, place=Place(cpu), stop_gradient=True,
#        [2.53102422, 3.22417140])""" .

"DESCRIPTION.The code initializes a Cauchy distribution with different types of inputs (float, 0-D tensor, N-D tensor) and samples from the distribution to generate random values. The shape of the samples varies depending on the input dimensions." <EXPLAINS> """CODE.import paddle
from paddle.distribution import Cauchy

# init Cauchy with float
rv = Cauchy(loc=0.1, scale=1.2)
print(rv.rsample([10]).shape)

# init Cauchy with 0-Dim tensor
rv = Cauchy(loc=paddle.full((), 0.1), scale=paddle.full((), 1.2))
print(rv.rsample([10]).shape)

# init Cauchy with N-Dim tensor
rv = Cauchy(loc=paddle.to_tensor(0.1), scale=paddle.to_tensor([1.0, 2.0]))
print(rv.rsample([10]).shape)

# sample 2-Dim data
rv = Cauchy(loc=0.1, scale=1.2)
print(rv.rsample([10, 2]).shape)

rv = Cauchy(loc=paddle.to_tensor(0.1), scale=paddle.to_tensor([1.0, 2.0]))
print(rv.rsample([10, 2]).shape)""" .

"DESCRIPTION.The code initializes a ClapAudioConfig object with specific style configuration and then initializes a ClapAudioModel object with random weights from the configuration. Finally, it accesses the configuration of the model." <EXPLAINS> """CODE.from transformers import ClapAudioConfig, ClapAudioModel

# Initializing a ClapAudioConfig with laion/clap-htsat-fused style configuration
configuration = ClapAudioConfig()

# Initializing a ClapAudioModel (with random weights) from the laion/clap-htsat-fused style configuration
model = ClapAudioModel(configuration)

# Accessing the model configuration
configuration = model.config
""" .

"DESCRIPTION.The code initializes a ClvpTokenizer object and then uses it to tokenize the input text \"Hello world\" and \" Hello world\", returning the input ids as lists of integers." <EXPLAINS> """CODE.from transformers import ClvpTokenizer

tokenizer = ClvpTokenizer.from_pretrained("susnato/clvp_dev")
tokenizer("Hello world")["input_ids"]
[62, 84, 28, 2, 179, 79]

tokenizer(" Hello world")["input_ids"]
[2, 62, 84, 28, 2, 179, 79]
""" .

"DESCRIPTION.The code initializes a CodeGen model with a 6B configuration and then accesses the model configuration." <EXPLAINS> """CODE.from transformers import CodeGenModel, CodeGenConfig

# Initializing a CodeGen 6B configuration
configuration = CodeGenConfig()

# Initializing a model from the configuration
model = CodeGenModel(configuration)

# Accessing the model configuration
configuration = model.config
""" .

"DESCRIPTION.The code initializes a Conditional DETR model using a configuration similar to the microsoft/conditional-detr-resnet-50 style and then accesses the configuration of the initialized model." <EXPLAINS> """CODE.from transformers import ConditionalDetrModel, ConditionalDetrConfig

# Initializing a Conditional DETR microsoft/conditional-detr-resnet-50 style configuration
configuration = ConditionalDetrConfig()

# Initializing a model from the microsoft/conditional-detr-resnet-50 style configuration
model = ConditionalDetrModel(configuration)

# Accessing the model configuration
configuration = model.config
""" .

"DESCRIPTION.The code initializes a Counter object with the key \"name\", sets the default tags for the Counter object to be {\"a\": \"b\"}, and then reassigns the Counter object with the key \"name\" and default tags {\"a\": \"b\"}." <EXPLAINS> """CODE.counter = Counter("name")
counter2 = counter.set_default_tags({"a": "b"})
counter = Counter("name").set_default_tags({"a": "b"})""" .

"DESCRIPTION.The code initializes a Cvt (Convoluted Vision Transformer) model with a specific configuration and then accesses the configuration of the model." <EXPLAINS> """CODE.from transformers import CvtModel, CvtConfig

# Initializing a Cvt msft/cvt style configuration
configuration = CvtConfig()

# Initializing a model from the msft/cvt style configuration
model = CvtModel(configuration)

# Accessing the model configuration
configuration = model.config
""" .

"DESCRIPTION.The code initializes a DINOv2 configuration and model, then retrieves the configuration from the model." <EXPLAINS> """CODE.from transformers import Dinov2Config, Dinov2Model
configuration = Dinov2Config()
model = Dinov2Model(configuration)
configuration = model.config
""" .

"DESCRIPTION.The code initializes a DPT (Data Preparation Tool) model using a large style configuration and accesses the model configuration." <EXPLAINS> """CODE.from transformers import DPTModel, DPTConfig

# Initializing a DPT dpt-large style configuration
configuration = DPTConfig()

# Initializing a model from the dpt-large style configuration
model = DPTModel(configuration)

# Accessing the model configuration
configuration = model.config
""" .

"DESCRIPTION.The code initializes a Data2VecVision model with a base configuration of data2vec_vision-base-patch16-224-in22k and accesses its configuration." <EXPLAINS> """CODE.from transformers import Data2VecVisionModel, Data2VecVisionConfig

# Initializing a Data2VecVision data2vec_vision-base-patch16-224-in22k style configuration
configuration = Data2VecVisionConfig()

# Initializing a model from the data2vec_vision-base-patch16-224-in22k style configuration
model = Data2VecVisionModel(configuration)

# Accessing the model configuration
configuration = model.config
""" .

"DESCRIPTION.The code initializes a Deep Q-Network (DQN) with a neural network architecture defined as a Sequential model with specific input and output dimensions." <EXPLAINS> """CODE.DQN(10, 5)
DQN(
  (net): Sequential(...)
)""" .

"DESCRIPTION.The code initializes a Donut model using a DonutSwinConfig configuration and then accesses the model's configuration." <EXPLAINS> """CODE.from transformers import DonutSwinConfig, DonutSwinModel

# Initializing a Donut naver-clova-ix/donut-base style configuration
configuration = DonutSwinConfig()

# Randomly initializing a model from the naver-clova-ix/donut-base style configuration
model = DonutSwinModel(configuration)

# Accessing the model configuration
configuration = model.config
""" .

"DESCRIPTION.The code initializes a Double Deep Q-Network (DQN) with two neural networks (online network and target network) for training on the CartPole-v1 environment." <EXPLAINS> """CODE.DQNLightning(env="CartPole-v1")
DQNLightning(
  (net): DQN(
    (net): Sequential(...)
  )
  (target_net): DQN(
    (net): Sequential(...)
  )
)""" .

"DESCRIPTION.The code initializes a Draft202012Validator object and uses it to evolve a schema object with the type \"number\"." <EXPLAINS> """CODE.validator = Draft202012Validator({})
validator.evolve(schema={"type": "number"})""" .

"DESCRIPTION.The code initializes a FastSpeech2ConformerHifiGan model with random weights and accesses its configuration." <EXPLAINS> """CODE.from transformers import FastSpeech2ConformerHifiGan, FastSpeech2ConformerHifiGanConfig

# Initializing a FastSpeech2ConformerHifiGan configuration
configuration = FastSpeech2ConformerHifiGanConfig()

# Initializing a model (with random weights) from the configuration
model = FastSpeech2ConformerHifiGan(configuration)

# Accessing the model configuration
configuration = model.config
""" .

"DESCRIPTION.The code initializes a FlavaConfig object with style configuration, then creates a FlavaModel and FlavaForPreTraining model based on the style configuration. Finally, it accesses the configuration of both models." <EXPLAINS> """CODE.from transformers import FlavaModel, FlavaForPreTraining, FlavaConfig

# Initializing a FlavaConfig with style configuration
configuration = FlavaConfig()

# Initializing a FlavaModel and FlavaForPreTraining model from the style configuration
model = FlavaModel(configuration)
model_pre = FlavaForPreTraining(configuration)

# Accessing the model configuration
configuration = model.config
configuration_pre = model_pre.config
""" .

"DESCRIPTION.The code initializes a FlavaTextModel with a style configuration and then accesses the model's configuration." <EXPLAINS> """CODE.from transformers import FlavaTextModel, FlavaTextConfig

# Initializing a FlavaTextModel with  style configuration
configuration = FlavaTextConfig()

# Initializing a FlavaTextConfig from the  style configuration
model = FlavaTextModel(configuration)

# Accessing the model configuration
configuration = model.config
""" .

"DESCRIPTION.The code initializes a Flax Encoder-Decoder model using a BERT encoder and a GPT-2 decoder. It then tokenizes a text input using a BERT tokenizer, encodes the input text using the tokenizer, and generates encoder outputs for the input text." <EXPLAINS> """CODE.model = FlaxEncoderDecoderModel.from_encoder_decoder_pretrained('bert-base-cased', 'gpt2')
tokenizer = BertTokenizer.from_pretrained('bert-base-cased')
text = "My friends are cool but they eat too many carbs."
input_ids = tokenizer.encode(text, return_tensors='np')
encoder_outputs = model.encode(input_ids)""" .

"DESCRIPTION.The code initializes a FlaxBart model for conditional generation and a tokenizer for the model. It encodes the input text using the tokenizer and the model, then decodes the input to generate an output. Lastly, it retrieves the last hidden states of the decoder." <EXPLAINS> """CODE.model = FlaxBartForConditionalGeneration.from_pretrained('facebook/bart-large-cnn')
tokenizer = BartTokenizer.from_pretrained('facebook/bart-large-cnn')
text = "My friends are cool but they eat too many carbs."
inputs = tokenizer(text, max_length=1024, return_tensors='jax')
encoder_outputs = model.encode(**inputs)
decoder_start_token_id = model.config.decoder_start_token_id
decoder_input_ids = jnp.ones((inputs.input_ids.shape[0], 1), dtype="i4") * decoder_start_token_id
outputs = model.decode(decoder_input_ids, encoder_outputs)
last_decoder_hidden_states = outputs.last_hidden_state""" .

"DESCRIPTION.The code initializes a FlaxBlenderbotSmallForConditionalGeneration model and a BlenderbotSmallTokenizer for text generation tasks. It then encodes the input text using the tokenizer and the model, generates decoder input ids, and finally decodes the input to output logits for further processing." <EXPLAINS> """CODE.model = FlaxBlenderbotSmallForConditionalGeneration.from_pretrained('facebook/blenderbot_small-90M')
tokenizer = BlenderbotSmallTokenizer.from_pretrained('facebook/blenderbot_small-90M')
text = "My friends are cool but they eat too many carbs."
inputs = tokenizer(text, max_length=1024, return_tensors='np')
encoder_outputs = model.encode(**inputs)
decoder_start_token_id = model.config.decoder_start_token_id
decoder_input_ids = jnp.ones((inputs.input_ids.shape[0], 1), dtype="i4") * decoder_start_token_id
outputs = model.decode(decoder_input_ids, encoder_outputs)
logits = outputs.logits""" .

"DESCRIPTION.The code initializes a FlaxEncoderDecoderModel using a pre-trained BERT model as the encoder and a pre-trained GPT-2 model as the decoder. It then uses a pre-trained BERT tokenizer to tokenize a given text and encode it. The encoded input is passed to the encoder to obtain encoder outputs. The decoder starts with a special token ID and generates an output sequence using the encoder outputs. Finally, the logits of the generated output sequence are obtained." <EXPLAINS> """CODE.from transformers import FlaxEncoderDecoderModel, BertTokenizer
import jax.numpy as jnp

model = FlaxEncoderDecoderModel.from_encoder_decoder_pretrained('bert-base-cased', 'gpt2')

tokenizer = BertTokenizer.from_pretrained('bert-base-cased')

text = "My friends are cool but they eat too many carbs."
input_ids = tokenizer.encode(text, max_length=1024, return_tensors='np')
encoder_outputs = model.encode(input_ids)

decoder_start_token_id = model.config.decoder.bos_token_id
decoder_input_ids = jnp.ones((input_ids.shape[0], 1), dtype="i4") * decoder_start_token_id

outputs = model.decode(decoder_input_ids, encoder_outputs)
logits = outputs.logits""" .

"DESCRIPTION.The code initializes a FlaxPegasusForConditionalGeneration model and a PegasusTokenizer for text generation. It then encodes the input text using the model and decodes it to generate output logits." <EXPLAINS> """CODE.model = FlaxPegasusForConditionalGeneration.from_pretrained('google/pegasus-large')
tokenizer = PegasusTokenizer.from_pretrained('google/pegasus-large')
text = "My friends are cool but they eat too many carbs."
inputs = tokenizer(text, max_length=1024, return_tensors='np')
encoder_outputs = model.encode(**inputs)
decoder_start_token_id = model.config.decoder_start_token_id
decoder_input_ids = jnp.ones((inputs.input_ids.shape[0], 1), dtype="i4") * decoder_start_token_id
outputs = model.decode(decoder_input_ids, encoder_outputs)
logits = outputs.logits""" .

"DESCRIPTION.The code initializes a FlaxVisionEncoderDecoder model using pre-trained ViT and GPT2 models. It then processes an image to extract pixel values using a ViT feature extractor, encodes the pixel values using the model's encoder, generates decoder input tokens, and finally decodes the input to obtain logits." <EXPLAINS> """CODE.from transformers import FlaxVisionEncoderDecoderModel
import jax.numpy as jnp
from PIL import Image
import requests

url = 'http://images.cocodataset.org/val2017/000000039769.jpg'
image = Image.open(requests.get(url, stream=True).raw)

feature_extractor = ViTFeatureExtractor.from_pretrained('google/vit-base-patch16-224-in21k')

# initialize a vit-gpt2 from pretrained ViT and GPT2 models. Note that the cross-attention layers will be randomly initialized
model = FlaxVisionEncoderDecoderModel.from_encoder_decoder_pretrained('vit', 'gpt2')

pixel_values = feature_extractor(images=image, return_tensors="np").pixel_values
encoder_outputs = model.encode(pixel_values)

decoder_start_token_id = model.config.decoder.bos_token_id
decoder_input_ids = jnp.ones((pixel_values.shape[0], 1), dtype="i4") * decoder_start_token_id

outputs = model.decode(decoder_input_ids, encoder_outputs)
logits = outputs.logits""" .

"DESCRIPTION.The code initializes a FlaxVisionEncoderDecoderModel using a pre-trained encoder-decoder model (ViT-base-patch16-224-in21k and GPT-2), saves the model weights to a specified directory, and then reloads the model from the saved directory." <EXPLAINS> """CODE.from transformers import FlaxVisionEncoderDecoderModel
model = FlaxVisionEncoderDecoderModel.from_encoder_decoder_pretrained('google/vit-base-patch16-224-in21k', 'gpt2')
model.save_pretrained("./vit-gpt2")
model = FlaxVisionEncoderDecoderModel.from_pretrained("./vit-gpt2")
""" .

"DESCRIPTION.The code initializes a GIT microsoft/git-base style configuration and a model with random weights based on that configuration. Then it accesses the configuration of the model." <EXPLAINS> """CODE.from transformers import GitConfig, GitModel

# Initializing a GIT microsoft/git-base style configuration
configuration = GitConfig()

# Initializing a model (with random weights) from the microsoft/git-base style configuration
model = GitModel(configuration)

# Accessing the model configuration
configuration = model.config
""" .

"DESCRIPTION.The code initializes a GLPN model and configuration based on the kaist/gdpdepth-kitti style, and then accesses the model configuration." <EXPLAINS> """CODE.from transformers import GLPNModel, GLPNConfig

# Initializing a GLPN kaist/gdpdepth-kitti style configuration
configuration = GLPNConfig()

# Initializing a model from the kaist/gdpdepth-kitti style configuration
model = GLPNModel(configuration)

# Accessing the model configuration
configuration = model.config
""" .

"DESCRIPTION.The code initializes a GPTNeoX model with a gpt-neox-20b style configuration and accesses the model configuration." <EXPLAINS> """CODE.from transformers import GPTNeoXModel, GPTNeoXConfig

# Initializing a GPTNeoX gpt-neox-20b style configuration
configuration = GPTNeoXConfig()

# Initializing a model from the gpt-neox-20b style configuration
model = GPTNeoXModel(configuration)

# Accessing the model configuration
configuration = model.config
""" .

"DESCRIPTION.The code initializes a GPTNeoXJapaneseModel with a specific configuration and then accesses the configuration of the model." <EXPLAINS> """CODE.from transformers import GPTNeoXJapaneseModel, GPTNeoXJapaneseConfig

# Initializing a GPTNeoXJapanese gpt-neox-japanese-2.7b style configuration
configuration = GPTNeoXJapaneseConfig()

# Initializing a model from the gpt-neox-japanese-2.7b style configuration
model = GPTNeoXJapaneseModel(configuration)

# Accessing the model configuration
configuration = model.config
""" .

"DESCRIPTION.The code initializes a GRU decoder cell with a hidden size of 128 and uses a training helper to decode the target embedding sequence based on the target sequence length provided. The decoder processes the input sequence dynamically and returns the outputs of the decoding process." <EXPLAINS> """CODE.import paddle.fluid as fluid
import paddle.fluid.layers as layers
trg_emb = fluid.data(name="trg_emb",
                     shape=[None, None, 128],
                     dtype="float32")
trg_seq_length = fluid.data(name="trg_seq_length",
                            shape=[None],
                            dtype="int64")
helper = layers.TrainingHelper(trg_emb, trg_seq_length)
decoder_cell = layers.GRUCell(hidden_size=128)
decoder = layers.BasicDecoder(decoder_cell, helper)
outputs = layers.dynamic_decode(
    decoder,
    inits=decoder_cell.get_initial_states(trg_emb),
    is_test=False)""" .

"DESCRIPTION.The code initializes a GlorotNormal initializer and uses it to initialize a Dense layer with 3 units." <EXPLAINS> """CODE.initializer = tf.keras.initializers.GlorotNormal()
values = initializer(shape=(2, 2))

initializer = tf.keras.initializers.GlorotNormal()
layer = tf.keras.layers.Dense(3, kernel_initializer=initializer)""" .

"DESCRIPTION.The code initializes a GlorotUniform initializer and uses it to create a Dense layer with 3 units in TensorFlow." <EXPLAINS> """CODE.initializer = tf.keras.initializers.GlorotUniform()
values = initializer(shape=(2, 2))

initializer = tf.keras.initializers.GlorotUniform()
layer = tf.keras.layers.Dense(3, kernel_initializer=initializer)""" .

"DESCRIPTION.The code initializes a Graphormer model for graph classification using a specific configuration, then accesses the configuration of the model." <EXPLAINS> """CODE.from transformers import GraphormerForGraphClassification, GraphormerConfig

# Initializing a Graphormer graphormer-base-pcqm4mv2 style configuration
configuration = GraphormerConfig()

# Initializing a model from the graphormer-base-pcqm4mv1 style configuration
model = GraphormerForGraphClassification(configuration)

# Accessing the model configuration
configuration = model.config
""" .

"DESCRIPTION.The code initializes a HeUniform initializer and uses it to define the initial values for a 2x2 tensor. Then it initializes a Dense layer with 3 units using the HeUniform initializer for the kernel weights." <EXPLAINS> """CODE.initializer = tf.keras.initializers.HeUniform()
values = initializer(shape=(2, 2))

initializer = tf.keras.initializers.HeUniform()
layer = tf.keras.layers.Dense(3, kernel_initializer=initializer)""" .

"DESCRIPTION.The code initializes a HeUniform initializer for weight values and assigns it to a layer in a neural network model with 3 output units." <EXPLAINS> """CODE.initializer = tf.keras.initializers.HeUniform()
values = initializer(shape=(2, 2))

initializer = tf.keras.initializers.HeUniform()
layer = tf.keras.layers.Dense(3, kernel_initializer=initializer)""" .

"DESCRIPTION.The code initializes a Hugging Face Agent object with a specified model URL and runs the agent to generate an output based on the input query \"Draw me a picture of rivers and lakes\"." <EXPLAINS> """CODE.from transformers import HfAgent

agent = HfAgent("https://api-inference.huggingface.co/models/bigcode/starcoder")
agent.run("Draw me a picture of rivers and lakes")
""" .

"DESCRIPTION.The code initializes a HyperSelection object with a shape of (10, 20) representing 200 points. It then updates the selection by setting a range of columns to False, reducing the number of points to 100. Next, it sets a specific column to True, increasing the number of points to 110. Finally, it updates the selection using XOR operation, resulting in 90 points." <EXPLAINS> """CODE.sel = HyperSelection((10,20))  # Initially 200 points
sel[:,5:15] = False            # Now 100 points
sel[:,10]   = True             # Now 110 points
sel[...]    = XOR              # Now 90 points
""" .

"DESCRIPTION.The code initializes a Idefics idefics-9b style configuration, then initializes a model based on that configuration, and finally accesses the configuration of the model." <EXPLAINS> """CODE.from transformers import IdeficsModel, IdeficsConfig

# Initializing a Idefics idefics-9b style configuration
configuration = IdeficsConfig()

# Initializing a model from the idefics-9b style configuration
model = IdeficsModel(configuration)

# Accessing the model configuration
configuration = model.config
""" .

"DESCRIPTION.The code initializes a Kosmos-2 model with a specific configuration and then accesses the configuration of the model." <EXPLAINS> """CODE.from transformers import Kosmos2Config, Kosmos2Model

# Initializing a Kosmos-2 kosmos-2-patch14-224 style configuration
configuration = Kosmos2Config()

# Initializing a model (with random weights) from the kosmos-2-patch14-224 style configuration
model = Kosmos2Model(configuration)

# Accessing the model configuration
configuration = model.config
""" .

"DESCRIPTION.The code initializes a LayoutLMv3 model with a configuration based on the microsoft/layoutlmv3-base style, and then accesses the model's configuration." <EXPLAINS> """CODE.from transformers import LayoutLMv3Model, LayoutLMv3Config

# Initializing a LayoutLMv3 microsoft/layoutlmv3-base style configuration
configuration = LayoutLMv3Config()

# Initializing a model from the microsoft/layoutlmv3-base style configuration
model = LayoutLMv3Model(configuration)

# Accessing the model configuration
configuration = model.config
""" .

"DESCRIPTION.The code initializes a LeViT model and configuration based on the levit-base-192 style, and then accesses the model configuration." <EXPLAINS> """CODE.from transformers import LevitModel, LevitConfig

# Initializing a LeViT levit-base-192 style configuration
configuration = LevitConfig()

# Initializing a model from the levit-base-192 style configuration
model = LevitModel(configuration)

# Accessing the model configuration
configuration = model.config
""" .

"DESCRIPTION.The code initializes a Lecun Normal initializer and uses it to initialize the weights of a Dense layer with 3 units." <EXPLAINS> """CODE.initializer = tf.keras.initializers.LecunNormal()
values = initializer(shape=(2, 2))

initializer = tf.keras.initializers.LecunNormal()
layer = tf.keras.layers.Dense(3, kernel_initializer=initializer)""" .

"DESCRIPTION.The code initializes a Lecun Uniform initializer and uses it to initialize a Dense layer with 3 units." <EXPLAINS> """CODE.initializer = tf.keras.initializers.LecunUniform()
values = initializer(shape=(2, 2))

initializer = tf.keras.initializers.LecunUniform()
layer = tf.keras.layers.Dense(3, kernel_initializer=initializer)""" .

"DESCRIPTION.The code initializes a Mask2Former model using a configuration." <EXPLAINS> """CODE.from transformers import Mask2FormerConfig, Mask2FormerModel

# Initializing a Mask2Former facebook/mask2former-swin-small-coco-instance configuration
configuration = Mask2FormerConfig()

# Initializing a model (with random weights) from the facebook/mask2former-swin-small-coco-instance style configuration
model = Mask2FormerModel(configuration)

# Accessing the model configuration
configuration = model.config
""" .

"DESCRIPTION.The code initializes a MaskFormer model using a configuration based on the facebook/maskformer-swin-base-ade style, and then accesses the configuration of the model." <EXPLAINS> """CODE.from transformers import MaskFormerConfig, MaskFormerModel

# Initializing a MaskFormer facebook/maskformer-swin-base-ade configuration
configuration = MaskFormerConfig()

# Initializing a model from the facebook/maskformer-swin-base-ade style configuration
model = MaskFormerModel(configuration)

# Accessing the model configuration
configuration = model.config
""" .

"DESCRIPTION.The code initializes a Mean Squared Error (MSE) object, calculates the loss using the MSE between two sets of values, creates a model with specified inputs and outputs, and compiles the model using Stochastic Gradient Descent (SGD) optimizer with MSE as the loss function." <EXPLAINS> """CODE.mse = keras.losses.MeanSquaredError()
loss = mse([0., 0., 1., 1.], [1., 1., 1., 0.])
model = keras.Model(inputs, outputs)
model.compile('sgd', loss=keras.losses.MeanSquaredError())
""" .

"DESCRIPTION.The code initializes a MobileViTV2Config configuration object and a MobileViTV2Model model object based on that configuration. Then it accesses the configuration of the model." <EXPLAINS> """CODE.from transformers import MobileViTV2Config, MobileViTV2Model

# Initializing a mobilevitv2-small style configuration
configuration = MobileViTV2Config()

# Initializing a model from the mobilevitv2-small style configuration
model = MobileViTV2Model(configuration)

# Accessing the model configuration
configuration = model.config
""" .

"DESCRIPTION.The code initializes a ModelSearchArguments object and sets the author attribute to \"huggingface\" and the language attribute to \"en\"." <EXPLAINS> """CODE.args = ModelSearchArguments()
args.author.huggingface
args.language.en
""" .

"DESCRIPTION.The code initializes a Mpt configuration, then uses it to initialize a Mpt model with random weights, and finally accesses the configuration of the model." <EXPLAINS> """CODE.from transformers import MptConfig, MptModel

# Initializing a Mpt configuration
configuration = MptConfig()

# Initializing a model (with random weights) from the configuration
model = MptModel(configuration)

# Accessing the model configuration
configuration = model.config
""" .

"DESCRIPTION.The code initializes a Nystromformer model using a specific configuration (uw-madison/nystromformer-512 style) and then accesses the configuration of the model." <EXPLAINS> """CODE.from transformers import NystromformerModel, NystromformerConfig

# Initializing a Nystromformer uw-madison/nystromformer-512 style configuration
configuration = NystromformerConfig()

# Initializing a model from the uw-madison/nystromformer-512 style configuration
model = NystromformerModel(configuration)

# Accessing the model configuration
configuration = model.config
""" .

"DESCRIPTION.The code initializes a OneFormer model using a specific configuration for the shi-labs/oneformer_ade20k_swin_tiny style, and then accesses the configuration of the model." <EXPLAINS> """CODE.from transformers import OneFormerConfig, OneFormerModel

# Initializing a OneFormer shi-labs/oneformer_ade20k_swin_tiny configuration
configuration = OneFormerConfig()
# Initializing a model (with random weights) from the shi-labs/oneformer_ade20k_swin_tiny style configuration
model = OneFormerModel(configuration)
# Accessing the model configuration
configuration = model.config
""" .

"DESCRIPTION.The code initializes a PLBART model using the uclanlp/plbart-base style configuration and then accesses the configuration parameters of the model." <EXPLAINS> """CODE.from transformers import PLBartModel, PLBartConfig

# Initializing a PLBART uclanlp/plbart-base style configuration
configuration = PLBartConfig()
# Initializing a model from the uclanlp/plbart-base style configuration
model = PLBartModel(configuration)
# Accessing the model configuration
configuration = model.config
""" .

"DESCRIPTION.The code initializes a PPO (Proximal Policy Optimization) model for the CartPole-v0 environment and trains the model using a Trainer." <EXPLAINS> """CODE.model = PPOLightning("CartPole-v0")
trainer = Trainer()
trainer.fit(model)""" .

"DESCRIPTION.The code initializes a PPOLightning model for the \"CartPole-v0\" environment and trains the model using a Trainer." <EXPLAINS> """CODE.model = PPOLightning("CartPole-v0")
trainer = Trainer()
trainer.fit(model)""" .

"DESCRIPTION.The code initializes a PPOLightning model for the CartPole-v0 environment and then trains the model using a Trainer object." <EXPLAINS> """CODE.model = PPOLightning("CartPole-v0")
trainer = Trainer()
trainer.fit(model)""" .

"DESCRIPTION.The code initializes a PatchTSMixer model with a default configuration and then accesses the configuration of the model." <EXPLAINS> """CODE.from transformers import PatchTSMixerConfig, PatchTSMixerModel

# Initializing a default PatchTSMixer configuration
configuration = PatchTSMixerConfig()

# Randomly initializing a model (with random weights) from the configuration
model = PatchTSMixerModel(configuration)

# Accessing the model configuration
configuration = model.config
""" .

"DESCRIPTION.The code initializes a Persimmon configuration object using the Persimmon persimmon-7b style configuration." <EXPLAINS> """CODE.from transformers import PersimmonModel, PersimmonConfig

# Initializing a Persimmon persimmon-7b style configuration
configuration = PersimmonConfig()
""" .

"DESCRIPTION.The code initializes a Phi-1 style configuration and model, then accesses the model's configuration." <EXPLAINS> """CODE.from transformers import PhiModel, PhiConfig

# Initializing a Phi-1 style configuration
configuration = PhiConfig.from_pretrained("susnato/phi-1_dev")

# Initializing a model from the configuration
model = PhiModel(configuration)

# Accessing the model configuration
configuration = model.config
""" .

"DESCRIPTION.The code initializes a PyFileSystem object with a FSSpecHandler object that handles the FSSpec file system." <EXPLAINS> "CODE.PyFileSystem(FSSpecHandler(fsspec_fs))" .

"DESCRIPTION.The code initializes a RMSNorm layer using Flax, generates random input data, initializes the layer's variables with another random key, and applies the RMSNorm layer to the input data to obtain the output." <EXPLAINS> """CODE.import jax.numpy as jnp
import jax
import flax.linen as nn

x = jax.random.uniform(jax.random.PRNGKey(0), (2, 3))
layer = nn.RMSNorm()
variables = layer.init(jax.random.PRNGKey(1), x)
y = layer.apply(variables, x)""" .

"DESCRIPTION.The code initializes a RandomUniform object with a range of -1 to 1, gets its configuration, and then recreates the RandomUniform object using the configuration." <EXPLAINS> """CODE.initializer = RandomUniform(-1, 1)
config = initializer.get_config()
initializer = RandomUniform.from_config(config)
""" .

"DESCRIPTION.The code initializes a Ray instance, creates a placement group with 2 CPUs, and checks if the current placement group is None." <EXPLAINS> """CODE.@ray.remote
def f():
    pg = get_current_placement_group()
    pg = placement_group([{"CPU": 2}])
    f.options(placement_group=pg).remote()

ray.init()
assert get_current_placement_group() is None""" .

"DESCRIPTION.The code initializes a ResNet50 backbone model using the Timm library and then accesses the configuration of the model." <EXPLAINS> """CODE.from transformers import TimmBackboneConfig, TimmBackbone

# Initializing a timm backbone
configuration = TimmBackboneConfig("resnet50")

# Initializing a model from the configuration
model = TimmBackbone(configuration)

# Accessing the model configuration
configuration = model.config
""" .

"DESCRIPTION.The code initializes a SamConfig and a SamModel with the configuration set to \"facebook/sam-vit-huge\". It also allows for initializing a SamConfig from separate SamVisionConfig, SamPromptEncoderConfig, and SamMaskDecoderConfig configurations." <EXPLAINS> """CODE.from transformers import (
    SamVisionConfig,
    SamPromptEncoderConfig,
    SamMaskDecoderConfig,
    SamModel,
)

# Initializing a SamConfig with `"facebook/sam-vit-huge"` style configuration
configuration = SamConfig()

# Initializing a SamModel (with random weights) from the `"facebook/sam-vit-huge"` style configuration
model = SamModel(configuration)

# Accessing the model configuration
configuration = model.config

# We can also initialize a SamConfig from a SamVisionConfig, SamPromptEncoderConfig, and SamMaskDecoderConfig

# Initializing SAM vision, SAM Q-Former and language model configurations
vision_config = SamVisionConfig()
prompt_encoder_config = SamPromptEncoderConfig()
mask_decoder_config = SamMaskDecoderConfig()

config = SamConfig(vision_config, prompt_encoder_config, mask_decoder_config)
""" .

"DESCRIPTION.The code initializes a SiglipConfig object with a google/siglip-base-patch16-224 style configuration, followed by initializing a SiglipModel object with random weights based on the previously defined configuration. It then accesses the configuration of the model. Additionally, the code demonstrates initializing a SiglipConfig object from separate SiglipTextConfig and SiglipVisionConfig objects." <EXPLAINS> """CODE.from transformers import SiglipConfig, SiglipModel

# Initializing a SiglipConfig with google/siglip-base-patch16-224 style configuration
configuration = SiglipConfig()

# Initializing a SiglipModel (with random weights) from the google/siglip-base-patch16-224 style configuration
model = SiglipModel(configuration)

# Accessing the model configuration
configuration = model.config

# We can also initialize a SiglipConfig from a SiglipTextConfig and a SiglipVisionConfig
from transformers import SiglipTextConfig, SiglipVisionConfig

# Initializing a SiglipText and SiglipVision configuration
config_text = SiglipTextConfig()
config_vision = SiglipVisionConfig()

config = SiglipConfig.from_text_vision_configs(config_text, config_vision)
""" .

"DESCRIPTION.The code initializes a SwiftFormer model with random weights and then accesses the model's configuration." <EXPLAINS> """CODE.from transformers import SwiftFormerConfig, SwiftFormerModel

# Initializing a SwiftFormer swiftformer-base-patch16-224 style configuration
configuration = SwiftFormerConfig()

# Initializing a model (with random weights) from the swiftformer-base-patch16-224 style configuration
model = SwiftFormerModel(configuration)

# Accessing the model configuration
configuration = model.config
""" .

"DESCRIPTION.The code initializes a Swin2SR model with a random weight and then accesses the configuration of the model." <EXPLAINS> """CODE.from transformers import Swin2SRConfig, Swin2SRModel

# Initializing a Swin2SR caidas/swin2sr-classicalsr-x2-64 style configuration
configuration = Swin2SRConfig()

# Initializing a model (with random weights) from the caidas/swin2sr-classicalsr-x2-64 style configuration
model = Swin2SRModel(configuration)

# Accessing the model configuration
configuration = model.config
""" .

"DESCRIPTION.The code initializes a TPU cluster resolver, retrieves the TPU system metadata, and then assigns the number of hosts in the TPU system metadata to the variable num_hosts." <EXPLAINS> """CODE.resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='')
tpu_system_medata = resolver.get_tpu_system_metadata()
num_hosts = tpu_system_medata.num_hosts
""" .

"DESCRIPTION.The code initializes a TPU system with 2 logical devices and 4 replicas, creates a distributed training strategy for TPU, splits input data across multiple logical devices, executes a model on 8 logical devices with input split in specific ways, replicates labels and outputs for loss calculation, and runs a step function using the strategy with input from an iterator." <EXPLAINS> """CODE.# Initializing TPU system with 2 logical devices and 4 replicas.
resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='')
tf.config.experimental_connect_to_cluster(resolver)
topology = tf.tpu.experimental.initialize_tpu_system(resolver)
device_assignment = tf.tpu.experimental.DeviceAssignment.build(
    topology,
    computation_shape=[1, 1, 2],
    num_replicas=4)
strategy = tf.distribute.experimental.TPUStrategy(
    resolver, device_assignment=device_assignment)

iterator = iter(inputs)

@tf.function()
def step_fn(inputs):
  images, labels = inputs
  images = strategy.experimental_split_to_logical_devices(
    inputs, [1, 2, 4, 1])

  // model() function will be executed on 8 logical devices with `inputs`
  // split 2 * 4  ways.
  output = model(inputs)

  // For loss calculation, all logical devices share the same logits
  // and labels.
  labels = strategy.experimental_replicate_to_logical_devices(labels)
  output = strategy.experimental_replicate_to_logical_devices(output)
  loss = loss_fn(labels, output)

  return loss

strategy.run(step_fn, args=(next(iterator),))
""" .

"DESCRIPTION.The code initializes a TPU system with 2 logical devices and 4 replicas, creates a strategy for distributing computation on the TPU, defines a step function that adds input values together and assigns the operation to be executed on a specific logical device, then runs the step function using the defined strategy." <EXPLAINS> """CODE.# Initializing TPU system with 2 logical devices and 4 replicas.
resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='')
tf.config.experimental_connect_to_cluster(resolver)
topology = tf.tpu.experimental.initialize_tpu_system(resolver)
device_assignment = tf.tpu.experimental.DeviceAssignment.build(
    topology,
    computation_shape=[1, 1, 2],
    num_replicas=4)
strategy = tf.distribute.experimental.TPUStrategy(
    resolver, device_assignment=device_assignment)
iterator = iter(inputs)

@tf.function()
def step_fn(inputs):
  output = tf.add(inputs, inputs)

  // Add operation will be executed on logical device 0.
  output = strategy.experimental_assign_to_logical_device(output, 0)
  return output

strategy.run(step_fn, args=(next(iterator),))
""" .

"DESCRIPTION.The code initializes a TPUStrategy and creates a checkpoint manager to save checkpoints during training. It also defines a preemption handler to save checkpoints if preemption occurs. The training loop runs a multi-step training function until the trained step reaches a specified number of steps." <EXPLAINS> """CODE.strategy = tf.distribute.TPUStrategy(tpu_cluster_resolver)
# initialization omitted

with strategy.scope():
  # Save in the checkpoint.
  trained_step = tf.Variable(initial_value=tf.constant(0, dtype=tf.dtypes.int64), name='trained_step', aggregation=tf.VariableAggregation.ONLY_FIRST_REPLICA)

  checkpoint_manager = tf.train.CheckpointManager(checkpoint, directory, max_to_keep=1)
  preemption_handler = tf.distribute.experimental.PreemptionCheckpointHandler(cluster_resolver, checkpoint_manager)

while trained_step.numpy() < NUM_STEPS:
  # Train STEPS_IN_FUNCTION steps at once.
  train_multi_step_function()
  trained_step.assign_add(STEPS_IN_FUNCTION)
  preemption_handler.save_checkpoint_if_preempted()""" .

"DESCRIPTION.The code initializes a TensorFlow variable named 'example_var' with a NumPy array 'val' containing values [[1, 2], [3, 4]] of datatype float64. The code then retrieves and prints the datatype of the variable 'kvar'." <EXPLAINS> """CODE.val = np.array([[1, 2], [3, 4]])
kvar = tf.keras.backend.variable(value=val, dtype='float64', name='example_var')
tf.keras.backend.dtype(kvar)
print(kvar)""" .

"DESCRIPTION.The code initializes a TimeSformer model with a base style configuration, randomly initializes the model, and then accesses the model's configuration." <EXPLAINS> """CODE.from transformers import TimesformerConfig, TimesformerModel

# Initializing a TimeSformer timesformer-base style configuration
configuration = TimesformerConfig()

# Randomly initializing a model from the configuration
model = TimesformerModel(configuration)

# Accessing the model configuration
configuration = model.config
""" .

"DESCRIPTION.The code initializes a Trainer object from pytorch_lightning and assigns a LambdaCallback to it with a setup function that prints 'setup' when called." <EXPLAINS> """CODE.from pytorch_lightning import Trainer
from pytorch_lightning.callbacks import LambdaCallback
trainer = Trainer(callbacks=[LambdaCallback(setup=lambda *args: print('setup'))])""" .

"""DESCRIPTION.The code initializes a Trainer object from the pytorch_lightning library with the callback function RichModelSummary(), which provides a detailed summary of the model during training.

The code initializes a Trainer object from the pytorch_lightning library with the callback function RichProgressBar(), which displays a progress bar during model training.""" <EXPLAINS> """CODE.from pytorch_lightning import Trainer
from pytorch_lightning.callbacks import RichModelSummary

trainer = Trainer(callbacks=RichModelSummary())


from pytorch_lightning import Trainer
from pytorch_lightning.callbacks import RichProgressBar

trainer = Trainer(callbacks=RichProgressBar())
""" .

"DESCRIPTION.The code initializes a TrajectoryTransformer model using a specific configuration and then accesses the configuration of the model." <EXPLAINS> """CODE.from transformers import TrajectoryTransformerModel, TrajectoryTransformerConfig

# Initializing a TrajectoryTransformer CarlCochet/trajectory-transformer-halfcheetah-medium-v2 style configuration
configuration = TrajectoryTransformerConfig()

# Initializing a model from the CarlCochet/trajectory-transformer-halfcheetah-medium-v2 style configuration
model = TrajectoryTransformerModel(configuration)

# Accessing the model configuration
configuration = model.config
""" .

"DESCRIPTION.The code initializes a TruncatedNormal initializer with mean 0 and standard deviation 1, then creates a Dense layer with 3 units using the initializer for the kernel weights." <EXPLAINS> """CODE.initializer = tf.keras.initializers.TruncatedNormal(mean=0., stddev=1.)
values = initializer(shape=(2, 2))
initializer = tf.keras.initializers.TruncatedNormal(mean=0., stddev=1.)
layer = tf.keras.layers.Dense(3, kernel_initializer=initializer)""" .

"DESCRIPTION.The code initializes a UNet model with 3 layers and 2 output classes. It consists of multiple layers including DoubleConv, Down, and Up layers, and ends with a final Conv2d layer that reduces the output channels to 2." <EXPLAINS> """CODE.UNet(num_classes=2, num_layers=3)
(layers): ModuleList(
(0): DoubleConv(...)
(1): Down(...)
(2): Down(...)
(3): Up(...)
(4): Up(...)
(5): Conv2d(64, 2, kernel_size=(1, 1), stride=(1, 1))""" .

"DESCRIPTION.The code initializes a UnitNorm constraint, gets the configuration of the constraint, and then reinitializes the UnitNorm constraint using the obtained configuration." <EXPLAINS> """CODE.constraint = UnitNorm()
config = constraint.get_config()
constraint = UnitNorm.from_config(config)
""" .

"DESCRIPTION.The code initializes a UnivNet model with random weights and accesses its configuration." <EXPLAINS> """CODE.from transformers import UnivNetModel, UnivNetConfig

# Initializing a Tortoise TTS style configuration
configuration = UnivNetConfig()

# Initializing a model (with random weights) from the Tortoise TTS style configuration
model = UnivNetModel(configuration)

# Accessing the model configuration
configuration = model.config
""" .

"DESCRIPTION.The code initializes a UniversalDetector object, feeds it with some bytes of data, closes the object, and then retrieves the result of the detection process." <EXPLAINS> """CODE.        u = UniversalDetector()
        u.feed(some_bytes)
        u.close()
        detected = u.result""" .

"DESCRIPTION.The code initializes a VanModel object with a VanConfig object and then retrieves the configuration from the model." <EXPLAINS> """CODE.from transformers import VanModel, VanConfig
configuration = VanConfig()
model = VanModel(configuration)
configuration = model.config
""" .

"DESCRIPTION.The code initializes a ViTMatte model with a specific configuration for image matting." <EXPLAINS> """CODE.from transformers import VitMatteConfig, VitMatteForImageMatting

# Initializing a ViTMatte hustvl/vitmatte-small-composition-1k style configuration
configuration = VitMatteConfig()

# Initializing a model (with random weights) from the hustvl/vitmatte-small-composition-1k style configuration
model = VitMatteForImageMatting(configuration)

# Accessing the model configuration
configuration = model.config
""" .

"DESCRIPTION.The code initializes a Voting Classifier with Logistic Regression and Random Forest Classifier as its estimators, and sets the parameters of the Random Forest Classifier to None." <EXPLAINS> """CODE.clf1 = LogisticRegression()
clf2 = RandomForestClassifier()
eclf = VotingClassifier(estimators=[('lr', clf1), ('rf', clf2)]
eclf.set_params(rf=None)""" .

"DESCRIPTION.The code initializes a Wav2Vec2Conformer model with a specific configuration and then accesses the configuration of the model." <EXPLAINS> """CODE.from transformers import Wav2Vec2ConformerModel, Wav2Vec2ConformerConfig

# Initializing a Wav2Vec2Conformer facebook/wav2vec2-conformer-large-rel-pos style configuration
configuration = Wav2Vec2ConformerConfig()

# Initializing a model from the facebook/wav2vec2-conformer-large-rel-pos style configuration
model = Wav2Vec2ConformerModel(configuration)

# Accessing the model configuration
configuration = model.config
""" .

"DESCRIPTION.The code initializes a Yoso model with a specific configuration, and then retrieves the configuration of the initialized model." <EXPLAINS> """CODE.from transformers import YosoModel, YosoConfig

# Initializing a YOSO uw-madison/yoso-4096 style configuration
configuration = YosoConfig()

# Initializing a model from the uw-madison/yoso-4096 style configuration
model = YosoModel(configuration)

# Accessing the model configuration
configuration = model.config
""" .

"""DESCRIPTION.The code initializes a cache object with ContextValueCache class using integer type as its value type. It then accesses the cache with a key of None and increments its value by 2 and then by 4. The code then checks if the value associated with key None is 6.

Next, it creates a TensorFlow graph and updates the value associated with key None in the cache by adding 5. It then accesses the cache with the graph object as the key and increments its value by 3. Finally, the code asserts that the value associated with the graph object key is 8.

Later, the code reinitializes the cache with a lambda function that increments the input value by 1. It gets the default TensorFlow graph object and sets it as the key for the cache with key g and kwargs {'x': 3}. It then checks if the value associated with the graph object key is 4.""" <EXPLAINS> """CODE.cache = ContextValueCache(int)
cache[None] += 2
cache[None] += 4
assert cache[None] == 6

with tf.Graph().as_default() as g:
  cache[None] += 5
  cache[g] += 3
assert cache[g] == 8

cache = ContextValueCache(lambda x: x + 1)
g = tf.get_default_graph()

value = cache.setdefault(key=g, kwargs={'x': 3})
assert cache[g] == 4""" .

"DESCRIPTION.The code initializes a code generation tokenizer from a pretrained model and tokenizes the input strings \"Hello world\" and \" Hello world\" to generate input IDs." <EXPLAINS> """CODE.from transformers import CodeGenTokenizer
tokenizer = CodeGenTokenizer.from_pretrained("Salesforce/codegen-350M-mono")
tokenizer("Hello world")['input_ids']
tokenizer(" Hello world")['input_ids']""" .

"DESCRIPTION.The code initializes a configuration for a PatchTST model with 12 time steps for prediction, then randomly initializes a model with random weights based on the configuration, and finally accesses the configuration of the model." <EXPLAINS> """CODE.from transformers import PatchTSTConfig, PatchTSTModel

# Initializing an PatchTST configuration with 12 time steps for prediction
configuration = PatchTSTConfig(prediction_length=12)

# Randomly initializing a model (with random weights) from the configuration
model = PatchTSTModel(configuration)

# Accessing the model configuration
configuration = model.config
""" .

"DESCRIPTION.The code initializes a configuration for a model using the DetaConfig class and then initializes a model with random weights using the DetaModel class based on the configuration. Finally, it accesses the configuration of the model." <EXPLAINS> """CODE.from transformers import DetaConfig, DetaModel

# Initializing a DETA SenseTime/deformable-detr style configuration
configuration = DetaConfig()

# Initializing a model (with random weights) from the SenseTime/deformable-detr style configuration
model = DetaModel(configuration)

# Accessing the model configuration
configuration = model.config
""" .

"DESCRIPTION.The code initializes a configuration object for a OwlV2 Vision model and creates the corresponding model using that configuration. Finally, it retrieves the configuration from the created model." <EXPLAINS> """CODE.from transformers import Owlv2VisionConfig, Owlv2VisionModel
configuration = Owlv2VisionConfig()
model = Owlv2VisionModel(configuration)
configuration = model.config
""" .

"DESCRIPTION.The code initializes a configuration object for a vision model in the style of BridgeTower/bridgetower-base." <EXPLAINS> """CODE.from transformers import BridgeTowerVisionConfig

# Initializing a BridgeTower BridgeTower/bridgetower-base style configuration for the vision model
configuration = BridgeTowerVisionConfig()

# Accessing the configuration
configuration
""" .

"DESCRIPTION.The code initializes a configuration object for the Fuyu fuyu-7b style model." <EXPLAINS> """CODE.from transformers import FuyuConfig

# Initializing a Fuyu fuyu-7b style configuration
configuration = FuyuConfig()
""" .

"DESCRIPTION.The code initializes a constant tensor 'a' with values [-20, -1.0, 0.0, 1.0, 20] of float32 data type. It then applies the softplus activation function from the TensorFlow Keras API to 'a' and stores the result in tensor 'b'. Finally, it converts 'b' to a numpy array." <EXPLAINS> """CODE.a = tf.constant([-20, -1.0, 0.0, 1.0, 20], dtype = tf.float32)
b = tf.keras.activations.softplus(a)
b.numpy()""" .

"DESCRIPTION.The code initializes a constant tensor with a value of -7 using Jax library in Python." <EXPLAINS> """CODE.import jax, jax.numpy as jnp
initializer = jax.nn.initializers.constant(-7)
initializer(jax.random.PRNGKey(42), (2, 3), jnp.float32)""" .

"DESCRIPTION.The code initializes a constant tensor with values [-3.0, -1.0, 0.0, 1.0, 3.0] of float type and applies the hyperbolic tangent activation function to it, then converts the result to a numpy array." <EXPLAINS> """CODE.a = tf.constant([-3.0,-1.0, 0.0,1.0,3.0], dtype = tf.float32)
b = tf.keras.activations.tanh(a)
b.numpy()""" .

"DESCRIPTION.The code initializes a constant value of 3 and uses it to initialize a weight matrix for a dense neural network layer with 3 units." <EXPLAINS> """CODE.initializer = tf.keras.initializers.Constant(3.)
values = initializer(shape=(2, 2))
initializer = tf.keras.initializers.Constant(3.)
layer = tf.keras.layers.Dense(3, kernel_initializer=initializer)""" .

"DESCRIPTION.The code initializes a convolutional neural network model using PaddlePaddle, creates a stochastic gradient descent optimizer with a learning rate of 0.01, sets up a gradient scaler with an initial loss scaling of 1024, generates random data of shape [10, 3, 32, 32], performs automatic mixed precision training with the model and data, calculates the mean of the convolution output as the loss, scales the loss using the gradient scaler, performs backward propagation, and updates the model parameters using the optimizer." <EXPLAINS> """CODE.import paddle

model = paddle.nn.Conv2D(3, 2, 3, bias_attr=True)
optimizer = paddle.optimizer.SGD(learning_rate=0.01, parameters=model.parameters())
scaler = paddle.amp.GradScaler(init_loss_scaling=1024)
data = paddle.rand([10, 3, 32, 32])

with paddle.amp.auto_cast():
    conv = model(data)
    loss = paddle.mean(conv)

scaled = scaler.scale(loss)  # scale the loss
scaled.backward()            # do backward
scaler.minimize(optimizer, scaled)  # update parameters""" .

"DESCRIPTION.The code initializes a custom HTTP header dictionary with the key 'foo' and the value 'bar'. It then adds another value 'baz' to the same key 'foo' in the dictionary. Finally, it retrieves the value associated with the key 'foo' from the dictionary." <EXPLAINS> """CODE.headers = HTTPHeaderDict(foo='bar')
headers.add('Foo', 'baz')
headers['foo']""" .

"DESCRIPTION.The code initializes a custom initializer using the LecunUniform method and then uses this initializer to define the kernel_initializer of a dense layer with 3 units." <EXPLAINS> """CODE.initializer = tf.keras.initializers.LecunUniform()
values = initializer(shape=(2, 2))

initializer = tf.keras.initializers.LecunUniform()
layer = tf.keras.layers.Dense(3, kernel_initializer=initializer)""" .

"DESCRIPTION.The code initializes a dataset object and sets the HDFS configuration with the specified file system name and user group information." <EXPLAINS> """CODE.import paddle
dataset = paddle.distributed.fleet.DatasetBase()
dataset._set_hdfs_config("my_fs_name", "my_fs_ugi")""",
        """CODE.import paddle.base as base
dataset = base.DatasetFactory().create_dataset()
dataset.set_hdfs_config("my_fs_name", "my_fs_ugi")""" .

"DESCRIPTION.The code initializes a debug object for detecting underflows and overflows in a given model, with optional parameters such as the maximum number of frames to save, specific batch numbers to trace, and the number of batches to abort after." <EXPLAINS> """CODE.debug_overflow = DebugUnderflowOverflow(model)
debug_overflow = DebugUnderflowOverflow(model, max_frames_to_save=100)
debug_overflow = DebugUnderflowOverflow(model, trace_batch_nums=[1,3])
debug_overflow = DebugUnderflowOverflow(model, trace_batch_nums=[1,3], abort_after_batch_num=3)
""" .

"DESCRIPTION.The code initializes a decoder start token ID from a model configuration and then creates decoder input IDs by filling a 2D array with the decoder start token ID. It then uses the model to decode the input IDs and obtain the last hidden states of the decoder." <EXPLAINS> """CODE.decoder_start_token_id = model.config.decoder_start_token_id
decoder_input_ids = jnp.ones((inputs.input_ids.shape[0], 1), dtype="i4") * decoder_start_token_id

outputs = model.decode(decoder_input_ids, encoder_outputs)
last_decoder_hidden_states = outputs.last_hidden_state""" .

"DESCRIPTION.The code initializes a deep learning model with 1000 layers, each consisting of a linear transformation from 10000 inputs to 10000 outputs, without using any RAM." <EXPLAINS> """CODE.pyton
import torch.nn as nn
from accelerate import init_empty_weights

# Initialize a model with 100 billions parameters in no time and without using any RAM.
with init_empty_weights():
    tst = nn.Sequential(*[nn.Linear(10000, 10000) for _ in range(1000)])
""" .

"DESCRIPTION.The code initializes a default Autoformer configuration and creates a model with random weights based on that configuration. It then accesses the configuration of the model." <EXPLAINS> """CODE.from transformers import AutoformerConfig, AutoformerModel

# Initializing a default Autoformer configuration
configuration = AutoformerConfig()

# Randomly initializing a model (with random weights) from the configuration
model = AutoformerModel(configuration)

# Accessing the model configuration
configuration = model.config
""" .

"DESCRIPTION.The code initializes a dense neural network layer with 3 units using the Lecun Normal initialization method for the kernel weights." <EXPLAINS> """CODE.initializer = tf.keras.initializers.LecunNormal()
values = initializer(shape=(2, 2))
initializer = tf.keras.initializers.LecunNormal()
layer = tf.keras.layers.Dense(3, kernel_initializer=initializer)""" .

"DESCRIPTION.The code initializes a dictionary with various data types as values, including float, int, string, bool, list, namespace, and a torch.nn.BatchNorm1d layer. It then prints a sanitized version of the parameters using a method from the LightningLoggerBase class." <EXPLAINS> """CODE.params = {"float": 0.3,
          "int": 1,
          "string": "abc",
          "bool": True,
          "list": [1, 2, 3],
          "namespace": Namespace(foo=3),
          "layer": torch.nn.BatchNorm1d}
import pprint
pprint.pprint(LightningLoggerBase._sanitize_params(params))  # doctest: +NORMALIZE_WHITESPACE""" .

"DESCRIPTION.The code initializes a distributed RPC framework with 2 workers, performs remote procedure calls to calculate the sum and difference of two tensors, and profiles the execution time of the operations before shutting down the RPC framework." <EXPLAINS> """CODE.import torch
import torch.distributed.rpc as rpc
rpc.init_rpc("worker0", rank=0, world_size=2)
x, y = torch.tensor(1), torch.tensor(2)
outer_profile_rref = rpc.remote(dst_worker_name, rpc._server_process_global_profile)
outer_profile_rref.rpc_sync().__enter__()
rpc.rpc_sync(dst_worker_name, torch.add, (x, y))
inner_profile_rref = rpc.remote(dst_worker_name, rpc._server_process_global_profile)
inner_profile_rref.rpc_sync().__enter__()
rpc.rpc_sync(dst_worker_name, torch.sub, (x, y))
inner_profile_rref.rpc_sync().__exit__(None, None, None)
outer_profile_rref.rpc_sync().__exit__(None, None, None
rpc.shutdown()""" .

"DESCRIPTION.The code initializes a distributed computing environment with 2 workers on the IP address '10.0.0.1' and ports 1234, assigning worker 0 and worker 1 respectively." <EXPLAINS> """CODE.jax.distributed.initialize('10.0.0.1:1234', 2, 0)  # doctest: +SKIP
jax.distributed.initialize('10.0.0.1:1234', 2, 1)  # doctest: +SKIP""" .

"DESCRIPTION.The code initializes a distributed process group using the 'gloo' backend with 2 processes. It then establishes remote procedure calls (RPC) between 'worker0' and 'worker1', with 'worker1' performing addition operations on tensors. Finally, it retrieves the results from 'worker1', performs addition on them locally, and then terminates the RPC." <EXPLAINS> """CODE.import torch.distributed as dist
dist.init_process_group(backend='gloo', rank=0, world_size=2)
dist.init_rpc("worker0")
worker1 = dist.get_worker_id("worker1")
rref1 = dist.remote(worker1, torch.add, args=(torch.ones(2), 3))
rref2 = dist.remote(worker1, torch.add, args=(torch.ones(2), 1))
x = rref1.to_here() + rref2.to_here()
dist.join_rpc()

import torch.distributed as dist
dist.init_process_group(backend='gloo', rank=1, world_size=2)
dist.init_rpc("worker1")
dist.join_rpc()""" .

"DESCRIPTION.The code initializes a distributed process group using the 'gloo' backend with a specified number of processes (world_size), creates a ToyModel and initializes a Python Distributed Data Parallel (DDP) model with asynchronous reduction. It then defines a Mean Squared Error (MSE) loss function and a Stochastic Gradient Descent (SGD) optimizer. The DDP model is used to forward pass a random input tensor, calculate loss between the outputs and random labels, and perform backpropagation. Finally, it reduces parameter gradients across processes, updates the model parameters using the optimizer, and executes a training step." <EXPLAINS> """CODE.torch.distributed.init_process_group(
    backend='gloo', world_size=N, init_method='...'
)
pg = dist.distributed_c10d._get_default_group()
async_reduction = True
module = ToyModel()
ddp_model = PythonDDP(module, pg, async_reduction)
loss_fn = nn.MSELoss()
optimizer = optim.SGD(ddp_model.parameters(), lr=0.001)
outputs = ddp_model(torch.randn(20, 10).to(rank))
labels = torch.randn(20, 10).to(rank)
loss_fn(outputs, labels).backward()

# Reduce param grads
ddp_model.all_reduce_grads()
optimizer.step()""" .

"DESCRIPTION.The code initializes a distributed training process using NCCL backend with 16 processes. It creates a linear neural network model with 1 input and 1 output without bias and distributes it among the processes. It defines a subgroup for communication, sets up a post-local SGD state, and registers a communication hook. Additionally, it sets up a periodic model averager with a period of 4 and warmup steps of 100. During training, it updates the model parameters using the optimizer, calculates the loss, and averages the model parameters periodically using the averager." <EXPLAINS> """CODE.import torch
import torch.distributed as dist
import torch.distributed.algorithms.ddp_comm_hooks.post_localSGD_hook as post_localSGD
import torch.distributed.algorithms.model_averaging.averagers as averagers
import torch.nn as nn

dist.init_process_group("nccl", rank=rank, world_size=16)
torch.cuda.set_device(rank)
module = nn.Linear(1, 1, bias=False).to(rank)
model = nn.parallel.DistributedDataParallel(
    module, device_ids=[rank], output_device=rank
)
subgroup, subgroups = dist.new_subgroups()
state = PostLocalSGDState(subgroup=subgroup, start_localSGD_iter=100)
model.register_comm_hook(state, post_localSGD_hook)

averager = averagers.PeriodicModelAverager(period=4, warmup_steps=100)
for step in range(0, 20):
    optimizer.zero_grad()
    loss = loss_fn(output, labels)
    loss.backward()
    optimizer.step()
    averager.average_parameters(model.parameters())""" .

"DESCRIPTION.The code initializes a distributed training process with two workers using NCCL backend. Each worker creates a distributed data parallel model with a linear layer and an optimizer using ZeRO. The workers share parameters and gradients and perform model training with different numbers of input tensors. Finally, the code ensures that all ranks complete the training process without any hanging or errors." <EXPLAINS> """CODE.import os
import torch
import torch.distributed as dist
import torch.multiprocessing as mp
import torch.nn.parallel.DistributedDataParallel as DDP
import torch.distributed.optim.ZeroRedundancyOptimizer as ZeRO
from torch.distributed.algorithms.join import Join

# On each spawned worker
def worker(rank):
    dist.init_process_group("nccl", rank=rank, world_size=2)
    model = DDP(torch.nn.Linear(1, 1).to(rank), device_ids=[rank])
    optim = ZeRO(model.parameters(), torch.optim.Adam, lr=0.01)
    # Rank 1 gets one more input than rank 0
    inputs = [torch.tensor([1.]).to(rank) for _ in range(10 + rank)]
    with Join([model, optim]):
        for input in inputs:
            loss = model(input).sum()
            loss.backward()
            optim.step()
    # All ranks reach here without hanging/erroring
""" .

"DESCRIPTION.The code initializes a layer with a dense neural network with 3 units using a VarianceScaling initializer with scale of 0.1, fan_in mode, and uniform distribution." <EXPLAINS> """CODE.initializer = tf.keras.initializers.VarianceScaling(
    scale=0.1, mode='fan_in', distribution='uniform')

values = initializer(shape=(2, 2))

initializer = tf.keras.initializers.VarianceScaling(
    scale=0.1, mode='fan_in', distribution='uniform')

layer = tf.keras.layers.Dense(3, kernel_initializer=initializer)""" .

"DESCRIPTION.The code initializes a learning rate decay function using the CosineDecayRestarts scheduling method in TensorFlow's keras.optimizers module." <EXPLAINS> """CODE.first_decay_steps = 1000
lr_decayed_fn = (
  tf.keras.optimizers.schedules.CosineDecayRestarts(
      initial_learning_rate,
      first_decay_steps))
""" .

"DESCRIPTION.The code initializes a linear layer with input size 10 and output size 10, and sets up a momentum optimizer with a learning rate of 0.1, weight decay using L2 regularizer with a regularization coefficient of 0.0001 for the linear layer parameters." <EXPLAINS> """CODE.from paddle.regularizer import L2Decay
linear = paddle.nn.Linear(10, 10)
momentum = paddle.optimizer.Momentum(
    learning_rate=0.1,
    parameters=linear.parameters(),
    weight_decay=L2Decay(0.0001))
""" .

"DESCRIPTION.The code initializes a local agent model from the \"bigcode/starcoder\" pretrained model, and then prompts the agent to execute a task by providing the input sentence \"Draw me a picture of rivers and lakes.\"" <EXPLAINS> """CODE.import torch
from transformers import LocalAgent

agent = LocalAgent.from_pretrained("bigcode/starcoder", device_map="auto", torch_dtype=torch.bfloat16)
agent.run("Draw me a picture of rivers and lakes.")
""" .

"DESCRIPTION.The code initializes a local agent with a pretrained AutoModelForCausalLM model and AutoTokenizer for generating text based on the input prompt \"Draw me a picture of rivers and lakes.\"" <EXPLAINS> """CODE.import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, LocalAgent

checkpoint = "bigcode/starcoder"
model = AutoModelForCausalLM.from_pretrained(checkpoint, device_map="auto", torch_dtype=torch.bfloat16)
tokenizer = AutoTokenizer.from_pretrained(checkpoint)

agent = LocalAgent(model, tokenizer)
agent.run("Draw me a picture of rivers and lakes.")""" .

"DESCRIPTION.The code initializes a matrix of shape (2, 3) with Glorot normal distribution using JAX library." <EXPLAINS> """CODE.import jax, jax.numpy as jnp
initializer = jax.nn.initializers.glorot_normal()
initializer(jax.random.PRNGKey(42), (2, 3), jnp.float32)  # doctest: +SKIP""" .

"DESCRIPTION.The code initializes a model and tokenizer for a FlaxPegasus model for conditional text generation, encodes a given input text, decodes it to generate an output, and extracts the last hidden states from the decoder." <EXPLAINS> """CODE.model = FlaxPegasusForConditionalGeneration.from_pretrained('google/pegasus-large')
tokenizer = PegasusTokenizer.from_pretrained('google/pegasus-large')
text = "My friends are cool but they eat too many carbs."
inputs = tokenizer(text, max_length=1024, return_tensors='np')
encoder_outputs = model.encode(**inputs)
decoder_start_token_id = model.config.decoder_start_token_id
decoder_input_ids = jnp.ones((inputs.input_ids.shape[0], 1), dtype="i4") * decoder_start_token_id
outputs = model.decode(decoder_input_ids, encoder_outputs)
last_decoder_hidden_states = outputs.last_hidden_state""" .

"DESCRIPTION.The code initializes a model and tokenizer for aligning text and images, tokenizes two input sentences (\"a photo of a cat\" and \"a photo of a dog\") using the tokenizer, and then extracts text features using the model based on the tokenized input sentences." <EXPLAINS> """CODE.from transformers import AutoTokenizer, AlignModel

model = AlignModel.from_pretrained("kakaobrain/align-base")
tokenizer = AutoTokenizer.from_pretrained("kakaobrain/align-base")

inputs = tokenizer(["a photo of a cat", "a photo of a dog"], padding=True, return_tensors="pt")
text_features = model.get_text_features(**inputs)
""" .

"DESCRIPTION.The code initializes a model based on a specific configuration (OPTConfig) and then accesses the configuration of the model." <EXPLAINS> """CODE.from transformers import OPTModel, OPTConfig

# Initializing a OPT facebook/opt-large style configuration
configuration = OPTConfig()

# Initializing a model from the facebook/opt-large style configuration
model = OPTModel(configuration)

# Accessing the model configuration
configuration = model.config
""" .

"DESCRIPTION.The code initializes a model class with input, hidden, output, and value layers using TensorFlow's Keras layers, and registers the variables of the base model." <EXPLAINS> """CODE.def __init__(self, *args, **kwargs):
    super(MyModelClass, self).__init__(*args, **kwargs)
    input_layer = tf.keras.layers.Input(...)
    hidden_layer = tf.keras.layers.Dense(...)(input_layer)
    output_layer = tf.keras.layers.Dense(...)(hidden_layer)
    value_layer = tf.keras.layers.Dense(...)(hidden_layer)
    self.base_model = tf.keras.Model(
        input_layer, [output_layer, value_layer])
    self.register_variables(self.base_model.variables)""" .

"DESCRIPTION.The code initializes a model for next sentence prediction using BERT-based pre-trained weights and configurations." <EXPLAINS> """CODE.model = AutoModelForNextSentencePrediction.from_pretrained('bert-base-uncased')
model = AutoModelForNextSentencePrediction.from_pretrained('bert-base-uncased', output_attentions=True)
config = AutoConfig.from_json_file('./tf_model/bert_tf_model_config.json')
model = AutoModelForNextSentencePrediction.from_pretrained('./tf_model/bert_tf_checkpoint.ckpt.index', from_tf=True, config=config)""" .

"DESCRIPTION.The code initializes a model with random weights based on a specific configuration, and then accesses the configuration of the model." <EXPLAINS> """CODE.from transformers import EncodecModel, EncodecConfig

# Initializing a "facebook/encodec_24khz" style configuration
configuration = EncodecConfig()

# Initializing a model (with random weights) from the "facebook/encodec_24khz" style configuration
model = EncodecModel(configuration)

# Accessing the model configuration
configuration = model.config
""" .

"DESCRIPTION.The code initializes a named sharding strategy for data distribution in JAX computation, using the specified mesh and sharding configuration." <EXPLAINS> "CODE.named_sharding = jax.sharding.NamedSharding(mesh, spec)" .

"DESCRIPTION.The code initializes a neural network model with 1 layer of 10 neurons and compiles it using RMSprop optimizer with mean squared error loss. It then trains the model on a dataset of shape (5, 20) with zero labels. After training, it sets new weights for the optimizer and returns the number of iterations performed." <EXPLAINS> """CODE.opt = tf.keras.optimizers.RMSprop()
m = tf.keras.models.Sequential([tf.keras.layers.Dense(10)])
m.compile(opt, loss='mse')
data = np.arange(100).reshape(5, 20)
labels = np.zeros(5)
print('Training'); results = m.fit(data, labels)
new_weights = [np.array(10), np.ones([20, 10]), np.zeros([10])]
opt.set_weights(new_weights)
opt.iterations""" .

"DESCRIPTION.The code initializes a neural network model with 256 features and initializes its variables using a random number generator and a shape data structure of size (1, 128) with float32 data type." <EXPLAINS> """CODE.model = nn.Dense(features=256)
variables = model.lazy_init(rng, jax.ShapeDtypeStruct((1, 128), jnp.float32))
""" .

"DESCRIPTION.The code initializes a neural network model with a linear layer of input size 20 and output size 100, followed by a batch normalization layer with a dimension of 100. It then moves the model to the GPU for computation. Additionally, it creates a distributed process group with the given process IDs and converts the batch normalization module to synchronize across the process group." <EXPLAINS> """CODE.torch.nn.Sequential(
            torch.nn.Linear(20, 100),
            torch.nn.BatchNorm1d(100)
          ).cuda()

process_group = torch.distributed.new_group(process_ids)
sync_bn_module = convert_sync_batchnorm(module, process_group)""" .

"DESCRIPTION.The code initializes a neural network module with a linear layer of input size 100 and output size 100 on a CUDA device." <EXPLAINS> """CODE.import torch.nn as nn
from accelerate import init_on_device

with init_on_device(device=torch.device("cuda")):
    tst = nn.Liner(100, 100)  # on `cuda` device
""" .

"DESCRIPTION.The code initializes a parallel computing environment using PaddlePaddle, assigns data based on the local_rank, reduces the data across all ranks using the sum operation, and then converts the result back to a numpy array." <EXPLAINS> """CODE.import numpy as np
import paddle
from paddle.distributed import ReduceOp
from paddle.distributed import init_parallel_env

paddle.set_device('gpu:%d'%paddle.distributed.ParallelEnv().dev_id)
init_parallel_env()
if paddle.distributed.ParallelEnv().local_rank == 0:
    np_data = np.array([[4, 5, 6], [4, 5, 6]])
else:
    np_data = np.array([[1, 2, 3], [1, 2, 3]])
data = paddle.to_tensor(np_data)
paddle.distributed.all_reduce(data, op=ReduceOp.SUM)
out = data.numpy()""" .

"DESCRIPTION.The code initializes a parallel environment using PaddlePaddle and defines an orthogonal strategy for distributed training with data parallelism (DP), model parallelism (MP), and pipeline parallelism (PP) of 2 processes each, with a fused strategy dict for checking MP and PP processes." <EXPLAINS> """CODE.import paddle
import paddle.distributed as dist
from paddle.distributed.fleet.base.strategy_group import DPGroup, MPGroup, PPGroup
from paddle.distributed.fleet.base.orthogonal_strategy import OrthogonalStrategy

dist.init_parallel_env()
strategy = OrthogonalStrategy([("dp", 2, DPGroup), ("mp", 2, MPGroup), ("pp", 2, PPGroup)], fused_strategy_dict={"check": ["mp", "pp"]})""" .

"DESCRIPTION.The code initializes a parallel environment using PaddlePaddle's dygraph module and prints the device ID." <EXPLAINS> """CODE.import paddle.distributed as dist
env = dist.ParallelEnv()
print("The device id are %d" % env.device_id)""",
        """CODE.import paddle.fluid as fluid
env = fluid.dygraph.ParallelEnv()
print("The device id are %d" % env.dev_id)""" .

"DESCRIPTION.The code initializes a parallel environment using PaddlePaddle, generates a 2x3 tensor filled with random numbers, performs an all_reduce operation on the tensor across all parallel processes, and waits for the operation to complete." <EXPLAINS> """CODE.import paddle

paddle.distributed.init_parallel_env()
tindata = paddle.randn(shape=[2, 3])
paddle.distributed.all_reduce(tindata, use_calc_stream=True)
paddle.distributed.wait(tindata)""" .

"DESCRIPTION.The code initializes a parallel environment using paddle.distributed, defines a strategy group consisting of two subgroups [0, 1] and [2, 3], and then prints the world size of the strategy group, which is 2." <EXPLAINS> """CODE.import paddle.distributed as dist
from paddle.distributed.fleet.base.strategy_group import StrategyGroupBase

dist.init_parallel_env()
strategy_group = dist.fleet.base.strategy_group.StrategyGroupBase([[0, 1], [2, 3]])
print(strategy_group.world_size)  # 2""" .

"DESCRIPTION.The code initializes a parallel environment using the PaddlePaddle library and prints the device ID." <EXPLAINS> """CODE.import paddle.fluid as fluid
env = fluid.dygraph.ParallelEnv()
print("The device id are %d" % env.dev_id)""" .

"DESCRIPTION.The code initializes a parallel environment using the PaddlePaddle library and prints the number of ranks in the environment." <EXPLAINS> """CODE.import paddle.fluid as fluid
env = fluid.dygraph.ParallelEnv()
print("The nranks is %d" % env.nranks)""" .

"DESCRIPTION.The code initializes a parallel environment using the `paddle.distributed` module, generates a random tensor of shape [2, 3], creates a new process group consisting of ranks 2, 4, and 6, and performs an all-reduce operation on the `tindata` tensor using the created process group." <EXPLAINS> """CODE.import paddle
paddle.distributed.init_parallel_env()
tindata = paddle.randn(shape=[2, 3])
gp = paddle.distributed.new_group([2,4,6])
paddle.distributed.all_reduce(tindata, group=gp, use_calc_stream=False)""" .

"DESCRIPTION.The code initializes a pbag object with a list containing the integer 1. It then adds the integer 1 to the pbag object s, creating a new pbag object s2. Finally, it adds the integer 2 to the pbag object s, creating a new pbag object s3." <EXPLAINS> """CODE.s = pbag([1])
s2 = s.add(1)
s3 = s.add(2)""" .

"DESCRIPTION.The code initializes a pipe input object and sends the data 'inputdata' through the input pipe." <EXPLAINS> """CODE.input = PipeInput()
input.send('inputdata')""" .

"DESCRIPTION.The code initializes a pre-trained EfficientNet model with the architecture efficientnet-b0." <EXPLAINS> "CODE.model = EfficientNet.from_pretrained('efficientnet-b0')" .

"DESCRIPTION.The code initializes a pre-trained Faster R-CNN model using the MobileNet V3 architecture with a large backbone and a feature pyramid network of 320x320 input size. The model is set to evaluation mode. It then generates random input tensors of different sizes (3x300x400 and 3x500x400) and runs the model on these inputs to make object detection predictions." <EXPLAINS> """CODE.model = torchvision.models.detection.fasterrcnn_mobilenet_v3_large_320_fpn(pretrained=True)
model.eval()
x = [torch.rand(3, 300, 400), torch.rand(3, 500, 400)]
predictions = model(x)""" .

"DESCRIPTION.The code initializes a pre-trained FlaxBlenderbotSmall model and tokenizer from the Facebook Blenderbot Small model. It then encodes the input text using the tokenizer and the model's encoder. Next, it prepares the decoder input ids and decodes the generated output using the model. Lastly, it retrieves the last hidden states of the decoder from the outputs." <EXPLAINS> """CODE.model = FlaxBlenderbotSmallForConditionalGeneration.from_pretrained('facebook/blenderbot_small-90M')
tokenizer = BlenderbotSmallTokenizer.from_pretrained('facebook/blenderbot_small-90M')
text = "My friends are cool but they eat too many carbs."
inputs = tokenizer(text, max_length=1024, return_tensors='np')
encoder_outputs = model.encode(**inputs)
decoder_start_token_id = model.config.decoder_start_token_id
decoder_input_ids = jnp.ones((inputs.input_ids.shape[0], 1), dtype="i4") * decoder_start_token_id
outputs = model.decode(decoder_input_ids, encoder_outputs)
last_decoder_hidden_states = outputs.last_hidden_state""" .

"DESCRIPTION.The code initializes a pretrained Fasterrcnn model with MobileNetV3 as the backbone architecture, sets the model to evaluation mode, and makes predictions on two input tensors of images with different dimensions." <EXPLAINS> """CODE.model = torchvision.models.detection.fasterrcnn_mobilenet_v3_large_fpn(pretrained=True)
model.eval()
x = [torch.rand(3, 300, 400), torch.rand(3, 500, 400)]
predictions = model(x)
""" .

"DESCRIPTION.The code initializes a pretrained FlaxBlenderbot model and tokenizer from the 'facebook/blenderbot-400M-distill' checkpoint. It then encodes input text using the tokenizer and the model, generates initial decoder input ids, decodes the input using the model, and retrieves the last hidden states of the decoder." <EXPLAINS> """CODE.model = FlaxBlenderbotForConditionalGeneration.from_pretrained('facebook/blenderbot-400M-distill')
tokenizer = BlenderbotTokenizer.from_pretrained('facebook/blenderbot-400M-distill')
inputs = tokenizer(text, max_length=1024, return_tensors='jax')
encoder_outputs = model.encode(**inputs)
decoder_start_token_id = model.config.decoder_start_token_id
decoder_input_ids = jnp.ones((inputs.input_ids.shape[0], 1), dtype="i4") * decoder_start_token_id
outputs = model.decode(decoder_input_ids, encoder_outputs)
last_decoder_hidden_states = outputs.last_hidden_state""" .

"DESCRIPTION.The code initializes a program translator for converting imperative code to symbolic code in PaddlePaddle framework and retrieves the program cache." <EXPLAINS> """CODE.import paddle.fluid as fluid
prog_trans = fluid.dygraph.ProgramTranslator()
prog_cache = prog_trans.get_program_cache()""" .

"DESCRIPTION.The code initializes a queue with a maximum size of 100, creates a read operation from the queue, combines the write and read operations to run concurrently in asynchronous mode, and then executes the combined operation." <EXPLAINS> """CODE.queue = queue.Queue(100)
read_op = Dequeue(queue)
combined_op = Concurrently([write_op, read_op], mode="async")
next(combined_op)""" .

"DESCRIPTION.The code initializes a random normal distribution with mean 0 and standard deviation 1, creates a 2x2 tensor using this random normal distribution, and then initializes a dense layer with 3 units using the same random normal distribution." <EXPLAINS> """CODE.initializer = tf.keras.initializers.RandomNormal(mean=0., stddev=1.)
values = initializer(shape=(2, 2))
initializer = tf.keras.initializers.RandomNormal(mean=0., stddev=1.)
layer = tf.keras.layers.Dense(3, kernel_initializer=initializer)""" .

"DESCRIPTION.The code initializes a random number generator for statistics, then creates a variable named 'mean' with a LeCun normal distribution with shape (2, 2) using the random number generator." <EXPLAINS> """CODE.key = self.make_rng('stats')
mean = self.variable('stats', 'mean', lecun_normal(), key, (2, 2))
""" .

"DESCRIPTION.The code initializes a random uniform function with range from -1 to 1, retrieves the configuration of the initialized function, and creates a new random uniform function using the retrieved configuration." <EXPLAINS> """CODE.initializer = RandomUniform(-1, 1)
config = initializer.get_config()
initializer = RandomUniform.from_config(config)""" .

"DESCRIPTION.The code initializes a replay buffer with a capacity of 1000, creates parallel rollouts, stores the rollouts data in the replay buffer, and advances the storage operation." <EXPLAINS> """CODE.buf = ReplayBuffer(1000)
rollouts = ParallelRollouts(...)
store_op = rollouts.for_each(StoreToReplayBuffer(buf))
next(store_op)""" .

"DESCRIPTION.The code initializes a role for a worker in a distributed training setup using PaddlePaddle. It sets up the worker with specific endpoints for communication and prints a message indicating it is worker 0." <EXPLAINS> """CODE.import paddle.distributed.fleet as fleet
from paddle.distributed.fleet import UserDefinedRoleMaker

role = UserDefinedRoleMaker(
    is_collective=False,
    init_gloo=False,
    current_id=0,
    role=fleet.Role.WORKER,
    worker_endpoints=["127.0.0.1:6003", "127.0.0.1:6004"],
    server_endpoints=["127.0.0.1:6001", "127.0.0.1:6002"])
fleet.init(role)

fleet.util.print_on_rank("I'm worker 0", 0)""" .

"DESCRIPTION.The code initializes a sampling process using an initial set of arguments and tensors. It then iterates over time steps, generating sample IDs based on cell output and state. At each time step, it updates the sampling process and generates the next inputs, state, and a boolean flag indicating if the process has finished." <EXPLAINS> """CODE.sampler = Sampler(init_args)
(initial_finished, initial_inputs) = sampler.initialize(input_tensors)
for time_step in range(time):
  cell_output, cell_state = cell.call(cell_input, previous_state)
  sample_ids = sampler.sample(time_step, cell_output, cell_state)
  (finished, next_inputs, next_state) = sampler.next_inputs(
      time_step,cell_output, cell_state)""" .

"DESCRIPTION.The code initializes a saver object with a root directory, restores data from a specified path, asserts that the data has been consumed, and then runs additional restore operations." <EXPLAINS> """CODE.saver = Saver(root)
saver.restore(path).assert_consumed()

saver.restore(path).assert_consumed().run_restore_ops()
""" .

"DESCRIPTION.The code initializes a sequence categorical column named 'watches' with 1000 buckets, creates a sequence embedding column with a dimension of 10 for 'watches', and sets 'watches' as a column. It then parses the example features using the column specification, creates a sequence input layer using the features and columns, defines a BasicRNNCell with a specified hidden size, and performs dynamic RNN computation using the input layer and sequence lengths to generate outputs and states." <EXPLAINS> """CODE.watches = sequence_categorical_column_with_identity(
    'watches', num_buckets=1000)
watches_embedding = _sequence_embedding_column(watches, dimension=10)
columns = [watches]

features = tf.parse_example(..., features=make_parse_example_spec(columns))
input_layer, sequence_length = sequence_input_layer(features, columns)

rnn_cell = tf.nn.rnn_cell.BasicRNNCell(hidden_size)
outputs, state = tf.nn.dynamic_rnn(
    rnn_cell, inputs=input_layer, sequence_length=sequence_length)
""" .

"DESCRIPTION.The code initializes a sequential model in Python using the Keras library. It adds a masking layer to the model with a specified mask value of 0 and input shape of (timesteps, features). It then adds an LSTM layer with 32 units to the model." <EXPLAINS> """CODE.model = Sequential()
model.add(Masking(mask_value=0., input_shape=(timesteps, features)))
model.add(LSTM(32))
""" .

"DESCRIPTION.The code initializes a setup function that loads data, splits it into training, validation, and testing datasets, and assigns them to respective variables for later use." <EXPLAINS> """CODE.def setup(self, stage):
    data = load_data(...)
    self.train_ds, self.val_ds, self.test_ds = split_data(data)""" .

"DESCRIPTION.The code initializes a sparse lookup table that can be accessed using the input data and retrieves embeddings for the input sequence." <EXPLAINS> """CODE.import paddle.fluid as fluid
data = fluid.layers.data(name='sequence', shape=[1], dtype='int64', lod_level=1)
emb = fluid.layers.nn._pull_sparse(
    input=data, size=11, table_id=0, accessor_class="DownpourCtrAccessor")""" .

"DESCRIPTION.The code initializes a speech encoder-decoder model using the pre-trained wav2vec2-large-lv60 encoder and bart-large decoder. It then encodes an input sequence, generates decoder input tokens, decodes the input using the model, and retrieves the logits as outputs." <EXPLAINS> """CODE.from transformers import FlaxSpeechEncoderDecoderModel
import jax.numpy as jnp

model = FlaxSpeechEncoderDecoderModel.from_encoder_decoder_pretrained(
    "facebook/wav2vec2-large-lv60", "facebook/bart-large"
)

inputs = jnp.ones((2, 5000), dtype=jnp.float32)
encoder_outputs = model.encode(inputs)

decoder_start_token_id = model.config.decoder.bos_token_id
decoder_input_ids = jnp.ones((inputs.shape[0], 1), dtype="i4") * decoder_start_token_id

outputs = model.decode(decoder_input_ids, encoder_outputs)
logits = outputs.logits""" .

"DESCRIPTION.The code initializes a strategy for distributed training using TensorFlow. It creates a function that returns constant values based on the replica id or the number of replicas in sync. The values are distributed across multiple workers or replicas based on the strategy used (MirroredStrategy or TPUStrategy). The local results of the distributed values are then retrieved." <EXPLAINS> """CODE.strategy = tf.distribute.MirroredStrategy()
def value_fn(ctx):
  return tf.constant(1.)
distributed_values = (
     strategy.experimental_distribute_values_from_function(
       value_fn))
local_result = strategy.experimental_local_results(distributed_values)
local_result

strategy = tf.distribute.MirroredStrategy()
array_value = np.array([3., 2., 1.])
def value_fn(ctx):
  return array_value[ctx.replica_id_in_sync_group]
distributed_values = (
     strategy.experimental_distribute_values_from_function(
       value_fn))
local_result = strategy.experimental_local_results(distributed_values)
local_result

strategy = tf.distribute.MirroredStrategy()
def value_fn(ctx):
  return ctx.num_replicas_in_sync
distributed_values = (
     strategy.experimental_distribute_values_from_function(
       value_fn))
local_result = strategy.experimental_local_results(distributed_values)
local_result

strategy = tf.distribute.TPUStrategy()
worker_devices = strategy.extended.worker_devices
multiple_values = []
for i in range(strategy.num_replicas_in_sync):
  with tf.device(worker_devices[i]):
    multiple_values.append(tf.constant(1.0))

def value_fn(ctx):
  return multiple_values[ctx.replica_id]

distributed_values = strategy.
  experimental_distribute_values_from_function(
  value_fn)""" .

"DESCRIPTION.The code initializes a string to index table from a vocabulary file, maps the strings in the 'features' constant to their corresponding indices using the table, and then initializes the table." <EXPLAINS> """CODE.features = tf.constant(["emerson", "lake", "and", "palmer"])
table = tf.contrib.lookup.index_table_from_file(
    vocabulary_file="test.txt", num_oov_buckets=1)
ids = table.lookup(features)
...
tf.tables_initializer().run()

ids.eval()  ==> [0, 1, 3, 2]  # where 3 is the out-of-vocabulary bucket
""",
        """CODE.features = tf.constant(["emerson", "lake", "and", "palmer"])
table = tf.contrib.lookup.string_to_index_table_from_file(
    vocabulary_file="test.txt", num_oov_buckets=1)
ids = table.lookup(features)
...
tf.tables_initializer().run()
""" .

"DESCRIPTION.The code initializes a tokenizer from a pre-trained model for code generation and then tokenizes the input text \"Hello world\" twice, returning the input_ids for each tokenization." <EXPLAINS> """CODE.from transformers import CodeGenTokenizerFast
tokenizer = CodeGenTokenizerFast.from_pretrained("Salesforce/codegen-350M-mono")
tokenizer("Hello world")['input_ids']
tokenizer(" Hello world")['input_ids']""" .

"DESCRIPTION.The code initializes a trainer for training a model using a specific training program, optimizer function, and checkpoint configuration, and then starts the training process." <EXPLAINS> """CODE.config = fluid.CheckpointConfig("./checkpoints")
trainer = fluid.Trainer(train_func=train_program,
                        place=place,
                        optimizer_func=optimizer_func,
                        checkpoint_config=config)
trainer.train(...)""" .

"DESCRIPTION.The code initializes a trainer object, then trains the model 10 times using the train method of the trainer object. Finally, it exports the policy checkpoint to the specified directory." <EXPLAINS> """CODE.trainer = MyTrainer()
for _ in range(10):
    trainer.train()
trainer.export_policy_checkpoint("/tmp/export_dir")""" .

"DESCRIPTION.The code initializes a transformer model (T5) and a tokenizer, then encodes a given text input using the model to generate logits as outputs." <EXPLAINS> """CODE.from transformers import T5Tokenizer, FlaxLongT5ForConditionalGeneration
import jax.numpy as jnp

tokenizer = T5Tokenizer.from_pretrained("t5-base")
model = FlaxLongT5ForConditionalGeneration.from_pretrained("google/long-t5-local-base")

text = "My friends are cool but they eat too many carbs."
inputs = tokenizer(text, return_tensors="np")
encoder_outputs = model.encode(**inputs)

decoder_start_token_id = model.config.decoder_start_token_id
decoder_input_ids = jnp.ones((inputs.input_ids.shape[0], 1), dtype="i4") * decoder_start_token_id

outputs = model.decode(decoder_input_ids, encoder_outputs)
logits = outputs.logits""" .

"DESCRIPTION.The code initializes a transformer model encoder using a configuration based on susnato/clvp_dev style, then accesses the configuration of the model." <EXPLAINS> """CODE.from transformers import ClvpEncoderConfig, ClvpEncoder

# Initializing a ClvpEncoderConfig with susnato/clvp_dev style configuration
encoder_configuration = ClvpEncoderConfig()

# Initializing a ClvpEncoder (with random weights) from the susnato/clvp_dev style configuration
model = ClvpEncoder(encoder_configuration)

# Accessing the model configuration
configuration = model.config
""" .

"DESCRIPTION.The code initializes a transformer model for encoder-decoder architecture and loads pre-trained weights from the \"ydshieh/bert2bert-cnn_dailymail-fp16\" checkpoint." <EXPLAINS> """CODE.from transformers import TFEncoderDecoderModel
model = TFEncoderDecoderModel.from_pretrained("ydshieh/bert2bert-cnn_dailymail-fp16")""" .

"DESCRIPTION.The code initializes a user-defined role maker for distributed training in a worker role. It sets specific worker and server endpoints for communication and then initializes the fleet. It also uses fleet utility functions to get file shards for files \"file1\", \"file2\", and \"file3\", and prints the result." <EXPLAINS> """CODE.import paddle.distributed.fleet as fleet
from paddle.distributed.fleet import UserDefinedRoleMaker

role = UserDefinedRoleMaker(
    is_collective=False,
    init_gloo=False,
    current_id=0,
    role=fleet.Role.WORKER,
    worker_endpoints=["127.0.0.1:6003", "127.0.0.1:6004"],
    server_endpoints=["127.0.0.1:6001", "127.0.0.1:6002"])
fleet.init(role)

files = fleet.util.get_file_shard(["file1", "file2", "file3"])
print(files)""" .

"DESCRIPTION.The code initializes a variable called \"first\" with a Word object containing numbers, then initializes a variable called \"second\" with the result of matching a previous expression with the \"first\" object. Finally, it creates a new expression called \"matchExpr\" by concatenating the values of \"first\", a colon symbol \":\" and \"second\"." <EXPLAINS> """CODE.first = Word(nums)
second = matchPreviousExpr(first)
matchExpr = first + ":" + second""" .

"DESCRIPTION.The code initializes a variable v1 with values 1, 2, and a dictionary with keys 'a' and 'b' set to 5 and 6 respectively. Then it sets the value at index 2 with key 'b' to 17. Finally, it sets the value at index 2 with keys 'c' and 'd' to 17." <EXPLAINS> """CODE.v1 = v(1, 2, m(a=5, b=6))
v1.set_in((2, 'b'), 17)
v1.set_in((2, 'c', 'd'), 17)""" .

"DESCRIPTION.The code initializes a variable with a 2x3 matrix of random numbers using Keras backend. Then, it creates a new variable initialized with zeros having the same shape as the previous variable. Finally, it evaluates and prints out the zeros variable." <EXPLAINS> """CODE.from keras import backend as K
kvar = K.variable(np.random.random((2,3)))
kvar_zeros = K.zeros_like(kvar)
K.eval(kvar_zeros)
""" .

"DESCRIPTION.The code initializes a weight matrix, creates an EmbeddingBag module using the weights, creates an input tensor, and then applies the EmbeddingBag module to the input tensor, returning the embeddings for the indices in the input tensor." <EXPLAINS> """CODE.weight = torch.FloatTensor([[1, 2.3, 3], [4, 5.1, 6.3]])
embeddingbag = nn.EmbeddingBag.from_pretrained(weight)
input = torch.LongTensor([[1, 0]])
embeddingbag(input)""" .

"DESCRIPTION.The code initializes a weight tensor of shape (3, 2) with all values set to one using JAX framework." <EXPLAINS> """CODE.import jax, jax.numpy as jnp
jax.nn.initializers.ones(jax.random.PRNGKey(42), (3, 2), jnp.float32)""" .

"DESCRIPTION.The code initializes an Accelerator object for mixed precision computation using FP8 format with Hybrid mode." <EXPLAINS> """CODE.from accelerate import Accelerator
from accelerate.utils import FP8RecipeKwargs

kwargs = FP8RecipeKwargs(fp8_format="HYBRID")
accelerator = Accelerator(mixed_precision="fp8", kwargs_handlers=[kwargs])
""" .

"DESCRIPTION.The code initializes an Accelerator object with AutocastKwargs configuration for caching enabled." <EXPLAINS> """CODE.from accelerate import Accelerator
from accelerate.utils import AutocastKwargs

kwargs = AutocastKwargs(cache_enabled=True)
accelerator = Accelerator(kwargs_handlers=[kwargs])
""" .

"DESCRIPTION.The code initializes an AlignTextModel with random weights based on the kakaobrain/align-base style configuration and then accesses the model configuration." <EXPLAINS> """CODE.from transformers import AlignTextConfig, AlignTextModel

# Initializing a AlignTextConfig with kakaobrain/align-base style configuration
configuration = AlignTextConfig()

# Initializing a AlignTextModel (with random weights) from the kakaobrain/align-base style configuration
model = AlignTextModel(configuration)

# Accessing the model configuration
configuration = model.config
""" .

"DESCRIPTION.The code initializes an AltCLIP configuration, model, and text and vision configurations, and then creates a new AltCLIP configuration using the text and vision configurations." <EXPLAINS> """CODE.from transformers import AltCLIPConfig, AltCLIPModel
configuration = AltCLIPConfig()
model = AltCLIPModel(configuration)
configuration = model.config
config_text = AltCLIPTextConfig()
config_vision = AltCLIPVisionConfig()
config = AltCLIPConfig.from_text_vision_configs(config_text, config_vision)
""" .

"DESCRIPTION.The code initializes an ESM (Efficient Speech Model) model with a specific configuration, then accesses and stores the model's configuration." <EXPLAINS> """CODE.from transformers import EsmModel, EsmConfig

# Initializing a ESM esm-base-uncased style configuration configuration = EsmConfig()

# Initializing a model from the configuration model = ESMModel(configuration)

# Accessing the model configuration configuration = model.config
""" .

"DESCRIPTION.The code initializes an ImageFilenameProvider class that generates filenames for rows in a dataset based on specific parameters. It then utilizes the ImageFilenameProvider class to read image data from a parquet file and write the images to a specified directory with the specified file format (png) and naming convention." <EXPLAINS> """CODE.import ray
from ray.data.datasource import FilenameProvider

class ImageFilenameProvider(FilenameProvider):

    def __init__(self, file_format: str):
        self.file_format = file_format

    def get_filename_for_row(self, row, task_index, block_index, row_index):
        return (
            f"{row['label']}_{task_index:06}_{block_index:06}"
            f"_{row_index:06}.{self.file_format}"
        )

ds = ray.data.read_parquet("s3://anonymous@ray-example-data/images.parquet")
ds.write_images(
    "/tmp/results",
    column="image",
    filename_provider=ImageFilenameProvider("png")
)""" .

"DESCRIPTION.The code initializes an IpuStrategy object and then disables the \"ViewSimplifyPattern\" pattern within the strategy." <EXPLAINS> """CODE.ipu_strategy = static.IpuStrategy()
ipu_strategy.disable_pattern("ViewSimplifyPattern")""" .

"DESCRIPTION.The code initializes an RMSprop optimizer, creates a neural network model with a single layer of 10 neurons, compiles the model using the optimizer and mean squared error loss, prepares data and labels for training, prints 'Training' to the console, fits the model to the data and labels, and then returns the length of the weights in the optimizer." <EXPLAINS> """CODE.opt = tf.keras.optimizers.RMSprop()
m = tf.keras.models.Sequential([tf.keras.layers.Dense(10)])
m.compile(opt, loss='mse')
data = np.arange(100).reshape(5, 20)
labels = np.zeros(5)
print('Training'); results = m.fit(data, labels)
len(opt.get_weights())""" .

"DESCRIPTION.The code initializes an agent for reinforcement learning using the CartPole-v1 environment and a replay buffer that can store up to 10 transitions." <EXPLAINS> """CODE.env = gym.make("CartPole-v1")
buffer = ReplayBuffer(10)
Agent(env, buffer)  # doctest: +ELLIPSIS
<...reinforce_learn_Qnet.Agent object at ...>""" .

"DESCRIPTION.The code initializes an agent with the Hugging Face API for a model called StarCoder, and then interacts with the agent by providing different chat prompts to generate corresponding outputs." <EXPLAINS> """CODE.from transformers import HfAgent

agent = HfAgent("https://api-inference.huggingface.co/models/bigcode/starcoder")
agent.chat("Draw me a picture of rivers and lakes")

agent.chat("Transform the picture so that there is a rock in there")
""" .

"DESCRIPTION.The code initializes an argument parser using the argparse module, adds arguments specific to a Trainer class to the parser, parses the input arguments provided ([]) and prints the parsed arguments using the pprint module." <EXPLAINS> """CODE.parser = argparse.ArgumentParser()
parser = Trainer.add_argparse_args(parser)
args = parser.parse_args([])
pprint.pprint(vars(args))  # doctest: +ELLIPSIS +NORMALIZE_WHITESPACE""" .

"DESCRIPTION.The code initializes an array [1.0, 2.0, 3.0] using JAX library and retrieves the value of the array from the device." <EXPLAINS> """CODE.import jax
x = jax.numpy.array([1., 2., 3.])
jax.device_get(x)

import jax
jax.device_get(1)
""" .

"DESCRIPTION.The code initializes an encoder-decoder model from a pretrained checkpoint, saves the encoder and decoder components separately, and then loads the encoder and decoder models from the saved checkpoints. Finally, it sets the configuration of the loaded model to match the configuration of the original pretrained model." <EXPLAINS> """CODE._model = EncoderDecoderModel.from_pretrained("patrickvonplaten/bert2bert-cnn_dailymail-fp16")
_model.encoder.save_pretrained("./encoder")
_model.decoder.save_pretrained("./decoder")
model = TFEncoderDecoderModel.from_encoder_decoder_pretrained(
...     "./encoder", "./decoder", encoder_from_pt=True, decoder_from_pt=True
... )
model.config = _model.config""" .

"DESCRIPTION.The code initializes an in-memory dataset for distributed training in PaddlePaddle framework and sets the merging strategy to merge data by line ID." <EXPLAINS> """CODE.import paddle
paddle.enable_static()
dataset = paddle.distributed.InMemoryDataset()
dataset._set_merge_by_lineid()""",
        """CODE.import paddle.base as base
dataset = base.DatasetFactory().create_dataset("InMemoryDataset")
dataset.set_merge_by_lineid()""" .

"DESCRIPTION.The code initializes an initial state for an attention wrapper using a specified data type and batch size and then creates a copy of this initial state with the cell state set to the encoder state." <EXPLAINS> """CODE.initial_state = attention_wrapper.zero_state(dtype=..., batch_size=...)
initial_state = initial_state.clone(cell_state=encoder_state)""" .

"DESCRIPTION.The code initializes an input shape of (2, 4, 5, 3), creates a random tensor of that shape using TensorFlow, applies a global average pooling operation along the spatial dimensions of the tensor, and prints the shape of the resulting tensor which is (2, 3)." <EXPLAINS> """CODE.input_shape = (2, 4, 5, 3)
x = tf.random.normal(input_shape)
y = tf.keras.layers.GlobalAveragePooling2D()(x)
print(y.shape)
(2, 3)""" .

"DESCRIPTION.The code initializes an instance of AzureOpenAiAgent with a specific deployment ID, API key, and resource name. It then runs the agent on a given text in Spanish to determine if it is positive or negative." <EXPLAINS> """CODE.from transformers import AzureOpenAiAgent

agent = AzureAiAgent(deployment_id="Davinci-003", api_key=xxx, resource_name=yyy)
agent.run("Is the following `text` (in Spanish) positive or negative?", text="Â¡Este es un API muy agradable!")
""" .

"DESCRIPTION.The code initializes an iterator for a dataset and then runs the initialization operation in a session." <EXPLAINS> """CODE.dataset = ...
iterator = dataset.make_initializable_iterator()
# ...
sess.run(iterator.initializer)
""" .

"DESCRIPTION.The code initializes an iterator from a dataset and runs the iterator's initializer using a session object in TensorFlow." <EXPLAINS> """CODE.dataset = ...
iterator = Iterator.from_dataset(dataset)
# ...
sess.run(iterator.initializer)
""" .

"DESCRIPTION.The code initializes an iterator from a dataset and then runs the iterator's initializer in a TensorFlow session." <EXPLAINS> """CODE.dataset = ...
iterator = Iterator.from_dataset(dataset)
# ...
sess.run(iterator.initializer)
""" .

"DESCRIPTION.The code initializes an observation space with a dimensionality of 5 using binary values, and then generates a random sample from this observation space." <EXPLAINS> """CODE.self.observation_space = spaces.MultiBinary(5)
self.observation_space.sample()""" .

"DESCRIPTION.The code initializes and accesses a Owlv2TextModel with a google/owlv2-base-patch32 style configuration." <EXPLAINS> """CODE.from transformers import Owlv2TextConfig, Owlv2TextModel

# Initializing a Owlv2TextModel with google/owlv2-base-patch32 style configuration
configuration = Owlv2TextConfig()

# Initializing a Owlv2TextConfig from the google/owlv2-base-patch32 style configuration
model = Owlv2TextModel(configuration)

# Accessing the model configuration
configuration = model.config
""" .

"DESCRIPTION.The code initializes and accesses a XLMRobertaXL model and its configuration." <EXPLAINS> """CODE.from transformers import XLMRobertaXLModel, XLMRobertaXLConfig

# Initializing a XLM_ROBERTA_XL bert-base-uncased style configuration
configuration = XLMRobertaXLConfig()

# Initializing a model from the bert-base-uncased style configuration
model = XLMRobertaXLModel(configuration)

# Accessing the model configuration
configuration = model.config
""" .

"DESCRIPTION.The code initializes and accesses configurations for an ALIGN model using AlignConfig, AlignModel, AlignTextConfig, and AlignVisionConfig from the transformers library." <EXPLAINS> """CODE.from transformers import AlignConfig, AlignModel

# Initializing a AlignConfig with kakaobrain/align-base style configuration
configuration = AlignConfig()

# Initializing a AlignModel (with random weights) from the kakaobrain/align-base style configuration
model = AlignModel(configuration)

# Accessing the model configuration
configuration = model.config

# We can also initialize a AlignConfig from a AlignTextConfig and a AlignVisionConfig
from transformers import AlignTextConfig, AlignVisionConfig

# Initializing ALIGN Text and Vision configurations
config_text = AlignTextConfig()
config_vision = AlignVisionConfig()

config = AlignConfig.from_text_vision_configs(config_text, config_vision)
""" .

"DESCRIPTION.The code initializes and accesses the configuration of a Data2VecAudio model based on the facebook/data2vec-audio-base-960h style configuration." <EXPLAINS> """CODE.from transformers import Data2VecAudioModel, Data2VecAudioConfig

# Initializing a Data2VecAudio facebook/data2vec-audio-base-960h style configuration
configuration = Data2VecAudioConfig()

# Initializing a model from the facebook/data2vec-audio-base-960h style configuration
model = Data2VecAudioModel(configuration)

# Accessing the model configuration
configuration = model.config
""" .

"DESCRIPTION.The code initializes and accesses the configuration of a Decision Transformer model." <EXPLAINS> """CODE.from transformers import DecisionTransformerModel, DecisionTransformerConfig

# Initializing a DecisionTransformer configuration
configuration = DecisionTransformerConfig()

# Initializing a model from the configuration
model = DecisionTransformerConfig(configuration)

# Accessing the model configuration
configuration = model.config
""" .

"DESCRIPTION.The code initializes and accesses the configuration of a Swin model based on the microsoft/swin-tiny-patch4-window7-224 style." <EXPLAINS> """CODE.from transformers import SwinModel, SwinConfig

# Initializing a Swin microsoft/swin-tiny-patch4-window7-224 style configuration
configuration = SwinConfig()

# Initializing a model from the microsoft/swin-tiny-patch4-window7-224 style configuration
model = SwinModel(configuration)

# Accessing the model configuration
configuration = model.config
""" .

"DESCRIPTION.The code initializes and calculates the probability density function (pdf) of Vector Laplace distributions with different parameterizations, including calculating the covariance matrix, defining the distribution with location and scale parameters, and computing the pdf for single and batch observations in R^3 space." <EXPLAINS> """CODE.ds = tf.contrib.distributions
la = tf.contrib.linalg

# Initialize a single 3-variate VectorLaplace with some desired covariance.
mu = [1., 2, 3]
cov = [[ 0.36,  0.12,  0.06],
       [ 0.12,  0.29, -0.13],
       [ 0.06, -0.13,  0.26]]

scale = tf.cholesky(cov)
# ==> [[ 0.6,  0. ,  0. ],
#      [ 0.2,  0.5,  0. ],
#      [ 0.1, -0.3,  0.4]])

# Divide scale by sqrt(2) so that the final covariance will be what we want.
vla = ds.VectorLaplaceLinearOperator(
    loc=mu,
    scale=la.LinearOperatorTriL(scale / tf.sqrt(2)))

# Covariance agrees with cholesky(cov) parameterization.
vla.covariance().eval()
# ==> [[ 0.36,  0.12,  0.06],
#      [ 0.12,  0.29, -0.13],
#      [ 0.06, -0.13,  0.26]]

# Compute the pdf of an`R^3` observation; return a scalar.
vla.prob([-1., 0, 1]).eval()  # shape: []

# Initialize a 2-batch of 3-variate Vector Laplace's.
mu = [[1., 2, 3],
      [11, 22, 33]]              # shape: [2, 3]
scale_diag = [[1., 2, 3],
              [0.5, 1, 1.5]]     # shape: [2, 3]

vla = ds.VectorLaplaceLinearOperator(
    loc=mu,
    scale=la.LinearOperatorDiag(scale_diag))

# Compute the pdf of two `R^3` observations; return a length-2 vector.
x = [[-0.9, 0, 0.1],
     [-10, 0, 9]]     # shape: [2, 3]
vla.prob(x).eval()    # shape: [2]
""" .

"DESCRIPTION.The code initializes and configures a trainer for a machine learning model training process. It sets various training parameters such as the number of epochs, steps, early stopping callback, checkpointing, and distributed backend options." <EXPLAINS> """CODE.parser = argparse.ArgumentParser()
parser = Trainer.add_argparse_args(parser)
args = parser.parse_args([])
pprint.pprint(vars(args))  # doctest: +ELLIPSIS +NORMALIZE_WHITESPACE
{...
 'check_val_every_n_epoch': 1,
 'checkpoint_callback': True,
 'default_root_dir': None,
 'deterministic': False,
 'distributed_backend': None,
 'early_stop_callback': False,
 ...
 'logger': True,
 'max_epochs': 1000,
 'max_steps': None,
 'min_epochs': 1,
 'min_steps': None,
 ...
 'profiler': None,
 'progress_bar_refresh_rate': 1,
 ...}""" .

"DESCRIPTION.The code initializes and defines a linear layer with general features, including different shapes and axes, and then initializes the parameters for the layer using random keys and input shapes. Finally, it maps the shapes of the parameters using JAX." <EXPLAINS> """CODE.import flax.linen as nn
import jax, jax.numpy as jnp

layer = nn.LinearGeneral(features=4)
layer = nn.LinearGeneral(features=(4, 5))
params = layer.init(jax.random.key(0), jnp.ones((1, 3)))
jax.tree_map(jnp.shape, params)
layer = nn.LinearGeneral(features=(4, 5), axis=(1, -1))
params = layer.init(jax.random.key(0), jnp.ones((1, 3, 6, 7))
jax.tree_map(jnp.shape, params)""" .

"DESCRIPTION.The code initializes and fine-tunes a speech encoder-decoder model by combining pretrained wav2vec2 and BART models. The model is saved and loaded for future use." <EXPLAINS> """CODE.from transformers import FlaxSpeechEncoderDecoderModel

# initialize a wav2vec2-2-bart from pretrained wav2vec2 and bart models. Note that the cross-attention layers will be randomly initialized
model = FlaxSpeechEncoderDecoderModel.from_encoder_decoder_pretrained(
...     "facebook/wav2vec2-large-lv60", "facebook/bart-large"
... )
# saving model after fine-tuning
model.save_pretrained("./wav2vec2-2-bart-large")
# load fine-tuned model
model = FlaxSpeechEncoderDecoderModel.from_pretrained("./wav2vec2-2-bart-large")
""" .

"DESCRIPTION.The code initializes and finishes the integration of a WandbTracer for tracking code execution in a notebook or script." <EXPLAINS> """CODE.
from wandb.integration.langchain import WandbTracer
WandbTracer.init()
# ...
# end of notebook / script:
WandbTracer.finish()
""" .

"DESCRIPTION.The code initializes and loads pre-trained models for table question answering using `tapas-base-finetuned-wtq` and a checkpoint file `tapas_tf_checkpoint.ckpt.index` with configuration settings from a JSON file." <EXPLAINS> """CODE.model = AutoModelForTableQuestionAnswering.from_pretrained('google/tapas-base-finetuned-wtq')
model = AutoModelForTableQuestionAnswering.from_pretrained('google/tapas-base-finetuned-wtq', output_attentions=True)
config = AutoConfig.from_json_file('./tf_model/tapas_tf_checkpoint.json')
model = AutoModelForQuestionAnswering.from_pretrained('./tf_model/tapas_tf_checkpoint.ckpt.index', from_tf=True, config=config)""" .

"DESCRIPTION.The code initializes and retrieves an instance of the ProgramTranslator class in the paddle.fluid.dygraph module, which is used for translating imperative-style program into declarative-style program in PaddlePaddle." <EXPLAINS> """CODE.import paddle.fluid as fluid
fluid.dygraph.ProgramTranslator()
fluid.dygraph.ProgramTranslator.get_instance()""" .

"DESCRIPTION.The code initializes and runs a Hugging Face Agent using the \"bigcode/starcoder\" model to determine if the input text (in Spanish) is positive or negative." <EXPLAINS> """CODE.from transformers import HfAgent

agent = HfAgent("https://api-inference.huggingface.co/models/bigcode/starcoder")
agent.run("Is the following `text` (in Spanish) positive or negative?", text="Â¡Este es un API muy agradable!")
""" .

"DESCRIPTION.The code initializes configurations and models for the Blip model for visual question answering (VQA) using the Salesforce/blip-vqa-base style configuration. The code also demonstrates how to access model configurations and initialize configurations using text and vision configurations." <EXPLAINS> """CODE.from transformers import BlipConfig, BlipModel

# Initializing a BlipConfig with Salesforce/blip-vqa-base style configuration
configuration = BlipConfig()

# Initializing a BlipPModel (with random weights) from the Salesforce/blip-vqa-base style configuration
model = BlipModel(configuration)

# Accessing the model configuration
configuration = model.config

# We can also initialize a BlipConfig from a BlipTextConfig and a BlipVisionConfig

# Initializing a BLIPText and BLIPVision configuration
config_text = BlipTextConfig()
config_vision = BlipVisionConfig()

config = BlipConfig.from_text_vision_configs(config_text, config_vision)
""" .

"DESCRIPTION.The code initializes deterministic vectors in R^2 and calculates the probability of these vectors. It first initializes a single deterministic vector supported at [0., 2.] and calculates the probability of [0., 2.] (which is 1) and [0., 3.] (which is 0). Then it initializes a batch of three deterministic vectors at different locations in R^2 and calculates the probabilities of three input vectors, returning a list of corresponding probabilities." <EXPLAINS> """CODE.# Initialize a single VectorDeterministic supported at [0., 2.] in R^2.
constant = tf.contrib.distributions.Deterministic([0., 2.])
constant.prob([0., 2.])
==> 1.
constant.prob([0., 3.])
==> 0.

# Initialize a [3] batch of constants on R^2.
loc = [[0., 1.], [2., 3.], [4., 5.]]
constant = constant_lib.VectorDeterministic(loc)
constant.prob([[0., 1.], [1.9, 3.], [3.99, 5.]])
==> [1., 0., 0.]
""" .

"DESCRIPTION.The code initializes different versions of a software package and accesses the public attribute of each version." <EXPLAINS> """CODE.Version("1.2.3").public
Version("1.2.3+abc").public
Version("1.2.3+abc.dev1").public""" .

"DESCRIPTION.The code initializes iterators for two datasets - one containing a range of 10 numbers and the other containing even numbers from the range. It then defines a model function that takes input from the iterator. The code trains the model for a specified number of epochs by iterating over the range dataset and the even dataset alternatively, fetching predictions and loss values for each iteration." <EXPLAINS> """CODE.iterator = Iterator.from_structure(tf.int64, tf.TensorShape([]))

dataset_range = Dataset.range(10)
range_initializer = iterator.make_initializer(dataset_range)

dataset_evens = dataset_range.filter(lambda x: x % 2 == 0)
evens_initializer = iterator.make_initializer(dataset_evens)

# Define a model based on the iterator; in this example, the model_fn
# is expected to take scalar tf.int64 Tensors as input (see
# the definition of 'iterator' above).
prediction, loss = model_fn(iterator.get_next())

# Train for `num_epochs`, where for each epoch, we first iterate over
# dataset_range, and then iterate over dataset_evens.
for _ in range(num_epochs):
  # Initialize the iterator to `dataset_range`
  sess.run(range_initializer)
  while True:
    try:
      pred, loss_val = sess.run([prediction, loss])
    except tf.errors.OutOfRangeError:
      break

  # Initialize the iterator to `dataset_evens`
  sess.run(evens_initializer)
  while True:
    try:
      pred, loss_val = sess.run([prediction, loss])
    except tf.errors.OutOfRangeError:
      break
""" .

"DESCRIPTION.The code initializes multivariate Gaussian distributions with diagonal and low-rank covariance structures. It computes the probability density function and covariance matrix for the specified distributions." <EXPLAINS> """CODE.ds = tf.contrib.distributions

# Initialize a single 3-variate Gaussian with covariance `cov = S @ S.T`,
# `S = diag(d) + U @ diag(m) @ U.T`. The perturbation, `U @ diag(m) @ U.T`, is
# a rank-2 update.
mu = [-0.5., 0, 0.5]   # shape: [3]
d = [1.5, 0.5, 2]      # shape: [3]
U = [[1., 2],
     [-1, 1],
     [2, -0.5]]        # shape: [3, 2]
m = [4., 5]            # shape: [2]
mvn = ds.MultivariateNormalDiagPlusLowRank(
    loc=mu
    scale_diag=d
    scale_perturb_factor=U,
    scale_perturb_diag=m)

# Evaluate this on an observation in `R^3`, returning a scalar.
mvn.prob([-1, 0, 1]).eval()  # shape: []

# Initialize a 2-batch of 3-variate Gaussians; `S = diag(d) + U @ U.T`.
mu = [[1.,  2,  3],
      [11, 22, 33]]      # shape: [b, k] = [2, 3]
U = [[[1., 2],
      [3,  4],
      [5,  6]],
     [[0.5, 0.75],
      [1,0, 0.25],
      [1.5, 1.25]]]      # shape: [b, k, r] = [2, 3, 2]
m = [[0.1, 0.2],
     [0.4, 0.5]]         # shape: [b, r] = [2, 2]

mvn = ds.MultivariateNormalDiagPlusLowRank(
    loc=mu,
    scale_perturb_factor=U,
    scale_perturb_diag=m)

mvn.covariance().eval()   # shape: [2, 3, 3]
# ==> [[[  15.63   31.57    48.51]
#       [  31.57   69.31   105.05]
#       [  48.51  105.05   162.59]]
#
#      [[   2.59    1.41    3.35]
#       [   1.41    2.71    3.34]
#       [   3.35    3.34    8.35]]]

# Compute the pdf of two `R^3` observations (one from each batch);
# return a length-2 vector.
x = [[-0.9, 0, 0.1],
     [-10, 0, 9]]     # shape: [2, 3]
mvn.prob(x).eval()    # shape: [2]
""" .

"DESCRIPTION.The code initializes the Ray framework, creates a linear neural network model, defines a stochastic gradient descent optimizer, sets up data loaders for training and validation data, creates a TorchTrainer instance with the specified components (model, data, optimizer, loss), and then trains the model for 4 epochs using the specified batch size and GPU acceleration." <EXPLAINS> """CODE.ray.init()

def model_creator(config):
    return nn.Linear(1, 1)


def optimizer_creator(model, config):
    return torch.optim.SGD(
        model.parameters(), lr=config.get("lr", 1e-4))


def data_creator(config):
    batch_size = config["batch_size"]
    train_data, val_data = LinearDataset(2, 5), LinearDataset(2, 5)
    train_loader = DataLoader(train_data, batch_size=batch_size)
    val_loader = DataLoader(val_data, batch_size=batch_size)
    return train_loader, val_loader


trainer = TorchTrainer(
    model_creator=model_creator,
    data_creator=data_creator,
    optimizer_creator=optimizer_creator,
    loss_creator=nn.MSELoss,
    config={"batch_size": 32},
    use_gpu=True
)
for i in range(4):
    trainer.train()
""" .

"DESCRIPTION.The code initializes the weight tensor 'w' by applying a truncated normal distribution." <EXPLAINS> "CODE.nn.init.trunc_normal_(w)" .

"DESCRIPTION.The code initializes two 2x2 matrices with ones and zeros, adds them together element-wise, and prints the result in the form of a numpy array." <EXPLAINS> """CODE.import paddle.fluid as fluid

fluid.enable_imperative()  # Now we are in imperative mode
x = fluid.layers.ones( (2, 2), "float32")
y = fluid.layers.zeros( (2, 2), "float32")
z = x + y
print( z.numpy() )   #[[1, 1], [1, 1]]
fluid.disable_imperative() # Now we are in declarative mode""" .

"DESCRIPTION.The code initializes two 2x2 matrices, x with all elements set to 1 and y with all elements set to 0, adds them together element-wise, and then prints the resulting matrix." <EXPLAINS> """CODE.import paddle.fluid as fluid

fluid.enable_imperative()  # Now we are in imperative mode
x = fluid.layers.ones( (2, 2), "float32")
y = fluid.layers.zeros( (2, 2), "float32")
z = x + y
print( z.numpy() )   #[[1, 1], [1, 1]]""" .

"DESCRIPTION.The code initializes two Dense layers (layer_a and layer_b) with different constant initializers for their kernel weights. It then calculates the output of both layers using input tensors, retrieves the weights of layer_a, sets the weights of layer_b to be the same as those of layer_a, and finally retrieves the weights of layer_b to confirm that they have been updated to match those of layer_a." <EXPLAINS> """CODE.layer_a = tf.keras.layers.Dense(1,
  kernel_initializer=tf.constant_initializer(1.))
a_out = layer_a(tf.convert_to_tensor([[1., 2., 3.]]))
layer_a.get_weights()
[array([[1.],
   [1.],
   [1.]], dtype=float32), array([0.], dtype=float32)]
layer_b = tf.keras.layers.Dense(1,
  kernel_initializer=tf.constant_initializer(2.))
b_out = layer_b(tf.convert_to_tensor([[10., 20., 30.]]))
layer_b.get_weights()
[array([[2.],
   [2.],
   [2.]], dtype=float32), array([0.], dtype=float32)]
layer_b.set_weights(layer_a.get_weights())
layer_b.get_weights()
[array([[1.],
   [1.],
   [1.]], dtype=float32), array([0.], dtype=float32)]""" .

"DESCRIPTION.The code initializes two Dense layers with different constant initializers, applies the layers to input tensors, retrieves the weights of the layers, sets the weights of the second layer to be the same as the weights of the first layer, and then retrieves the updated weights of the second layer." <EXPLAINS> """CODE.layer_a = tf.keras.layers.Dense(1,
  kernel_initializer=tf.constant_initializer(1.))
a_out = layer_a(tf.convert_to_tensor([[1., 2., 3.]]))
layer_a.get_weights()
layer_b = tf.keras.layers.Dense(1,
  kernel_initializer=tf.constant_initializer(2.))
b_out = layer_b(tf.convert_to_tensor([[10., 20., 30.]]))
layer_b.get_weights()
layer_b.set_weights(layer_a.get_weights())
layer_b.get_weights()""" .

"DESCRIPTION.The code initializes two LSTM cells with 128 and 64 units respectively, stacks them together to create a MultiRNN cell." <EXPLAINS> """CODE.num_units = [128, 64]
cells = [BasicLSTMCell(num_units=n) for n in num_units]
stacked_rnn_cell = MultiRNNCell(cells)""" .

"DESCRIPTION.The code initializes two arrays of ones, x and y, each with a shape of (10, 1), and then packs them into a dataset using the pack_x_y_sample_weight function from the tf.keras.utils module. Finally, it unpacks the data into variables x and y." <EXPLAINS> """CODE.x = tf.ones((10, 1))
data = tf.keras.utils.pack_x_y_sample_weight(x)
y = tf.ones((10, 1))
data = tf.keras.utils.pack_x_y_sample_weight(x, y)
x, y = data""" .

"DESCRIPTION.The code initializes two dense layers with different constant initializers, assigns weights to the first layer, applies the first layer to input data, retrieves the weights of the first layer, assigns the weights of the first layer to the second layer, and retrieves the weights of the second layer." <EXPLAINS> """CODE.layer_a = tf.keras.layers.Dense(1,
  kernel_initializer=tf.constant_initializer(1.))
a_out = layer_a(tf.convert_to_tensor([[1., 2., 3.]]))
layer_a.get_weights()
[array([[1.],
   [1.],
   [1.]], dtype=float32), array([0.], dtype=float32)]
layer_b = tf.keras.layers.Dense(1,
  kernel_initializer=tf.constant_initializer(2.))
b_out = layer_b(tf.convert_to_tensor([[10., 20., 30.]]))
layer_b.get_weights()
[array([[2.],
   [2.],
   [2.]], dtype=float32), array([0.], dtype=float32)]
layer_b.set_weights(layer_a.get_weights())
layer_b.get_weights()
[array([[1.],
   [1.],
   [1.]], dtype=float32), array([0.], dtype=float32)]""" .

"DESCRIPTION.The code initializes two dense layers, `layer_a` and `layer_b`, with different constant initializers. It then applies `layer_a` on input data `[1., 2., 3.]` and retrieves its weights. Next, it applies `layer_b` on input data `[10., 20., 30.]` and retrieves its weights. Finally, it sets the weights of `layer_b` to be the same as the weights of `layer_a` and retrieves the updated weights of `layer_b`." <EXPLAINS> """CODE.layer_a = tf.keras.layers.Dense(1,
  kernel_initializer=tf.constant_initializer(1.))
a_out = layer_a(tf.convert_to_tensor([[1., 2., 3.]]))
layer_a.get_weights()
layer_b = tf.keras.layers.Dense(1,
  kernel_initializer=tf.constant_initializer(2.))
b_out = layer_b(tf.convert_to_tensor([[10., 20., 30.]]))
layer_b.get_weights()
layer_b.set_weights(layer_a.get_weights())
layer_b.get_weights()""" .

"DESCRIPTION.The code initializes two dense layers, layer_a and layer_b, with different constant initializers for their kernels. It then calculates the output of layer_a with input [[1., 2., 3.]], retrieves the weights of layer_a, calculates the output of layer_b with input [[10., 20., 30.]], retrieves the weights of layer_b, sets the weights of layer_b to be the same as the weights of layer_a, and finally retrieves the weights of layer_b again." <EXPLAINS> """CODE.layer_a = tf.keras.layers.Dense(1,
  kernel_initializer=tf.constant_initializer(1.))
a_out = layer_a(tf.convert_to_tensor([[1., 2., 3.]]))
layer_a.get_weights()
layer_b = tf.keras.layers.Dense(1,
  kernel_initializer=tf.constant_initializer(2.))
b_out = layer_b(tf.convert_to_tensor([[10., 20., 30.]]))
layer_b.get_weights()
layer_b.set_weights(layer_a.get_weights())
layer_b.get_weights()""" .

"DESCRIPTION.The code initializes two dense layers, layer_a and layer_b, with different constant initializers. It then calculates the output of each layer for input values. Finally, it sets the weights of layer_b to be equal to the weights of layer_a." <EXPLAINS> """CODE.layer_a = tf.keras.layers.Dense(1,
  kernel_initializer=tf.constant_initializer(1.))
a_out = layer_a(tf.convert_to_tensor([[1., 2., 3.]]))
layer_a.get_weights()
[array([[1.],
       [1.],
       [1.]], dtype=float32), array([0.], dtype=float32)]
layer_b = tf.keras.layers.Dense(1,
  kernel_initializer=tf.constant_initializer(2.))
b_out = layer_b(tf.convert_to_tensor([[10., 20., 30.]]))
layer_b.get_weights()
[array([[2.],
       [2.],
       [2.]], dtype=float32), array([0.], dtype=float32)]
layer_b.set_weights(layer_a.get_weights())
layer_b.get_weights()
[array([[1.],
       [1.],
       [1.]], dtype=float32), array([0.], dtype=float32)]""" .

"DESCRIPTION.The code initializes two models: one using the Fully Sharded Data Parallel (FSDP) technique and the other using Distributed Data Parallel (DDP) technique. It then initializes Adam optimizers for both models with a learning rate of 1e-3. Finally, it retrieves the state dictionaries for both models and optimizers, asserting that the state dictionaries of the DDP and FSDP models are equal, ensuring the consistency of the model and optimizer parameters." <EXPLAINS> """CODE.import torch
from torch.distributed.fsdp import FullyShardedDataParallel as FSDP
from torch.nn.parallel import DistributedDataParallel as DDP
from torch.distributed.checkpoint.state_dict import get_state_dict

fsdp_model = FSDP(copy.deepcopy(model))
fsdp_optim = torch.optim.Adam(model.parameters(), lr=1e-3)
ddp_model = DDP(copy.deepcopy(model))
ddp_optim = torch.optim.Adam(model.parameters(), lr=1e-3)


ddp_state_dict, ddp_optim_state_dict = get_state_dict(ddp_model, ddp_optim)
fsdp_state_dict, fsdp_optim_state_dict = get_state_dict(fsdp_model, fsdp_optim)

# if we simply call ddp_model.state_dict() and fsdp_model.state_dict(),
# the asserts will fail.
assert ddp_state_dict == fsdp_state_dict
assert ddp_optim_state == fsdp_optim_state_dict""" .

"DESCRIPTION.The code initializes various configurations and models for Conditional Language Generation using the CLVP (Conditional Language Generation via Phonetics) architecture. The code demonstrates initializing configurations for text, speech, and decoder components, and combining them into a unified configuration." <EXPLAINS> """CODE.from transformers import ClvpConfig, ClvpModelForConditionalGeneration

# Initializing a ClvpConfig with susnato/clvp_dev style configuration
configuration = ClvpConfig()

# Initializing a ClvpModelForConditionalGeneration (with random weights) from the susnato/clvp_dev style configuration
model = ClvpModelForConditionalGeneration(configuration)

# Accessing the model configuration
configuration = model.config

# We can also initialize a CLVPConfig from a CLVPTextConfig, CLVPSpeechConfig and a CLVPAutoRegressiveConfig
from transformers import ClvpEncoderConfig, ClvpDecoderConfig

# Initializing a CLVP text, CLVP speech and CLVP decoder configuration
config_text = ClvpEncoderConfig()
config_speech = ClvpEncoderConfig()
decoder_config = ClvpDecoderConfig()

config = ClvpConfig.from_sub_model_configs(config_text, config_speech, decoder_config)
""" .

"DESCRIPTION.The code initializes weights and biases for a neural network layer using the zeros initializer." <EXPLAINS> """CODE.initializer = tf.keras.initializers.Zeros()
values = initializer(shape=(2, 2))
initializer = tf.keras.initializers.Zeros()
layer = tf.keras.layers.Dense(3, kernel_initializer=initializer)""" .

"DESCRIPTION.The code initializes weights for a layer using the HeNormal initializer in a neural network." <EXPLAINS> """CODE.initializer = tf.keras.initializers.HeNormal()
values = initializer(shape=(2, 2))

initializer = tf.keras.initializers.HeNormal()
layer = tf.keras.layers.Dense(3, kernel_initializer=initializer)
""" .

"DESCRIPTION.The code initiates a trip, runs a workflow asynchronously with the id \"trip1\", retrieves the output of the workflow with the id \"trip1\", and finally asserts that the results of the asynchronous run and the retrieved output are equal." <EXPLAINS> """CODE.trip = start_trip.step()
res1 = trip.async_run(workflow_id="trip1")
res2 = workflow.get_output("trip1")
assert ray.get(res1) == ray.get(res2)
""" .

"DESCRIPTION.The code initiates a workflow step for a trip, runs the workflow step with the specified workflow ID, and then asserts that the workflow status for the trip is successful." <EXPLAINS> """CODE.workflow_step = trip.step()
output = workflow_step.run(workflow_id="trip")
assert workflow.SUCCESSFUL == workflow.get_status("trip")""" .

"DESCRIPTION.The code inserts dictionaries into a sorted list based on the value of the key \"a\" within each dictionary." <EXPLAINS> """CODE._insert_into_sorted_list(list, {"a": 1, "b": 0}, lambda x: x["a"])
_insert_into_sorted_list(list, {"a": 3, "b": 1}, lambda x: x["a"])
_insert_into_sorted_list(list, {"a": 4, "b": 2}, lambda x: x["a"])
_insert_into_sorted_list(list, {"a": 1, "b": 3}, lambda x: x["a"])""" .

"DESCRIPTION.The code installs the XLA (Accelerated Linear Algebra) library with the option to upgrade if available." <EXPLAINS> """CODE.from accelerate.utils import install_xla
install_xla(upgrade=True)
""" .

"DESCRIPTION.The code instantiates a ConvNext model with its default configuration settings and retrieves the configuration from the model." <EXPLAINS> """CODE.from transformers import ConvNextModel, ConvNextConfig
configuration = ConvNextConfig()
model = ConvNextModel(configuration)
configuration = model.config
""" .

"DESCRIPTION.The code interpolates missing values in the provided array using linear method, with a limit of 3 consecutive missing values filled in one go in the forward direction." <EXPLAINS> """CODE.arr = pd.arrays.NumpyExtensionArray(np.array([0, 1, np.nan, 3]))
arr.interpolate(method="linear",
                limit=3,
                limit_direction="forward",
                index=pd.Index([1, 2, 3, 4]),
                fill_value=1,
                copy=False,
                axis=0,
                limit_area="inside"
                )""" .

"DESCRIPTION.The code is a distributed training script using PaddlePaddle's fleet API. It initializes a role maker for PaddlePaddle cloud training, checks if the current process is a server or a worker, performs allreduce operation on the input data based on the role, and prints the output. Finally, it performs an allreduce operation involving all processes and prints the final result." <EXPLAINS> """CODE.import paddle.distributed.fleet as fleet
from paddle.distributed.fleet import PaddleCloudRoleMaker
import sys
import numpy as np
import os

os.environ["PADDLE_WITH_GLOO"] = "2"

def train():
    role = PaddleCloudRoleMaker(
        is_collective=False,
        init_gloo=True,
        path="./tmp_gloo")
    fleet.init(role)

    if fleet.is_server():
        input = [1, 2]
        output = fleet.util.all_reduce(input, "sum", "server")
        print(output)
        # [2, 4]
    elif fleet.is_worker():
        input = np.array([3, 4])
        output = fleet.util.all_reduce(input, "sum", "worker")
        print(output)
        # [6, 8]
    output = fleet.util.all_reduce(input, "sum", "all")
    print(output)
    # [8, 12]
if __name__ == "__main__":
    train()""" .

"DESCRIPTION.The code is a test wrapper function that uses a parameterized test case to check if the input argument \"arg1\" is greater than 0.0." <EXPLAINS> """CODE.@RunIf(...)
@pytest.mark.parametrize("arg1", [1, 2.0])
def test_wrapper(arg1):
    assert arg1 > 0.0""" .

"DESCRIPTION.The code is adding scalar values of the functions xsinx, xcosx, and arctanx with their respective calculated values to a writer." <EXPLAINS> """CODE.writer.add_scalars('run_14h',{'xsinx':i*np.sin(i/r),
                              'xcosx':i*np.cos(i/r),
                              'arctanx': numsteps*np.arctan(i/r)}, i)""" .

"DESCRIPTION.The code is augmenting requirements for the \"arrow\" package with different unfreeze options." <EXPLAINS> """CODE._augment_requirement("arrow<=1.2.2,>=1.2.0  # anything", unfreeze="")
_augment_requirement("arrow<=1.2.2,>=1.2.0  # strict", unfreeze="")
_augment_requirement("arrow<=1.2.2,>=1.2.0  # my name", unfreeze="all")
_augment_requirement("arrow>=1.2.0, <=1.2.2  # strict", unfreeze="all")
_augment_requirement("arrow", unfreeze="all")
_augment_requirement("arrow>=1.2.0, <=1.2.2  # cool", unfreeze="major")
_augment_requirement("arrow>=1.2.0, <=1.2.2  # strict", unfreeze="major")
_augment_requirement("arrow>=1.2.0", unfreeze="major")
_augment_requirement("arrow", unfreeze="major")""" .

"DESCRIPTION.The code is calculating the False Positive Rate (FPR), True Positive Rate (TPR), and thresholds for a Receiver Operating Characteristic (ROC) curve using predicted values and target labels. The first part of the code calculates FPR, TPR, and thresholds for a binary classification task, while the second part calculates them for a multi-class classification task with four classes." <EXPLAINS> """CODE.pred = torch.tensor([0, 1, 2, 3])
target = torch.tensor([0, 1, 1, 1])
roc = ROC(pos_label=1)
fpr, tpr, thresholds = roc(pred, target)
fpr
tensor([0., 0., 0., 0., 1.])
tpr
tensor([0.0000, 0.3333, 0.6667, 1.0000, 1.0000])
thresholds
tensor([4, 3, 2, 1, 0])

pred = torch.tensor([[0.75, 0.05, 0.05, 0.05],
...                      [0.05, 0.75, 0.05, 0.05],
...                      [0.05, 0.05, 0.75, 0.05],
...                      [0.05, 0.05, 0.05, 0.75]])
target = torch.tensor([0, 1, 3, 2])
roc = ROC(num_classes=4)
fpr, tpr, thresholds = roc(pred, target)
fpr
[tensor([0., 0., 1.]), tensor([0., 0., 1.]), tensor([0.0000, 0.3333, 1.0000]), tensor([0.0000, 0.3333, 1.0000])]
tpr
[tensor([0., 1., 1.]), tensor([0., 1., 1.]), tensor([0., 0., 1.]), tensor([0., 0., 1.])]
thresholds # doctest: +NORMALIZE_WHITESPACE
[tensor([1.7500, 0.7500, 0.0500]),
 tensor([1.7500, 0.7500, 0.0500]),
 tensor([1.7500, 0.7500, 0.0500]),
 tensor([1.7500, 0.7500, 0.0500)]
""" .

"DESCRIPTION.The code is checking if a given input requires conversion to an integer type (int64). It checks whether the input is a datetime64, a Series with timedelta64 datatype, or a DatetimeIndex with a specific timezone." <EXPLAINS> """CODE.needs_i8_conversion(np.datetime64)
needs_i8_conversion(pd.Series([], dtype="timedelta64[ns]"))
needs_i8_conversion(pd.DatetimeIndex([1, 2, 3], tz="US/Eastern")""" .

"DESCRIPTION.The code is checking if the input data type is a real numeric data type." <EXPLAINS> """CODE.from pandas.api.types import is_any_real_numeric_dtype
is_any_real_numeric_dtype(int)
is_any_real_numeric_dtype(float)
is_any_real_numeric_dtype(object)
is_any_real_numeric_dtype(str)
is_any_real_numeric_dtype(complex(1, 2))
is_any_real_numeric_dtype(bool)""" .

"DESCRIPTION.The code is checking whether two values are approximately equal within certain specified relative and absolute tolerances." <EXPLAINS> """CODE.
0.1 + 0.2 == approx(0.3)
(0.1 + 0.2, 0.2 + 0.4) == approx((0.3, 0.6))
1.0001 == approx(1, rel=1e-3)
1.0001 == approx(1, abs=1e-3)
1 + 1e-8 == approx(1)
1 + 1e-8 == approx(1, abs=1e-12)
1 + 1e-8 == approx(1, rel=1e-6, abs=1e-12)
""" .

"DESCRIPTION.The code is comparing the DeviceType enum value CPU with the result of calling the from_str method on the string 'cpu', and also comparing the DeviceType enum value GPU with the string 'GPU', and the DeviceType enum value TPU with the string 'tpu'." <EXPLAINS> """CODE.DeviceType.CPU == DeviceType.from_str('cpu')
DeviceType.GPU == 'GPU'
DeviceType.TPU == 'tpu'""" .

"DESCRIPTION.The code is converting a pandas DataFrame into an Apache Arrow Table for optimized storage and processing. It also has an option to specify row and column limits for the conversion. The last line of code is converting a styled DataFrame into an Apache Arrow Table." <EXPLAINS> """CODE.st._arrow_dataframe(df)
st._arrow_dataframe(df, 200, 100)
st._arrow_dataframe(df.style.highlight_max(axis=0))""" .

"DESCRIPTION.The code is copying the weights from one neural network (ev1) to another neural network (ev2)." <EXPLAINS> """CODE.weights = ev1.get_weights()
ev2.set_weights(weights)""" .

"DESCRIPTION.The code is creating a Counter object to count the occurrences of each character in the string 'which'. Then it subtracts the counts of characters in the string 'witch' and the counts of characters in the Counter object created from the string 'watch'. Finally, it checks the count of the character 'h', which is 0, and the count of the character 'w', which is -1." <EXPLAINS> """CODE.c = Counter('which')
c.subtract('witch')
c.subtract(Counter('watch'))
c['h']
0
c['w']
-1""" .

"DESCRIPTION.The code is creating a DeckGL chart visualization using two layers: HexagonLayer and ScatterplotLayer. The HexagonLayer displays data in a hexagonal grid format with certain radius and elevation settings, while the ScatterplotLayer displays data points with customizable attributes such as radius and encoding. The visualization is centered at a specific latitude and longitude with zoom and pitch settings." <EXPLAINS> """CODE.st.deck_gl_chart(my_data_frame)
st.deck_gl_chart(
    viewport={
        'latitude': 37.76,
        'longitude': -122.4,
        'zoom': 11,
        'pitch': 50,
    },
    layers=[{
        'type': 'HexagonLayer',
        'data': my_dataframe,
        'radius': 200,
        'elevationScale': 4,
        'elevationRange': [0, 1000],
        'pickable': True,
        'extruded': True,
    }, {
        'type': 'ScatterplotLayer',
        'data': my_other_dataframe,
        'pickable': True,
        'autoHighlight': True,
        'radiusScale': 0.02,
        'encoding': {
            'radius': 'exits',
        },
    }])""" .

"DESCRIPTION.The code is creating a SequenceEnqueuer object, starting it, getting data from it, processing the data (e.g., training, evaluating, predicting), and finally closing the SequenceEnqueuer." <EXPLAINS> """CODE.enqueuer = SequenceEnqueuer(...)
enqueuer.start()
datas = enqueuer.get()
for data in datas:
    # Use the inputs; training, evaluating, predicting.
    # ... stop sometime.
enqueuer.close()
""" .

"DESCRIPTION.The code is creating a lookup table from a vocabulary file and converting a list of strings into their corresponding indices in the table." <EXPLAINS> """CODE.features = tf.constant(["emerson", "lake", "and", "palmer"])
table = tf.contrib.lookup.string_to_index_table_from_file(
    vocabulary_file="test.txt", num_oov_buckets=1)
ids = table.lookup(features)
...
tf.tables_initializer().run()
""" .

"DESCRIPTION.The code is creating a neural network model using TensorFlow for predicting a numerical value based on input features. It first defines the input features, including a numerical column for 'price' and an embedded column for 'keywords'. It then constructs a feature layer using dense features with a fixed-size partitioner. The code then parses the input data using the specified feature columns and applies multiple dense layers with ReLU activation functions before making the final prediction with a dense layer outputting a single value." <EXPLAINS> """CODE.price = tf.feature_column.numeric_column('price')
keywords_embedded = tf.feature_column.embedding_column(
    tf.feature_column.categorical_column_with_hash_bucket("keywords", 10K),
    dimension=16)
columns = [price, keywords_embedded, ...]
partitioner = tf.compat.v1.fixed_size_partitioner(num_shards=4)
feature_layer = tf.compat.v1.keras.layers.DenseFeatures(
    feature_columns=columns, partitioner=partitioner)

features = tf.io.parse_example(
    ..., features=tf.feature_column.make_parse_example_spec(columns))
dense_tensor = feature_layer(features)
for units in [128, 64, 32]:
  dense_tensor = tf.compat.v1.keras.layers.Dense(
                     units, activation='relu')(dense_tensor)
prediction = tf.compat.v1.keras.layers.Dense(1)(dense_tensor)
""" .

"DESCRIPTION.The code is creating a random dataset with a size of 10 and a length of 20." <EXPLAINS> """CODE.RandomDataset(size=10, length=20)  # doctest: +ELLIPSIS
<...bug_report_model.RandomDataset object at ...>""" .

"DESCRIPTION.The code is creating a synthetic Kitti dataset with image dimensions of 1024x512 and then initializing the dataset for training." <EXPLAINS> """CODE.from examples import DATASETS_PATH
dataset_path = os.path.join(DATASETS_PATH, "Kitti")
_create_synth_kitti_dataset(dataset_path, image_dims=(1024, 512))
KITTI(dataset_path, 'train')
""" .

"DESCRIPTION.The code is creating and executing multiple commits on a repository named \"my-cool-model\" using the Hugging Face Hub API. These commits include additions and deletions specified by the CommitOperationAdd and CommitOperationDelete operations. The verbose parameter is set to True to display detailed information during the process." <EXPLAINS> """CODE.from huggingface_hub import HfApi, plan_multi_commits
addition_commits, deletion_commits = plan_multi_commits(
...     operations=[
...          CommitOperationAdd(...),
...          CommitOperationAdd(...),
...          CommitOperationDelete(...),
...          CommitOperationDelete(...),
...          CommitOperationAdd(...),
...     ],
... )
HfApi().create_commits_on_pr(
...     repo_id="my-cool-model",
...     addition_commits=addition_commits,
...     deletion_commits=deletion_commits,
...     (...)
...     verbose=True,
... )
""" .

"DESCRIPTION.The code is creating input and target datasets for a time series using TensorFlow's `timeseries_dataset_from_array` function. The input data is sliced to generate sequences of a specified length, and then it is used to create datasets for training a time series model with the corresponding targets. The code includes assertions to ensure that the dataset batches are correctly structured with the expected input and target sequences." <EXPLAINS> """CODE.input_data = data[:-10]
targets = data[10:]
dataset = tf.keras.preprocessing.timeseries_dataset_from_array(
    input_data, targets, sequence_length=10)
for batch in dataset:
  inputs, targets = batch
  assert np.array_equal(inputs[0], data[:10])  # First sequence: steps [0-9]
  assert np.array_equal(targets[0], data[10])  # Corresponding target: step 10
  break


X = np.arange(100)
Y = X*2

sample_length = 20
input_dataset = tf.keras.preprocessing.timeseries_dataset_from_array(
  X, None, sequence_length=sample_length, sequence_stride=sample_length)
target_dataset = tf.keras.preprocessing.timeseries_dataset_from_array(
  Y, None, sequence_length=sample_length, sequence_stride=sample_length)

for batch in zip(input_dataset, target_dataset):
  inputs, targets = batch
  assert np.array_equal(inputs[0], X[:sample_length])

  # second sample equals output timestamps 20-40
  assert np.array_equal(targets[1], Y[sample_length:2*sample_length])
  break
""" .

"DESCRIPTION.The code is defining a class Flow that inherits from LightningFlow. Within the class, there is a method configure_layout that returns a StaticWebFrontend object with a specified path to a folder to serve." <EXPLAINS> """CODE.from lightning_app.frontend import StaticWebFrontend

class Flow(LightningFlow):
    ...

    def configure_layout(self):
        return StaticWebFrontend("path/to/folder/to/serve")

from lightning_app.frontend import StaticWebFrontend

class Flow(LightningFlow):
    ...

    def configure_layout(self):
        return StreamlitFrontend(render_fn=my_streamlit_ui)

def my_streamlit_ui(state):
    # add your streamlit code here!
    import streamlit as st

class Flow(LightningFlow):
    def configure_layout(self):
        return [
            dict(name="First Tab", content=self.child0),
            dict(name="Second Tab", content=self.child1),
            dict(name="Lightning", content="https://lightning.ai"),
        ]
""" .

"DESCRIPTION.The code is defining a function argument named \"foo\" with the data type integer." <EXPLAINS> "CODE._as_arg_name_and_type(\"foo (int)\")" .

"DESCRIPTION.The code is defining a method to get a resolution value from a frequency input. It then checks if the resolution value obtained using frequency 'H' is equal to the resolution value for 'HR'." <EXPLAINS> """CODE.Resolution.get_reso_from_freq('H')
Resolution.get_reso_from_freq('H') == Resolution.RESO_HR""" .

"DESCRIPTION.The code is defining layouts for the kernels and biases of dense layers in three different types of models: subclassed model, functional model, and sequential model. The layout_map object is used to map the layouts to specific layers in each model type." <EXPLAINS> """CODE.layout_map = layout_map_lib.LayoutMap(mesh=self.mesh)
layout_map['d1.kernel'] = layout_1
layout_map['d1.bias'] = layout_2
layout_map['d2.kernel'] = layout_3
layout_map['d2.bias'] = layout_4

## Subclassed model
class SubclassModel(tf.keras.Model):

  def __init__(self, name=None):
    super().__init__(name=name)
    self.d1 = tf.keras.layers.Dense(1000)
    self.d2 = tf.keras.layers.Dense(1000)

  def call(self, inputs):
    x = self.d1(inputs)
    return self.d2(x)

with layout_map.scope():
  model = SubclassModel()
inputs = tf.zeros((10, 10))
results = model(inputs)

model.d1.kernel.layout == layout_1
model.d1.bias.layout == layout_2
model.d2.kernel.layout == layout_3
model.d2.bias.layout == layout_4

## Functional model
with layout_map.scope():
  inputs = tf.keras.Input((10,), batch_size=10)
  x = tf.keras.layers.Dense(20, name='d1')(inputs)
  output = tf.keras.layers.Dense(30, name='d2')(x)

  model = tf.keras.Model(inputs, output)

d1 = model.layers[1]
d2 = model.layers[2]

d1.kernel.layout == layout_1
d1.bias.layout == layout_2
d1.kernel.layout == layout_3
d1.bias.layout == layout_4

## Sequential model
with layout_map.scope():
  model = tf.keras.Sequential([
      tf.keras.layers.Dense(20, name='d1', input_shape=(10,)),
      tf.keras.layers.Dense(30, name='d2')
  ])

d1 = model.layers[0]
d2 = model.layers[1]

d1.kernel.layout == layout_1
d1.bias.layout == layout_2
d1.kernel.layout == layout_3
d1.bias.layout == layout_4
""" .

"DESCRIPTION.The code is demonstrating the creation, retrieval, and modification of Token objects, storing information such as type, value, and position." <EXPLAINS> """CODE.repr(Token(1, "test", (1, 1)))
"<Token: ('NAME', 'test', (1, 1))>"
Token(1, 'bar', (3, 4)).__getstate__()
(1, 'bar', 3, 4)
a = Token(0, 'baz', (0, 0))
a.__setstate__((1, 'foo', 3, 4))
a
<Token: ('NAME', 'foo', (3, 4))>
a.start_pos
(3, 4)
a.string
'foo'
a._start_pos_col
4
Token(1, u("ð·"), (1 ,1)).string + "p" == u("ð·p")
True""" .

"DESCRIPTION.The code is encoding the fields dictionary into a multipart form data format." <EXPLAINS> """CODE.fields = {
    'foo': 'bar',
    'foofile': ('foofile.txt', 'contents of foofile'),
}

body, content_type = encode_multipart_formdata(fields)""" .

"DESCRIPTION.The code is ensuring the creation of an index from the input sequences with specified names." <EXPLAINS> """CODE._ensure_index_from_sequences([[1, 2, 3]], names=['name'])
_ensure_index_from_sequences([['a', 'a'], ['a', 'b']], names=['L1', 'L2'])""" .

"DESCRIPTION.The code is escaping special characters in a given string to their corresponding HTML entities." <EXPLAINS> """CODE.value = escape("<User 1>")
value
Markup('&lt;User 1&gt;')
escape(str(value))
Markup('&amp;lt;User 1&amp;gt;')
escape(soft_str(value))
Markup('&lt;User 1&gt;')""",
        """CODE.value = escape('<User 1>')
value
Markup('&lt;User 1&gt;')
escape(str(value))
Markup('&amp;lt;User 1&amp;gt;')
escape(soft_unicode(value))
Markup('&lt;User 1&gt;')""" .

"DESCRIPTION.The code is escaping special characters in a string, such as < and >, by replacing them with their corresponding HTML entities." <EXPLAINS> """CODE.value = escape('<User 1>')
value
Markup('&lt;User 1&gt;')
escape(str(value))
Markup('&amp;lt;User 1&amp;gt;')
escape(soft_unicode(value))
Markup('&lt;User 1&gt;')""" .

"DESCRIPTION.The code is formatting the elapsed time in seconds into a human-readable format. It takes the input number of seconds and returns a formatted string indicating the number of minutes and seconds." <EXPLAINS> """CODE._format_elapsed_seconds(5)
_format_elapsed_seconds(60)""" .

"DESCRIPTION.The code is implementing a bracketing algorithm to optimize a certain function with specified parameters. The algorithm updates the brackets based on the results of trials and determines a cutoff value for stopping the optimization process." <EXPLAINS> """CODE.b = _Bracket(1, 10, 2, 3)
b.on_result(trial1, 1, 2)  # CONTINUE
b.on_result(trial2, 1, 4)  # CONTINUE
b.cutoff(b._rungs[-1][1]) == 3.0  # rungs are reversed
b.on_result(trial3, 1, 1)  # STOP
b.cutoff(b._rungs[0][1]) == 2.0""" .

"DESCRIPTION.The code is implementing a neural network model in TensorFlow using dense layers to make predictions on a dataset with features including a numeric column \"price\" and an embedded column \"keywords\" after parsing the input data." <EXPLAINS> """CODE.price = numeric_column('price')
keywords_embedded = embedding_column(
    categorical_column_with_hash_bucket("keywords", 10K), dimensions=16)
columns = [price, keywords_embedded, ...]
features = tf.parse_example(..., features=make_parse_example_spec(columns))
dense_tensor = input_layer(features, columns)
for units in [128, 64, 32]:
  dense_tensor = tf.layers.dense(dense_tensor, units, tf.nn.relu)
prediction = tf.layers.dense(dense_tensor, 1)
""" .

"DESCRIPTION.The code is inferring the data type of an integer value with 64-bit precision." <EXPLAINS> "CODE._maybe_infer_dtype_type(Foo(np.dtype(\"i8\")))",
        "CODE.maybe_infer_dtype_type(Foo(np.dtype(\"i8\")))" .

"DESCRIPTION.The code is iterating and executing a training step using JAX profiler to trace the execution of the training process." <EXPLAINS> """CODE.import jax

while global_step < NUM_STEPS:
    with jax.profiler.StepTraceContext("train", step_num=global_step):
        train_step()
        global_step += 1
""" .

"DESCRIPTION.The code is loading and reading the description from the readme file located at the project root directory and displaying it as a centered HTML element." <EXPLAINS> """CODE._load_readme_description(_PROJECT_ROOT)  # doctest: +ELLIPSIS +NORMALIZE_WHITESPACE
'<div align="center">...""" .

"DESCRIPTION.The code is loading the requirements for a project located at PROJECT_ROOT." <EXPLAINS> """CODE._load_requirements(PROJECT_ROOT)  # doctest: +ELLIPSIS +NORMALIZE_WHITESPACE
['numpy...', 'torch...', ...]""" .

"DESCRIPTION.The code is marking the function \"Size\" as not differentiable in TensorFlow." <EXPLAINS> "CODE.tf.NotDifferentiable(\"Size\")" .

"DESCRIPTION.The code is monitoring preemption checkpoints while training a model in a distributed setting, incrementing the loss and trained steps until reaching a specified number of steps." <EXPLAINS> """CODE.with preemption_checkpoint_handler.watch_preemption_scope():
  while trained_step.numpy() < NUM_STEPS:

    # distributed_train_function contains a call to strategy.run.
    loss += preemption_checkpoint_handler.run(distributed_train_function, args=(next(iterator),))
    trained_step.assign_add(STEPS_PER_TRAIN_FUNCTION)""" .

"DESCRIPTION.The code is obtaining the frequency group for a specified time unit, in this case 'day'." <EXPLAINS> "CODE.f.Resolution.get_freq_group('day')" .

"DESCRIPTION.The code is parsing a header and extracting key-value pairs. It extracts the value corresponding to the key 'file' and decodes it to 'TÃ¤st'. It also retrieves the value corresponding to the key 'foo', which is 'bar'." <EXPLAINS> """CODE.d = _parse_header("CD: fd; foo=\\"bar\\"; file*=utf-8''T%C3%A4st")[1]
d['file'] == 'T\\u00e4st'
d['foo']
'bar'""" .

"DESCRIPTION.The code is parsing a string input and extracting all sequences of one or more alphabetic characters separated by whitespace characters, then returning them as a list. The default whitespace characters used for parsing are space and tab." <EXPLAINS> """CODE.OneOrMore(Word(alphas)).parseString("abc def\\nghi jkl")  # -> ['abc', 'def', 'ghi', 'jkl']
ParserElement.setDefaultWhitespaceChars(" \\t")
OneOrMore(Word(alphas)).parseString("abc def\\nghi jkl")  # -> ['abc', 'def']""" .

"DESCRIPTION.The code is patching the submodule \"os.path.join\" in the snli module with a custom function xjoin and then starting the patcher." <EXPLAINS> """CODE.import importlib
from datasets.load import prepare_module
from datasets.streaming import patch_submodule, xjoin

snli_module_path, _ = prepare_module("snli")
snli_module = importlib.import_module(snli_module_path)
patcher = patch_submodule(snli_module, "os.path.join", xjoin)
patcher.start()
assert snli_module.os.path.join is xjoin""" .

"DESCRIPTION.The code is performing bootstrapping cross-validation with 9 samples and a random state of 0. It iterates through the bootstrap samples, printing the indices of the training and test sets for each iteration." <EXPLAINS> """CODE.from sklearn import cross_validation
bs = cross_validation.Bootstrap(9, random_state=0)
for train_index, test_index in bs:
    print("TRAIN:", train_index, "TEST:", test_index)""" .

"DESCRIPTION.The code is performing category encoding using different output modes: \"one_hot\", \"multi_hot\", and \"count\" for encoding input values into numerical representation based on the specified parameters." <EXPLAINS> """CODE.layer = tf.keras.layers.CategoryEncoding(
          num_tokens=4, output_mode="one_hot")
layer([3, 2, 0, 1])


layer = tf.keras.layers.CategoryEncoding(
          num_tokens=4, output_mode="multi_hot")
layer([[0, 1], [0, 0], [1, 2], [3, 1]])


layer = tf.keras.layers.CategoryEncoding(
          num_tokens=4, output_mode="count")
count_weights = np.array([[.1, .2], [.1, .1], [.2, .3], [.4, .2]])
layer([[0, 1], [0, 0], [1, 2], [3, 1]], count_weights=count_weights)
""" .

"DESCRIPTION.The code is performing multi-label binarization on a list of sets. The MultiLabelBinarizer is being used to transform the input data into a binary matrix where each element represents the presence or absence of a particular label. The \"fit_transform\" method is fitting the MultiLabelBinarizer to the input data and transforming it into a binary matrix. The \"classes_\" attribute of the MultiLabelBinarizer returns the unique classes present in the input data." <EXPLAINS> """CODE.mlb = MultiLabelBinarizer()
mlb.fit_transform([(1, 2), (3,)])
array([[1, 1, 0],
       [0, 0, 1]])
mlb.classes_
array([1, 2, 3])

mlb.fit_transform([set(['sci-fi', 'thriller']), set(['comedy'])])
array([[0, 1, 1],
       [1, 0, 0]])
list(mlb.classes_)
['comedy', 'sci-fi', 'thriller']""" .

"DESCRIPTION.The code is performing table question answering using a specified pre-trained model to answer the query \"How many stars does the transformers repository have?\" on the provided table data. The output provides the answer along with relevant information such as coordinates and aggregator." <EXPLAINS> """CODE.from huggingface_hub import InferenceClient
client = InferenceClient()
query = "How many stars does the transformers repository have?"
table = {"Repository": ["Transformers", "Datasets", "Tokenizers"], "Stars": ["36542", "4512", "3934"]}
client.table_question_answering(table, query, model="google/tapas-base-finetuned-wtq")
{'answer': 'AVERAGE > 36542', 'coordinates': [[0, 1]], 'cells': ['36542'], 'aggregator': 'AVERAGE'}
""" .

"DESCRIPTION.The code is performing text summarization using the FlaxMT5 model. It takes an input article in German and generates a summary of the article in German using the model." <EXPLAINS> """CODE.from transformers import FlaxMT5ForConditionalGeneration, T5Tokenizer
model = FlaxMT5ForConditionalGeneration.from_pretrained("google/mt5-small")
tokenizer = T5Tokenizer.from_pretrained("google/mt5-small")
article = "UN Offizier sagt, dass weiter verhandelt werden muss in Syrien."
summary = "Weiter Verhandlung in Syrien."
inputs = tokenizer(article, return_tensors="np")
with tokenizer.as_target_tokenizer():
...     decoder_input_ids = tokenizer(summary, return_tensors="np").input_ids
outputs = model(**inputs, decoder_input_ids=decoder_input_ids)
logits = outputs.logits""" .

"DESCRIPTION.The code is preparing a sequence-to-sequence batch for text summarization using the T5 model. It takes an input article in German and generates a summary in German using the trained T5 model. The loss is calculated based on the output summary generated by the model." <EXPLAINS> """CODE.from transformers import TFMT5ForConditionalGeneration, T5Tokenizer
model = TFMT5ForConditionalGeneration.from_pretrained("google/mt5-small")
tokenizer = T5Tokenizer.from_pretrained("google/mt5-small")
article = "UN Offizier sagt, dass weiter verhandelt werden muss in Syrien."
summary = "Weiter Verhandlung in Syrien."
batch = tokenizer.prepare_seq2seq_batch(src_texts=[article], tgt_texts=[summary], return_tensors="tf")
outputs = model(batch)
loss = outputs.loss""" .

"DESCRIPTION.The code is rendering a template with the string 'knights' set to 'that say nih'." <EXPLAINS> """CODE.template.render(knights='that say nih')
template.render({'knights': 'that say nih'})""" .

"DESCRIPTION.The code is reversing the order of elements in a deque containing the elements 1, 2, and 3." <EXPLAINS> """CODE.pdeque([1, 2, 3]).reverse()
reversed(pdeque([1, 2, 3]))""" .

"DESCRIPTION.The code is scanning a table within a specified range of rows to retrieve data from specific columns." <EXPLAINS> """CODE.
ds1 = table.scan_range("row_start", "row_end", columns=[("cfa", "c1"),
                                                        ("cfa", "c2"),
                                                        ("cfb", "c3")])
ds2 = table.scan_range("row_start", "row_end", cfa=["c1", "c2"], cfb="c3")
""" .

"DESCRIPTION.The code is setting up a mock object named 'mock_dao' to return a mock result when the 'GetUsersInfo' function is called with the parameter 'expectedUserName'." <EXPLAINS> "CODE.mock_dao.GetUsersInfo(In('expectedUserName')).AndReturn(mock_result)" .

"DESCRIPTION.The code is testing floating point equality using the pytest's `approx` assertion helper. It checks whether two values are approximately equal within certain relative and absolute tolerances." <EXPLAINS> """CODE.abs((0.1 + 0.2) - 0.3) < 1e-6
True
from pytest import approx
0.1 + 0.2 == approx(0.3)
True
(0.1 + 0.2, 0.2 + 0.4) == approx((0.3, 0.6))
True
1.0001 == approx(1)
False
1.0001 == approx(1, rel=1e-3)
True
1.0001 == approx(1, abs=1e-3)
True
1 + 1e-8 == approx(1)
True
1 + 1e-8 == approx(1, abs=1e-12)
False
1 + 1e-8 == approx(1, rel=1e-6, abs=1e-12)
True""" .

"DESCRIPTION.The code is training a LightGBM model using either numpy arrays or pandas dataframes as input, and then making predictions using the trained model. It also demonstrates how to specify which columns to use as features for prediction." <EXPLAINS> """CODE.import numpy as np
    import lightgbm as lgbm
    from ray.ml.predictors.lightgbm import LightGBMPredictor

    train_X = np.array([[1, 2], [3, 4]])
    train_y = np.array([0, 1])

    model = lgbm.LGBMClassifier().fit(train_X, train_y)
    predictor = LightGBMPredictor(model=model.booster_)

    data = np.array([[1, 2], [3, 4]])
    predictions = predictor.predict(data)

    # Only use first and second column as the feature
    data = np.array([[1, 2, 8], [3, 4, 9]])
    predictions = predictor.predict(data, feature_columns=[0, 1])

import pandas as pd
    import lightgbm as lgbm
    from ray.ml.predictors.lightgbm import LightGBMPredictor

    train_X = pd.DataFrame([[1, 2], [3, 4]], columns=["A", "B"])
    train_y = pd.Series([0, 1])

    model = lgbm.LGBMClassifier().fit(train_X, train_y)
    predictor = LightGBMPredictor(model=model.booster_)

    # Pandas dataframe.
    data = pd.DataFrame([[1, 2], [3, 4]], columns=["A", "B"])
    predictions = predictor.predict(data)

    # Only use first and second column as the feature
    data = pd.DataFrame([[1, 2, 8], [3, 4, 9]], columns=["A", "B", "C"])
    predictions = predictor.predict(data, feature_columns=["A", "B"])""" .

"DESCRIPTION.The code is training a RandomForestClassifier model using sklearn and making predictions on new data using the trained model. It also demonstrates how to specify which columns to use as features for prediction." <EXPLAINS> """CODE.import numpy as np
    from sklearn.ensemble import RandomForestClassifier
    from ray.ml.predictors.sklearn import SklearnPredictor

    train_X = np.array([[1, 2], [3, 4]])
    train_y = np.array([0, 1])

    model = RandomForestClassifier().fit(train_X, train_y)
    predictor = SklearnPredictor(model=model)

    data = np.array([[1, 2], [3, 4]])
    predictions = predictor.predict(data)

    # Only use first and second column as the feature
    data = np.array([[1, 2, 8], [3, 4, 9]])
    predictions = predictor.predict(data, feature_columns=[0, 1])

import pandas as pd
    from sklearn.ensemble import RandomForestClassifier
    from ray.ml.predictors.sklearn import SklearnPredictor

    train_X = pd.DataFrame([[1, 2], [3, 4]], columns=["A", "B"])
    train_y = pd.Series([0, 1])

    model = RandomForestClassifier().fit(train_X, train_y)
    predictor = SklearnPredictor(model=model)

    # Pandas dataframe.
    data = pd.DataFrame([[1, 2], [3, 4]], columns=["A", "B"])
    predictions = predictor.predict(data)

    # Only use first and second column as the feature
    data = pd.DataFrame([[1, 2, 8], [3, 4, 9]], columns=["A", "B", "C"])
    predictions = predictor.predict(data, feature_columns=["A", "B"])""" .

"DESCRIPTION.The code is used to build a project named \"clang7-build\" with specific compiler versions and flags. It also allows building all targets including tests for the project." <EXPLAINS> """CODE.archery build --cc=clang-7 --cxx=clang++-7 --cxx-flags=-mavx2 clang7-build
archery build --targets=all --targets=test build""" .

"DESCRIPTION.The code is used to create a Version object with the input string as the version number. The \".local\" attribute of the Version object returns the local version part of the input string if present, otherwise it returns None." <EXPLAINS> """CODE.print(Version("1.2.3").local)
None
Version("1.2.3+abc").local
'abc'""" .

"DESCRIPTION.The code is used to evaluate the performance of the model \"facebook/bart-large-cnn\" on the task of summarization using the data from the \"cnn_dailymail\" dataset. It computes the evaluation metrics based on the model's predictions compared to the actual highlights in the dataset." <EXPLAINS> """CODE.from evaluate import evaluator
from datasets import load_dataset
task_evaluator = evaluator("summarization")
data = load_dataset("cnn_dailymail", "3.0.0", split="validation[:40]")
results = task_evaluator.compute(
    model_or_pipeline="facebook/bart-large-cnn",
    data=data,
    input_column="article",
    label_column="highlights",
)""" .

"DESCRIPTION.The code is used to load a pre-trained Vision Encoder-Decoder model, extract features from an image, generate captions for the image, and then decode the generated captions to obtain the final output. The model combines a Vision Transformer (ViT) and GPT-2 for image captioning tasks." <EXPLAINS> """CODE.# a workaround to load from pytorch checkpoint
_model = VisionEncoderDecoderModel.from_pretrained("ydshieh/vit-gpt2-coco-en")
_model.encoder.save_pretrained("./encoder")
_model.decoder.save_pretrained("./decoder")
model = TFVisionEncoderDecoderModel.from_encoder_decoder_pretrained(
...     "./encoder", "./decoder", encoder_from_pt=True, decoder_from_pt=True
... )
# This is only for copying some specific attributes of this particular model.
model.config = _model.config

from transformers import TFVisionEncoderDecoderModel, ViTFeatureExtractor, GPT2Tokenizer
from PIL import Image
import requests

feature_extractor = ViTFeatureExtractor.from_pretrained("ydshieh/vit-gpt2-coco-en")
decoder_tokenizer = GPT2Tokenizer.from_pretrained("ydshieh/vit-gpt2-coco-en")
model = TFVisionEncoderDecoderModel.from_pretrained("ydshieh/vit-gpt2-coco-en")

url = "http://images.cocodataset.org/val2017/000000039769.jpg"
img = Image.open(requests.get(url, stream=True).raw)
pixel_values = feature_extractor(images=img, return_tensors="tf").pixel_values  # Batch size 1

output_ids = model.generate(
...     pixel_values, max_length=16, num_beams=4, return_dict_in_generate=True
).sequences

preds = decoder_tokenizer.batch_decode(output_ids, skip_special_tokens=True)
preds = [pred.strip() for pred in preds]

assert preds == ["a cat laying on top of a couch next to another cat"]
""" .

"DESCRIPTION.The code is used to scan and delete specific cache revisions in the cache directory. It calculates the expected freed size before executing the deletion of the specified cache revisions." <EXPLAINS> """CODE.from huggingface_hub import scan_cache_dir
cache_info = scan_cache_dir()
delete_strategy = cache_info.delete_revisions(
    "81fd1d6e7847c99f5862c9fb81387956d99ec7aa"
)
print(f"Will free {delete_strategy.expected_freed_size_str}.")
delete_strategy.execute()


from huggingface_hub import scan_cache_dir
scan_cache_dir().delete_revisions(
    "81fd1d6e7847c99f5862c9fb81387956d99ec7aa",
    "e2983b237dccf3ab4937c97fa717319a9ca1a96d",
    "6c0e6080953db56375760c0471a8c5f2929baf11",
).execute()
""" .

"DESCRIPTION.The code is used to unescape HTML markup within the given string." <EXPLAINS> "CODE.Markup(\"Main &raquo; <em>About</em>\").unescape()" .

"DESCRIPTION.The code is using a context manager to temporarily patch the attribute value of a class C. The attribute value is changed to 'patched' within the context manager block, and then reset back to 'original' once the context manager exits." <EXPLAINS> """CODE.class C(object):
    attribute = 'original'

with patch(C, 'attribute', 'patched'):
    in_context = C.attribute

in_context
'patched'
C.attribute  # the value is reset when the context manager exists
'original'

with patch(C, 'attribute', 'patched'):
    in_context = C.attribute
    raise ValueError()

in_context
'patched'
C.attribute
'original'
""" .

"DESCRIPTION.The code is using a pre-trained MT5 model to generate a summary of an input article about negotiations in Syria. It tokenizes the input article and summary, prepares a sequence-to-sequence batch, and then uses the model to generate hidden states for the output summary." <EXPLAINS> """CODE.from transformers import MT5Model, T5Tokenizer
model = MT5Model.from_pretrained("google/mt5-small")
tokenizer = T5Tokenizer.from_pretrained("google/mt5-small")
article = "UN Offizier sagt, dass weiter verhandelt werden muss in Syrien."
summary = "Weiter Verhandlung in Syrien."
batch = tokenizer.prepare_seq2seq_batch(src_texts=[article], tgt_texts=[summary], return_tensors="pt")
outputs = model(input_ids=batch.input_ids, decoder_input_ids=batch.labels)
hidden_states = outputs.last_hidden_state""" .

"DESCRIPTION.The code is using the CategoryEncoding layer from the TensorFlow.keras library to encode categorical inputs into numerical representations. The functionality described includes one-hot encoding, multi-hot encoding, and count encoding modes based on the specified parameters in each code snippet." <EXPLAINS> """CODE.layer = tf.keras.layers.experimental.preprocessing.CategoryEncoding(
          num_tokens=4, output_mode="one_hot")
layer([3, 2, 0, 1])


layer = tf.keras.layers.experimental.preprocessing.CategoryEncoding(
          num_tokens=4, output_mode="multi_hot")
layer([[0, 1], [0, 0], [1, 2], [3, 1]])


layer = tf.keras.layers.experimental.preprocessing.CategoryEncoding(
          num_tokens=4, output_mode="count")
count_weights = np.array([[.1, .2], [.1, .1], [.2, .3], [.4, .2]])
layer([[0, 1], [0, 0], [1, 2], [3, 1]], count_weights=count_weights)
""" .

"DESCRIPTION.The code is using the Jedi library to infer the full name of the imported 'os.path.join' module and print it out." <EXPLAINS> """CODE.import os
os.path.join

from jedi import Script
source = '''
import os
os.path.join'''
script = Script(source, path='example.py')
print(script.infer(3, len('os.path.join'))[0].full_name)""" .

"DESCRIPTION.The code is using the PoolManager from the requests library to make HTTP GET requests to 'http://example.com/' with different retry settings." <EXPLAINS> """CODE.retries = Retry(connect=5, read=2, redirect=5)
http = PoolManager(retries=retries)
response = http.request('GET', 'http://example.com/')

response = http.request('GET', 'http://example.com/', retries=Retry(10))

response = http.request('GET', 'http://example.com/', retries=False)
""" .

"DESCRIPTION.The code is using the testutils library to pickle a list containing integers, strings, and None, with protocol version 2." <EXPLAINS> "CODE.testutils.subprocess_pickle_string([1, 'a', None], protocol=2)" .

"DESCRIPTION.The code is utilizing Random Forest Classifier to train a model and make predictions on given data. It uses features from the data to make predictions based on the trained model. It also demonstrates how to specify specific feature columns for prediction. The code also shows examples of using numpy arrays and pandas dataframes for training and predicting." <EXPLAINS> """CODE.import numpy as np
from sklearn.ensemble import RandomForestClassifier
from ray.ml.predictors.sklearn import SklearnPredictor

train_X = np.array([[1, 2], [3, 4]])
train_y = np.array([0, 1])

model = RandomForestClassifier().fit(train_X, train_y)
predictor = SklearnPredictor(model=model)

data = np.array([[1, 2], [3, 4]])
predictions = predictor.predict(data)

# Only use first and second column as the feature
data = np.array([[1, 2, 8], [3, 4, 9]])
predictions = predictor.predict(data, feature_columns=[0, 1])

import pandas as pd
from sklearn.ensemble import RandomForestClassifier
from ray.ml.predictors.sklearn import SklearnPredictor

train_X = pd.DataFrame([[1, 2], [3, 4]], columns=["A", "B"])
train_y = pd.Series([0, 1])

model = RandomForestClassifier().fit(train_X, train_y)
predictor = SklearnPredictor(model=model)

# Pandas dataframe.
data = pd.DataFrame([[1, 2], [3, 4]], columns=["A", "B"])
predictions = predictor.predict(data)

# Only use first and second column as the feature
data = pd.DataFrame([[1, 2, 8], [3, 4, 9]], columns=["A", "B", "C"])
predictions = predictor.predict(data, feature_columns=["A", "B"])""" .

"DESCRIPTION.The code iterates over the items in the dictionary returned by the `get_input_devices()` function and prints each key-value pair in the format \"key: value\". It displays information about input devices, such as \"avfoundation: AVFoundation input device\" and \"lavfi: Libavfilter virtual input device\"." <EXPLAINS> """CODE.for k, v in get_input_devices().items():
    print(f"{k}: {v}")
avfoundation: AVFoundation input device
lavfi: Libavfilter virtual input device""" .

"DESCRIPTION.The code iterates through a dictionary returned by the function get_video_decoders() and prints out each key-value pair in the format \"key: value\"." <EXPLAINS> """CODE.for k, v in get_video_decoders().items():
    print(f"{k}: {v}")
aasc: Autodesk RLE
aic: Apple Intermediate Codec
alias_pix: Alias/Wavefront PIX image
agm: Amuse Graphics Movie
amv: AMV Video
anm: Deluxe Paint Animation""" .

"DESCRIPTION.The code iterates through the getters in the process namespace and calls each function associated with the getter." <EXPLAINS> """CODE.ns = process_namespace(psutil.Process())
for fun, name in ns.iter(ns.getters):
    fun()""" .

"DESCRIPTION.The code iterates through the input string 'test' and prints the lines that start with the string 'AAA'." <EXPLAINS> """CODE.test = '''        AAA this line
    AAA and this line
      AAA but not this one
    B AAA and definitely not this one
    '''

for t in (LineStart() + 'AAA' + restOfLine).searchString(test):
    print(t)""" .

"DESCRIPTION.The code joins the _PROJECT_ROOT path with \"requirements\" to create a path, then loads the requirements from that path." <EXPLAINS> """CODE.path_req = os.path.join(_PROJECT_ROOT, "requirements")
load_requirements(path_req)  # doctest: +ELLIPSIS +NORMALIZE_WHITESPACE
['numpy...', 'torch...', ...]""" .

"DESCRIPTION.The code joins the _PROJECT_ROOT path with \"requirements\" to obtain the path_req variable. It then loads the requirements from the specified path and returns a list of required dependencies for the project." <EXPLAINS> """CODE.path_req = os.path.join(_PROJECT_ROOT, "requirements")
load_requirements(path_req)  # doctest: +ELLIPSIS +NORMALIZE_WHITESPACE
['numpy...', 'torch...', ...]""" .

"DESCRIPTION.The code likely involves a function related to balloons." <EXPLAINS> "CODE.st.balloons()" .

"DESCRIPTION.The code loads a LiteScriptModule from a saved file path or from a BytesIO object and loads all tensors to the original device." <EXPLAINS> """CODE.import torch
import io

# Load LiteScriptModule from saved file path
torch.jit._load_for_lite_interpreter('lite_script_module.pt')

# Load LiteScriptModule from io.BytesIO object
with open('lite_script_module.pt', 'rb') as f:
    buffer = io.BytesIO(f.read())

# Load all tensors to the original device
torch.jit.mobile._load_for_lite_interpreter(buffer)""" .

"DESCRIPTION.The code loads a Python session from a saved file using the Dill library and returns a copy of the variables in the session. It checks if certain variables have been successfully loaded and compares them to the original values before saving." <EXPLAINS> """CODE.lambda filename: vars(dill.load_module(filename)).copy()
import dill
alist = [1, 2, 3]
anum = 42
dill.dump_module()
anum = 0
new_var = 'spam'
main = dill.load_module_asdict()
main['__name__'], main['__session__']
('__main__', '/tmp/session.pkl')
main is globals() # loaded objects don't reference globals
False
main['alist'] == alist
True
main['alist'] is alist # was saved by value
False
main['anum'] == anum # changed after the session was saved
False
new_var in main # would be True if the option 'update' was set""" .

"DESCRIPTION.The code loads a RepoCard object for the repository \"nateraw/food\" from the Hugging Face Model Hub and checks that the tags associated with the repository are [\"generated_from_trainer\", \"image-classification\", \"pytorch\"]." <EXPLAINS> """CODE.from huggingface_hub.repocard import RepoCard
card = RepoCard.load("nateraw/food")
assert card.data.tags == ["generated_from_trainer", "image-classification", "pytorch"]
""" .

"DESCRIPTION.The code loads a TFRecord dataset from a file, checks if the cardinality of the dataset is unknown, then asserts that the dataset has a cardinality of 42." <EXPLAINS> """CODE.dataset = tf.data.TFRecordDataset("examples.tfrecord")
cardinality = tf.data.experimental.cardinality(dataset)
print((cardinality == tf.data.experimental.UNKNOWN_CARDINALITY).numpy())
True
dataset = dataset.apply(tf.data.experimental.assert_cardinality(42))
print(tf.data.experimental.cardinality(dataset).numpy())
42""" .

"DESCRIPTION.The code loads a dataset in parallel using Apache Spark with 2 processing cores." <EXPLAINS> """CODE.with parallel_backend('spark'):
  dataset = load_dataset(..., num_proc=2)
""" .

"DESCRIPTION.The code loads a dataset named \"rotten_tomatoes\" for the validation split and selects the first 4 rows with the corresponding indices mapping. The dataset contains features 'text' and 'label' with 4 rows." <EXPLAINS> """CODE.from datasets import load_dataset
ds = load_dataset("rotten_tomatoes", split="validation")
ds._select_with_indices_mapping(range(4))
Dataset({
    features: ['text', 'label'],
    num_rows: 4
})
""" .

"DESCRIPTION.The code loads a long description for a project and aligns it to the center of a div element." <EXPLAINS> """CODE._load_long_description(PROJECT_ROOT)  # doctest: +ELLIPSIS +NORMALIZE_WHITESPACE
'<div align="center">...""" .

"DESCRIPTION.The code loads a package, prints the runtime environment configuration, and defines and interacts with a remote actor and remote function using Ray." <EXPLAINS> """CODE.my_pkg = load_package("~/path/to/my_pkg.yaml")
my_pkg = ray.util.load_package(
  "https://raw.githubusercontent.com/user/repo/refspec"
  "/path/to/package/my_pkg.yaml")
print(my_pkg._runtime_env)
my_pkg.my_func.remote(1, 2)
actor = my_pkg.MyActor.remote(3, 4)
@ray.remote(runtime_env=my_pkg._runtime_env)
def f(): ...
""" .

"DESCRIPTION.The code loads a pre-trained BERT model for next sentence prediction using TensorFlow and converts a PyTorch model to TensorFlow format for next sentence prediction." <EXPLAINS> """CODE.model = TFAutoModelForNextSentencePrediction.from_pretrained('bert-base-uncased')
model = TFAutoModelForNextSentencePrediction.from_pretrained('bert-base-uncased', output_attentions=True)
config = AutoConfig.from_json_file('./pt_model/bert_pt_model_config.json')
model = TFAutoModelForNextSentencePrediction.from_pretrained('./pt_model/bert_pytorch_model.bin', from_pt=True, config=config)""" .

"DESCRIPTION.The code loads a pre-trained Blip2 model for text generation from the \"Salesforce/blip2-opt-2.7b\" repository, sets the model to run on either a GPU (if available) or CPU, and tokenizes input text data (\"a photo of a cat\" and \"a photo of a dog\") using the AutoTokenizer. Finally, it extracts text features using the loaded model." <EXPLAINS> """CODE.import torch
from transformers import AutoTokenizer, Blip2Model

device = "cuda" if torch.cuda.is_available() else "cpu"

model = Blip2Model.from_pretrained("Salesforce/blip2-opt-2.7b", torch_dtype=torch.float16)

model.to(device)  # doctest: +IGNORE_RESULT

tokenizer = AutoTokenizer.from_pretrained("Salesforce/blip2-opt-2.7b")
inputs = tokenizer(["a photo of a cat", "a photo of a dog"], padding=True, return_tensors="pt").to(device)
text_features = model.get_text_features(**inputs)
""" .

"DESCRIPTION.The code loads a pre-trained MT5 model and tokenizer from Google's MT5-small checkpoint. It then encodes a given article using the tokenizer and extracts the last hidden state from the model's outputs." <EXPLAINS> """CODE.from transformers import MT5EncoderModel, T5Tokenizer
model = MT5EncoderModel.from_pretrained("google/mt5-small")
tokenizer = T5Tokenizer.from_pretrained("google/mt5-small")
article = "UN Offizier sagt, dass weiter verhandelt werden muss in Syrien."
input_ids = tokenizer(article, return_tensors="pt").input_ids
outputs = model(input_ids)
hidden_state = outputs.last_hidden_state""" .

"DESCRIPTION.The code loads a pre-trained Owlv2 model from Google and a corresponding processor, then opens an image from a URL, processes the image using the processor, and extracts image features using the Owlv2 model." <EXPLAINS> """CODE.from PIL import Image
import requests
from transformers import AutoProcessor, Owlv2Model

model = Owlv2Model.from_pretrained("google/owlv2-base-patch16-ensemble")
processor = AutoProcessor.from_pretrained("google/owlv2-base-patch16-ensemble")
url = "http://images.cocodataset.org/val2017/000000039769.jpg"
image = Image.open(requests.get(url, stream=True).raw)
inputs = processor(images=image, return_tensors="pt")
image_features = model.get_image_features(**inputs)
""" .

"DESCRIPTION.The code loads a pre-trained ResNet18 model from a specified URL." <EXPLAINS> "CODE.state_dict = torch.utils.model_zoo.load_url('https://s3.amazonaws.com/pytorch/models/resnet18-5c106cde.pth')" .

"DESCRIPTION.The code loads a pre-trained ResNet50 model from a local path." <EXPLAINS> "CODE._load_local(path, 'resnet50', pretrained=True)" .

"DESCRIPTION.The code loads a pre-trained T5 model for encoding text and a tokenizer, processes input text \"UN Offizier sagt, dass weiter verhandelt werden muss in Syrien\", and calculates the last hidden state representation of the input text using the loaded model." <EXPLAINS> """CODE.from transformers import TFMT5EncoderModel, T5Tokenizer
model = TFMT5EncoderModel.from_pretrained("google/mt5-small")
tokenizer = T5Tokenizer.from_pretrained("google/mt5-small")
article = "UN Offizier sagt, dass weiter verhandelt werden muss in Syrien."
input_ids = tokenizer(article, return_tensors="tf").input_ids
outputs = model(input_ids)
hidden_state = outputs.last_hidden_state""" .

"DESCRIPTION.The code loads a pre-trained Wav2Vec2 model, saves its state dictionary to a file, and then loads the model state dictionary back into a new Wav2Vec2 model with a specified number of output units." <EXPLAINS> """CODE.from torchaudio.models.wav2vec2.utils import import_huggingface_model

original = Wav2Vec2ForCTC.from_pretrained("facebook/wav2vec2-large-960h")
model = import_huggingface_model(original)
torch.save(model.state_dict(), "wav2vec2-base-960h.pt")

model = wav2vec2_large(num_out=32)
model.load_state_dict(torch.load("wav2vec2-base-960h.pt"))
""" .

"DESCRIPTION.The code loads a pre-trained encoder-decoder model using BERT as encoder and GPT-2 as decoder, saves the model to a specific directory, and then loads the saved model." <EXPLAINS> """CODE.from transformers import FlaxEncoderDecoderModel
model = FlaxEncoderDecoderModel.from_encoder_decoder_pretrained('bert-base-cased', 'gpt2')
model.save_pretrained("./bert2gpt2")
model = FlaxEncoderDecoderModel.from_pretrained("./bert2gpt2")
""" .

"DESCRIPTION.The code loads a pretrained CLAP model and feature extractor, generates random audio data, extracts audio features from the data using the feature extractor, and then uses the model to get audio features." <EXPLAINS> """CODE.from transformers import AutoFeatureExtractor, ClapModel
import torch

model = ClapModel.from_pretrained("laion/clap-htsat-unfused")
feature_extractor = AutoFeatureExtractor.from_pretrained("laion/clap-htsat-unfused")
random_audio = torch.rand((16_000))
inputs = feature_extractor(random_audio, return_tensors="pt")
audio_features = model.get_audio_features(**inputs)
""" .

"DESCRIPTION.The code loads a stereo audio file and extracts the left and right channels into a NumPy array with a shape of (2, 1355168). It then converts the stereo audio into a monophonic audio signal and returns a NumPy array with a shape of (1355168,)." <EXPLAINS> """CODE.y, sr = librosa.load(librosa.util.example_audio_file(), mono=False)
y.shape
(2, 1355168)
y_mono = librosa.to_mono(y)
y_mono.shape""" .

"DESCRIPTION.The code loads an audio file 'file.mp3' and separates the harmonic components from the input audio signal 'y'." <EXPLAINS> """CODE.y, sr = librosa.load('file.mp3')
y_harmonic = librosa.effects.harmonic(y)""" .

"DESCRIPTION.The code loads an audio file, extracts chroma cens and chroma cq features from the audio, and displays them using matplotlib.pyplot. Each chroma feature is shown in a separate subplot with corresponding title and colorbar." <EXPLAINS> """CODE.y, sr = librosa.load(librosa.util.example_audio_file(),
...                      offset=10, duration=15)
chroma_cens = librosa.feature.chroma_cens(y=y, sr=sr)
chroma_cq = librosa.feature.chroma_cqt(y=y, sr=sr)

import matplotlib.pyplot as plt
plt.figure()
plt.subplot(2,1,1)
librosa.display.specshow(chroma_cq, y_axis='chroma')
plt.title('chroma_cq')
plt.colorbar()
plt.subplot(2,1,2)
librosa.display.specshow(chroma_cens, y_axis='chroma', x_axis='time')
plt.title('chroma_cens')
plt.colorbar()
plt.tight_layout()""" .

"DESCRIPTION.The code loads an audio file, extracts the harmonic components of the audio signal, computes the tonnetz features, and displays the tonnetz and chroma features in separate subplots using a spectrogram-like visualization." <EXPLAINS> """CODE.y, sr = librosa.load(librosa.util.example_audio_file())
y = librosa.effects.harmonic(y)
tonnetz = librosa.feature.tonnetz(y=y, sr=sr)
tonnetz
array([[-0.073, -0.053, ..., -0.054, -0.073],
       [ 0.001,  0.001, ..., -0.054, -0.062],
       ...,
       [ 0.039,  0.034, ...,  0.044,  0.064],
       [ 0.005,  0.002, ...,  0.011,  0.017]])

import matplotlib.pyplot as plt
plt.subplot(2, 1, 1)
librosa.display.specshow(tonnetz, y_axis='tonnetz')
plt.colorbar()
plt.title('Tonal Centroids (Tonnetz)')
plt.subplot(2, 1, 2)
librosa.display.specshow(librosa.feature.chroma_cqt(y, sr=sr),
...                          y_axis='chroma', x_axis='time')
plt.colorbar()
plt.title('Chroma')
plt.tight_layout()""" .

"DESCRIPTION.The code loads an evaluation suite from a specified file location and runs the evaluation on a trained model for sentiment analysis on the IMDb dataset." <EXPLAINS> """CODE.from evaluate import EvaluationSuite
suite = EvaluationSuite.load("evaluate/evaluation-suite-ci")
results = suite.run("lvwerra/distilbert-imdb")
""" .

"DESCRIPTION.The code loads an image from a given path, converts the image to a numpy array, converts the single image array into a batch array, and then uses a model to make predictions on the input image batch." <EXPLAINS> """CODE.image = tf.keras.preprocessing.image.load_img(image_path)
input_arr = keras.preprocessing.image.img_to_array(image)
input_arr = np.array([input_arr])  # Convert single image to a batch.
predictions = model.predict(input_arr)
""" .

"DESCRIPTION.The code loads an image from a specified path, converts the image to an array, reshapes the array to represent a batch of images, and then uses a machine learning model to make predictions on the input image batch." <EXPLAINS> """CODE.image = tf.keras.preprocessing.image.load_img(image_path)
input_arr = tf.keras.preprocessing.image.img_to_array(image)
input_arr = np.array([input_arr])  # Convert single image to a batch.
predictions = model.predict(input_arr)
""" .

"DESCRIPTION.The code loads and reads the README file located in the project root directory and displays its content in HTML format with center alignment." <EXPLAINS> """CODE._load_readme_description(_PROJECT_ROOT, "", "")  # doctest: +ELLIPSIS +NORMALIZE_WHITESPACE
'<div align="center">...""",
        """CODE.load_readme_description(_PROJECT_ROOT, "", "")  # doctest: +ELLIPSIS +NORMALIZE_WHITESPACE
'<div align="center">...""" .

"DESCRIPTION.The code loads and reads the description from the README file of a project located at the root directory and displays it in HTML format with aligned center." <EXPLAINS> """CODE.load_readme_description(_PROJECT_ROOT, "", "")  # doctest: +ELLIPSIS +NORMALIZE_WHITESPACE
'<div align="center">...'""" .

"DESCRIPTION.The code loads images from a directory, creates a dataset with the images for training, shuffles the files, and displays some examples from the dataset." <EXPLAINS> """CODE.builder = tfds.ImageFolder('path/to/image_dir/')
print(builder.info)
ds = builder.as_dataset(split='train', shuffle_files=True)
tfds.show_examples(ds, builder.info)
""" .

"DESCRIPTION.The code loads pre-trained weights for a large Wav2Vec 2.0 model with a vocabulary size of 60k and 32 output units." <EXPLAINS> """CODE.model = wav2vec2_large_lv60k(num_out=32)
model.load_state_dict(torch.load("wav2vec2-base-960h.pt"))
""" .

"DESCRIPTION.The code loads templates from a specified directory using two different loaders: ModuleLoader and FileSystemLoader." <EXPLAINS> """CODE.ModuleLoader('/path/to/compiled/templates'),
FileSystemLoader('/path/to/templates')""" .

"DESCRIPTION.The code loads the \"rotten_tomatoes\" dataset and selects the \"text\" column from the dataset." <EXPLAINS> """CODE.from datasets import load_dataset
ds = load_dataset("rotten_tomatoes")
ds.select_columns("text")
""" .

"DESCRIPTION.The code loads the \"rotten_tomatoes\" dataset and selects the \"text\" column from the training split. It then retrieves the next data sample, which consists of a text review and its corresponding label (1)." <EXPLAINS> """CODE.from datasets import load_dataset
ds = load_dataset("rotten_tomatoes", split="train", streaming=True)
next(iter(ds))
{'text': 'the rock is destined to be the 21st century's new " conan " and that he's going to make a splash even greater than arnold schwarzenegger , jean-claud van damme or steven segal .', 'label': 1}
ds = ds.select_columns("text")
next(iter(ds))
{'text': 'the rock is destined to be the 21st century's new " conan " and that he's going to make a splash even greater than arnold schwarzenegger , jean-claud van damme or steven segal .'}
""" .

"DESCRIPTION.The code loads the 'imagenet2012' dataset and retrieves one example from the 'train' split. It then checks if the image data type is a string." <EXPLAINS> """CODE.ds = ds.load(
    'imagenet2012',
    split='train',
    decoders={
        'image': tfds.decode.SkipDecoding(),
    }
)

for ex in ds.take(1):
  assert ex['image'].dtype == tf.string
""" .

"DESCRIPTION.The code loads the CIFAR-10 dataset and displays some example images from the training split." <EXPLAINS> """CODE.ds, ds_info = tfds.load('cifar10', split='train', with_info=True)
fig = tfds.show_examples(ds_info, ds)
""" .

"DESCRIPTION.The code loads the CIFAR-10 dataset using TensorFlow Datasets and creates a data pipeline for processing the dataset. It reads the dataset using Ray data, and then extracts the features and labels for the first example in the dataset. Finally, it prints the shape of the features and the corresponding label." <EXPLAINS> """CODE.import ray.data
from ray.data.datasource import SimpleTensorFlowDatasource
import tensorflow_datasets as tfds

def dataset_factory():
    return tfds.load("cifar10", split=["train"], as_supervised=True)[0]

dataset = ray.data.read_datasource(
    SimpleTensorFlowDatasource(),
    parallelism=1,
    dataset_factory=dataset_factory
)

features, label = dataset.take(1)[0]
features.shape
label""" .

"DESCRIPTION.The code loads the MNIST dataset using TensorFlow Datasets, shows the statistics of the dataset info before and after loading it." <EXPLAINS> """CODE.builder = tfds.builder('mnist')
tfds.show_statistics(builder.info)

ds, ds_info = tfds.load('mnist', with_info)
tfds.show_statistics(ds_info)
""" .

"DESCRIPTION.The code loads the MNIST dataset with different splits specified using percentage ranges or absolute values for the test and train sets." <EXPLAINS> """CODE.ds = tfds.load('mnist', split='test[:33%]')
ds = tfds.load('mnist', split=ReadInstruction.from_spec('test[:33%]'))
ds = tfds.load('mnist', split=ReadInstruction('test', to=33, unit='%'))
ds = tfds.load('mnist', split=ReadInstruction(
    'test', from_=0, to=33, unit='%'))

ds = tfds.load('mnist', split='test[:33%]+train[1:-1]')
ds = tfds.load('mnist', split=ReadInstruction.from_spec(
    'test[:33%]+train[1:-1]'))
ds = tfds.load('mnist', split=(
    ReadInstruction.('test', to=33, unit='%') +
    ReadInstruction.('train', from_=1, to=-1, unit='abs')))

tests = tfds.load(
    'mnist',
    [ReadInstruction('train', from_=k, to=k+10, unit='%')
     for k in range(0, 100, 10)])
trains = tfds.load(
    'mnist',
    [RI('train', to=k, unit='%') + RI('train', from_=k+10, unit='%')
     for k in range(0, 100, 10)])""" .

"DESCRIPTION.The code loads the MNIST dataset, splits it into a training set, creates batches of data with a batch size of 32, and prefetches the data for improved performance. It then benchmarks the dataset with a batch size of 32." <EXPLAINS> """CODE.ds = tfds.load('mnist', split='train').batch(32).prefetch()
tfds.core.benchmark(ds, batch_size=32)
""" .

"DESCRIPTION.The code loads the Wine dataset using sklearn, extracts the target values for the 10th, 80th, and 140th instances, and creates a list of target names." <EXPLAINS> """CODE.from sklearn.datasets import load_wine
data = load_wine()
data.target[[10, 80, 140]]
list(data.target_names)""" .

"DESCRIPTION.The code loads the breast cancer dataset from sklearn and retrieves the target labels of samples at indices 10, 50, and 85. It also retrieves the list of target names." <EXPLAINS> """CODE.from sklearn.datasets import load_breast_cancer
data = load_breast_cancer()
data.target[[10, 50, 85]]
list(data.target_names)""" .

"DESCRIPTION.The code loads the latest version of a previously logged model with the name \"my-simple-model\" and assigns the model object to the variable \"model\"." <EXPLAINS> """CODE.# assuming you have previously logged a model with the name "my-simple-model"
sm = use_model("my-simple-model:latest")
model = sm.model_obj()
""" .

"DESCRIPTION.The code loads the long description of a project from a specified location and formats it to be aligned center in HTML." <EXPLAINS> """CODE._load_long_description(PROJECT_ROOT)  # doctest: +ELLIPSIS +NORMALIZE_WHITESPACE
'<div align="center">...""" .

"DESCRIPTION.The code logs an experiment and interacts with a logging service called \"wandb\" by calling a function within it." <EXPLAINS> """CODE.self.logger.experiment.some_wandb_function()
""" .

"DESCRIPTION.The code logs an experiment and performs some training functionality." <EXPLAINS> "CODE.self.logger.experiment.some_trains_function()" .

"DESCRIPTION.The code logs the value of 'val_loss' and another specified value (name and value variables) during training, with the option to specify when to log (on_step, on_epoch), where to log (logger, prog_bar), and how to reduce the value (reduce_fx)." <EXPLAINS> """CODE.result.log('val_loss', loss)
result.log(
    name,
    value,
    on_step=False,
    on_epoch=True,
    logger=True,
    prog_bar=False,
    reduce_fx=torch.mean
)""" .

"DESCRIPTION.The code logs the value of a loss metric named 'val_loss' and another user-defined metric with specified parameters such as name, value, whether to log on each training step and/or epoch, use logger, show on progress bar, and use a specific function for reducing the value." <EXPLAINS> """CODE.result.log('val_loss', loss)

# defaults used
result.log(
    name,
    value,
    on_step=False,
    on_epoch=True,
    logger=True,
    prog_bar=False,
    reduce_fx=torch.mean
)
""" .

"DESCRIPTION.The code maps strings to indices using a lookup table and then looks up the indices for a set of input strings." <EXPLAINS> """CODE.mapping_strings = t.constant(["emerson", "lake", "palmer")
table = tf.contrib.lookup.index_table_from_tensor(
    mapping=mapping_strings, num_oov_buckets=1, default_value=-1)
features = tf.constant(["emerson", "lake", "and", "palmer"])
ids = table.lookup(features)
...
tf.tables_initializer().run()

ids.eval()  ==> [0, 1, 4, 2]
""",
        """CODE.mapping_strings = t.constant(["emerson", "lake", "palmer")
table = tf.contrib.lookup.string_to_index_table_from_tensor(
    mapping=mapping_strings, num_oov_buckets=1, default_value=-1)
features = tf.constant(["emerson", "lake", "and", "palmer"])
ids = table.lookup(features)
...
tf.tables_initializer().run()

ids.eval()  ==> [0, 1, 4, 2]
""" .

"DESCRIPTION.The code maps the parameters of an optimizer using a given state and specifications, transforming any non-parameter values to None." <EXPLAINS> """CODE.opt_specs = optax.tree_map_params(
    opt,
    lambda _, spec: spec,
    state,
    specs,
    transform_non_params=lambda _: None,
)""" .

"DESCRIPTION.The code masks specific timesteps in the input data for an LSTM model, setting the values at timesteps 3 and 5 to 0 to be skipped during the LSTM calculation." <EXPLAINS> """CODE.samples, timesteps, features = 32, 10, 8
inputs = np.random.random([samples, timesteps, features]).astype(np.float32)
inputs[:, 3, :] = 0.
inputs[:, 5, :] = 0.

model = tf.keras.models.Sequential()
model.add(tf.keras.layers.Masking(mask_value=0.,
                                  input_shape=(timesteps, features)))
model.add(tf.keras.layers.LSTM(32))

output = model(inputs)
# The time step 3 and 5 will be skipped from LSTM calculation.
""" .

"DESCRIPTION.The code may potentially modify lambda functions named 'sum' to avoid conflicts." <EXPLAINS> "CODE._maybe_mangle_lambdas('sum')\\n\\n_maybe_mangle_lambdas([lambda: 1, lambda: 2])  # doctest: +SKIP",
        """CODE.maybe_mangle_lambdas('sum')

maybe_mangle_lambdas([lambda: 1, lambda: 2])  # doctest: +SKIP""" .

"DESCRIPTION.The code measures the throughput of a process by monitoring the time taken to complete a certain number of batches and samples, and logs the computed throughput every 10 steps." <EXPLAINS> """CODE.    fabric = Fabric(logger=logger)
    throughput = ThroughputMonitor()
    t0 = time()
    for i in range(1, 100):
        do_work()
        if torch.cuda.is_available(): torch.cuda.synchronize()  # required or else time() won't be correct
        throughput.update(time=time() - t0, batches=i, samples=i)
        if i % 10 == 0:
            throughput.compute_and_log(step=i)""" .

"DESCRIPTION.The code melts the DataFrame 'df' by keeping the column 'A' as the identifier variable." <EXPLAINS> "CODE.melt(df, id_vars=['A'])" .

"DESCRIPTION.The code merges two dataframes A and B based on a specified fill method and left by group." <EXPLAINS> "CODE.ordered_merge(A, B, fill_method='ffill', left_by='group')" .

"DESCRIPTION.The code merges two dictionaries together by combining their values for matching keys, using the specified function for combining the values." <EXPLAINS> """CODE.from operator import add
m1 = m(a=1, b=2)
m1.merge_with(add, m(a=2))
pmap({'a': 3, 'b': 2})
m1 = m(a=1)
m1.merge_with(lambda l, r: l, m(a=2), {'a':3})
pmap({'a': 1})""" .

"DESCRIPTION.The code mocks the data access object to process users with specific names, 'stevepm' and 'salomaki'." <EXPLAINS> "CODE.mock_dao.ProcessUsers(SameElementsAs('stevepm', 'salomaki'))" .

"DESCRIPTION.The code modifies a dictionary object and a training state object by setting specific values at specified paths within the objects. The modified dictionary object is checked against an expected value." <EXPLAINS> """CODE.from flax.cursor import cursor
from flax.training import train_state
import optax

dict_obj = {'a': 1, 'b': (2, 3), 'c': [4, 5]}
modified_dict_obj = cursor(dict_obj)['b'][0].set(10)
assert modified_dict_obj == {'a': 1, 'b': (10, 3), 'c': [4, 5]}

state = train_state.TrainState.create(
    apply_fn=lambda x: x,
    params=dict_obj,
    tx=optax.adam(1e-3),
)
modified_state = cursor(state).params['b'][1].set(10)
assert modified_state.params == {'a': 1, 'b': (2, 10), 'c': [4, 5]}""" .

"DESCRIPTION.The code modifies a dictionary object by updating specific values within nested lists and tuples, and then creates a new TrainState object with updated parameters and a new apply function." <EXPLAINS> """CODE.from flax.cursor import cursor
from flax.training import train_state
import optax

dict_obj = {'a': 1, 'b': (2, 3), 'c': [4, 5]}
c = cursor(dict_obj)
c['b'][0] = 10
c['a'] = (100, 200)
modified_dict_obj = c.build()
assert modified_dict_obj == {'a': (100, 200), 'b': (10, 3), 'c': [4, 5]}

state = train_state.TrainState.create(
    apply_fn=lambda x: x,
    params=dict_obj,
    tx=optax.adam(1e-3),
)
new_fn = lambda x: x + 1
c = cursor(state)
c.params['b'][1] = 10
c.apply_fn = new_fn
modified_state = c.build()
assert modified_state.params == {'a': 1, 'b': (2, 10), 'c': [4, 5]}
assert modified_state.apply_fn == new_fn""" .

"DESCRIPTION.The code modifies the output of a linear layer by multiplying it by 2 using a forward hook." <EXPLAINS> """CODE.import paddle.fluid as fluid
import numpy as np

# the forward_post_hook change the output of the layer: output = output * 2
def forward_post_hook(layer, input, output):
    # user can use layer, input and output for information statistis tasks

    # change the output
    return output * 2

with fluid.dygraph.guard():
    linear = fluid.Linear(13, 5, dtype="float32")

    # register the hook
    forward_post_hook_handle = linear.register_forward_post_hook(forward_post_hook)

    value1 = np.arange(26).reshape(2, 13).astype("float32")
    in1 = fluid.dygraph.to_variable(value1)

    out0 = linear(in1)

    # remove the hook
    forward_post_hook_handle.remove()

    out1 = linear(in1)

    # hook change the linear's output to output * 2, so out0 is equal to out1 * 2.
    assert (out0.numpy() == (out1.numpy()) * 2).any()
""" .

"DESCRIPTION.The code multiplies all the elements in the Counter 'prime_factors' and returns the product, which is 1836." <EXPLAINS> """CODE.c = Counter('ABCABC')
sorted(c.elements())
['A', 'A', 'B', 'B', 'C', 'C']

prime_factors = Counter({2: 2, 3: 3, 17: 1})
product = 1
for factor in prime_factors.elements():
    product *= factor
product
1836""" .

"DESCRIPTION.The code multiplies two arrays obtained from reshaping two sequences of numbers and returns the result." <EXPLAINS> """CODE.x1 = tf.keras.layers.Dense(8)(np.arange(10).reshape(5, 2)
x2 = tf.keras.layers.Dense(8)(np.arange(10, 20).reshape(5, 2)
multiplied = tf.keras.layers.Multiply()([x1, x2])""" .

"DESCRIPTION.The code normalizes the JSON data into a flat table format using pandas, with different levels of normalization specified by the \"max_level\" parameter." <EXPLAINS> """CODE.pd.json_normalize(data)
pd.json_normalize(data, max_level=0)
pd.json_normalize(data, max_level=1)
result = pd.json_normalize(data, "counties", ["state", "shortname", ["info", "governor"]])
pd.json_normalize(data, "A", record_prefix="Prefix.")""" .

"DESCRIPTION.The code normalizes the data format between 'channels_first' and 'channels_last' for Keras backend." <EXPLAINS> """CODE.from keras import backend as K
K.normalize_data_format(None)
'channels_first'
K.normalize_data_format('channels_last')
'channels_last'
""" .

"DESCRIPTION.The code normalizes the data format for Keras backend, allowing for consistent data format usage." <EXPLAINS> """CODE.from keras import backend as K
K.normalize_data_format(None)
K.normalize_data_format('channels_last')
""" .

"DESCRIPTION.The code normalizes the input data and applies an activation function (leaky ReLU with an alpha value of 0.2) to the output of the normalization for a neural network model." <EXPLAINS> """CODE.hidden2 = fluid.layers.inplace_abn(input=hidden1)
hidden3 = fluid.layers.inplace_abn(input=hidden2, act='leaky_relu', act_alpha=0.2)""" .

"DESCRIPTION.The code normalizes the input data using the tf.keras.layers.Normalization layer. The adapt_data is used to calculate the mean and variance for normalization, and then the input_data is normalized based on the calculated mean and variance." <EXPLAINS> """CODE.adapt_data = np.array([1., 2., 3., 4., 5.], dtype='float32')
input_data = np.array([1., 2., 3.], dtype='float32')
layer = tf.keras.layers.Normalization(axis=None)
layer.adapt(adapt_data)
layer(input_data)


adapt_data = np.array([[0., 7., 4.],
                       [2., 9., 6.],
                       [0., 7., 4.],
                       [2., 9., 6.]], dtype='float32')
input_data = np.array([[0., 7., 4.]], dtype='float32')
layer = tf.keras.layers.Normalization(axis=-1)
layer.adapt(adapt_data)
layer(input_data)


input_data = np.array([[1.], [2.], [3.]], dtype='float32')
layer = tf.keras.layers.Normalization(mean=3., variance=2.)
layer(input_data)
""" .

"DESCRIPTION.The code normalizes the input data using unit normalization and then calculates the sum of the squared values of the first row in the normalized data." <EXPLAINS> """CODE.data = tf.constant(np.arange(6).reshape(2, 3), dtype=tf.float32)
normalized_data = tf.keras.layers.UnitNormalization()(data)
print(tf.reduce_sum(normalized_data[0, :] ** 2).numpy())
""" .

"DESCRIPTION.The code offloads three different models to a specified CUDA device and conducts forward passes using these models. Model 1 is offloaded to the CPU, model 2 stays on the GPU for a specified number of iterations, and model 3 is offloaded to the CPU again. Additionally, a manual offload method is called for model 3." <EXPLAINS> """CODE.model_1, hook_1 = cpu_offload_with_hook(model_1, cuda_device)
model_2, hook_2 = cpu_offload_with_hook(model_2, cuda_device, prev_module_hook=hook_1)
model_3, hook_3 = cpu_offload_with_hook(model_3, cuda_device, prev_module_hook=hook_2)

hid_1 = model_1(input)
for i in range(50):
    # model1 is offloaded on the CPU at the first iteration, model 2 stays on the GPU for this whole loop.
    hid_2 = model_2(hid_1)
# model2 is offloaded to the CPU just before this forward.
hid_3 = model_3(hid_3)

# For model3, you need to manually call the hook offload method.
hook_3.offload()
""" .

"DESCRIPTION.The code opens a web browser to visit the website \"http://click.pocoo.org/\" and then locates and opens the downloaded file '/my/downloaded/file'." <EXPLAINS> """CODE.click.launch('http://click.pocoo.org/')
click.launch('/my/downloaded/file', locate=True)""" .

"DESCRIPTION.The code opens an image file in read mode using a plugin and then closes the file." <EXPLAINS> """CODE.image_file = my_plugin(Request, "r")
...
image_file.close()""" .

"DESCRIPTION.The code opens an image file named 'sunrise.jpg' using the PIL library and displays it with the caption 'Sunrise by the mountains' in a streamlit app." <EXPLAINS> """CODE.from PIL import Image
image = Image.open('sunrise.jpg')

st.image(image, caption='Sunrise by the mountains', use_column_width=True)
""" .

"DESCRIPTION.The code optimizes a machine learning model using stochastic gradient descent with a learning rate scheduler that reduces the learning rate by a factor of 0.9 every epoch. Additionally, after a certain number of epochs, it switches to a different learning rate scheduler that linearly decreases the learning rate over 20 epochs. After reaching the specified epoch (swa_start = 160), it switches to the SWALR scheduler for further optimization." <EXPLAINS> """CODE.lr_lambda = lambda epoch: 0.9
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lr_lambda)
swa_scheduler = torch.optim.swa_utils.SWALR(optimizer, anneal_strategy="linear", anneal_epochs=20, swa_lr=0.05)
swa_start = 160
for i in range(300):
     for input, target in loader:
         optimizer.zero_grad()
         loss_fn(model(input), target).backward()
         optimizer.step()
     if i > swa_start:
         swa_scheduler.step()
     else:
         scheduler.step()""" .

"DESCRIPTION.The code optimizes a neural network model using automatic mixed precision (AMP) training with bfloat16 (bf16) data type. It includes operations such as batch normalization, convolution, pooling, and fully connected layers, and utilizes the Momentum optimizer with multi-precision settings for improved accuracy and convergence speed. The code also demonstrates the use of custom op lists and initialization processes for AMP training." <EXPLAINS> """CODE.import paddle
import paddle.static as static

paddle.enable_static()

data = static.data(name='X', shape=[None, 1], dtype='float32')
hidden = static.nn.fc(x=data, size=10)
loss = paddle.mean(hidden)
optimizer = paddle.optimizer.Adam(learning_rate=0.001)

mp_optimizer = static.amp.decorate_bf16(optimizer=optimizer)

ops, param_grads = mp_optimizer.minimize(loss)

place = paddle.CPUPlace(0)
exe = paddle.static.Executor(place)
data = paddle.static.data(name='X', shape=[None, 1, 28, 28], dtype='float32')
conv2d = paddle.static.nn.conv2d(input=data, num_filters=6, filter_size=3)
# 1) Use bf16_guard to control the range of bf16 kernels used.
with paddle.static.amp.bf16_guard():
    bn = paddle.static.nn.batch_norm(input=conv2d, act="relu")
    pool = F.max_pool2d(bn, kernel_size=2, stride=2)
    hidden = paddle.static.nn.fc(pool, size=10)
    loss = paddle.mean(hidden)
# 2) Create the optimizer and set `multi_precision` to True.
# Setting `multi_precision` to True can avoid the poor accuracy
# or the slow convergence in a way.
optimizer = paddle.optimizer.Momentum(learning_rate=0.01, multi_precision=True)
# 3) These ops in `custom_fp32_list` will keep in the float32 computation type.
amp_list = paddle.static.amp.CustomOpLists(
    custom_fp32_list=['pool2d'])
# 4) The entry of Paddle AMP.
# Enable pure bf16 training by setting `use_pure_bf16` to True.
optimizer = paddle.static.amp.decorate_bf16(
    optimizer,
    amp_list,
    use_pure_bf16=True)
# If you don't use the default_startup_program(), you sholud pass
# your defined `startup_program` into `minimize`.
optimizer.minimize(loss)
exe.run(paddle.static.default_startup_program())
# 5) Use `amp_init` after FP32 parameters initialization(such as `exe.run(startup_program)`).
# If you want to perform the testing process, you should pass `test_program` into `amp_init`.
optimizer.amp_init(place, scope=paddle.static.global_scope())""" .

"DESCRIPTION.The code pads a 3D input tensor with zeroes to increase its spatial dimensions." <EXPLAINS> """CODE.input_shape = (1, 1, 2, 2, 3)
x = np.arange(np.prod(input_shape)).reshape(input_shape)
y = tf.keras.layers.ZeroPadding3D(padding=2)(x)
print(y.shape)
(1, 5, 6, 6, 3)""" .

"DESCRIPTION.The code pads a 4-dimensional tensor with constant values." <EXPLAINS> """CODE.out = F.pad(t4d, p1d, "constant", 0)  # effectively zero padding
out = F.pad(t4d, p2d, "constant", 0)
out = F.pad(t4d, p3d, "constant", 0)""" .

"DESCRIPTION.The code pads sequences in a list to a specified length." <EXPLAINS> """CODE.sequence = [[1], [2, 3], [4, 5, 6]]
tf.keras.preprocessing.sequence.pad_sequences(sequence)

tf.keras.preprocessing.sequence.pad_sequences(sequence, value=-1)

tf.keras.preprocessing.sequence.pad_sequences(sequence, padding='post')

tf.keras.preprocessing.sequence.pad_sequences(sequence, maxlen=2)
""" .

"DESCRIPTION.The code pads sequences in a list to ensure they are of the same length either by adding zeros at the beginning ('pre') or end ('post') of the sequences, by setting a different value for padding, or by cutting off sequences that exceed a maximum length specified." <EXPLAINS> """CODE.sequence = [[1], [2, 3], [4, 5, 6]]
tf.keras.preprocessing.sequence.pad_sequences(sequence)
array([[0, 0, 1],
       [0, 2, 3],
       [4, 5, 6]], dtype=int32)

tf.keras.preprocessing.sequence.pad_sequences(sequence, value=-1)
array([[-1, -1,  1],
       [-1,  2,  3],
       [ 4,  5,  6]], dtype=int32)

tf.keras.preprocessing.sequence.pad_sequences(sequence, padding='post')
array([[1, 0, 0],
       [2, 3, 0],
       [4, 5, 6]], dtype=int32)

tf.keras.preprocessing.sequence.pad_sequences(sequence, maxlen=2)
array([[0, 1],
       [2, 3],
       [5, 6]], dtype=int32)
""" .

"DESCRIPTION.The code pads sequences with zeros, a specified value, or truncates them based on the provided parameters such as padding position, padding value, and maximum sequence length." <EXPLAINS> """CODE.sequence = [[1], [2, 3], [4, 5, 6]]
tf.keras.preprocessing.sequence.pad_sequences(sequence)
array([[0, 0, 1],
       [0, 2, 3],
       [4, 5, 6]], dtype=int32)

tf.keras.preprocessing.sequence.pad_sequences(sequence, value=-1)
array([[-1, -1,  1],
       [-1,  2,  3],
       [ 4,  5,  6]], dtype=int32)

tf.keras.preprocessing.sequence.pad_sequences(sequence, padding='post')
array([[1, 0, 0],
       [2, 3, 0],
       [4, 5, 6]], dtype=int32)

tf.keras.preprocessing.sequence.pad_sequences(sequence, maxlen=2)
array([[0, 1],
       [2, 3],
       [5, 6]], dtype=int32)
""" .

"DESCRIPTION.The code parses HTML content to extract and print the body content of div elements with class \"grid\" and div elements with any class attribute." <EXPLAINS> """CODE.div,div_end = makeHTMLTags("div")
div_grid = div().setParseAction(withClass("grid"))

grid_expr = div_grid + SkipTo(div | div_end)("body")
for grid_header in grid_expr.searchString(html):
    print(grid_header.body)

div_any_type = div().setParseAction(withClass(withAttribute.ANY_VALUE))
div_expr = div_any_type + SkipTo(div | div_end)("body")
for div_header in div_expr.searchString(html):
    print(div_header.body)""" .

"DESCRIPTION.The code parses HTML content to extract and print the body of div tags with specific attributes and values. It first targets div tags with a type attribute set to \"grid\", then moves on to match any div tags with a type attribute, regardless of the value." <EXPLAINS> """CODE.div,div_end = makeHTMLTags("div")

# only match div tag having a type attribute with value "grid"
div_grid = div().setParseAction(withAttribute(type="grid"))
grid_expr = div_grid + SkipTo(div | div_end)("body")
for grid_header in grid_expr.searchString(html):
    print(grid_header.body)

# construct a match with any div tag having a type attribute, regardless of the value
div_any_type = div().setParseAction(withAttribute(type=withAttribute.ANY_VALUE))
div_expr = div_any_type + SkipTo(div | div_end)("body")
for div_header in div_expr.searchString(html):
    print(div_header.body)""" .

"DESCRIPTION.The code parses a Jinja template containing a variable assignment and an expression with variables, then it finds all undeclared variables in the template." <EXPLAINS> """CODE.from jinja2 import Environment, meta
env = Environment()
ast = env.parse('{% set foo = 42 %}{{ bar + foo }}')
meta.find_undeclared_variables(ast)
""" .

"DESCRIPTION.The code parses a Jinja2 template string to extract any referenced template names." <EXPLAINS> """CODE.from jinja2 import Environment, meta
env = Environment()
ast = env.parse('{% extends "layout.html" %}{% include helper %}')
list(meta.find_referenced_templates(ast))""" .

"DESCRIPTION.The code parses a Unicode version string \"14.0.0\" and creates a UnicodeVersion object with major version 14, minor version 0, and micro version 0." <EXPLAINS> """CODE.UnicodeVersion.parse("14.0.0")
UnicodeVersion(major=14, minor=0, micro=0)""" .

"DESCRIPTION.The code parses a builder name with corresponding keyword arguments and returns the namespace, name, and version of the builder, as well as additional builder keyword arguments." <EXPLAINS> """CODE.ds_name, builder_kwargs = parse_builder_name_kwargs(
    'kaggle:ds/cfg:1.2.3', data_dir='...'
)
ds_name.namespace == 'kaggle'
ds_name.name == 'ds'
builder_kwargs == {'config': 'cfg', 'version': '1.2.3', 'data_dir': '...'}
""" .

"DESCRIPTION.The code parses a byte string containing information about ImageJ software and returns a dictionary with keys 'ImageJ', 'images', and 'hyperstack' mapped to their respective values." <EXPLAINS> """CODE.description = 'ImageJ=1.11a\\nimages=510\\nhyperstack=true\\n'
imagej_description_metadata(description)
{'ImageJ': '1.11a', 'images': 510, 'hyperstack': True}""",
        """CODE.description = b'ImageJ=1.11a\\nimages=510\\nhyperstack=true\\n'
imagej_description_dict(description)  # doctest: +SKIP
{'ImageJ': '1.11a', 'images': 510, 'hyperstack': True}""" .

"DESCRIPTION.The code parses a date and time string in ISO 8601 format and creates a datetime object with the corresponding date, time, and timezone information." <EXPLAINS> """CODE.parse_datetime('2022-08-19T07:19:38.123Z')
datetime.datetime(2022, 8, 19, 7, 19, 38, 123000, tzinfo=timezone.utc)
""" .

"DESCRIPTION.The code parses a date string in the format \"YYYY/MM/DD\" into a list of integers corresponding to the year, month, and day respectively." <EXPLAINS> """CODE.integer = Word(nums)
date_str = integer + '/' + integer + '/' + integer

date_str.parseString("1999/12/31")  # -> ['1999', '/', '12', '/', '31']

# use parse action to convert to ints at parse time
integer = Word(nums).setParseAction(lambda toks: int(toks[0]))
date_str = integer + '/' + integer + '/' + integer

# note that integer fields are now ints, not strings
date_str.parseString("1999/12/31")  # -> [1999, '/', 12, '/', 31]""" .

"DESCRIPTION.The code parses a date string in the format \"year/month/day\" and ensures that the year is greater than or equal to 2000." <EXPLAINS> """CODE.integer = Word(nums).setParseAction(lambda toks: int(toks[0]))
year_int = integer.copy()
year_int.addCondition(lambda toks: toks[0] >= 2000, message="Only support years 2000 and later")
date_str = year_int + '/' + integer + '/' + integer""" .

"DESCRIPTION.The code parses a given URL and extracts the scheme, host, port, path, and query parameters from it." <EXPLAINS> """CODE.def parse_url(url):
    from urllib.parse import urlparse
    parsed_url = urlparse(url)
    return Url(scheme=parsed_url.scheme, host=parsed_url.hostname, port=parsed_url.port, path=parsed_url.path, query=parsed_url.query, ...)

parse_url('http://google.com/mail/')
parse_url('google.com:80')
parse_url('/foo?bar')
""" .

"DESCRIPTION.The code parses a given string representing key-value pairs separated by commas within single quotes. It returns a dictionary containing the key-value pairs." <EXPLAINS> """CODE.d = parse_dict_header('foo="is a fish", bar="as well"')
type(d) is dict
sorted(d.items())

parse_dict_header('key_without_value')
""" .

"DESCRIPTION.The code parses a program protobuf file (.pbtxt) and saves the output in a specified directory." <EXPLAINS> """CODE.from paddle.fluid.incubate.fleet.utils.fleet_util import FleetUtil
fleet_util = FleetUtil()
program_path = "./program.pbtxt"
is_text = True
output_dir = "/tmp/"
fleet_util.parse_program_proto(program_path, is_text, output_dir)""" .

"DESCRIPTION.The code parses a source string to extract a list of words separated by commas, with the first version retaining the commas while the second version suppresses them." <EXPLAINS> """CODE.wd_list1 = wd + ZeroOrMore(',' + wd)
print(wd_list1.parseString(source))

wd_list2 = wd + ZeroOrMore(Suppress(',') + wd)
print(wd_list2.parseString(source))""" .

"DESCRIPTION.The code parses a string containing attribute-value pairs separated by colon, where each attribute is followed by a colon and the attribute-value pairs are grouped together." <EXPLAINS> """CODE.data_word = Word(alphas)
label = data_word + FollowedBy(':')
attr_expr = Group(label + Suppress(':') + OneOrMore(data_word, stopOn=label).setParseAction(' '.join))

OneOrMore(attr_expr).parseString("shape: SQUARE color: BLACK posn: upper left").pprint()""" .

"DESCRIPTION.The code parses a string containing numbers and 'N/A' or 'NA' strings, replacing them with integers and 'nan' respectively." <EXPLAINS> """CODE.num = Word(nums).setParseAction(lambda toks: int(toks[0])
na = oneOf("N/A NA").setParseAction(replaceWith(math.nan)
term = na | num
OneOrMore(term).parseString("324 234 N/A 234") # -> [324, 234, nan, 234]""" .

"DESCRIPTION.The code parses a string input and extracts one or more numeric words, then inserts the parse location at the beginning of the results." <EXPLAINS> """CODE.print(OneOrMore(Word(nums)).parseString("0 123 321")) # -> ['0', '123', '321']

# use a parse action to insert the parse location in the front of the parsed results
def insert_locn(locn, tokens):
    tokens.insert(0, locn)
print(OneOrMore(Word(nums)).addParseAction(insert_locn).parseString("0 123 321")) # -> [0, '0', '123', '321']""" .

"DESCRIPTION.The code parses a string representing a function call and returns the function name along with its arguments as a list. If the arguments are grouped together, they are returned as a sublist within the main list." <EXPLAINS> """CODE.ident = Word(alphas)
num = Word(nums)
term = ident | num
func = ident + Optional(delimitedList(term))
print(func.parseString("fn a,b,100"))  # -> ['fn', 'a', 'b', '100']

func = ident + Group(Optional(delimitedList(term)))
print(func.parseString("fn a,b,100"))  # -> ['fn', ['a', 'b', '100']]""" .

"DESCRIPTION.The code parses a string representing a nested function call with arguments and prints the result in a pretty format." <EXPLAINS> """CODE.ident = Word(alphas, alphanums)
num = Word(nums)
func = Forward()
term = ident | num | Group('(' + func + ')')
func <<= ident + Group(Optional(delimitedList(term)))
result = func.parseString("fna a,b,(fnb c,d,200),100")
result.pprint(width=40)""" .

"DESCRIPTION.The code parses a string that contains metadata information about intensity mapping for a specific channel and extracts and returns the mapping range for that channel." <EXPLAINS> """CODE.descr = ('[Intensity Mapping]\\nMap Ch0: Range=00000 to 02047\\n'
...          '[Intensity Mapping End]')
fluoview_description_metadata(descr)
{'Intensity Mapping': {'Map Ch0: Range': '00000 to 02047'}}""" .

"DESCRIPTION.The code parses a text string containing attribute-value pairs separated by colons, and stores the attributes and values in a dictionary. It then prints the parsed results and accesses specific attribute values from the dictionary." <EXPLAINS> """CODE.data_word = Word(alphas)
label = data_word + FollowedBy(':')
attr_expr = Group(label + Suppress(':') + OneOrMore(data_word).setParseAction(' '.join))

text = "shape: SQUARE posn: upper left color: light blue texture: burlap"
attr_expr = (label + Suppress(':') + OneOrMore(data_word, stopOn=label).setParseAction(' '.join))

print(OneOrMore(attr_expr).parseString(text).dump())

result = Dict(OneOrMore(Group(attr_expr))).parseString(text)
print(result.dump())

print(result['shape'])
print(result.asDict())""" .

"DESCRIPTION.The code parses and removes quotes from a quoted string input." <EXPLAINS> """CODE.quotedString.setParseAction(removeQuotes)
quotedString.parseString("'Now is the Winter of our Discontent'")
quotedString.setParseAction(removeQuotes)
quotedString.parseString("'Now is the Winter of our Discontent'")""" .

"DESCRIPTION.The code parses examples from a BigQuery table with specified schema, reads and processes the data including features like name, age, and state." <EXPLAINS> """CODE.# Assume a BigQuery has the following schema,
#     name      STRING,
#     age       INT,
#     state     STRING

# Create the parse_examples list of features.
features = dict(
  name=tf.FixedLenFeature([1], tf.string),
  age=tf.FixedLenFeature([1], tf.int32),
  state=tf.FixedLenFeature([1], dtype=tf.string, default_value="UNK"))

# Create a Reader.
reader = bigquery_reader_ops.BigQueryReader(project_id=PROJECT,
                                            dataset_id=DATASET,
                                            table_id=TABLE,
                                            timestamp_millis=TIME,
                                            num_partitions=NUM_PARTITIONS,
                                            features=features)

# Populate a queue with the BigQuery Table partitions.
queue = tf.training.string_input_producer(reader.partitions())

# Read and parse examples.
row_id, examples_serialized = reader.read(queue)
examples = tf.parse_example(examples_serialized, features=features)

# Process the Tensors examples["name"], examples["age"], etc...
""" .

"DESCRIPTION.The code parses repositories to extract metadata information about the model files stored within them. It distinguishes between repositories with a single weights file and those with a sharded model. If the repository is not recognized as a safetensors repo, an error message is returned indicating the absence of necessary files." <EXPLAINS> """CODE.# Parse repo with single weights file
metadata = get_safetensors_metadata("bigscience/bloomz-560m")
metadata
metadata.files_metadata["model.safetensors"].metadata
{'format': 'pt'}

# Parse repo with sharded model
metadata = get_safetensors_metadata("bigscience/bloom")
metadata
len(metadata.files_metadata)
72  # All safetensors files have been fetched

# Parse repo with sharded model
get_safetensors_metadata("runwayml/stable-diffusion-v1-5")
NotASafetensorsRepoError: 'runwayml/stable-diffusion-v1-5' is not a safetensors repo. Couldn't find 'model.safetensors.index.json' or 'model.safetensors' files.
""" .

"DESCRIPTION.The code parses text data into a dictionary format with attribute labels and values. It then prints the parsed result, accesses specific attributes ('shape'), and prints the result as a dictionary." <EXPLAINS> """CODE.attr_expr = (label + Suppress(':') + OneOrMore(data_word, stopOn=label).setParseAction(' '.join))
print(OneOrMore(attr_expr).parseString(text).dump())

attr_label = label
attr_value = Suppress(':') + OneOrMore(data_word, stopOn=label).setParseAction(' '.join)

result = dictOf(attr_label, attr_value).parseString(text)
print(result.dump())
print(result['shape'])
print(result.shape)
print(result.asDict())""" .

"DESCRIPTION.The code parses the arguments provided to the program." <EXPLAINS> "CODE.parse_args()" .

"DESCRIPTION.The code parses the input URL and extracts its components such as the protocol, domain, port, and path." <EXPLAINS> """CODE.parse_url('http://google.com/mail/')
parse_url('google.com:80')
parse_url('/foo?bar')""" .

"DESCRIPTION.The code parses the text to find all occurrences of HTML anchor tags (```<a></a>```), extracts the link text and the href attribute, and then prints them in the format: 'link text -> href'." <EXPLAINS> """CODE.a,a_end = makeHTMLTags("A")
link_expr = a + SkipTo(a_end)("link_text") + a_end

for link in link_expr.searchString(text):
    print(link.link_text, '->', link.href)""" .

"DESCRIPTION.The code parses user information from a string and categorizes it into age, social security number, or house number." <EXPLAINS> """CODE.integer = Word(nums)
ssn_expr = Regex(r"\\d\\d\\d-\\d\\d-\\d\\d\\d\\d")
house_number_expr = Suppress('#') + Word(nums, alphanums)
user_data = (Group(house_number_expr)("house_number")
            | Group(ssn_expr)("ssn")
            | Group(integer)("age"))
user_info = OneOrMore(user_data)

result = user_info.parseString("22 111-22-3333 #221B")
for item in result:
    print(item.getName(), ':', item[0])""" .

"DESCRIPTION.The code partitions a tensor or structured tensor based on given row lengths." <EXPLAINS> """CODE.partition = RowPartition.from_row_lengths([2, 0, 1])
_partition_outer_dimension(tf.constant([1, 2, 3]), partition)

struct_value = StructuredTensor.from_pyval(
    [{'x': 1}, {'x': 2}, {'x': 3}])
_partition_outer_dimension(struct_value, partition)
""" .

"DESCRIPTION.The code performs 1D max pooling on a given input tensor `x` with different pooling configurations - pooling size, strides, and padding. The max pooling operation reduces the dimensionality of the input tensor by selecting the maximum value within a specified window size along the specified axis." <EXPLAINS> """CODE.x = tf.constant([1., 2., 3., 4., 5.])
x = tf.reshape(x, [1, 5, 1])
max_pool_1d = tf.keras.layers.MaxPooling1D(pool_size=2,
...    strides=1, padding='valid')
max_pool_1d(x)
<tf.Tensor: shape=(1, 4, 1), dtype=float32, numpy=
array([[[2.],
        [3.],
        [4.],
        [5.]]], dtype=float32)>
x = tf.constant([1., 2., 3., 4., 5.])
x = tf.reshape(x, [1, 5, 1])
max_pool_1d = tf.keras.layers.MaxPooling1D(pool_size=2,
...    strides=2, padding='valid')
max_pool_1d(x)
<tf.Tensor: shape=(1, 2, 1), dtype=float32, numpy=
array([[[2.],
        [4.]]], dtype=float32)>
x = tf.constant([1., 2., 3., 4., 5.])
x = tf.reshape(x, [1, 5, 1])
max_pool_1d = tf.keras.layers.MaxPooling1D(pool_size=2,
...    strides=1, padding='same')
max_pool_1d(x)
<tf.Tensor: shape=(1, 5, 1), dtype=float32, numpy=
array([[[2.],
        [3.],
        [4.],
        [5.],
        [5.]]], dtype=float32)>""" .

"DESCRIPTION.The code performs 2D deformable convolution operation on the input tensor using the given offset and weight tensors, resulting in an output tensor with a specific shape." <EXPLAINS> """CODE.input = torch.rand(1, 3, 10, 10)
kh, kw = 3, 3
weight = torch.rand(5, 3, kh, kw)
offset = torch.rand(5, 2 * kh * kw, 8, 8)
out = deform_conv2d(input, offset, weight)
print(out.shape)
""" .

"DESCRIPTION.The code performs 3D convolution on randomly generated data using TensorFlow. It creates two 3D convolutional layers with 2 filters, relu activation, and different input shapes, and prints the shape of the output tensor for each convolutional layer." <EXPLAINS> """CODE.input_shape =(4, 28, 28, 28, 1)
x = tf.random.normal(input_shape)
y = tf.keras.layers.Conv3D(
2, 3, activation='relu', input_shape=input_shape[1:])(x)
print(y.shape)
(4, 26, 26, 26, 2)

input_shape = (4, 7, 28, 28, 28, 1)
x = tf.random.normal(input_shape)
y = tf.keras.layers.Conv3D(
2, 3, activation='relu', input_shape=input_shape[2:])(x)
print(y.shape)
(4, 7, 26, 26, 26, 2)
""" .

"DESCRIPTION.The code performs 3D convolution on randomly generated tensor data with specified input shapes, using 2 filters and a relu activation function. The resulting tensor shape is printed out for each input shape provided." <EXPLAINS> """CODE.input_shape =(4, 28, 28, 28, 1)
x = tf.random.normal(input_shape)
y = tf.keras.layers.Conv3D(
2, 3, activation='relu', input_shape=input_shape[1:])(x)
print(y.shape)
(4, 26, 26, 26, 2)

input_shape = (4, 7, 28, 28, 28, 1)
x = tf.random.normal(input_shape)
y = tf.keras.layers.Conv3D(
2, 3, activation='relu', input_shape=input_shape[2:])(x)
print(y.shape)
(4, 7, 26, 26, 26, 2)
""" .

"DESCRIPTION.The code performs 3D max pooling on the input tensor x with a pool size of 2 and strides of 2." <EXPLAINS> """CODE. y = tf.compat.v1.layers.max_pooling3d(x, pool_size=2, strides=2)

 x = tf.keras.Input((28, 28, 1))
 y = tf.keras.layers.MaxPooling3D(pool_size=2, strides=2)(x)
 model = tf.keras.Model(x, y)
""" .

"DESCRIPTION.The code performs KMeans clustering on a dataset X with 2 clusters initialized using the kmeans++ algorithm." <EXPLAINS> """CODE.from sklearn.cluster import kmeans_plusplus
import numpy as np
X = np.array([[1, 2], [1, 4], [1, 0],
              [10, 2], [10, 4], [10, 0]])
centers, indices = kmeans_plusplus(X, n_clusters=2, random_state=0)""" .

"DESCRIPTION.The code performs LU (Lower-Upper) factorization on a randomly generated 3x3 matrix using PyTorch. It then checks if the LU factorization succeeded for all samples and prints a success message if so." <EXPLAINS> """CODE.A = torch.randn(2, 3, 3)
    A_LU, pivots = torch.lu(A)
    A_LU
    tensor([[[ 1.3506,  2.5558, -0.0816],
             [ 0.1684,  1.1551,  0.1940],
             [ 0.1193,  0.6189, -0.5497]],

            [[ 0.4526,  1.2526, -0.3285],
             [-0.7988,  0.7175, -0.9701],
             [ 0.2634, -0.9255, -0.3459]]])
    pivots
    tensor([[ 3,  3,  3],
            [ 3,  3,  3]], dtype=torch.int32)
    A_LU, pivots, info = torch.lu(A, get_infos=True)
    if info.nonzero().size(0) == 0:
    ...   print('LU factorization succeeded for all samples!')""" .

"DESCRIPTION.The code performs LU decomposition on a randomly generated 2x3x3 tensor A, then unpacks the LU factors into permutation matrix P, lower triangular matrix A_L, and upper triangular matrix A_U. Finally, it reconstructs matrix A using P, A_L, and A_U." <EXPLAINS> """CODE.A = torch.randn(2, 3, 3)
A_LU, pivots = A.lu()
P, A_L, A_U = torch.lu_unpack(A_LU, pivots)

# can recover A from factorization
A_ = torch.bmm(P, torch.bmm(A_L, A_U))
""" .

"DESCRIPTION.The code performs LU decomposition on a randomly generated 2x3x3 tensor using the torch.lu() function in PyTorch. It then checks if the LU factorization succeeded for all samples and prints a success message if it did." <EXPLAINS> """CODE.A = torch.randn(2, 3, 3)
A_LU, pivots = torch.lu(A)
A_LU
tensor([[[ 1.3506,  2.5558, -0.0816],
         [ 0.1684,  1.1551,  0.1940],
         [ 0.1193,  0.6189, -0.5497]],

        [[ 0.4526,  1.2526, -0.3285],
         [-0.7988,  0.7175, -0.9701],
         [ 0.2634, -0.9255, -0.3459]]])
pivots
tensor([[ 3,  3,  3],
        [ 3,  3,  3]], dtype=torch.int32)
A_LU, pivots, info = torch.lu(A, get_infos=True)
if info.nonzero().size(0) == 0:
    print('LU factorization succeeded for all samples!')
""" .

"DESCRIPTION.The code performs Leave-One-Group-Out cross-validation, where the data is split into training and testing sets based on specified group labels. It splits the data into two sets, with samples from group 1 in one set and samples from group 2 in the other set." <EXPLAINS> """CODE.from sklearn.model_selection import LeaveOneGroupOut
X = np.array([[1, 2], [3, 4], [5, 6], [7, 8])
y = np.array([1, 2, 1, 2])
groups = np.array([1, 1, 2, 2])
lol = LeaveOneGroupOut()
lol.get_n_splits(X, y, groups)
2
for train_index, test_index in lol.split(X, y, groups):
...    X_train, X_test = X[train_index], X[test_index]
...    y_train, y_test = y[train_index], y[test_index]
...    print(X_train, X_test, y_train, y_test)""" .

"DESCRIPTION.The code performs Leave-P-Groups-Out cross-validation, where each test set contains P groups and the training set contains all groups except the test groups. It splits the input data X and labels y based on the group information provided and prints the training and test set indices along with the corresponding data points for each split." <EXPLAINS> """CODE.from sklearn.model_selection import LeavePGroupsOut
X = np.array([[1, 2], [3, 4], [5, 6])
y = np.array([1, 2, 1])
groups = np.array([1, 2, 3])
lpl = LeavePGroupsOut(n_groups=2)
lpl.get_n_splits(X, y, groups)
3
for train_index, test_index in lpl.split(X, y, groups):
...    print("TRAIN:", train_index, "TEST:", test_index)
...    X_train, X_test = X[train_index], X[test_index]
...    y_train, y_test = y[train_index], y[test_index]
...    print(X_train, X_test, y_train, y_test)
TRAIN: [2] TEST: [0 1]
[[5 6]] [[1 2]
 [3 4]] [1] [1 2]
TRAIN: [1] TEST: [0 2]
[[3 4]] [[1 2]
 [5 6]] [2] [1 1]
TRAIN: [0] TEST: [1 2]
[[1 2]] [[3 4]
 [5 6]] [1] [2 1]""" .

"DESCRIPTION.The code performs a distributed \"all_to_all\" operation, where each rank splits its input into chunks according to the specified input_splits, sends each chunk to a different rank, and gathers the corresponding chunks from other ranks to form the final output. The output tensor at each rank contains the data received from other ranks in a specific order." <EXPLAINS> """CODE.input = torch.arange(4) + rank * 4
input = list(input.chunk(4))
input
[tensor([0]), tensor([1]), tensor([2]), tensor([3])]     # Rank 0
[tensor([4]), tensor([5]), tensor([6]), tensor([7])]     # Rank 1
[tensor([8]), tensor([9]), tensor([10]), tensor([11])]   # Rank 2
[tensor([12]), tensor([13]), tensor([14]), tensor([15])] # Rank 3
output = list(torch.empty([4], dtype=torch.int64).chunk(4))
dist.all_to_all(output, input)
output
[tensor([0]), tensor([4]), tensor([8]), tensor([12])]    # Rank 0
[tensor([1]), tensor([5]), tensor([9]), tensor([13])]    # Rank 1
[tensor([2]), tensor([6]), tensor([10]), tensor([14])]   # Rank 2
[tensor([3]), tensor([7]), tensor([11]), tensor([15])]   # Rank 3

input
tensor([0, 1, 2, 3, 4, 5])                                       # Rank 0
tensor([10, 11, 12, 13, 14, 15, 16, 17, 18])                     # Rank 1
tensor([20, 21, 22, 23, 24])                                     # Rank 2
tensor([30, 31, 32, 33, 34, 35, 36])                             # Rank 3
input_splits
[2, 2, 1, 1]                                                     # Rank 0
[3, 2, 2, 2]                                                     # Rank 1
[2, 1, 1, 1]                                                     # Rank 2
[2, 2, 2, 1]                                                     # Rank 3
output_splits
[2, 3, 2, 2]                                                     # Rank 0
[2, 2, 1, 2]                                                     # Rank 1
[1, 2, 1, 2]                                                     # Rank 2
[1, 2, 1, 1]                                                     # Rank 3
input = list(input.split(input_splits))
input
[tensor([0, 1]), tensor([2, 3]), tensor([4]), tensor([5])]                   # Rank 0
[tensor([10, 11, 12]), tensor([13, 14]), tensor([15, 16]), tensor([17, 18])] # Rank 1
[tensor([20, 21]), tensor([22]), tensor([23]), tensor([24])]                 # Rank 2
[tensor([30, 31]), tensor([32, 33]), tensor([34, 35]), tensor([36])]         # Rank 3
output = ...
dist.all_to_all(output, input)
output
[tensor([0, 1]), tensor([10, 11, 12]), tensor([20, 21]), tensor([30, 31])]   # Rank 0
[tensor([2, 3]), tensor([13, 14]), tensor([22]), tensor([32, 33])]           # Rank 1
[tensor([4]), tensor([15, 16]), tensor([23]), tensor([34, 35])]              # Rank 2
[tensor([5]), tensor([17, 18]), tensor([24]), tensor([36])]                  # Rank 3""" .

"DESCRIPTION.The code performs a flat mapping operation on the elements of a dataset 'a' and creates a new dataset by slicing each element into individual tensors." <EXPLAINS> """CODE.a.flat_map(lambda x: Dataset.from_tensor_slices(x)) ==
  {[1,2,3,4,5,6,7,8,9,10]}
""" .

"DESCRIPTION.The code performs a function called `dense_to_sparse_batch` which converts dense batch data into sparse representation. The function takes a `batch_size` parameter and a `row_shape` parameter. The output of the function is a dictionary of two sparse representations, each containing a list of indices, values, and dense shapes." <EXPLAINS> """CODE.a.dense_to_sparse_batch(batch_size=2, row_shape=[6]) == {
    ([[0, 0], [0, 1], [0, 2], [1, 0], [1, 1]],  # indices
     ['a', 'b', 'c', 'a', 'b'],                 # values
     [2, 6]),                                   # dense_shape
    ([[2, 0], [2, 1], [2, 2], [2, 3]],
     ['a', 'b', 'c', 'd'],
     [1, 6])
}
""" .

"DESCRIPTION.The code performs a grid search using GridSearchCV to find the best hyperparameters for a Linear Support Vector Classifier (LinearSVC) model. It sets up a grid search with a parameter grid {'C': [-1, -2]} and an error_score of 0. Then, it attempts to fit the model to the data [[1, 2], [3, 4], [5, 6], [7, 8], [8, 9]] with corresponding labels [0, 0, 0, 1, 1]. In case of a ValueError being raised due to the parameter C being less than 0, the code catches the exception, prints the error message, and continues." <EXPLAINS> """CODE.from sklearn.model_selection import GridSearchCV
from sklearn.svm import LinearSVC
from sklearn.exceptions import FitFailedWarning
import warnings
warnings.simplefilter('always', FitFailedWarning)
gs = GridSearchCV(LinearSVC(), {'C': [-1, -2]}, error_score=0)
X, y = [[1, 2], [3, 4], [5, 6], [7, 8], [8, 9]], [0, 0, 0, 1, 1]
with warnings.catch_warnings(record=True) as w:
    try:
        gs.fit(X, y)   # This will raise a ValueError since C is < 0
    except ValueError:
        pass
    print(repr(w[-1].message))
""" .

"DESCRIPTION.The code performs a groupby operation on the DataFrame 's' based on the first level of the index and checks if each group is in a monotonic decreasing order." <EXPLAINS> "CODE.s.groupby(level=0).is_monotonic_decreasing" .

"DESCRIPTION.The code performs a mapping operation between the values in dataframe x and dataframe y based on a common key, where x values are mapped to y values." <EXPLAINS> """CODE.x
one   1
two   2
three 3

y
1  foo
2  bar
3  baz

x.map(y)""" .

"DESCRIPTION.The code performs a neural network training using local reparameterization for dense layers, softmax cross entropy loss function, and Adam optimizer." <EXPLAINS> """CODE.tfp = tf.contrib.bayesflow

net = tfp.layers.dense_local_reparameterization(
    features, 512, activation=tf.nn.relu)
logits = tfp.layers.dense_local_reparameterization(net, 10)
neg_log_likelihood = tf.nn.softmax_cross_entropy_with_logits(
    labels=labels, logits=logits)
kl = sum(tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES))
loss = neg_log_likelihood + kl
train_op = tf.train.AdamOptimizer().minimize(loss)
""" .

"DESCRIPTION.The code performs a string operation to strip off any HTML tags present in the input text." <EXPLAINS> """CODE.Markup("Main &raquo;  <em>About</em>").striptags()
u'Main \\xbb About'""" .

"DESCRIPTION.The code performs a zip operation on multiple datasets to combine elements from each dataset into tuples. The zip operation pairs corresponding elements from the input datasets together." <EXPLAINS> """CODE.a = { 1, 2, 3 }
b = { 4, 5, 6 }
c = { (7, 8), (9, 10), (11, 12) }
d = { 13, 14 }

Dataset.zip((a, b)) == { (1, 4), (2, 5), (3, 6) }
Dataset.zip((b, a)) == { (4, 1), (5, 2), (6, 3) }

Dataset.zip((a, b, c) == { (1, 4, (7, 8)),
                           (2, 5, (9, 10)),
                           (3, 6, (11, 12)) }

Dataset.zip((a, d)) == { (1, 13), (2, 14) }
""" .

"DESCRIPTION.The code performs addition of two floating point numbers using a custom FloatFunctional object." <EXPLAINS> """CODE.f_add = FloatFunctional()
a = torch.tensor(3.0)
b = torch.tensor(4.0)
f_add.add(a, b)  # Equivalent to ``torch.add(3, 4)
""" .

"DESCRIPTION.The code performs an in-place addition operation by adding the value of 6 to the elements of the array x located at the specified index range [2:4, 3:] using JAX indexing operations." <EXPLAINS> "CODE.jax.ops.index_add(x, jnp.index_exp[2:4, 3:], 6.)" .

"DESCRIPTION.The code performs an upsampling operation on a 4D array `x` using the `UpSampling2D` layer from TensorFlow's Keras API. The `size` parameter of the layer is set to (1, 2), resulting in each element in the input array being repeated twice vertically. The final output `y` is a 4D tensor with the specified upsampling applied." <EXPLAINS> """CODE.input_shape = (2, 2, 1, 3)
x = np.arange(np.prod(input_shape)).reshape(input_shape)
print(x)
[[[[ 0  1  2]]
  [[ 3  4  5]]]
 [[[ 6  7  8]]
  [[ 9 10 11]]]]
y = tf.keras.layers.UpSampling2D(size=(1, 2))(x)
print(y)
tf.Tensor(
  [[[[ 0  1  2]
     [ 0  1  2]]
    [[ 3  4  5]
     [ 3  4  5]]]
   [[[ 6  7  8]
     [ 6  7  8]]
    [[ 9 10 11]
     [ 9 10 11]]]], shape=(2, 2, 2, 3), dtype=int64)""" .

"DESCRIPTION.The code performs associative scanning on a provided function and input array to compute partial sums or products of the numbers or matrices respectively." <EXPLAINS> """CODE.# Example 1: Partials sums of numbers.

np.associative_scan(operator.add, np.arange(0, 4))
# ==> [ 0, 1, 3, 6]

# Example 2: Partial products of random matrices.

np.associative_scan(np.matmul, matrices)
""" .

"DESCRIPTION.The code performs authentication using a username and password, adds the authentication object to the requests module, and then sends a GET request to verify the account information on the Convore API." <EXPLAINS> """CODE.c_auth = requests.AuthObject('kennethreitz', 'xxxxxxx')
requests.add_autoauth('https://convore.com/api/', c_auth)
r = requests.get('https://convore.com/api/account/verify.json')
""" .

"DESCRIPTION.The code performs average pooling on a 1D input tensor with different configurations of pool size, strides, and padding. The output tensor shape varies based on these configurations." <EXPLAINS> """CODE.x = tf.constant([1., 2., 3., 4., 5.])
x = tf.reshape(x, [1, 5, 1])
avg_pool_1d = tf.keras.layers.AveragePooling1D(pool_size=2,
...    strides=1, padding='valid')
avg_pool_1d(x)
<tf.Tensor: shape=(1, 4, 1), dtype=float32, numpy=
array([[[1.5],
        [2.5],
        [3.5],
        [4.5]]], dtype=float32)>
x = tf.constant([1., 2., 3., 4., 5.])
x = tf.reshape(x, [1, 5, 1])
avg_pool_1d = tf.keras.layers.AveragePooling1D(pool_size=2,
...    strides=2, padding='valid')
avg_pool_1d(x)
<tf.Tensor: shape=(1, 2, 1), dtype=float32, numpy=
array([[[1.5],
        [3.5]]], dtype=float32)>
x = tf.constant([1., 2., 3., 4., 5.])
x = tf.reshape(x, [1, 5, 1])
avg_pool_1d = tf.keras.layers.AveragePooling1D(pool_size=2,
...    strides=1, padding='same')
avg_pool_1d(x)
<tf.Tensor: shape=(1, 5, 1), dtype=float32, numpy=
array([[[1.5],
        [2.5],
        [3.5],
        [4.5],
        [5.]]], dtype=float32)>""" .

"DESCRIPTION.The code performs average pooling on input tensors with different pool sizes, strides, and padding options. The AveragePooling2D layer is applied to the input tensors after reshaping them to specific shapes, resulting in output tensors with averaged values based on the specified pool size, strides, and padding settings." <EXPLAINS> """CODE.x = tf.constant([[1., 2., 3.],
...                  [4., 5., 6.],
...                  [7., 8., 9.]])
x = tf.reshape(x, [1, 3, 3, 1])
avg_pool_2d = tf.keras.layers.AveragePooling2D(pool_size=(2, 2),
...    strides=(1, 1), padding='valid')
avg_pool_2d(x)
<tf.Tensor: shape=(1, 2, 2, 1), dtype=float32, numpy=
  array([[[[3.],
           [4.]],
          [[6.],
           [7.]]]], dtype=float32)>
x = tf.constant([[1., 2., 3., 4.],
...                  [5., 6., 7., 8.],
...                  [9., 10., 11., 12.]])
x = tf.reshape(x, [1, 3, 4, 1])
avg_pool_2d = tf.keras.layers.AveragePooling2D(pool_size=(2, 2),
...    strides=(2, 2), padding='valid')
avg_pool_2d(x)
<tf.Tensor: shape=(1, 1, 2, 1), dtype=float32, numpy=
  array([[[[3.5],
           [5.5]]]], dtype=float32)>
x = tf.constant([[1., 2., 3.],
...                  [4., 5., 6.],
...                  [7., 8., 9.]])
x = tf.reshape(x, [1, 3, 3, 1])
avg_pool_2d = tf.keras.layers.AveragePooling2D(pool_size=(2, 2),
...    strides=(1, 1), padding='same')
avg_pool_2d(x)
<tf.Tensor: shape=(1, 3, 3, 1), dtype=float32, numpy=
  array([[[[3.],
           [4.],
           [4.5]],
          [[6.],
           [7.],
           [7.5]],
          [[7.5],
           [8.5],
           [9.]]]], dtype=float32)>""" .

"DESCRIPTION.The code performs average pooling operation on a 5-dimensional input tensor with a specified pool size of (3,3,3) resulting in an output tensor of shape (batch_size, 10, 10, 10, 3)." <EXPLAINS> """CODE.inputs = tf.keras.Input(shape=(depth, height, width, input_channels))
layer = tf.keras.layers.AveragePooling3D(pool_size=3)
outputs = layer(inputs)  # Shape: (batch_size, 10, 10, 10, 3)
""" .

"DESCRIPTION.The code performs average pooling operations on a tensor 'x' with different configurations of pool size, strides, and padding." <EXPLAINS> """CODE.x = tf.constant([1., 2., 3., 4., 5.])
x = tf.reshape(x, [1, 5, 1])
x
<tf.Tensor: shape=(1, 5, 1), dtype=float32, numpy=
  array([[[1.],
          [2.],
          [3.],
          [4.],
          [5.]], dtype=float32)>
avg_pool_1d = tf.keras.layers.AveragePooling1D(pool_size=2,
...    strides=1, padding='valid')
avg_pool_1d(x)
<tf.Tensor: shape=(1, 4, 1), dtype=float32, numpy=
array([[[1.5],
        [2.5],
        [3.5],
        [4.5]]], dtype=float32)>
x = tf.constant([1., 2., 3., 4., 5.])
x = tf.reshape(x, [1, 5, 1])
x
<tf.Tensor: shape=(1, 5, 1), dtype=float32, numpy=
  array([[[1.],
          [2.],
          [3.],
          [4.],
          [5.]], dtype=float32)>
avg_pool_1d = tf.keras.layers.AveragePooling1D(pool_size=2,
...    strides=2, padding='valid')
avg_pool_1d(x)
<tf.Tensor: shape=(1, 2, 1), dtype=float32, numpy=
array([[[1.5],
        [3.5]]], dtype=float32)>
x = tf.constant([1., 2., 3., 4., 5.])
x = tf.reshape(x, [1, 5, 1])
x
<tf.Tensor: shape=(1, 5, 1), dtype=float32, numpy=
  array([[[1.],
          [2.],
          [3.],
          [4.],
          [5.]], dtype=float32)>
avg_pool_1d = tf.keras.layers.AveragePooling1D(pool_size=2,
...    strides=1, padding='same')
avg_pool_1d(x)
<tf.Tensor: shape=(1, 5, 1), dtype=float32, numpy=
array([[[1.5],
        [2.5],
        [3.5],
        [4.5],
        [5.]]], dtype=float32)>)""" .

"""DESCRIPTION.The code performs batched matrix operations using PyTorch's torch.vmap function. It first calculates the dot product of two vectors using torch.dot function and then extends it to perform batched dot product for multiple pairs of vectors.

It also defines a simple linear model with activation function and applies this model to a batch of input examples. Finally, it computes the Jacobian matrix of the function y = f(x) with respect to x and provides a function get_vjp to compute the vector-Jacobian product. The Jacobian matrix is then calculated using torch.vmap function.""" <EXPLAINS> """CODE.torch.dot                            # [D], [D] -> []
batched_dot = torch.vmap(torch.dot)  # [N, D], [N, D] -> [N]
x, y = torch.randn(2, 5), torch.randn(2, 5)
batched_dot(x, y)

batch_size, feature_size = 3, 5
weights = torch.randn(feature_size, requires_grad=True)

def model(feature_vec):
    # Very simple linear model with activation
    return feature_vec.dot(weights).relu()

examples = torch.randn(batch_size, feature_size)
result = torch.vmap(model)(examples)

N = 5
f = lambda x: x ** 2
x = torch.randn(N, requires_grad=True)
y = f(x)
I_N = torch.eye(N)

jacobian_rows = [torch.autograd.grad(y, x, v, retain_graph=True)[0]
                 for v in I_N.unbind()]
jacobian = torch.stack(jacobian_rows)

def get_vjp(v):
    return torch.autograd.grad(y, x, v)
jacobian = torch.vmap(get_vjp)(I_N)""" .

"DESCRIPTION.The code performs broadcasting of matrices with batch dimensions to align their shapes for further computation." <EXPLAINS> """CODE.x = [[1, 2],
     [3, 4]]  # Shape [2, 2], no batch dims

y = [[[1]]]   # Shape [1, 1, 1], 1 batch dim of shape [1]

x_bc, y_bc = broadcast_matrix_batch_dims([x, y])

x_bc
==> [[[1, 2],
      [3, 4]]]  # Shape [1, 2, 2], 1 batch dim of shape [1].

y_bc
==> same as y


x = tf.random_normal(shape=(2, 3, 1, 4, 4))
y = tf.random_normal(shape=(1, 3, 2, 5, 5))
x_bc, y_bc = broadcast_matrix_batch_dims([x, y])

x_bc.shape
==> (2, 3, 2, 4, 4)

y_bc.shape
==> (2, 3, 2, 5, 5)
""" .

"DESCRIPTION.The code performs category crossing on two input lists by combining their elements." <EXPLAINS> """CODE.inp_1 = ['a', 'b', 'c']
inp_2 = ['d', 'e', 'f']
layer = tf.keras.layers.experimental.preprocessing.CategoryCrossing()
layer([inp_1, inp_2])

inp_1 = ['a', 'b', 'c']
inp_2 = ['d', 'e', 'f']
layer = tf.keras.layers.experimental.preprocessing.CategoryCrossing(
   separator='-')
layer([inp_1, inp_2])""" .

"DESCRIPTION.The code performs category crossing on two input lists, combining them into pairs using a specified separator." <EXPLAINS> """CODE.inp_1 = ['a', 'b', 'c']
inp_2 = ['d', 'e', 'f']
layer = tf.keras.layers.experimental.preprocessing.CategoryCrossing()
layer([inp_1, inp_2])

inp_1 = ['a', 'b', 'c']
inp_2 = ['d', 'e', 'f']
layer = tf.keras.layers.experimental.preprocessing.CategoryCrossing(
   separator='-')
layer([inp_1, inp_2])""" .

"DESCRIPTION.The code performs convolutional operation with a 1D kernel on randomly generated tensors of certain shapes. The Conv1D layer is applied with 32 filters, a kernel size of 3, and ReLU activation function. The code outputs the shape of the resulting tensor after convolution operation." <EXPLAINS> """CODE.input_shape = (4, 10, 128)
x = tf.random.normal(input_shape)
y = tf.keras.layers.Conv1D(
32, 3, activation='relu',input_shape=input_shape[1:])(x)
print(y.shape)

input_shape = (4, 7, 10, 128)
x = tf.random.normal(input_shape)
y = tf.keras.layers.Conv1D(
32, 3, activation='relu', input_shape=input_shape[2:])(x)
print(y.shape)
""" .

"DESCRIPTION.The code performs convolutional operations on randomly generated input tensors using different parameters such as activation functions, dilation rates, and padding modes, resulting in output tensors with specific shapes." <EXPLAINS> """CODE.input_shape = (4, 28, 28, 3)
x = tf.random.normal(input_shape)
y = tf.keras.layers.Conv2D(
2, 3, activation='relu', input_shape=input_shape[1:])(x)
print(y.shape)
(4, 26, 26, 2)

input_shape = (4, 28, 28, 3)
x = tf.random.normal(input_shape)
y = tf.keras.layers.Conv2D(
2, 3, activation='relu', dilation_rate=2, input_shape=input_shape[1:])(x)
print(y.shape)
(4, 24, 24, 2)

input_shape = (4, 28, 28, 3)
x = tf.random.normal(input_shape)
y = tf.keras.layers.Conv2D(
2, 3, activation='relu', padding="same", input_shape=input_shape[1:])(x)
print(y.shape)
(4, 28, 28, 2)

input_shape = (4, 7, 28, 28, 3)
x = tf.random.normal(input_shape)
y = tf.keras.layers.Conv2D(
2, 3, activation='relu', input_shape=input_shape[2:])(x)
print(y.shape)
(4, 7, 26, 26, 2)
""" .

"DESCRIPTION.The code performs convolutional operations using the Conv2D layer with varying parameters such as activation function, dilation rate, and padding. The output shape of the convolutional layers is printed for different input shapes and parameters." <EXPLAINS> """CODE.input_shape = (4, 28, 28, 3)
x = tf.random.normal(input_shape)
y = tf.keras.layers.Conv2D(
2, 3, activation='relu', input_shape=input_shape[1:])(x)
print(y.shape)
(4, 26, 26, 2)

input_shape = (4, 28, 28, 3)
x = tf.random.normal(input_shape)
y = tf.keras.layers.Conv2D(
2, 3, activation='relu', dilation_rate=2, input_shape=input_shape[1:])(x)
print(y.shape)
(4, 24, 24, 2)

input_shape = (4, 28, 28, 3)
x = tf.random.normal(input_shape)
y = tf.keras.layers.Conv2D(
2, 3, activation='relu', padding="same", input_shape=input_shape[1:])(x)
print(y.shape)
(4, 28, 28, 2)

input_shape = (4, 7, 28, 28, 3)
x = tf.random.normal(input_shape)
y = tf.keras.layers.Conv2D(
2, 3, activation='relu', input_shape=input_shape[2:])(x)
print(y.shape)
(4, 7, 26, 26, 2)
""" .

"DESCRIPTION.The code performs cross-validation on a dataset using the LabelKFold method from scikit-learn. It splits the dataset into k folds based on the labels provided, where k equals the number of folds specified. Each fold is then used as a training set and a test set for model evaluation." <EXPLAINS> """CODE.from sklearn.cross_validation import LabelKFold
X = np.array([[1, 2], [3, 4], [5, 6], [7, 8]])
y = np.array([1, 2, 3, 4])
labels = np.array([0, 0, 2, 2])
label_kfold = LabelKFold(labels, n_folds=2)
len(label_kfold)
2
print(label_kfold)
sklearn.cross_validation.LabelKFold(n_labels=4, n_folds=2)
for train_index, test_index in label_kfold:
...     print("TRAIN:", train_index, "TEST:", test_index)
...     X_train, X_test = X[train_index], X[test_index]
...     y_train, y_test = y[train_index], y[test_index]
...     print(X_train, X_test, y_train, y_test)""" .

"DESCRIPTION.The code performs cross-validation on the data using LabelKFold, splitting the data into training and testing sets based on the specified labels. It prints out the training and testing data for each fold." <EXPLAINS> """CODE.from sklearn.cross_validation import LabelKFold
import numpy as np

X = np.array([[1, 2], [3, 4], [5, 6], [7, 8])
y = np.array([1, 2, 3, 4])
labels = np.array([0, 0, 2, 2])
label_kfold = LabelKFold(labels, n_folds=2)

for train_index, test_index in label_kfold:
    X_train, X_test = X[train_index], X[test_index]
    y_train, y_test = y[train_index], y[test_index]
    print(X_train, X_test, y_train, y_test)""" .

"DESCRIPTION.The code performs data aggregation on a grouped dataset. The first block calculates the sum of grouped data. The second block calculates the sum, mean, and standard deviation of the grouped data. The third block calculates the result of the mean divided by standard deviation and the total sum of grouped data." <EXPLAINS> """CODE.grouped.aggregate(np.sum)
b    3.0
q    7.0

grouped.aggregate([np.sum, np.mean, np.std])
   mean  std  sum
b  1.5   0.5  3
q  3.5   0.5  7

grouped.agg({'result' : lambda x: x.mean() / x.std(),
...              'total' : np.sum})
   result  total
b  2.121   3
q  4.95    7""" .

"DESCRIPTION.The code performs data augmentation on image and mask data for training a model. It includes features like centering, normalization, rotation, shifting, and zooming of images. The model is trained using the augmented data with a specified number of epochs and steps per epoch." <EXPLAINS> """CODE.(x_train, y_train), (x_test, y_test) = cifar10.load_data()
y_train = utils.to_categorical(y_train, num_classes)
y_test = utils.to_categorical(y_test, num_classes)
datagen = ImageDataGenerator(
    featurewise_center=True,
    featurewise_std_normalization=True,
    rotation_range=20,
    width_shift_range=0.2,
    height_shift_range=0.2,
    horizontal_flip=True,
    validation_split=0.2)
datagen.fit(x_train)
model.fit(datagen.flow(x_train, y_train, batch_size=32, subset='training'), validation_data=datagen.flow(x_train, y_train, batch_size=8, subset='validation'), steps_per_epoch=len(x_train) / 32, epochs=epochs)
for e in range(epochs):
    print('Epoch', e)
    batches = 0
    for x_batch, y_batch in datagen.flow(x_train, y_train, batch_size=32):
        model.fit(x_batch, y_batch)
        batches += 1
        if batches >= len(x_train) / 32:
            break

train_datagen = ImageDataGenerator(
        rescale=1./255,
        shear_range=0.2,
        zoom_range=0.2,
        horizontal_flip=True)
test_datagen = ImageDataGenerator(rescale=1./255)
train_generator = train_datagen.flow_from_directory(
        'data/train',
        target_size=(150, 150),
        batch_size=32,
        class_mode='binary')
validation_generator = test_datagen.flow_from_directory(
        'data/validation',
        target_size=(150, 150),
        batch_size=32,
        class_mode='binary')
model.fit(
        train_generator,
        steps_per_epoch=2000,
        epochs=50,
        validation_data=validation_generator,
        validation_steps=800)

data_gen_args = dict(featurewise_center=True,
                     featurewise_std_normalization=True,
                     rotation_range=90,
                     width_shift_range=0.1,
                     height_shift_range=0.1,
                     zoom_range=0.2)
image_datagen = ImageDataGenerator(**data_gen_args)
mask_datagen = ImageDataGenerator(**data_gen_args)
seed = 1
image_datagen.fit(images, augment=True, seed=seed)
mask_datagen.fit(masks, augment=True, seed=seed)
image_generator = image_datagen.flow_from_directory(
    'data/images',
    class_mode=None,
    seed=seed)
mask_generator = mask_datagen.flow_from_directory(
    'data/masks',
    class_mode=None,
    seed=seed)
train_generator = zip(image_generator, mask_generator)
model.fit(
    train_generator,
    steps_per_epoch=2000,
    epochs=50)
""" .

"DESCRIPTION.The code performs data normalization using tf.keras.layers.experimental.preprocessing.Normalization. It adapts the normalization layer to the input data, maps the normalization layer over the input dataset, and recompiles the model before making predictions." <EXPLAINS> """CODE.layer = tf.keras.layers.experimental.preprocessing.Normalization(
...     axis=None)
layer.adapt([0, 2])
model = tf.keras.Sequential(layer)
model.predict([0, 1, 2])
array([-1.,  0.,  1.], dtype=float32)
layer.adapt([-1, 1])
model.compile() # This is needed to re-compile model.predict!
model.predict([0, 1, 2])
array([0., 1., 2.], dtype=float32)

layer = tf.keras.layers.experimental.preprocessing.Normalization(
...     axis=None)
layer.adapt([0, 2])
input_ds = tf.data.Dataset.range(3)
normalized_ds = input_ds.map(layer)
list(normalized_ds.as_numpy_iterator())
[array([-1.], dtype=float32),
 array([0.], dtype=float32),
 array([1.], dtype=float32)]
layer.adapt([-1, 1])
normalized_ds = input_ds.map(layer) # Re-map over the input dataset.
list(normalized_ds.as_numpy_iterator())
[array([0.], dtype=float32),
 array([1.], dtype=float32),
 array([2.], dtype=float32)""" .

"DESCRIPTION.The code performs data normalization using the Normalization class. The adapt method is used to calculate the mean and variance of the input data in order to normalize the input data. The normalized output is computed based on the mean and variance provided or calculated from the adapt_data." <EXPLAINS> """CODE.adapt_data = np.array([[1.], [2.], [3.], [4.], [5.]], dtype=np.float32)
input_data = np.array([[1.], [2.], [3.]], np.float32)
layer = Normalization()
layer.adapt(adapt_data)
layer(input_data)
<tf.Tensor: shape=(3, 1), dtype=float32, numpy=
array([[-1.4142135 ],
       [-0.70710677],
       [ 0.        ]], dtype=float32)>

input_data = np.array([[1.], [2.], [3.]], np.float32)
layer = Normalization(mean=3., variance=2.)
layer(input_data)
<tf.Tensor: shape=(3, 1), dtype=float32, numpy=
array([[-1.4142135 ],
       [-0.70710677],
       [ 0.        ]], dtype=float32)>""" .

"DESCRIPTION.The code performs data preprocessing tasks on the Iris dataset such as reading, iterating, and transforming the data into TensorFlow compatible format for machine learning models." <EXPLAINS> """CODE.import ray
ds = ray.data.read_csv(
    "s3://anonymous@air-example-data/iris.csv"
)
it = ds.iterator(); it
DataIterator(Dataset(
   num_blocks=1,
   num_rows=150,
   schema={
      sepal length (cm): double,
      sepal width (cm): double,
      petal length (cm): double,
      petal width (cm): double,
      target: int64
   }
))

it.to_tf(feature_columns="sepal length (cm)", label_columns="target")  # doctest: +SKIP
<_OptionsDataset element_spec=(TensorSpec(shape=(None,), dtype=tf.float64, name='sepal length (cm)'), TensorSpec(shape=(None,), dtype=tf.int64, name='target'))>

it.to_tf(["sepal length (cm)", "sepal width (cm)"], "target")  # doctest: +SKIP
<_OptionsDataset element_spec=({'sepal length (cm)': TensorSpec(shape=(None,), dtype=tf.float64, name='sepal length (cm)'), 'sepal width (cm)': TensorSpec(shape=(None,), dtype=tf.float64, name='sepal width (cm)')}, TensorSpec(shape=(None,), dtype=tf.int64, name='target'))>

from ray.data.preprocessors import Concatenator
preprocessor = Concatenator(output_column_name="features", exclude="target")
it = preprocessor.transform(ds).iterator()
it
DataIterator(Concatenator
+- Dataset(
      num_blocks=1,
      num_rows=150,
      schema={
         sepal length (cm): double,
         sepal width (cm): double,
         petal length (cm): double,
         petal width (cm): double,
         target: int64
      }
   ))
it.to_tf("features", "target")  # doctest: +SKIP
<_OptionsDataset element_spec=(TensorSpec(shape=(None, 4), dtype=tf.float64, name='features'), TensorSpec(shape=(None,), dtype=tf.int64, name='target'))>
""" .

"DESCRIPTION.The code performs deconvolutional operations on dummy input data using a Sequential model in Keras. The first deconvolution layer has an output shape of (None, 3, 14, 14) with valid border mode. The second deconvolution layer has an output shape of (None, 3, 25, 25) with a subsample of (2, 2) and valid border mode. Finally, the code prints the shape of the predictions generated by the deconvolution models." <EXPLAINS> """CODE.model = Sequential()
model.add(Deconvolution2D(3, 3, 3, output_shape=(None, 3, 14, 14),
                          border_mode='valid',
                          input_shape=(3, 12, 12)))
dummy_input = np.ones((32, 3, 12, 12))
preds = model.predict(dummy_input)
print(preds.shape)

model = Sequential()
model.add(Deconvolution2D(3, 3, 3, output_shape=(None, 3, 25, 25),
                          subsample=(2, 2),
                          border_mode='valid',
                          input_shape=(3, 12, 12)))
model.summary()
dummy_input = np.ones((32, 3, 12, 12))
preds = model.predict(dummy_input)
print(preds.shape)
""" .

"DESCRIPTION.The code performs distributed automatic differentiation using the torch library. It includes a forward pass, backward pass, and optimizer step within the context of distributed autograd." <EXPLAINS> """CODE.import torch.distributed.autograd as dist_autograd
with dist_autograd.context() as context_id:
     forward pass...
     backward pass...
     optimizer step...""" .

"DESCRIPTION.The code performs distributed matrix-vector dot product computation using JAX with specified axis resources and input axes mappings." <EXPLAINS> """CODE.devices = np.array(jax.devices())[:4].reshape((2, 2)
with mesh(devices, ('x', 'y')):
    distributed_out = xmap(
      jnp.vdot,
      in_axes=({0: 'left', 1: 'right'}),
      out_axes=['left', 'right', ...],
      axis_resources={'left': 'x', 'right': 'y'})(x, x.T)""" .

"DESCRIPTION.The code performs distributed model training using post-localSGD optimization technique. It initializes a model with distributed data parallelism, sets up post-localSGD communication hooks, creates a post-localSGD optimizer that wraps a local optimizer, and runs global model averaging every 4 steps after applying the local optimizer within each subgroup." <EXPLAINS> """CODE.import torch
import torch.distributed as dist
import torch.distributed.algorithms.model_averaging.averagers as averagers
import torch.nn as nn
from torch.distributed.optim import PostLocalSGDOptimizer

model = nn.parallel.DistributedDataParallel(
    module, device_ids=[rank], output_device=rank
)

# Register a post-localSGD communication hook.
subgroup, subgroups = dist.new_subgroups()
state = PostLocalSGDState(subgroup=subgroup, start_localSGD_iter=100)
model.register_comm_hook(state, post_localSGD_hook)

# Create a post-localSGD optimizer that wraps a local optimizer.
# Note that ``warmup_steps`` used in ``PostLocalSGDOptimizer`` must be the same as
# ``start_localSGD_iter`` used in ``PostLocalSGDState``.
local_optim = torch.optim.SGD(params=model.parameters(), lr=0.01)
opt = PostLocalSGDOptimizer(
    optim=local_optim,
    averager=averagers.PeriodicModelAverager(period=4, warmup_steps=100)
)

# In the first 100 steps, DDP runs global gradient averaging at every step.
# After 100 steps, DDP runs gradient averaging within each subgroup (intra-node by default),
# and post-localSGD optimizer runs global model averaging every 4 steps after applying the local optimizer.
for step in range(0, 20):
    opt.zero_grad()
    loss = loss_fn(output, labels)
    loss.backward()
    opt.step()""" .

"DESCRIPTION.The code performs diverse beam search using 6 beams to generate translations from English to German for the input sentence \"How old are you?\". It utilizes a T5 model pre-trained for sequence-to-sequence tasks and uses beam search along with specific decoder start token ids and encoder outputs to generate translations. The beam search process involves scoring beams and processing logits to ensure diversity and maintain a minimum length, resulting in the generation of translated sentences." <EXPLAINS> """CODE.from transformers import (
...    AutoTokenizer,
...    AutoModelForSeq2SeqLM,
...    LogitsProcessorList,
...    MinLengthLogitsProcessor,
...    HammingDiversityLogitsProcessor,
...    BeamSearchScorer,
... )
import torch

tokenizer = AutoTokenizer.from_pretrained("t5-base")
model = AutoModelForSeq2SeqLM.from_pretrained("t5-base")

encoder_input_str = "translate English to German: How old are you?"
encoder_input_ids = tokenizer(encoder_input_str, return_tensors="pt").input_ids


# lets run diverse beam search using 6 beams
num_beams = 6
# define decoder start token ids
input_ids = torch.ones((num_beams, 1), device=model.device, dtype=torch.long)
input_ids = input_ids * model.config.decoder_start_token_id

# add encoder_outputs to model keyword arguments
model_kwargs = {
...     "encoder_outputs": model.get_encoder()(encoder_input_ids.repeat_interleave(num_beams, dim=0), return_dict=True)
... }

# instantiate beam scorer
beam_scorer = BeamSearchScorer(
...     batch_size=1,
...     max_length=model.config.max_length,
...     num_beams=num_beams,
...     device=model.device,
...     num_beam_groups=3
... )

# instantiate logits processors
logits_processor = LogitsProcessorList([
...     HammingDiversityLogitsProcessor(5.5, num_beams=6, num_beam_groups=3),
...     MinLengthLogitsProcessor(5, eos_token_id=model.config.eos_token_id),
... ])

outputs = model.group_beam_search(input_ids, beam_scorer, logits_processor=logits_processor, **model_kwargs)

print("Generated:", tokenizer.batch_decode(outputs, skip_special_tokens=True))""" .

"DESCRIPTION.The code performs document question answering by providing an image of an invoice and asking for the invoice number. It returns a result containing the predicted invoice number, its score, and the positions of the start and end of the answer within the text." <EXPLAINS> """CODE.from huggingface_hub import InferenceClient
client = InferenceClient()
client.document_question_answering(image="https://huggingface.co/spaces/impira/docquery/resolve/2359223c1837a7587402bda0f2643382a6eefeab/invoice.png", question="What is the invoice number?")
[{'score': 0.42515629529953003, 'answer': 'us-001', 'start': 16, 'end': 16}]
""" .

"DESCRIPTION.The code performs dropout regularization with a dropout probability of 0.5 on the input tensor 'x' in both training and evaluation modes using the PaddlePaddle library." <EXPLAINS> """CODE.import paddle.fluid as fluid
from paddle.fluid.dygraph.base import to_variable
import numpy as np

x = np.random.random(size=(3, 10, 3, 7)).astype('float32')
with fluid.dygraph.guard():
    x = to_variable(x)
    m = fluid.dygraph.Dropout(p=0.5)
    droped_train = m(x)
    # switch to eval mode
    m.eval()
    droped_eval = m(x)""" .

"DESCRIPTION.The code performs dynamic quantization of a PyTorch model using per-channel quantization configuration. It first traces the model, defines a quantization configuration, and calibrates the model with a given data loader. Then it quantizes the model dynamically with the specified configuration and calibration function." <EXPLAINS> """CODE.import torch
from torch.quantization import per_channel_dynamic_qconfig
from torch.quantization import quantize_dynamic_fx

graph_module = torch._fx.symbolic_trace(float_model.eval())
qconfig = get_default_qconfig('fbgemm')
def calibrate(model, data_loader):
    model.eval()
    with torch.no_grad():
        for image, target in data_loader:
            model(image)

quantized_model = quantize_dynamic_fx(
    graph_module,
    {'': qconfig},
    calibrate,
    [data_loader_test])
""" .

"DESCRIPTION.The code performs element-wise clamping on a numpy array, setting values below 3.5 to 3.5 and values above 5.0 to 5.0 for the first clamp operation. For the second clamp operation, it sets values below 2.5 to 2.5." <EXPLAINS> """CODE.import paddle
import paddle.fluid as fluid
import numpy as np

in1 = np.array([[1.2,3.5],
                [4.5,6.4]]).astype('float32')
with fluid.dygraph.guard():
    x1 = fluid.dygraph.to_variable(in1)
    out1 = fluid.layers.clamp(x1, min=3.5, max=5.0)
    out2 = fluid.layers.clamp(x1, min=2.5)
    print(out1.numpy())
    # [[3.5, 3.5]
    # [4.5, 5.0]]
    print(out2.numpy())
    # [[2.5, 3.5]
    # [[4.5, 6.4]""" .

"DESCRIPTION.The code performs element-wise multiplication of tensor1 and tensor2, adds the result to the input tensor, and then scales the result by a constant value of 1.0." <EXPLAINS> """CODE.import paddle
import paddle.fluid as fluid
input = fluid.data(name='input', dtype='float32', shape=[3, 4])
tensor1 = fluid.data(name='tenosr1', dtype='float32', shape=[1, 4])
tensor2 = fluid.data(name='tensor2', dtype='float32', shape=[3, 4])
data = fluid.layers.addcmul(input, tensor1, tensor2, value=1.0)""" .

"DESCRIPTION.The code performs element-wise multiplication on a subset of elements in a 5x6 matrix x by multiplying them with the scalar value 6." <EXPLAINS> """CODE.x = jax.numpy.ones((5, 6))
jax.ops.index_mul(x, jax.ops.index[2:4, 3:], 6.)""" .

"DESCRIPTION.The code performs ensemble learning using a Voting Classifier with different base classifiers (Logistic Regression, Random Forest, and Naive Bayes). It uses both hard and soft voting strategies to make predictions based on the votes from individual classifiers. The weights can also be assigned to different classifiers when using the soft voting strategy." <EXPLAINS> """CODE.import numpy as np
from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import GaussianNB
from sklearn.ensemble import RandomForestClassifier
clf1 = LogisticRegression(random_state=1)
clf2 = RandomForestClassifier(random_state=1)
clf3 = GaussianNB()
X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])
y = np.array([1, 1, 1, 2, 2, 2])
eclf1 = VotingClassifier(estimators=[
...         ('lr', clf1), ('rf', clf2), ('gnb', clf3)], voting='hard')
eclf1 = eclf1.fit(X, y)
print(eclf1.predict(X))
[1 1 1 2 2 2]
eclf2 = VotingClassifier(estimators=[
...         ('lr', clf1), ('rf', clf2), ('gnb', clf3)],
...         voting='soft')
eclf2 = eclf2.fit(X, y)
print(eclf2.predict(X))
[1 1 1 2 2 2]
eclf3 = VotingClassifier(estimators=[
...        ('lr', clf1), ('rf', clf2), ('gnb', clf3)],
...        voting='soft', weights=[2,1,1])
eclf3 = eclf3.fit(X, y)
print(eclf3.predict(X))
[1 1 1 2 2 2]""" .

"DESCRIPTION.The code performs file system operations such as listing files, reading/writing files using a custom file system module called HfFileSystem." <EXPLAINS> """CODE.import hffs

fs = hffs.HfFileSystem()

# List files
fs.glob("my-username/my-model/*.bin")
fs.ls("datasets/my-username/my-dataset", detail=False)

# Read/write files
with fs.open("my-username/my-model/pytorch_model.bin") as f:
    data = f.read()
with fs.open("my-username/my-model/pytorch_model.bin", "wb") as f:
    f.write(data)
""" .

"DESCRIPTION.The code performs forward and inverse transformation using a square bijector on the input data." <EXPLAINS> """CODE.bijector.Square().forward(x=[[1., 0], [2, 1]])
bijector.Square().inverse(y=[[1., 4], [9, 1]])
""" .

"DESCRIPTION.The code performs forward and inverse transformations using an ordered bijector on input arrays." <EXPLAINS> """CODE.bijector.Ordered().forward([2, 3, 4])
# Result: [2., 0., 0.]

bijector.Ordered().inverse([0.06428002, -1.07774478, -0.71530371])
# Result: [0.06428002, 0.40464228, 0.8936858]
""" .

"DESCRIPTION.The code performs grid search cross-validation to find the best hyperparameters for a Linear Support Vector Machine (LinearSVC) model using an fbeta_score with beta=2 as the scoring metric." <EXPLAINS> """CODE.from sklearn.metrics import fbeta_score, make_scorer
ftwo_scorer = make_scorer(fbeta_score, beta=2)
from sklearn.grid_search import GridSearchCV
from sklearn.svm import LinearSVC
grid = GridSearchCV(LinearSVC(), param_grid={'C': [1, 10]},
                    scoring=ftwo_scorer)""" .

"DESCRIPTION.The code performs grouping and calculates the minimum value within each group based on different criteria such as modulo operation and specific keys in a dictionary." <EXPLAINS> """CODE.ray.data.range(100).groupby(lambda x: x % 3).min()
ray.data.from_items([
    (i % 3, i, i**2)
    for i in range(100)]).groupby(lambda x: x[0] % 3).min(lambda x: x[2)
ray.data.range_arrow(100).groupby("value").min()
ray.data.from_items([
    {"A": i % 3, "B": i, "C": i**2}
    for i in range(100)]).groupby("A").min(["B", "C"])
""" .

"DESCRIPTION.The code performs grouping and summation operations on different generated datasets." <EXPLAINS> """CODE.ray.data.range(100).groupby(lambda x: x % 3).sum()
ray.data.from_items([
    (i % 3, i, i**2)
    for i in range(100)]).groupby(lambda x: x[0] % 3).sum(lambda x: x[2)
ray.data.range_arrow(100).groupby("value").sum()
ray.data.from_items([
    {"A": i % 3, "B": i, "C": i**2}
    for i in range(100)]).groupby("A").sum(["B", "C"])
""" .

"DESCRIPTION.The code performs grouping operations on data and calculates the maximum value within each group based on different criteria." <EXPLAINS> """CODE.ray.data.range(100).groupby(lambda x: x % 3).max()
ray.data.from_items([
    (i % 3, i, i**2)
    for i in range(100)])
    .groupby(lambda x: x[0] % 3)
    .max(lambda x: x[2])
ray.data.range_arrow(100).groupby("value").max()
ray.data.from_items([
    {"A": i % 3, "B": i, "C": i**2}
    for i in range(100)])
    .groupby("A")
    .max(["B", "C"])
""" .

"DESCRIPTION.The code performs hashing of input data using a specified number of bins. It assigns each input value a corresponding bin number based on the hashing function." <EXPLAINS> """CODE.  layer = Hashing(num_bins=3)
  inp = np.asarray([['A'], ['B'], ['C'], ['D'], ['E']])
  layer(inputs)
  [[1], [0], [1], [1], [2]]


  layer = Hashing(num_bins=3, salt=[133, 137])
  inp = np.asarray([['A'], ['B'], ['C'], ['D'], ['E']])
  layer(inputs)
  [[1], [2], [1], [0], [2]]
""" .

"DESCRIPTION.The code performs hashing on input data using a specified number of bins for each example provided in the input list. The hashing can be affected by providing a salt value to the process." <EXPLAINS> """CODE.layer = tf.keras.layers.experimental.preprocessing.Hashing(num_bins=3)
inp = [['A'], ['B'], ['C'], ['D'], ['E']]
layer(inp)


layer = tf.keras.layers.experimental.preprocessing.Hashing(num_bins=3, mask_value='')
inp = [['A'], ['B'], [''], ['C'], ['D']]
layer(inp)


layer = tf.keras.layers.experimental.preprocessing.Hashing(num_bins=3, salt=[133, 137])
inp = [['A'], ['B'], ['C'], ['D'], ['E']]
layer(inp)


layer = tf.keras.layers.experimental.preprocessing.Hashing(num_bins=3, salt=133)
inp = [['A'], ['B'], ['C'], ['D'], ['E']]
layer(inp)
""" .

"DESCRIPTION.The code performs image classification using a pre-trained model. It takes an image as input and returns a list of dictionaries containing the predicted labels and their corresponding scores for that image." <EXPLAINS> """CODE.from huggingface_hub import InferenceClient
client = InferenceClient()
client.image_classification("https://upload.wikimedia.org/wikipedia/commons/thumb/4/43/Cute_dog.jpg/320px-Cute_dog.jpg")
[{'score': 0.9779096841812134, 'label': 'Blenheim spaniel'}, ...]
""" .

"DESCRIPTION.The code performs imputation by replacing missing values with the mean of the column in a 2D array." <EXPLAINS> """CODE.import numpy as np
from sklearn.impute import SimpleImputer
imp_mean = SimpleImputer(missing_values=np.nan, strategy='mean')
imp_mean.fit([[7, 2, 3], [4, np.nan, 6], [10, 5, 9])
X = [[np.nan, 2, 3], [4, np.nan, 6], [10, np.nan, 9]]
print(imp_mean.transform(X))""" .

"DESCRIPTION.The code performs incremental PCA (Principal Component Analysis) on the given dataset X, with a batch size of 3 and 2 components. It fits the incremental PCA model to the data and transforms the data into the new feature space." <EXPLAINS> """CODE.import numpy as np
from sklearn.decomposition import IncrementalPCA
X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])
ipca = IncrementalPCA(n_components=2, batch_size=3)
ipca.fit(X)
ipca.transform(X) # doctest: +SKIP""" .

"DESCRIPTION.The code performs incremental PCA (Principal Component Analysis) on the input data X, allowing for processing in batches instead of all at once." <EXPLAINS> """CODE.import numpy as np
from sklearn.decomposition import IncrementalPCA
X = np.array([[-1, -1], [-2, -1], [-3, -2],
              [1, 1], [2, 1], [3, 2]])
ipca = IncrementalPCA(n_components=2, batch_size=3)
ipca.fit(X)
ipca.transform(X) # doctest: +SKIP""" .

"DESCRIPTION.The code performs inference using different pre-trained models for natural language processing tasks such as masked word prediction, question answering, and text classification with candidate labels." <EXPLAINS> """CODE.from huggingface_hub.inference_api import InferenceApi
api = InferenceApi("bert-base-uncased")
api(inputs="The goal of life is [MASK].")
api = InferenceApi("deepset/roberta-base-squad2")
inputs = {"question":"What's my name?", "context":"My name is Clara and I live in Berkeley."}
api(inputs)
api = InferenceApi("typeform/distilbert-base-uncased-mnli")
inputs = "Hi, I recently bought a device from your company but it is not working as advertised and I would like to get reimbursed!"
params = {"candidate_labels":["refund", "legal", "faq"]}
api(inputs, params)
api = InferenceApi("bert-base-uncased", task="feature-extraction")""" .

"DESCRIPTION.The code performs instance normalization, layer normalization, and group normalization on the input tensor x, using the Flax library in Python. Instance normalization normalizes the features across each individual sample in the batch. Layer normalization normalizes the features across specified reduction axes. Group normalization normalizes the features across specified groups. The code then asserts that the output tensors y, y2, and y3 are approximately equal within a tolerance of 1e-7." <EXPLAINS> """CODE.import flax.linen as nn
import jax
import numpy as np

x = jax.random.normal(jax.random.key(0), (2, 3, 4, 5))
layer = nn.InstanceNorm()
variables = layer.init(jax.random.key(1), x)
y = layer.apply(variables, x)

y2 = nn.LayerNorm(reduction_axes=[1, 2], feature_axes=-1).apply(variables, x)
np.testing.assert_allclose(y, y2, atol=1e-7)
y3 = nn.GroupNorm(num_groups=x.shape[-1]).apply(variables, x)
np.testing.assert_allclose(y, y3, atol=1e-7)""" .

"DESCRIPTION.The code performs label spreading for semi-supervised learning on the Iris dataset." <EXPLAINS> """CODE.from sklearn import datasets
from sklearn.semi_supervised import LabelSpreading
label_prop_model = LabelSpreading()
iris = datasets.load_iris()
random_unlabeled_points = np.where(np.random.random_integers(0, 1,
...    size=len(iris.target)))
labels = np.copy(iris.target)
labels[random_unlabeled_points] = -1
label_prop_model.fit(iris.data, labels)""" .

"DESCRIPTION.The code performs language modeling from scratch using the GPT-2 model on the WikiText dataset. It tokenizes the text data, groups the texts into chunks based on a specified block size, and then trains the model using a distributed training approach with Ray and Hugging Face Trainer." <EXPLAINS> """CODE.# Based on
# huggingface/notebooks/examples/language_modeling_from_scratch.ipynb

# Hugging Face imports
from datasets import load_dataset
import transformers
from transformers import AutoConfig, AutoModelForCausalLM, AutoTokenizer

import ray
from ray.ml.train.integrations.huggingface import HuggingFaceTrainer

model_checkpoint = "gpt2"
tokenizer_checkpoint = "sgugger/gpt2-like-tokenizer"
block_size = 128

datasets = load_dataset("wikitext", "wikitext-2-raw-v1")
tokenizer = AutoTokenizer.from_pretrained(tokenizer_checkpoint)

def tokenize_function(examples):
    return tokenizer(examples["text"])

tokenized_datasets = datasets.map(
    tokenize_function, batched=True, num_proc=1, remove_columns=["text"]
)

def group_texts(examples):
    # Concatenate all texts.
    concatenated_examples = {
        k: sum(examples[k], []) for k in examples.keys()
    }
    total_length = len(concatenated_examples[list(examples.keys())[0]])
    # We drop the small remainder, we could add padding if the model
    # supported it.
    # instead of this drop, you can customize this part to your needs.
    total_length = (total_length // block_size) * block_size
    # Split by chunks of max_len.
    result = {
        k: [
            t[i : i + block_size]
            for i in range(0, total_length, block_size)
        ]
        for k, t in concatenated_examples.items()
    }
    result["labels"] = result["input_ids"].copy()
    return result

lm_datasets = tokenized_datasets.map(
    group_texts,
    batched=True,
    batch_size=1000,
    num_proc=1,
)
ray_train_ds = ray.data.from_huggingface(lm_datasets["train"])
ray_evaluation_ds = ray.data.from_huggingface(
    lm_datasets["evaluation"]
)

def trainer_init_per_worker(train_dataset, eval_dataset, **config):
    model_config = AutoConfig.from_pretrained(model_checkpoint)
    model = AutoModelForCausalLM.from_config(model_config)
    args = transformers.TrainingArguments(
        output_dir=f"{model_checkpoint}-wikitext2",
        evaluation_strategy="epoch",
        learning_rate=2e-5,
        weight_decay=0.01,
    )
    return transformers.Trainer(
        model=model,
        args=args,
        train_dataset=train_dataset,
        eval_dataset=eval_dataset,
    )

scaling_config = {"num_workers": 3}
# If using GPUs, use the below scaling config instead.
# scaling_config = {"num_workers": 3, "use_gpu": True}
trainer = HuggingFaceTrainer(
    trainer_init_per_worker=trainer_init_per_worker,
    scaling_config=scaling_config,
    datasets={"train": ray_train_ds, "evaluation": ray_evaluation_ds},
)
result = trainer.fit()
""" .

"DESCRIPTION.The code performs layer normalization on the input data x using the nn.LayerNorm function and then calculates the mean of the normalized data using torch.mean." <EXPLAINS> """CODE.AutoformerLayernorm(x) = nn.LayerNorm(x)
torch.mean(nn.LayerNorm(x))""" .

"DESCRIPTION.The code performs logical AND operation between two boolean tensors." <EXPLAINS> """CODE.a = tf.constant([True])
b = tf.constant([False])
tf.math.logical_and(a, b)


c = tf.constant([True])
x = tf.constant([False, True, True, False])
tf.math.logical_and(c, x)


y = tf.constant([False, False, True, True])
z = tf.constant([False, True, False, True])
tf.math.logical_and(y, z)
""" .

"DESCRIPTION.The code performs max pooling on a 1D tensor with varying pool sizes, strides, and padding modes. Max pooling reduces the size of the input tensor by only keeping the maximum value in each window of the specified size." <EXPLAINS> """CODE.x = tf.constant([1., 2., 3., 4., 5.])
x = tf.reshape(x, [1, 5, 1])
max_pool_1d = tf.keras.layers.MaxPooling1D(pool_size=2,
...    strides=1, padding='valid')
max_pool_1d(x)
<tf.Tensor: shape=(1, 4, 1), dtype=float32, numpy=
array([[[2.],
        [3.],
        [4.],
        [5.]]], dtype=float32)>
x = tf.constant([1., 2., 3., 4., 5.])
x = tf.reshape(x, [1, 5, 1])
max_pool_1d = tf.keras.layers.MaxPooling1D(pool_size=2,
...    strides=2, padding='valid')
max_pool_1d(x)
<tf.Tensor: shape=(1, 2, 1), dtype=float32, numpy=
array([[[2.],
        [4.]]], dtype=float32)>
x = tf.constant([1., 2., 3., 4., 5.])
x = tf.reshape(x, [1, 5, 1])
max_pool_1d = tf.keras.layers.MaxPooling1D(pool_size=2,
...    strides=1, padding='same')
max_pool_1d(x)
<tf.Tensor: shape=(1, 5, 1), dtype=float32, numpy=
array([[[2.],
        [3.],
        [4.],
        [5.],
        [5.]]], dtype=float32)>""" .

"DESCRIPTION.The code performs max pooling on input tensors using different pool sizes, strides, and padding methods. Max pooling is a type of pooling layer used in convolutional neural networks to reduce the spatial dimensions of the input tensor while retaining the most important information." <EXPLAINS> """CODE.x = tf.constant([[1., 2., 3.],
...                  [4., 5., 6.],
...                  [7., 8., 9.]])
x = tf.reshape(x, [1, 3, 3, 1])
max_pool_2d = tf.keras.layers.MaxPooling2D(pool_size=(2, 2),
...    strides=(1, 1), padding='valid')
max_pool_2d(x)
<tf.Tensor: shape=(1, 2, 2, 1), dtype=float32, numpy=
  array([[[[5.],
           [6.]],
          [[8.],
           [9.]]]], dtype=float32)>
x = tf.constant([[1., 2., 3., 4.],
...                  [5., 6., 7., 8.],
...                  [9., 10., 11., 12.]])
x = tf.reshape(x, [1, 3, 4, 1])
max_pool_2d = tf.keras.layers.MaxPooling2D(pool_size=(2, 2),
...    strides=(2, 2), padding='valid')
max_pool_2d(x)
<tf.Tensor: shape=(1, 1, 2, 1), dtype=float32, numpy=
  array([[[[6.],
           [8.]]]], dtype=float32)>
input_image = tf.constant([[[[1.], [1.], [2.], [4.]],
...                            [[2.], [2.], [3.], [2.]],
...                            [[4.], [1.], [1.], [1.]],
...                            [[2.], [2.], [1.], [4.]]])
output = tf.constant([[[[1], [0]],
...                       [[0], [1]]]])
model = tf.keras.models.Sequential()
model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2),
...    input_shape=(4, 4, 1)))
model.compile('adam', 'mean_squared_error')
model.predict(input_image, steps=1)
array([[[[2.],
         [4.]],
        [[4.],
         [4.]]]], dtype=float32)>
x = tf.constant([[1., 2., 3.],
...                  [4., 5., 6.],
...                  [7., 8., 9.]])
x = tf.reshape(x, [1, 3, 3, 1])
max_pool_2d = tf.keras.layers.MaxPooling2D(pool_size=(2, 2),
...    strides=(1, 1), padding='same')
max_pool_2d(x)
<tf.Tensor: shape=(1, 3, 3, 1), dtype=float32, numpy=
  array([[[[5.],
           [6.],
           [6.]],
          [[8.],
           [9.],
           [9.]],
          [[8.],
           [9.],
           [9.]]]], dtype=float32)>""" .

"DESCRIPTION.The code performs max pooling operation on input matrices using TensorFlow's MaxPooling2D layer with various pool sizes, strides, and padding options. The max pooling operation reduces the spatial dimensions of the input matrices while retaining the maximum values in each pooling window, resulting in downsampled feature maps with reduced spatial resolution." <EXPLAINS> """CODE.x = tf.constant([[1., 2., 3.],
...                  [4., 5., 6.],
...                  [7., 8., 9.]])
x = tf.reshape(x, [1, 3, 3, 1])
max_pool_2d = tf.keras.layers.MaxPooling2D(pool_size=(2, 2),
...    strides=(1, 1), padding='valid')
max_pool_2d(x)
<tf.Tensor: shape=(1, 2, 2, 1), dtype=float32, numpy=
  array([[[[5.],
           [6.]],
          [[8.],
           [9.]]]], dtype=float32)>
x = tf.constant([[1., 2., 3., 4.],
...                  [5., 6., 7., 8.],
...                  [9., 10., 11., 12.]])
x = tf.reshape(x, [1, 3, 4, 1])
max_pool_2d = tf.keras.layers.MaxPooling2D(pool_size=(2, 2),
...    strides=(2, 2), padding='valid')
max_pool_2d(x)
<tf.Tensor: shape=(1, 1, 2, 1), dtype=float32, numpy=
  array([[[[6.],
           [8.]]]], dtype=float32)>
input_image = tf.constant([[[[1.], [1.], [2.], [4.]],
...                            [[2.], [2.], [3.], [2.]],
...                            [[4.], [1.], [1.], [1.]],
...                            [[2.], [2.], [1.], [4.]]])
output = tf.constant([[[[1], [0]],
...                       [[0], [1]]]])
model = tf.keras.models.Sequential()
model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2),
...    input_shape=(4, 4, 1)))
model.compile('adam', 'mean_squared_error')
model.predict(input_image, steps=1)
array([[[[2.],
         [4.]],
        [[4.],
         [4.]]]], dtype=float32)>
x = tf.constant([[1., 2., 3.],
...                  [4., 5., 6.],
...                  [7., 8., 9.]])
x = tf.reshape(x, [1, 3, 3, 1])
max_pool_2d = tf.keras.layers.MaxPooling2D(pool_size=(2, 2),
...    strides=(1, 1), padding='same')
max_pool_2d(x)
<tf.Tensor: shape=(1, 3, 3, 1), dtype=float32, numpy=
  array([[[[5.],
           [6.],
           [6.]],
          [[8.],
           [9.],
           [9.]],
          [[8.],
           [9.],
           [9.]]]], dtype=float32)>""" .

"DESCRIPTION.The code performs max pooling operation on the input data x with a pool size of 2 and strides of 2." <EXPLAINS> """CODE. y = tf.compat.v1.layers.max_pooling2d(x, pool_size=2, strides=2)

 x = tf.keras.Input((28, 28, 1))
 y = tf.keras.layers.MaxPooling2D(pool_size=2, strides=2)(x)
 model = tf.keras.Model(x, y)
""" .

"DESCRIPTION.The code performs model loading and feature extraction using the Wav2Vec2 model from fairseq, then compares the extracted features with the original model's output. It also includes encoding using the Wav2Vec2 model and compares the encoded output with the original model's encoder output." <EXPLAINS> """CODE.from torchaudio.models.wav2vec2.utils import import_fairseq_model

# Load model using fairseq
model_file = 'wav2vec_small.pt'
model, _, _ = fairseq.checkpoint_utils.load_model_ensemble_and_task([model_file])
original = model[0]
imported = import_fairseq_model(original, num_out=28)

# Perform feature extraction
waveform, _ = torchaudio.load('audio.wav')
features, _ = imported.extract_features(waveform)

# Compare result with the original model from fairseq
reference = original.feature_extractor(waveform).transpose(1, 2)
torch.testing.assert_allclose(features, reference)

from torchaudio.models.wav2vec2.utils import import_fairseq_model

# Load model using fairseq
model_file = 'wav2vec_small_960h.pt'
model, _, _ = fairseq.checkpoint_utils.load_model_ensemble_and_task([model_file])
original = model[0]
imported = import_fairseq_model(original.w2v_encoder)

# Perform encoding
waveform, _ = torchaudio.load('audio.wav')
emission, _ = imported(waveform)

# Compare result with the original model from fairseq
mask = torch.zeros_like(waveform)
reference = original(waveform, mask)['encoder_out'].transpose(0, 1)
torch.testing.assert_allclose(emission, reference)""" .

"DESCRIPTION.The code performs model training with specified loss function, optimizer, and metrics. It also uses a ModelCheckpoint callback to save the best model weights based on validation accuracy during training. Finally, it loads the best weights from the saved checkpoint file." <EXPLAINS> """CODE.model.compile(loss=..., optimizer=...,
              metrics=['accuracy'])

EPOCHS = 10
checkpoint_filepath = '/tmp/checkpoint'
model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(
    filepath=checkpoint_filepath,
    save_weights_only=True,
    monitor='val_accuracy',
    mode='max',
    save_best_only=True)

model.fit(epochs=EPOCHS, callbacks=[model_checkpoint_callback])

model.load_weights(checkpoint_filepath)
""" .

"DESCRIPTION.The code performs multiple-choice question answering using the Camembert model. It encodes two choices, \"J'aime le camembert !\" and \"Je deteste le camembert !\", with special tokens using the CamembertTokenizer. These encoded choices are then inputted into the CamembertForMultipleChoice model along with the correct label. The model outputs the loss and classification scores for the given choices and label." <EXPLAINS> """CODE.tokenizer = CamembertTokenizer.from_pretrained('camembert-base')
model = CamembertForMultipleChoice.from_pretrained('camembert-base')
choices = ["J'aime le camembert !", "Je deteste le camembert !"]
input_ids = torch.tensor([tokenizer.encode(s, add_special_tokens=True) for s in choices]).unsqueeze(0)  # Batch size 1, 2 choices
labels = torch.tensor(1).unsqueeze(0)  # Batch size 1
outputs = model(input_ids, labels=labels)
loss, classification_scores = outputs[:2]""" .

"DESCRIPTION.The code performs non-maximum suppression on bounding boxes and corresponding scores to filter out redundant detections, utilizing matrix operations and setting parameters such as score threshold, post threshold, top K for nms, and top K to keep." <EXPLAINS> """CODE.import paddle.fluid as fluid
boxes = fluid.data(name='bboxes', shape=[None,81, 4],
                          dtype='float32', lod_level=1)
scores = fluid.data(name='scores', shape=[None,81],
                          dtype='float32', lod_level=1)
out = fluid.layers.matrix_nms(bboxes=boxes,
                              scores=scores,
                              background_label=0,
                              score_threshold=0.5,
                              post_threshold=0.1,
                              nms_top_k=400,
                              keep_top_k=200,
                              normalized=False)""" .

"DESCRIPTION.The code performs non-negative matrix factorization on the Short Time Fourier Transform (STFT) of an audio signal to decompose it into a set of basis components and their corresponding activations." <EXPLAINS> """CODE.S = np.abs(librosa.stft(y))
components, activations = librosa.decompose.decompose(S, n_components=32)


T = sklearn.decomposition.DictionaryLearning(n_components=32)
components, activations = librosa.decompose.decompose(S, transformer=T)
""" .

"DESCRIPTION.The code performs object detection on an image of people using the InferenceClient from the Hugging Face model hub. It returns a list of dictionaries containing information about detected objects such as the confidence score, label, and bounding box coordinates." <EXPLAINS> """CODE.from huggingface_hub import InferenceClient
client = InferenceClient()
client.object_detection("people.jpg"):
[{"score":0.9486683011054993,"label":"person","box":{"xmin":59,"ymin":39,"xmax":420,"ymax":510}}, ... ]
""" .

"DESCRIPTION.The code performs padding on a 4-dimensional tensor using different padding configurations specified by tuples. The padding is done with a constant value of 0. The output tensor size is adjusted based on the padding configuration provided." <EXPLAINS> """CODE.t4d = torch.empty(3, 3, 4, 2)
    p1d = (1, 1) # pad last dim by 1 on each side
    out = F.pad(t4d, p1d, "constant", 0)  # effectively zero padding
    print(out.data.size())
    torch.Size([3, 3, 4, 4])
    p2d = (1, 1, 2, 2) # pad last dim by (1, 1) and 2nd to last by (2, 2)
    out = F.pad(t4d, p2d, "constant", 0)
    print(out.data.size())
    torch.Size([3, 3, 8, 4])
    t4d = torch.empty(3, 3, 4, 2)
    p3d = (0, 1, 2, 1, 3, 3) # pad by (0, 1), (2, 1), and (3, 3)
    out = F.pad(t4d, p3d, "constant", 0)
    print(out.data.size())
    torch.Size([3, 9, 7, 3])""" .

"DESCRIPTION.The code performs parallel scanning of a table between specified row ranges, extracting specific columns from the table." <EXPLAINS> """CODE.
ds1 = table.parallel_scan_range("row_start",
                                "row_end",
                                columns=[("cfa", "c1"),
                                         ("cfa", "c2"),
                                         ("cfb", "c3")])
ds2 = table.parallel_scan_range("row_start", "row_end",
                                cfa=["c1", "c2"], cfb="c3")
""" .

"DESCRIPTION.The code performs parallel training using an Accelerator object in Python. It prepares the data loader, model, and optimizer for training. It then computes the loss for two different inputs in separate forward passes. Gradients are accumulated for the first forward pass, and gradients are synchronized and applied for the second forward pass. Finally, the optimizer updates the model parameters and resets the gradients to zero." <EXPLAINS> """CODE.        from accelerate import Accelerator

        accelerator = Accelerator()
        dataloader, model, optimizer = accelerator.prepare(dataloader, model, optimizer)

        with accelerator.no_sync():
        ...     loss_a = loss_func(model(input_a))  # first forward pass
        ...     loss_b = loss_func(model(input_b))  # second forward pass
        accelerator.backward(loss_a)  # No synchronization across processes, only accumulate gradients
        with accelerator.trigger_sync_in_backward(model):
        ...     accelerator.backward(loss_b)  # Synchronization across all processes
        optimizer.step()
        optimizer.zero_grad()
""" .

"DESCRIPTION.The code performs parallelized sum scatter operations on the input array `x` along the specified axis 'i' with various configurations such as using tiled scatter and custom axis index groups for parallel processing using JAX library in Python." <EXPLAINS> """CODE.x = np.arange(16).reshape(4,4)
y = jax.pmap(lambda x: jax.lax.psum_scatter(x, 'i'), axis_name='i')(x)
y = jax.pmap(lambda x: jax.lax.psum_scatter(x, 'i', tiled=True), axis_name='i')(x)
def f(x):
...   return jax.lax.psum_scatter(
...       x, 'i', axis_index_groups=[[0, 2], [3, 1]], tiled=True)
y = jax.pmap(f, axis_name='i')(x)""" .

"DESCRIPTION.The code performs pivoting on the DataFrame 'df' with columns 'foo' and 'bar', and displays the values in the column 'baz'." <EXPLAINS> """CODE.df.pivot('foo', 'bar', 'baz')
df.pivot('foo', 'bar')['baz']""" .

"""DESCRIPTION.The code performs pre-emphasis filtering on the input audio signal 'y' in three steps:
1. Applying pre-emphasis to the entire signal 'y'.
2. Applying pre-emphasis to the first 1000 samples of 'y' and returning the final filter state 'zf'.
3. Applying pre-emphasis to the remaining samples of 'y' using the final filter state 'zf' from step 2 and returning the updated filter state 'zf'.""" <EXPLAINS> """CODE.y_filt = librosa.effects.preemphasis(y)
y_filt_1, zf = librosa.effects.preemphasis(y[:1000], return_zf=True)
y_filt_2, zf = librosa.effects.preemphasis(y[1000:], zi=zf, return_zf=True)""" .

"DESCRIPTION.The code performs prefetching, offloading, and memory copying operations before utilizing the ADAM optimization algorithm." <EXPLAINS> """CODE.(m1, m2) = prefetch(m1@offload, m2@offload)
(m1out, m2out, pout) = adam(m1, m2, p)
(m1@offload, m2@offload) = memcpy(m1, m2)""" .

"DESCRIPTION.The code performs question answering by providing a question and a context, and outputs the answer along with the confidence score, start and end positions of the answer in the context." <EXPLAINS> """CODE.from huggingface_hub import InferenceClient
client = InferenceClient()
client.question_answering(question="What's my name?", context="My name is Clara and I live in Berkeley.")
{'score': 0.9326562285423279, 'start': 11, 'end': 16, 'answer': 'Clara'}
""" .

"DESCRIPTION.The code performs renaming of columns in the dataframe x to either be in uppercase or using a dictionary to map new names to existing columns." <EXPLAINS> """CODE.x.rename(str.upper)
x.rename({'foo' : 'a', 'bar' : 'b', 'baz' : 'c'})""" .

"DESCRIPTION.The code performs repeated k-fold cross-validation on the dataset X with corresponding labels y. It splits the dataset into train and test sets multiple times, prints the indices of the train and test sets, and assigns the data points to X_train, X_test, y_train, and y_test for each iteration." <EXPLAINS> """CODE.from sklearn.model_selection import RepeatedKFold
X = np.array([[1, 2], [3, 4], [1, 2], [3, 4]])
y = np.array([0, 0, 1, 1])
rkf = RepeatedKFold(n_splits=2, n_repeats=2, random_state=2652124)
for train_index, test_index in rkf.split(X):
...     print("TRAIN:", train_index, "TEST:", test_index)
...     X_train, X_test = X[train_index], X[test_index]
...     y_train, y_test = y[train_index], y[test_index]""" .

"DESCRIPTION.The code performs repeated stratified k-fold cross-validation on the input data array X with corresponding labels y. It splits the data into training and testing sets multiple times while ensuring class balance in each fold. The training and testing indices are then used to split the data into training and testing subsets." <EXPLAINS> """CODE.from sklearn.model_selection import RepeatedStratifiedKFold
X = np.array([[1, 2], [3, 4], [1, 2], [3, 4]])
y = np.array([0, 0, 1, 1])
rskf = RepeatedStratifiedKFold(n_splits=2, n_repeats=2,
...     random_state=36851234)
for train_index, test_index in rskf.split(X, y):
...     print("TRAIN:", train_index, "TEST:", test_index)
...     X_train, X_test = X[train_index], X[test_index]
...     y_train, y_test = y[train_index], y[test_index]""" .

"DESCRIPTION.The code performs resampling on a pandas Series or DataFrame with a specified frequency and backfills missing values within the resampled intervals." <EXPLAINS> """CODE.s = pd.Series([1, 2, 3],
              index=pd.date_range('20180101', periods=3, freq='h'))
s
2018-01-01 00:00:00    1
2018-01-01 01:00:00    2
2018-01-01 02:00:00    3
Freq: H, dtype: int64

s.resample('30min').backfill()
2018-01-01 00:00:00    1
2018-01-01 00:30:00    2
2018-01-01 01:00:00    2
2018-01-01 01:30:00    3
2018-01-01 02:00:00    3
Freq: 30T, dtype: int64

s.resample('15min').backfill(limit=2)
2018-01-01 00:00:00    1.0
2018-01-01 00:15:00    NaN
2018-01-01 00:30:00    2.0
2018-01-01 00:45:00    2.0
2018-01-01 01:00:00    2.0
2018-01-01 01:15:00    NaN
2018-01-01 01:30:00    3.0
2018-01-01 01:45:00    3.0
2018-01-01 02:00:00    3.0
Freq: 15T, dtype: float64

df = pd.DataFrame({'a': [2, np.nan, 6], 'b': [1, 3, 5]},
                  index=pd.date_range('20180101', periods=3,
                                      freq='h'))
df
                   a  b
2018-01-01 00:00:00  2.0  1
2018-01-01 01:00:00  NaN  3
2018-01-01 02:00:00  6.0  5

df.resample('30min').backfill()
                   a  b
2018-01-01 00:00:00  2.0  1
2018-01-01 00:30:00  NaN  3
2018-01-01 01:00:00  NaN  3
2018-01-01 01:30:00  6.0  5
2018-01-01 02:00:00  6.0  5

df.resample('15min').backfill(limit=2)
                   a    b
2018-01-01 00:00:00  2.0  1.0
2018-01-01 00:15:00  NaN   NaN
2018-01-01 00:30:00   NaN   3.0
2018-01-01 00:45:00   NaN   3.0
2018-01-01 01:00:00   NaN   3.0
2018-01-01 01:15:00   NaN   NaN
2018-01-01 01:30:00   6.0   5.0
2018-01-01 01:45:00   6.0   5.0
2018-01-01 02:00:00   6.0   5.0""",
        """CODE.s = pd.Series([1, 2, 3],
...               index=pd.date_range('20180101', periods=3, freq='h'))
s
2018-01-01 00:00:00    1
2018-01-01 01:00:00    2
2018-01:01 02:00:00    3
Freq: H, dtype: int64

s.resample('30min').bfill()
2018-01-01 00:00:00    1
2018-01-01 00:30:00    2
2018-01-01 01:00:00    2
2018-01-01 01:30:00    3
2018-01-01 02:00:00    3
Freq: 30T, dtype: int64

s.resample('15min').bfill(limit=2)
2018-01-01 00:00:00    1.0
2018-01-01 00:15:00    NaN
2018-01-01 00:30:00    2.0
2018-01-01 00:45:00    2.0
2018-01-01 01:00:00    2.0
2018-01-01 01:15:00    NaN
2018-01-01 01:30:00    3.0
2018-01-01 01:45:00    3.0
2018-01-01 02:00:00    3.0
Freq: 15T, dtype: float64

df = pd.DataFrame({'a': [2, np.nan, 6], 'b': [1, 3, 5]},
...                   index=pd.date_range('20180101', periods=3,
...                                       freq='h'))
df
                       a    b
2018-01-01 00:00:00  2.0  1
2018-01-01 01:00:00  NaN  3
2018-01-01 02:00:00  6.0  5

df.resample('30min').bfill()
                       a    b
2018-01-01 00:00:00  2.0   1
2018-01-01 00:30:00   NaN   3
2018-01-01 01:00:00   NaN   3
2018-01-01 01:30:00   6.0   5
2018-01-01 02:00:00   6.0   5

df.resample('15min').bfill(limit=2)
                       a    b
2018-01-01 00:00:00   2.0   1.0
2018-01-01 00:15:00   NaN   NaN
2018-01-01 00:30:00   NaN   3.0
2018-01-01 00:45:00   NaN   3.0
2018-01-01 01:00:00   NaN   3.0
2018-01-01 01:15:00   NaN   NaN
2018-01-01 01:30:00   6.0   5.0
2018-01-01 01:45:00   6.0   5.0
2018-01-02 02:00:00   6.0   5.0""" .

"DESCRIPTION.The code performs scaling and shifting operations on the input data to transform it according to the specified mean and variance parameters." <EXPLAINS> """CODE.E[Y] = matmul(scale, E[X])
Cov[Y] = matmul(scale, matmul(Cov[X], scale, transpose_b=True)

mu = 0
sigma = 1
b = ScaleAndShift(shift=mu, scale=sigma)

mu = ...
sigma = ...
b = ScaleAndShift(shift=mu, scale=sigma)

mu = ...
sigma = ...
b = ScaleAndShift(shift=mu, scale=sigma, event_ndims=1)

mu = ...
sigma = ...
b = ScaleAndShift(shift=mu, scale=sigma, event_ndims=1)

mu = 1
sigma = [I, I]
b = ScaleAndShift(shift=mu, scale=sigma, event_ndims=1)
x = numpy.ones(S + sigma.shape)
b.forward(x)
""" .

"DESCRIPTION.The code performs semi-supervised label propagation on the Iris dataset." <EXPLAINS> """CODE.from sklearn import datasets
from sklearn.semi_supervised import LabelPropagation
label_prop_model = LabelPropagation()
iris = datasets.load_iris()
random_unlabeled_points = np.where(np.random.random_integers(0, 1,
   size=len(iris.target)))
labels = np.copy(iris.target)
labels[random_unlabeled_points] = -1
label_prop_model.fit(iris.data, labels)
""" .

"DESCRIPTION.The code performs sequence classification using a pre-trained Xxx model and tokenizer from the Transformers library, using the input text \"Hello, my dog is cute\" to generate logits as the output." <EXPLAINS> """CODE.import tensorflow as tf
from transformers import XxxTokenizer, TFXxxForSequenceClassification

tokenizer = XxxTokenizer.from_pretrained('xxx-base-uncased')
model = TFXxxForSequenceClassification.from_pretrained('xxx-base-uncased')
input_ids = tf.constant(tokenizer.encode("Hello, my dog is cute"))[None, :]  # Batch size 1
outputs = model(input_ids)
logits = outputs[0]""" .

"DESCRIPTION.The code performs sequence processing using a basic RNN cell in TensorFlow. It creates a categorical column for colors with a specified vocabulary list, creates an indicator column for the colors, and then parses the input features according to the specified columns. Finally, it constructs a dynamic RNN model with the specified RNN cell and input layer." <EXPLAINS> """CODE.colors = sequence_categorical_column_with_vocabulary_list(
    key='colors', vocabulary_list=('R', 'G', 'B', 'Y'))
colors_indicator = _sequence_indicator_column(colors)
columns = [colors]

features = tf.parse_example(..., features=make_parse_example_spec(columns))
input_layer, sequence_length = sequence_input_layer(features, columns)

rnn_cell = tf.nn.rnn_cell.BasicRNNCell(hidden_size)
outputs, state = tf.nn.dynamic_rnn(
    rnn_cell, inputs=input_layer, sequence_length=sequence_length)
""" .

"DESCRIPTION.The code performs signal processing on an audio signal by first obtaining the short-time Fourier transform (STFT) magnitude representation, converting it to a Mel spectrogram, and then inverting the Mel spectrogram back to STFT domain. The code then visualizes the original STFT and the residual error between the inverted STFT and the original STFT using matplotlib." <EXPLAINS> """CODE.S = np.abs(librosa.stft(y))
mel_spec = librosa.feature.melspectrogram(S=S, sr=sr)
S_inv = librosa.feature.inverse.mel_to_stft(mel_spec, sr=sr)
import matplotlib.pyplot as plt
plt.figure()
plt.subplot(2,1,1)
librosa.display.specshow(librosa.amplitude_to_db(S, ref=np.max, top_db=None),
...                          y_axis='log', x_axis='time')
plt.colorbar()
plt.title('Original STFT')
plt.subplot(2,1,2)
librosa.display.specshow(librosa.amplitude_to_db(np.abs(S_inv - S),
...                                                  ref=S.max(), top_db=None),
...                          vmax=0, y_axis='log', x_axis='time', cmap='magma')
plt.title('Residual error (dB)')
plt.colorbar()
plt.tight_layout()
plt.show()""" .

"DESCRIPTION.The code performs spline transformation on a 1-dimensional array X with 6 elements, using a quadratic spline with 3 knots." <EXPLAINS> """CODE.import numpy as np
from sklearn.preprocessing import SplineTransformer
X = np.arange(6).reshape(6, 1)
spline = SplineTransformer(degree=2, n_knots=3)
spline.fit_transform(X)""" .

"DESCRIPTION.The code performs tabular regression using the given table data with features like Height, Length1, Length2, Length3, Width, and predicts the weight of fish species based on the provided model from the Hugging Face Hub." <EXPLAINS> """CODE.from huggingface_hub import InferenceClient
client = InferenceClient()
table = {
...     "Height": ["11.52", "12.48", "12.3778"],
...     "Length1": ["23.2", "24", "23.9"],
...     "Length2": ["25.4", "26.3", "26.5"],
...     "Length3": ["30", "31.2", "31.1"],
...     "Species": ["Bream", "Bream", "Bream"],
...     "Width": ["4.02", "4.3056", "4.6961"],
... }
client.tabular_regression(table, model="scikit-learn/Fish-Weight")
[110, 120, 130]
""" .

"DESCRIPTION.The code performs text classification using a predefined text classification model to predict whether the input text is positive or negative." <EXPLAINS> """CODE.from transformers.tools import TextClassificationTool
classifier = TextClassificationTool()
classifier("This is a super nice API!", labels=["positive", "negative"])""" .

"DESCRIPTION.The code performs text encoding using a VisionTextDualEncoderModel trained on Italian data. It takes input text in Italian describing images of a cat and a dog, tokenizes the text using the Italian tokenizer, pads the sequences, and returns the text features extracted by the model." <EXPLAINS> """CODE.from transformers import VisionTextDualEncoderModel, AutoTokenizer

model = VisionTextDualEncoderModel.from_pretrained("clip-italian/clip-italian")
tokenizer = AutoTokenizer.from_pretrained("clip-italian/clip-italian")

inputs = tokenizer(["una foto di un gatto", "una foto di un cane"],  padding=True, return_tensors="pt")
text_features = model.get_text_features(**inputs)""" .

"DESCRIPTION.The code performs text generation using a pre-trained XLMProphetNet model and a pre-trained Bert model for encoder-decoder architecture. It takes an input text \"Hello, my dog is cute\", tokenizes it using the XLMProphetNetTokenizer and BertTokenizer, and generates text using XLMProphetNetForCausalLM and EncoderDecoderModel. Finally, it calculates the loss after generating the output text." <EXPLAINS> """CODE.from transformers import XLMProphetNetTokenizer, XLMProphetNetForCausalLM
import torch

tokenizer = XLMProphetNetTokenizer.from_pretrained('microsoft/xprophetnet-large-wiki100-cased')
model = XLMProphetNetForCausalLM.from_pretrained('patrickvonplaten/xprophetnet-decoder-clm-large-uncased', return_dict=True)
assert model.config.is_decoder, f"{model.__class__} has to be configured as a decoder."
inputs = tokenizer("Hello, my dog is cute", return_tensors="pt")
outputs = model(**inputs)

logits = outputs.logits

from transformers import BertTokenizer, EncoderDecoderModel
import torch

tokenizer = BertTokenizer.from_pretrained('bert-uncased-large')
model = EncoderDecoderModel.from_encoder_decoder_pretrained("bert-uncased-large", "patrickvonplaten/xprophetnet-decoder-clm-large-uncased")

inputs = tokenizer("Hello, my dog is cute", return_tensors="pt")
outputs = model(input_ids=inputs["input_ids"], labels=inputs["input_ids"])

loss = outputs.loss""" .

"DESCRIPTION.The code performs text generation using the Hugging Face Hub library, generating a sequence of tokens based on the input text \"The huggingface_hub library is\". The generated text is \"100% open source and built to be easy to use.\" The code demonstrates different options for text generation, including generating text with or without details, streaming the generated tokens, and providing detailed information about the generation process." <EXPLAINS> """CODE.from huggingface_hub import InferenceClient
client = InferenceClient()
client.text_generation("The huggingface_hub library is ", max_new_tokens=12)
'100% open source and built to be easy to use.'
for token in client.text_generation("The huggingface_hub library is ", max_new_tokens=12, stream=True):
...     print(token)
100
%
open
source
and
built
to
be
easy
to
use
.
client.text_generation("The huggingface_hub library is ", max_new_tokens=12, details=True)
TextGenerationResponse(
    generated_text='100% open source and built to be easy to use.',
    details=Details(
        finish_reason=<FinishReason.Length: 'length'>,
        generated_tokens=12,
        seed=None,
        prefill=[
            InputToken(id=487, text='The', logprob=None),
            InputToken(id=53789, text=' hugging', logprob=-13.171875),
            (...)
            InputToken(id=204, text=' ', logprob=-7.0390625)
        ],
        tokens=[
            Token(id=1425, text='100', logprob=-1.0175781, special=False),
            Token(id=16, text='%', logprob=-0.0463562, special=False),
            (...)
            Token(id=25, text='.', logprob=-0.5703125, special=False)
        ],
        best_of_sequences=None
    )
)
for details in client.text_generation("The huggingface_hub library is ", max_new_tokens=12, details=True, stream=True):
...     print(details)
...
TextGenerationStreamResponse(token=Token(id=1425, text='100', logprob=-1.0175781, special=False), generated_text=None, details=None)
TextGenerationStreamResponse(token=Token(id=16, text='%', logprob=-0.0463562, special=False), generated_text=None, details=None)
TextGenerationStreamResponse(token=Token(id=1314, text=' open', logprob=-1.3359375, special=False), generated_text=None, details=None)
TextGenerationStreamResponse(token=Token(id=3178, text=' source', logprob=-0.28100586, special=False), generated_text=None, details=None)
TextGenerationStreamResponse(token=Token(id=273, text=' and', logprob=-0.5961914, special=False), generated_text=None, details=None)
TextGenerationStreamResponse(token=Token(id=3426, text=' built', logprob=-1.9423828, special=False), generated_text=None, details=None)
TextGenerationStreamResponse(token=Token(id=271, text=' to', logprob=-1.4121094, special=False), generated_text=None, details=None)
TextGenerationStreamResponse(token=Token(id=314, text=' be', logprob=-1.5224609, special=False), generated_text=None, details=None)
TextGenerationStreamResponse(token=Token(id=1833, text=' easy', logprob=-2.1132812, special=False), generated_text=None, details=None)
TextGenerationStreamResponse(token=Token(id=271, text=' to', logprob=-0.08520508, special=False), generated_text=None, details=None)
TextGenerationStreamResponse(token=Token(id=745, text=' use', logprob=-0.39453125, special=False), generated_text=None, details=None)
TextGenerationStreamResponse(token=Token(
    id=25,
    text='.',
    logprob=-0.5703125,
    special=False),
    generated_text='100% open source and built to be easy to use.',
    details=StreamDetails(finish_reason=<FinishReason.Length: 'length'>, generated_tokens=12, seed=None)
)
""" .

"DESCRIPTION.The code performs text summarization using a pre-trained model from the Hugging Face Hub on the input text \"The Eiffel tower\", returning a summarized version of the input text." <EXPLAINS> """CODE.from huggingface_hub import InferenceClient
client = InferenceClient()
client.summarization("The Eiffel tower...")
'The Eiffel tower is one of the most famous landmarks in the world....'
""" .

"DESCRIPTION.The code performs time series cross-validation by splitting the data into training and testing sets using the TimeSeriesSplit function. It iterates through the splits and assigns the corresponding training and testing data for both X and y arrays." <EXPLAINS> """CODE.from sklearn.model_selection import TimeSeriesSplit
X = np.array([[1, 2], [3, 4], [1, 2], [3, 4]])
y = np.array([1, 2, 3, 4])
tscv = TimeSeriesSplit(n_splits=3)
for train_index, test_index in tscv.split(X):
...    X_train, X_test = X[train_index], X[test_index]
...    y_train, y_test = y[train_index], y[test_index]""" .

"DESCRIPTION.The code performs timestamp casting on a given array, specifying the unit of the timestamp as milliseconds." <EXPLAINS> """CODE.cast(arr, pa.timestamp('ms'))
cast(arr, pa.timestamp('ms')).type
arr.cast('timestamp[ms]')
arr.cast('timestamp[ms]').type""" .

"DESCRIPTION.The code performs token classification on the input text \"My name is Sarah Jessica Parker but you can call me Jessica\" and identifies the entities in the text. It returns a list of dictionaries containing information about the identified entities, including the entity group, score, word, start index, and end index of each entity in the text." <EXPLAINS> """CODE.from huggingface_hub import InferenceClient
client = InferenceClient()
client.token_classification("My name is Sarah Jessica Parker but you can call me Jessica")
[{'entity_group': 'PER',
'score': 0.9971321225166321,
'word': 'Sarah Jessica Parker',
'start': 11,
'end': 31},
{'entity_group': 'PER',
'score': 0.9773476123809814,
'word': 'Jessica',
'start': 52,
'end': 59}]
""" .

"DESCRIPTION.The code performs token embedding for a categorical sequence column using a hash bucket, creates an input layer and sequence length for a neural network model, defines a recurrent neural network cell with a specified hidden size, and runs dynamic RNN on the input data to get outputs and final state." <EXPLAINS> """CODE.tokens = sequence_categorical_column_with_hash_bucket(
    'tokens', hash_bucket_size=1000)
tokens_embedding = embedding_column(tokens, dimension=10)
columns = [tokens_embedding]

features = tf.parse_example(..., features=make_parse_example_spec(columns))
input_layer, sequence_length = sequence_input_layer(features, columns)

rnn_cell = tf.nn.rnn_cell.BasicRNNCell(hidden_size)
outputs, state = tf.nn.dynamic_rnn(
    rnn_cell, inputs=input_layer, sequence_length=sequence_length)""" .

"DESCRIPTION.The code performs training a Bernoulli Restricted Boltzmann Machine model on a 3-dimensional binary input matrix X." <EXPLAINS> """CODE.import numpy as np
from sklearn.neural_network import BernoulliRBM
X = np.array([[0, 0, 0], [0, 1, 1], [1, 0, 1], [1, 1, 1]])
model = BernoulliRBM(n_components=2)
model.fit(X)
""" .

"DESCRIPTION.The code performs training and prediction using the XGBoost algorithm. It first trains a model on the provided training data and then uses the trained model to make predictions on new data. The code also demonstrates how to specify which columns to use as features for prediction." <EXPLAINS> """CODE.import numpy as np
    import xgboost as xgb
    from ray.ml.predictors.xgboost import XGBoostPredictor

    train_X = np.array([[1, 2], [3, 4]])
    train_y = np.array([0, 1])

    model = xgb.XGBClassifier().fit(train_X, train_y)
    predictor = XGBoostPredictor(model=model.get_booster())

    data = np.array([[1, 2], [3, 4]])
    predictions = predictor.predict(data)

    # Only use first and second column as the feature
    data = np.array([[1, 2, 8], [3, 4, 9]])
    predictions = predictor.predict(data, feature_columns=[0, 1])

import pandas as pd
    import xgboost as xgb
    from ray.ml.predictors.xgboost import XGBoostPredictor

    train_X = pd.DataFrame([[1, 2], [3, 4]], columns=["A", "B"])
    train_y = pd.Series([0, 1])

    model = xgb.XGBClassifier().fit(train_X, train_y)
    predictor = XGBoostPredictor(model=model.get_booster())

    # Pandas dataframe.
    data = pd.DataFrame([[1, 2], [3, 4]], columns=["A", "B"])
    predictions = predictor.predict(data)

    # Only use first and second column as the feature
    data = pd.DataFrame([[1, 2, 8], [3, 4, 9]], columns=["A", "B", "C"])
    predictions = predictor.predict(data, feature_columns=["A", "B"])""" .

"DESCRIPTION.The code performs upsampling on a 1D array by duplicating each element based on the specified size." <EXPLAINS> """CODE.input_shape = (2, 2, 3)
x = np.arange(np.prod(input_shape)).reshape(input_shape)
print(x)
[[[ 0  1  2]
  [ 3  4  5]]
 [[ 6  7  8]
  [ 9 10 11]]]
y = tf.keras.layers.UpSampling1D(size=2)(x)
print(y)
tf.Tensor(
  [[[ 0  1  2]
    [ 0  1  2]
    [ 3  4  5]
    [ 3  4  5]]
   [[ 6  7  8]
    [ 6  7  8]
    [ 9 10 11]
    [ 9 10 11]]], shape=(2, 4, 3), dtype=int64)""" .

"DESCRIPTION.The code performs various operations using TensorFlow library functions. It initializes and manipulates matrices and performs banded triangular solves and compares the results." <EXPLAINS> """CODE.x = [[2., 3., 4.], [1., 2., 3.]]
x2 = [[2., 3., 4.], [10000., 2., 3.]]
y = tf.zeros([3, 3])
z = tf.linalg.set_diag(y, x, align='LEFT_RIGHT', k=(-1, 0))
soln = tf.linalg.banded_triangular_solve(x, tf.ones([3, 1]))
are_equal = soln == tf.linalg.banded_triangular_solve(x2, tf.ones([3, 1]))
are_equal = soln == tf.linalg.triangular_solve(z, tf.ones([3, 1]))
x = [[2., 3., 4., 5.], [-1., -2., -3., -4.]]
y = tf.zeros([4, 4])
z = tf.linalg.set_diag(y, x, align='LEFT_RIGHT', k=(0, 1))
soln = tf.linalg.banded_triangular_solve(x, tf.ones([4, 1]), lower=False)
are_equal = (soln == tf.linalg.triangular_solve(z, tf.ones([4, 1]), lower=False))
""" .

"DESCRIPTION.The code performs various pop operations on a deque object, removing and returning elements based on the specified index." <EXPLAINS> """CODE.pdeque([1, 2]).pop()
pdeque([1])
pdeque([1, 2]).pop(2)
pdeque([])
pdeque([1, 2]).pop(-1)
pdeque([2])""" .

"DESCRIPTION.The code performs zero padding on a 1-dimensional array `x` with a padding value of 2 on each side." <EXPLAINS> """CODE.input_shape = (2, 2, 3)
x = np.arange(np.prod(input_shape)).reshape(input_shape)
print(x)
[[[ 0  1  2]
  [ 3  4  5]]
 [[ 6  7  8]
  [ 9 10 11]]]
y = tf.keras.layers.ZeroPadding1D(padding=2)(x)
print(y)
tf.Tensor(
  [[[ 0  0  0]
    [ 0  0  0]
    [ 0 1 2]
    [ 3 4 5]
    [ 0 0 0]
    [ 0 0 0]]
   [[ 0 0 0]
    [ 0 0 0]
    [ 6 7 8]
    [ 9 10 11]
    [ 0 0 0]
    [ 0 0 0]]], shape=(2, 6, 3), dtype=int64)""" .

"DESCRIPTION.The code performs zero-shot classification on a given text with pre-defined labels, providing the likelihood scores for each label. It uses a model to predict the relevance of the text to each label without specific training data. The `zero_shot_classification` function returns a list of dictionaries containing the label and corresponding score for the input text." <EXPLAINS> """CODE.from huggingface_hub import InferenceClient
client = InferenceClient()
text = (
...     "A new model offers an explanation for how the Galilean satellites formed around the solar system's"
...     "largest world. Konstantin Batygin did not set out to solve one of the solar system's most puzzling"
...     " mysteries when he went for a run up a hill in Nice, France."
... )
labels = ["space & cosmos", "scientific discovery", "microbiology", "robots", "archeology"]
client.zero_shot_classification(text, labels)
[
    {"label": "scientific discovery", "score": 0.7961668968200684},
    {"label": "space & cosmos", "score": 0.18570658564567566},
    {"label": "microbiology", "score": 0.00730885099619627},
    {"label": "archeology", "score": 0.006258360575884581},
    {"label": "robots", "score": 0.004559356719255447},
]
client.zero_shot_classification(text, labels, multi_label=True)
[
    {"label": "scientific discovery", "score": 0.9829297661781311},
    {"label": "space & cosmos", "score": 0.755190908908844},
    {"label": "microbiology", "score": 0.0005462635890580714},
    {"label": "archeology", "score": 0.00047131875180639327},
    {"label": "robots", "score": 0.00030448526376858354},
]
""" .

"DESCRIPTION.The code pivots the dataframe 'df' based on the columns 'foo' and 'bar' with the values in the column 'baz'. The first line pivots the entire dataframe, while the second line accesses the 'baz' values after pivoting the dataframe." <EXPLAINS> """CODE.df.pivot('foo', 'bar', 'baz')
df.pivot('foo', 'bar')['baz']""" .

"DESCRIPTION.The code plots the Receiver Operating Characteristic (ROC) curve for a classifier on the test data and displays the plot." <EXPLAINS> """CODE.metrics.plot_roc_curve(clf, X_test, y_test)  # doctest: +SKIP
plt.show()                                   # doctest: +SKIP""" .

"DESCRIPTION.The code plots the harmonic frequencies of a signal by displaying the spectrogram and plotting the harmonics of the fundamental frequency against time." <EXPLAINS> """CODE.harmonics = np.arange(1, 13)
f0_harm = librosa.f0_harmonics(S, freqs=freqs, f0=f0, harmonics=harmonics)
import matplotlib.pyplot as plt
fig, ax =plt.subplots(nrows=2, sharex=True)
librosa.display.specshow(librosa.amplitude_to_db(S, ref=np.max),
...                          x_axis='time', y_axis='log', ax=ax[0])
times = librosa.times_like(f0)
for h in harmonics:
...     ax[0].plot(times, h * f0, label=f"{h}*f0")
ax[0].legend(ncols=4, loc='lower right')
ax[0].label_outer()
librosa.display.specshow(librosa.amplitude_to_db(f0_harm, ref=np.max),
...                          x_axis='time', ax=ax[1])
ax[1].set_yticks(harmonics-1)
ax[1].set_yticklabels(harmonics)
ax[1].set(ylabel='Harmonics')""" .

"DESCRIPTION.The code populates metadata and associated files to a model file or model buffer and then writes the updated model buffer to a file." <EXPLAINS> """CODE.  # Populating a metadata file (or a metadta buffer) and associated files to
  a model file:
  populator = MetadataPopulator.with_model_file(model_file)
  # For metadata buffer (bytearray read from the metadata file), use:
  # populator.load_metadata_buffer(metadata_buf)
  populator.load_metadata_file(metadata_file)
  populator.load_associated_files([label.txt])
  populator.populate()

  # Populating a metadata file (or a metadta buffer) and associated files to
  a model buffer:
  populator = MetadataPopulator.with_model_buffer(model_buf)
  populator.load_metadata_file(metadata_file)
  populator.load_associated_files([label.txt])
  populator.populate()
  # Writing the updated model buffer into a file.
  updated_model_buf = populator.get_model_buffer()
  with open("updated_model.tflite", "wb") as f:
    f.write(updated_model_buf)
""" .

"DESCRIPTION.The code prepares a model by swapping modules from a float model to a quantized model, then invokes the quantized model on data and creates a logger dictionary using the quantized model." <EXPLAINS> """CODE.prepare_model_with_stubs(float_model, q_model, module_swap_list, Logger)
q_model(data)
ob_dict = get_logger_dict(q_model)""" .

"DESCRIPTION.The code prepares and applies linear regression model to predict based on the input data with embedding of the categorical column \"keywords\"." <EXPLAINS> """CODE.keywords = categorical_column_with_hash_bucket("keywords", 10K)
columns = [keywords, ...]
features = tf.parse_example(..., features=make_parse_example_spec(columns))
linear_prediction = linear_model(features, columns)

keywords_embedded = embedding_column(keywords, 16)
columns = [keywords_embedded, ...]
features = tf.parse_example(..., features=make_parse_example_spec(columns))
dense_tensor = input_layer(features, columns)
""" .

"DESCRIPTION.The code prepares the data for a machine learning model by downloading the data, tokenizing it, and performing other necessary data preparation steps. It also sets up the data split and some initial state. It then specifies how the data preparation is performed based on different parameters passed to the Trainer class. Finally, it initializes the model and sets up the data loaders for training, validation, and testing." <EXPLAINS> """CODE.def prepare_data(self):
    # good
    download_data()
    tokenize()
    etc()

    # bad
    self.split = data_split
    self.some_state = some_other_state()

# DEFAULT
# called once per node on LOCAL_RANK=0 of that node
Trainer(prepare_data_per_node=True)

# call on GLOBAL_RANK=0 (great for shared file systems)
Trainer(prepare_data_per_node=False)

model.prepare_data()
    if ddp/tpu: init()
model.setup(stage)
model.train_dataloader()
model.val_dataloader()
model.test_dataloader()
""" .

"DESCRIPTION.The code preprocesses 4 files concurrently and interleaves blocks of 16 records from each file." <EXPLAINS> """CODE.# Preprocess 4 files concurrently, and interleave blocks of 16 records from
# each file.
filenames = ["/var/data/file1.txt", "/var/data/file2.txt", ...]
dataset = (Dataset.from_tensor_slices(filenames)
           .interleave(lambda x:
               TextLineDataset(x).map(parse_fn, num_parallel_calls=1),
               cycle_length=4, block_length=16))


# NOTE: The following examples use `{ ... }` to represent the
# contents of a dataset.
a = { 1, 2, 3, 4, 5 }

# NOTE: New lines indicate "block" boundaries.
a.interleave(lambda x: Dataset.from_tensors(x).repeat(6),
             cycle_length=2, block_length=4) == {
    1, 1, 1, 1,
    2, 2, 2, 2,
    1, 1,
    2, 2,
    3, 3, 3, 3,
    4, 4, 4, 4,
    3, 3,
    4, 4,
    5, 5, 5, 5,
    5, 5,
}
""" .

"DESCRIPTION.The code prints \"Hello World!\" in green color using the Click library in Python." <EXPLAINS> """CODE.click.secho('Hello World!', fg='green')
click.echo(click.style('Hello World!', fg='green'))""" .

"DESCRIPTION.The code prints a list of input protocols." <EXPLAINS> "CODE.print(get_input_protocols())" .

"DESCRIPTION.The code prints out colored and styled text messages using different styles such as changing text color, making the text blink, and reversing the text." <EXPLAINS> """CODE.click.echo(click.style('Hello World!', fg='green'))
click.echo(click.style('ATTENTION!', blink=True))
click.echo(click.style('Some things', reverse=True, fg='cyan'))""" .

"DESCRIPTION.The code prints the local rank of the current process in a parallel environment using the PaddlePaddle framework." <EXPLAINS> """CODE.import paddle.fluid as fluid
env = fluid.dygraph.ParallelEnv()
print("The local rank is %d" % env.local_rank)""" .

"DESCRIPTION.The code prints the names of audio encoders along with their descriptions." <EXPLAINS> """CODE.for k, v in get_audio_encoders().items():
    print(f"{k}: {v}")
comfortnoise: RFC 3389 comfort noise generator
s302m: SMPTE 302M
aac: AAC (Advanced Audio Coding)
ac3: ATSC A/52A (AC-3)
ac3_fixed: ATSC A/52A (AC-3)
alac: ALAC (Apple Lossless Audio Codec)""" .

"DESCRIPTION.The code prints the output of the sample method from the ev object." <EXPLAINS> "CODE.print(ev.sample())" .

"DESCRIPTION.The code prints the output protocols that are available." <EXPLAINS> "CODE.print(get_output_protocols())" .

"DESCRIPTION.The code prints the verbosity level specified by the config object, and then prints the verbosity level specified by Config.VERBOSITY_ASSERTIONS in the config object, with a default verbosity level of 2." <EXPLAINS> """CODE.print(config.get_verbosity())
print(config.get_verbosity(Config.VERBOSITY_ASSERTIONS))  # 2""" .

"DESCRIPTION.The code processes an audio file 'file.mp3' by separating its percussive elements from the rest of the audio signal." <EXPLAINS> """CODE.y, sr = librosa.load('file.mp3')
y_percussive = librosa.effects.percussive(y)""" .

"DESCRIPTION.The code processes input text data using a pretrained Owlv2Model to generate text features." <EXPLAINS> """CODE.from transformers import AutoProcessor, Owlv2Model

model = Owlv2Model.from_pretrained("google/owlv2-base-patch16-ensemble")
processor = AutoProcessor.from_pretrained("google/owlv2-base-patch16-ensemble")
inputs = processor(
    text=[["a photo of a cat", "a photo of a dog"], ["photo of a astranaut"]], return_tensors="pt"
)
text_features = model.get_text_features(**inputs)
""" .

"DESCRIPTION.The code processes text summarization by using a pre-trained TFMT5Model to generate summaries of input articles." <EXPLAINS> """CODE.from transformers import TFMT5Model, T5Tokenizer
model = TFMT5Model.from_pretrained("google/mt5-small")
tokenizer = T5Tokenizer.from_pretrained("google/mt5-small")
article = "UN Offizier sagt, dass weiter verhandelt werden muss in Syrien."
summary = "Weiter Verhandlung in Syrien."
batch = tokenizer.prepare_seq2seq_batch(src_texts=[article], tgt_texts=[summary], return_tensors="tf")
batch["decoder_input_ids"] = batch["labels"]
del batch["labels"]
outputs = model(batch)
hidden_states = outputs.last_hidden_state""" .

"DESCRIPTION.The code processes user data for users with usernames 'stevepm' and 'salomaki'." <EXPLAINS> "CODE.mock_dao.ProcessUsers(SameElementsAs('stevepm', 'salomaki'))" .

"DESCRIPTION.The code profiles the training process using PyTorch profiler and records the shapes of tensors, along with stack information. It runs profiling for a specified number of wait, warmup, and active iterations, then repeats the profiling process. The code also uses Weights & Biases profiling tool to handle tracing." <EXPLAINS> """CODE.with torch.profiler.profile(
    schedule=torch.profiler.schedule(wait=1, warmup=1, active=3, repeat=1),
    on_trace_ready=wandb.profiler.torch_trace_handler(),
    record_shapes=True,
    with_stack=True,
) as prof:
    for i, batch in enumerate(dataloader):
        if step >= 5:
            break
        train(batch)
        prof.step()
""" .

"DESCRIPTION.The code prompts the user to agree by displaying a checkbox. If the user checks the box, the code will write 'Great!' to the screen." <EXPLAINS> """CODE.agree = st.checkbox('I agree')
if agree:
    st.write('Great!')""" .

"DESCRIPTION.The code prompts the user to input a movie title. It then displays the current movie title that the user has entered." <EXPLAINS> """CODE.title = st.text_input('Movie title', 'Life of Brian')
st.write('The current movie title is', title)""" .

"DESCRIPTION.The code prompts the user to input a number and then displays the inputted number on the screen." <EXPLAINS> """CODE.number = st.number_input('Insert a number')
st.write('The current number is ', number)""" .

"DESCRIPTION.The code prompts the user to input a number, then displays the input number on the screen." <EXPLAINS> """CODE.number = st.number_input('Insert a number')
st.write('The current number is ', number)""" .

"DESCRIPTION.The code prompts the user to input their birthday date, then displays the inputted date in the format \"Your birthday is: [date]\"." <EXPLAINS> """CODE.st.date_input(
    "When's your birthday",
    datetime.date(2019, 7, 6))
st.write('Your birthday is:', d)""" .

"DESCRIPTION.The code quantizes a PyTorch model using dynamic quantization with per-channel quantization configuration. It first converts a floating-point model to a TorchScript module, then defines a function to calibrate the model using a data loader. Finally, it quantizes the model using dynamic quantization with the defined calibration function and per-channel quantization configuration." <EXPLAINS> """CODE.import torch
from torch.quantization import per_channel_dynamic_qconfig
from torch.quantization import quantize_dynamic_jit

ts_model = torch.jit.script(float_model.eval())  # or torch.jit.trace(float_model, input)
qconfig = get_default_qconfig('fbgemm')
def calibrate(model, data_loader):
    model.eval()
    with torch.no_grad():
        for image, target in data_loader:
            model(image)

quantized_model = quantize_dynamic_jit(
    ts_model,
    {'': qconfig},
    calibrate,
    [data_loader_test])
""" .

"DESCRIPTION.The code quantizes a given PyTorch model using JIT quantization with a default QConfig, and calibrates the quantized model using a provided data loader." <EXPLAINS> """CODE.import torch
from torch.quantization import get_default_qconfig
from torch.quantization import quantize_jit

ts_model = torch.jit.script(float_model.eval())  # or torch.jit.trace(float_model, input)
qconfig = get_default_qconfig('fbgemm')
def calibrate(model, data_loader):
    model.eval()
    with torch.no_grad():
        for image, target in data_loader:
            model(image)

quantized_model = quantize_jit(
    ts_model,
    {'': qconfig},
    calibrate,
    [data_loader_test])
""" .

"DESCRIPTION.The code quantizes a saved model by creating a representative dataset, saving it to a file, specifying quantization options, and then quantizing the saved model using the provided options." <EXPLAINS> """CODE.representative_dataset = [{"input": tf.random.uniform(shape=(3, 3))}
                      for _ in range(256)]

dataset_file_map = (
  tf.quantization.experimental.TfRecordRepresentativeDatasetSaver(
        path_map={'serving_default': '/tmp/representative_dataset_path'}
    ).save({'serving_default': representative_dataset})
)

quantization_options = tf.quantization.experimental.QuantizationOptions(
    signature_keys=['serving_default'],
    representative_datasets=dataset_file_map,
)
tf.quantization.experimental.quantize_saved_model(
    '/tmp/input_model',
    '/tmp/output_model',
    quantization_options=quantization_options,
)
""" .

"DESCRIPTION.The code randomly shuffles the elements in the data structure \"ds\"." <EXPLAINS> """CODE.ds.random_shuffle()
ds.random_shuffle(seed=12345)""" .

"DESCRIPTION.The code randomly shuffles the elements of a dataset. The second line of code shuffles the dataset with a specified seed value for reproducibility." <EXPLAINS> """CODE.ds.random_shuffle()
ds.random_shuffle(seed=12345)""" .

"DESCRIPTION.The code reads MNIST dataset using Ray's SimpleTorchDatasource with parallelism of 1 and returns the first element from the dataset." <EXPLAINS> """CODE.import ray
from ray.data.datasource import SimpleTorchDatasource

dataset_factory = lambda: torchvision.datasets.MNIST("data", download=True)
dataset = ray.data.read_datasource(
    SimpleTorchDatasource(), parallelism=1, dataset_factory=dataset_factory
)
dataset.take(1)""" .

"DESCRIPTION.The code reads a Python file containing functions and imports, removes unnecessary imports, and removes empty statements from the file." <EXPLAINS> """CODE.import os

py_file = os.path.join(_PROJECT_ROOT, "src", "pytorch_lightning", "utilities", "cli.py")
import_path = ".".join(["pytorch_lightning", "utilities", "cli"])

with open(py_file, encoding="utf-8") as fp:
    lines = [ln.rstrip() for ln in fp.readlines()]

lines = prune_imports_callables(lines)
lines = prune_empty_statements(lines)""" .

"DESCRIPTION.The code reads a Python file from a specified path, removes any trailing whitespace from each line, and then prunes any imports and callables from the lines of code." <EXPLAINS> """CODE.import os

py_file = os.path.join(_PROJECT_ROOT, "src", "pytorch_lightning", "utilities", "cli.py")
import_path = ".".join(["pytorch_lightning", "utilities", "cli"])

with open(py_file, encoding="utf-8") as fp:
    lines = [ln.rstrip() for ln in fp.readlines()]

lines = prune_imports_callables(lines)""" .

"DESCRIPTION.The code reads a Python file specified by the variable `py_file`, removes comments and docstrings from each line, and stores the cleaned lines in the `lines` variable." <EXPLAINS> """CODE.py_file = os.path.join(_PROJECT_ROOT, "src", "pytorch_lightning", "loggers", "csv_logs.py")
import_path = ".".join(["pytorch_lightning", "loggers", "csv_logs"])
with open(py_file, encoding="utf-8") as fp:
...     lines = [ln.rstrip() for ln in fp.readlines()]
lines = prune_comments_docstrings(lines)""" .

"DESCRIPTION.The code reads a Python file, removes any trailing whitespace from each line, and replaces variables with imports based on the specified import path." <EXPLAINS> """CODE.py_file = os.path.join(_PROJECT_ROOT, "src", "pytorch_lightning", "utilities", "imports.py")
import_path = ".".join(["pytorch_lightning", "utilities", "imports"])
with open(py_file, encoding="utf-8") as fp:
...     lines = [ln.rstrip() for ln in fp.readlines()]
lines = replace_vars_with_imports(lines, import_path)""" .

"DESCRIPTION.The code reads a file in chunks of specified size by utilizing the SliceFileObj class, allowing the program to read parts of the file sequentially without loading the entire file into memory at once." <EXPLAINS> """CODE.with open("path/to/file", "rb") as file:
...     with SliceFileObj(file, seek_from=128, read_limit=200) as fslice:
...         fslice.read(...)

import os
chunk_size = 512
file_size = os.getsize("path/to/file")
with open("path/to/file", "rb") as file:
...     for chunk_idx in range(ceil(file_size / chunk_size)):
...         with SliceFileObj(file, seek_from=chunk_idx * chunk_size, read_limit=chunk_size) as fslice:
...             chunk = fslice.read(...)
""" .

"DESCRIPTION.The code reads an audio file in Ogg format, converts it into bytes, and then plays the audio using Streamlit's 'st.audio' function." <EXPLAINS> """CODE.audio_file = open('myaudio.ogg', 'rb')
audio_bytes = audio_file.read()

st.audio(audio_bytes, format='audio/ogg')
""" .

"DESCRIPTION.The code reads an audio file in blocks using librosa, computes a Short-Time Fourier Transform (STFT) of each block without centering, and then computes a mel spectrogram of each block with specific parameters." <EXPLAINS> """CODE.stream librosa.stream(filename,
                      block_length=256,
                      frame_length=4096,
                      hop_length=1024)
for y_block in stream:
    D_block = librosa.stft(y_block, center=False)

stream = librosa.stream(filename,
                        block_length=256,
                        frame_length=2048,
                        hop_length=2048)
for y_block in stream:
    m_block = librosa.feature.melspectrogram(y_block, sr=sr,
                                             n_fft=2048,
                                             hop_length=2048,
                                             center=False)""" .

"DESCRIPTION.The code reads an audio file named 'file.wav' and computes the instantaneous frequency and magnitude spectrogram." <EXPLAINS> """CODE.y, sr = librosa.load('file.wav')
frequencies, D = librosa.ifgram(y, sr=sr)
""" .

"DESCRIPTION.The code reads an image from a specific URL, extracts image features using a Vision Transformer (ViT) model, and then encodes the features using an Encoder-Decoder model composed of ViT and GPT2 models." <EXPLAINS> """CODE.from transformers import FlaxVisionEncoderDecoderModel
from PIL import Image
import requests

url = 'http://images.cocodataset.org/val2017/000000039769.jpg'
image = Image.open(requests.get(url, stream=True).raw)

feature_extractor = ViTFeatureExtractor.from_pretrained('google/vit-base-patch16-224-in21k')

# initialize a vit-gpt2 from pretrained ViT and GPT2 models. Note that the cross-attention layers will be randomly initialized
model = FlaxVisionEncoderDecoderModel.from_encoder_decoder_pretrained('vit', 'gpt2')

pixel_values = feature_extractor(images=image, return_tensors="np").pixel_values
encoder_outputs = model.encode(pixel_values)
""" .

"DESCRIPTION.The code reads an input file located at `data_path` using UTF-8 encoding and creates a reader object to read the CSV file contents as unicode strings." <EXPLAINS> """CODE.from torchaudio.datasets.utils import unicode_csv_reader
import io
with io.open(data_path, encoding="utf8") as f:
    reader = unicode_csv_reader(f)""" .

"DESCRIPTION.The code reads and cleans up the contents of a specific Python file, storing each line in a list, and then removes certain function calls from the lines." <EXPLAINS> """CODE.import os

py_file = os.path.join(_PROJECT_ROOT, "src", "pytorch_lightning", "loggers", "__init__.py")
import_path = ".".join(["pytorch_lightning", "loggers"])

with open(py_file, encoding="utf-8") as fp:
    lines = [ln.rstrip() for ln in fp.readlines()]

lines = prune_func_calls(lines)""" .

"DESCRIPTION.The code reads and pre-processes the MNIST training dataset in batches of 32 with distributed batch reader support." <EXPLAINS> """CODE.import paddle
import paddle.fluid as fluid

train_reader = paddle.batch(paddle.dataset.mnist.train(),
            batch_size=32,drop_last=True)
train_reader = fluid.contrib.reader.distributed_batch_reader(
            train_reader)""" .

"DESCRIPTION.The code reads data from a TFRecord file and applies a function to parse each example in the dataset." <EXPLAINS> """CODE.
ds = tf.data.TFRecordDataset(filepath)
ds = ds.map(file_adapter.parse_example)
""" .

"DESCRIPTION.The code reads data from a binary data source located at \"/path/to/dir\" using the ray.data.read_datasource function and then takes the data." <EXPLAINS> """CODE.source = BinaryDatasource()
ray.data.read_datasource(source, paths="/path/to/dir").take()""" .

"DESCRIPTION.The code reads data from an HDF5 file and uses a machine learning model to make predictions on the data." <EXPLAINS> """CODE.    x_data = HDF5Matrix('input/file.hdf5', 'data')
    model.predict(x_data)
""" .

"DESCRIPTION.The code reads financial data for Goldman Sachs (GS), the VIX volatility index, and various Fama-French factors and portfolios." <EXPLAINS> """CODE.gs = DataReader("GS", "yahoo")
vix = DataReader("VIXCLS", "fred")
ff = DataReader("F-F_Research_Data_Factors", "famafrench")
ff = DataReader("F-F_Research_Data_Factors_weekly", "famafrench")
ff = DataReader("6_Portfolios_2x3", "famafrench")
ff = DataReader("F-F_ST_Reversal_Factor", "famafrench")""" .

"DESCRIPTION.The code reads frames from a test video located at the specified path. It allows the user to seek to a specific timestamp in the video and retrieve frames either one by one or in batches based on certain conditions such as timestamp or a specified number of frames. The frames are stored in a list called frames." <EXPLAINS> """CODE.import torchvision
video_path = "path_to_a_test_video"

reader = torchvision.io.VideoReader(video_path, "video")
reader.seek(2.0)
frame = next(reader)

reader.seek(2)
for frame in reader:
    frames.append(frame['data'])
# additionally, `seek` implements a fluent API, so we can do
for frame in reader.seek(2):
    frames.append(frame['data'])

for frame in itertools.takewhile(lambda x: x['pts'] <= 5, reader.seek(2)):
    frames.append(frame['data'])

for frame in itertools.islice(reader.seek(2), 10):
    frames.append(frame['data'])""" .

"DESCRIPTION.The code reads in a table with information about different animals and their number of legs, writes this table to a parquet file named 'example.parquet', and then reads the parquet file to get the number of row groups in the file." <EXPLAINS> """CODE.import pyarrow as pa
table = pa.table({'n_legs': [2, 2, 4, 4, 5, 100],
                  'animal': ["Flamingo", "Parrot", "Dog", "Horse",
                             "Brittle stars", "Centipede"]})
import pyarrow.parquet as pq
pq.write_table(table, 'example.parquet')
parquet_file = pq.ParquetFile('example.parquet')

parquet_file.num_row_groups""" .

"DESCRIPTION.The code reads input files matching a specified pattern, shuffles and replicates the dataset, shuffles it again, interleaves it with TFRecord datasets, and finally maps a function to each element of the dataset in parallel." <EXPLAINS> """CODE.d = tf.data.TFRecordDataset(input_file)
d = d.shard(num_workers, worker_index)
d = d.repeat(num_epochs)
d = d.shuffle(shuffle_buffer_size)
d = d.map(parser_fn, num_parallel_calls=num_map_threads)


d = Dataset.list_files(pattern)
d = d.shard(num_workers, worker_index)
d = d.repeat(num_epochs)
d = d.shuffle(shuffle_buffer_size)
d = d.interleave(tf.data.TFRecordDataset,
                 cycle_length=num_readers, block_length=1)
d = d.map(parser_fn, num_parallel_calls=num_map_threads)
""" .

"DESCRIPTION.The code reads items from a generator and adds them to an output buffer. If the buffer reaches a certain size, it yields the buffered items and then continues adding more items. Finally, it yields any remaining items in the buffer." <EXPLAINS> """CODE.output = BlockOutputBuffer(udf, 500 * 1024 * 1024)
for item in generator():
    output.add(item)
    if output.has_next():
        yield output.next()
output.finalize()
if output.has_next():
    yield output.next()""" .

"DESCRIPTION.The code reads logs from different sockets synchronously and asynchronously and prints them using the function print_log_msg." <EXPLAINS> """CODE.# Synchronous reading, run_forever() is blocking

def print_log_msg(ws_app, msg):
    print(msg)

flow_logs_socket = client.create_lightning_logs_socket("project_id", "app_id", "flow", print_log_msg)
flow_socket.run_forever()

# Asynchronous reading (with Threads)

def print_log_msg(ws_app, msg):
    print(msg)

flow_logs_socket = client.create_lightning_logs_socket("project_id", "app_id", "flow", print_log_msg)
work_logs_socket = client.create_lightning_logs_socket("project_id", "app_id", "work_1", print_log_msg)

flow_logs_thread = Thread(target=flow_logs_socket.run_forever)
work_logs_thread = Thread(target=work_logs_socket.run_forever)

flow_logs_thread.start()
work_logs_thread.start()
# .......

flow_logs_socket.close()
work_logs_thread.close()
""" .

"DESCRIPTION.The code reads numpy arrays from specified file paths or S3 bucket locations." <EXPLAINS> """CODE.ray.data.read_numpy("s3://bucket/path")
ray.data.read_numpy(["/path/to/file1", "/path/to/file2"])
ray.data.read_numpy(["s3://bucket/path1", "s3://bucket/path2"])""" .

"DESCRIPTION.The code reads records from a TFRecord file using a random access reader. It reads the first record at the initial offset, then reads the second record at the updated offset, and can also jump back to read the first record again if needed." <EXPLAINS> """CODE.reader = tf_record_random_reader(file_path)

record_1, offset_1 = reader.read(0)  # 0 is the initial offset.
# offset_1 is the ending offset of the 1st record and the starting offset of
# the next.

record_2, offset_2 = reader.read(offset_1)
# offset_2 is the ending offset of the 2nd record and the starting offset of
# the next.
# We can jump back and read the first record again if so desired.
reader.read(0)
""" .

"DESCRIPTION.The code reads text data from either a file stored on Amazon S3 or from local files specified in a list." <EXPLAINS> """CODE.ray.data.read_text("s3://bucket/path")
ray.data.read_text(["/path/to/file1", "/path/to/file2"])""" .

"DESCRIPTION.The code reads the content of a file called 'file.txt' with encoding 'AES' and prints the content. It then imports the 'sys' module and sets the encoding of standard input to 'AES', then checks if the standard input encoding is 'AES'." <EXPLAINS> """CODE.with open('file.txt', 'r', encoding='AES') as f:
...     data = f.read()
...
print(data)
'AES'
import sys
_encoding = sys.stdin.encoding
with stdin_encoding('AES'): sys.stdin.encoding
'AES'
sys.stdin.encoding==_encoding
True""" .

"DESCRIPTION.The code reassigns NaN values to elements in an array based on specified indexes." <EXPLAINS> """CODE.indexer, mask = index.get_indexer(new_index)
new_values = cur_values.take(indexer)
new_values[-mask] = np.nan""" .

"DESCRIPTION.The code reassigns values in the 'codes' array based on the mapping between the 'old_cat' and 'new_cat' Index objects." <EXPLAINS> """CODE.old_cat = pd.Index(['b', 'a', 'c'])
new_cat = pd.Index(['a', 'b'])
codes = np.array([0, 1, 1, 2])
_recode_for_categories(codes, old_cat, new_cat)
array([ 1,  0,  0, -1], dtype=int8)""",
        """CODE.old_cat = pd.Index(['b', 'a', 'c'])
new_cat = pd.Index(['a', 'b'])
codes = np.array([0, 1, 1, 2])
recode_for_categories(codes, old_cat, new_cat)
array([ 1,  0,  0, -1], dtype=int8)""" .

"DESCRIPTION.The code records the time taken to process each batch of work, computes the throughput in terms of batches and samples processed, and logs the throughput every 10 batches processed. If CUDA is available, it ensures synchronization to accurately measure time." <EXPLAINS> """CODE.    fabric = Fabric(logger=logger)
    throughput = ThroughputMonitor()
    t0 = time()
    for i in range(1, 100):
        do_work()
        if torch.cuda.is_available(): torch.cuda.synchronize()  # required or else time() won't be correct
        throughput.update(time=time() - t0, batches=i, samples=i)
        if i % 10 == 0:
            throughput.compute_and_log(step=i)""" .

"DESCRIPTION.The code recursively applies a transformation function to the key-value pairs of a nested dictionary, where the transformation function is used to update the parameters of a model using the Adam and SGD optimizers." <EXPLAINS> """CODE.import optax
import jax
import jax.numpy as jnp

def map_nested_fn(fn):
    '''Recursively apply `fn` to the key-value pairs of a nested dict'''
    def map_fn(nested_dict):
        return {k: (map_fn(v) if isinstance(v, dict) else fn(k, v))
                for k, v in nested_dict.items()}
    return map_fn

params = {'linear_1': {'w': jnp.zeros((5, 6)), 'b': jnp.zeros(5)},
          'linear_2': {'w': jnp.zeros((6, 1)), 'b': jnp.zeros(1)}}
gradients = jax.tree_map(jnp.ones_like, params)  # dummy gradients

label_fn = map_nested_fn(lambda k, _: k)
tx = optax.multi_transform({'w': optax.adam(1.0), 'b': optax.sgd(1.0)},
                           label_fn)
state = tx.init(params)
updates, new_state = tx.update(gradients, state, params)
new_params = optax.apply_updates(params, updates)""" .

"DESCRIPTION.The code recursively flattens a nested structure, optionally expanding composites, and then applies a custom function to flatten and filter the composite elements." <EXPLAINS> """CODE.nest.flatten(data, expand_composites=True)
nest.flatten(nest.map(
    data, lambda x: _flatten_and_filter_composite(x, False, True)))
""" .

"DESCRIPTION.The code reduces the learning rate of a machine learning model if the validation loss plateaus for 5 consecutive epochs during training." <EXPLAINS> """CODE.reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2,
                              patience=5, min_lr=0.001)
model.fit(X_train, Y_train, callbacks=[reduce_lr])
""" .

"DESCRIPTION.The code registers a communication hook for Distributed Data Parallel (DDP) with the specified type as FP16 compression, using the provided model and state." <EXPLAINS> "CODE.register_ddp_comm_hook(DDPCommHookType.FP16_COMPRESS, model, state)" .

"DESCRIPTION.The code registers a key binding for the key 'j' in vi insert mode, so that when the 'j' key is pressed, the function prefix_meta will be called." <EXPLAINS> "CODE.registry.add_key_binding('j', 'j', filter=ViInsertMode())(prefix_meta)" .

"DESCRIPTION.The code registers and manages datasets from different namespaces using a DataDirRegister class. It can list all registered datasets and load a specific dataset by calling the builder method with the dataset name." <EXPLAINS> """CODE.register = DataDirRegister(path='/path/to/namespaces.toml')

# List all registered datasets: ['kaggle:ds0', 'kaggle:ds1',...]
register.list_builders()

# Load a specific dataset
builder = register.builder('tensorflow_graphics:shapenet')
""" .

"DESCRIPTION.The code registers and runs trainable functions with Ray Tune for hyperparameter tuning." <EXPLAINS> """CODE.from ray import tune

analysis = tune.run(
    tune.durable("PPO"),
    config={"env": "CartPole-v0"},
    checkpoint_freq=1,
    sync_config=tune.SyncConfig(
        sync_to_driver=False,
        upload_dir="s3://your-s3-bucket/durable-ppo/",
    ))

tune.run(
    tune.durable(your_training_fn),
    # ...
)

tune.run(
    tune.durable(YourTrainableClass),
    # ...
)
""" .

"DESCRIPTION.The code registers different communication hooks for DistributedDataParallel (DDP) training. The first register_ddp_comm_hook function registers a hook for using fp16 data compression. The second register_ddp_comm_hook function registers a hook for powerSGD algorithm with a specific state configuration. The third register_ddp_comm_hook function registers a hook for powerSGD algorithm with a specific state configuration and also includes a wrapper for fp16 data compression." <EXPLAINS> """CODE.register_ddp_comm_hook(
    model=ddp_model,
    ddp_comm_hook=default.fp16_compress_hook,
)

register_ddp_comm_hook(
    model=ddp_model,
    ddp_comm_state=powerSGD.PowerSGDState(
        process_group=None,
        matrix_approximation_rank=1,
        start_powerSGD_iter=5000,
    ),
    ddp_comm_hook=powerSGD.powerSGD_hook,
)

register_ddp_comm_hook(
    model=ddp_model,
    ddp_comm_state=powerSGD.PowerSGDState(
        process_group=None,
        matrix_approximation_rank=1,
        start_powerSGD_iter=5000,
    ),
    ddp_comm_hook=powerSGD.powerSGD_hook,
    ddp_comm_wrapper=default.fp16_compress_wrapper,
)""" .

"DESCRIPTION.The code registers different distributed data parallel communication hooks for a deep learning model. The hooks include fp16_compress_hook for compressing gradients, powerSGD_hook for a specific communication optimization technique, and post_localSGD_hook for another communication optimization technique. Additionally, it demonstrates combining fp16_compress_wrapper with other communication hooks for further optimization." <EXPLAINS> """CODE.from torch.distributed.algorithms.ddp_comm_hooks import ( # doctest: +SKIP
    ...     default_hooks as default,
    ...     powerSGD_hook as powerSGD,
    ...     post_localSGD_hook as post_localSGD,
    ... )

    # fp16_compress_hook for compress gradients
    ddp_model = ...
    _register_ddp_comm_hook( # doctest: +SKIP
    ...     model=ddp_model,
    ...     ddp_comm_hook=default.fp16_compress_hook,
    ... )

    # powerSGD_hook
    ddp_model = ...
    _register_ddp_comm_hook( # doctest: +SKIP
    ...     model=ddp_model,
    ...     ddp_comm_state=powerSGD.PowerSGDState(
    ...         process_group=None,
    ...         matrix_approximation_rank=1,
    ...         start_powerSGD_iter=5000,
    ...     ),
    ...     ddp_comm_hook=powerSGD.powerSGD_hook,
    ... )

    # post_localSGD_hook
    subgroup, _ = torch.distributed.new_subgroups() # doctest: +SKIP
    ddp_model = ...
    _register_ddp_comm_hook( # doctest: +SKIP
    ...     model=ddp_model,
    ...     state=post_localSGD.PostLocalSGDState(
    ...         process_group=None,
    ...         subgroup=subgroup,
    ...         start_localSGD_iter=1_000,
    ...     ),
    ...     ddp_comm_hook=post_localSGD.post_localSGD_hook,
    ... )

    # fp16_compress_wrapper combined with other communication hook
    ddp_model = ...
    _register_ddp_comm_hook( # doctest: +SKIP
    ...     model=ddp_model,
    ...     ddp_comm_state=powerSGD.PowerSGDState(
    ...         process_group=None,
    ...         matrix_approximation_rank=1,
    ...         start_powerSGD_iter=5000,
    ...     ),
    ...     ddp_comm_hook=powerSGD.powerSGD_hook,
    ...     ddp_comm_wrapper=default.fp16_compress_wrapper,
    ... )""",
        """CODE.from torch.distributed.algorithms.ddp_comm_hooks import ( # doctest: +SKIP
    ...     default_hooks as default,
    ...     powerSGD_hook as powerSGD,
    ...     post_localSGD_hook as post_localSGD,
    ... )

    # fp16_compress_hook for compress gradients
    ddp_model = ...
    register_ddp_comm_hook( # doctest: +SKIP
    ...     model=ddp_model,
    ...     ddp_comm_hook=default.fp16_compress_hook,
    ... )

    # powerSGD_hook
    ddp_model = ...
    register_ddp_comm_hook( # doctest: +SKIP
    ...     model=ddp_model,
    ...     ddp_comm_state=powerSGD.PowerSGDState(
    ...         process_group=None,
    ...         matrix_approximation_rank=1,
    ...         start_powerSGD_iter=5000,
    ...     ),
    ...     ddp_comm_hook=powerSGD.powerSGD_hook,
    ... )

    # post_localSGD_hook
    subgroup, _ = torch.distributed.new_subgroups() # doctest: +SKIP
    ddp_model = ...
    register_ddp_comm_hook( # doctest: +SKIP
    ...     model=ddp_model,
    ...     state=post_localSGD.PostLocalSGDState(
    ...         process_group=None,
    ...         subgroup=subgroup,
    ...         start_localSGD_iter=1_000,
    ...     ),
    ...     ddp_comm_hook=post_localSGD.post_localSGD_hook,
    ... )

    # fp16_compress_wrapper combined with other communication hook
    ddp_model = ...
    register_ddp_comm_hook( # doctest: +SKIP
    ...     model=ddp_model,
    ...     ddp_comm_state=powerSGD.PowerSGDState(
    ...         process_group=None,
    ...         matrix_approximation_rank=1,
    ...         start_powerSGD_iter=5000,
    ...     ),
    ...     ddp_comm_hook=powerSGD.powerSGD_hook,
    ...     ddp_comm_wrapper=default.fp16_compress_wrapper,
    ... )""" .

"DESCRIPTION.The code remixes the audio signal y by rearranging the segments defined by the intervals in reverse order." <EXPLAINS> """CODE.intervals = [(0, 100), (200, 300), (400, 500)]
y_remix = librosa.effects.remix(y, intervals[::-1])""" .

"DESCRIPTION.The code removes and returns the leftmost element from a deque containing the integers 1 and 2." <EXPLAINS> "CODE.pdeque([1, 2]).popleft()" .

"DESCRIPTION.The code removes any HTML tags from the string \"Main &raquo; <em>About</em>\"." <EXPLAINS> """CODE.Markup("Main &raquo;  <em>About</em>").striptags()
u'Main \\xbb About'""" .

"DESCRIPTION.The code removes duplicate characters from a string input and returns a sorted version of the unique characters." <EXPLAINS> """CODE.@traceParseAction
def remove_duplicate_chars(tokens):
    return ''.join(sorted(set(''.join(tokens)))

wds = OneOrMore(wd).setParseAction(remove_duplicate_chars)
print(wds.parseString("slkdjs sld sldd sdlf sdljf"))""" .

"DESCRIPTION.The code removes empty strings from a list of sentences." <EXPLAINS> """CODE.import jiwer
sentences = ["", "this is an example", " ",  "                "]
print(jiwer.RemoveEmptyStrings()(sentences))""" .

"DESCRIPTION.The code removes leading and trailing spaces from each sentence in the list." <EXPLAINS> """CODE.import jiwer

sentences = [" this is an example ", "  hello goodbye  ", "  "]

print(jiwer.Strip()(sentences))
# prints: ['this is an example', "hello goodbye", ""]
# note that there is an empty string left behind which might need to be cleaned up
""" .

"DESCRIPTION.The code removes multiple spaces from the input sentences." <EXPLAINS> """CODE.import jiwer
sentences = ["this is   an   example ", "  hello goodbye  ", "  "]
print(jiwer.RemoveMultipleSpaces()(sentences))""" .

"DESCRIPTION.The code removes punctuation from each sentence in the list and returns the cleaned sentences as a new list." <EXPLAINS> """CODE.import jiwer

sentences = ["this is an example!", "hello. goodbye"]

print(jiwer.RemovePunctuation()(sentences))
# prints: ['this is an example', "hello goodbye"]
""" .

"DESCRIPTION.The code removes the first occurrence of the value 2 from a deque object and returns the updated deque object without the removed value." <EXPLAINS> """CODE.pdeque([2, 1, 2]).remove(2)
pdeque([1, 2])""" .

"DESCRIPTION.The code removes the specified subpath from the URI and returns the updated URI as a string." <EXPLAINS> """CODE.Example:
uri = URI("s3://bucket/a/b/c/?param=1")
print(str(uri.rstrip_subpath(Path("b/c")))

uri = URI("/tmp/a/b/c/")
print(str(uri.rstrip_subpath(Path("/b/c/.//")))""" .

"DESCRIPTION.The code removes white spaces from the given list of sentences or replaces them with spaces based on the specified configuration." <EXPLAINS> """CODE.import jiwer

sentences = ["this is an example", "hello world "]

print(jiwer.RemoveWhiteSpace()(sentences))
# prints: ["thisisanexample", "helloworld"]

print(jiwer.RemoveWhiteSpace(replace_by_space=True)(sentences))
# prints: ["this is an example", "hello world  "]
# note the trailing spaces
""" .

"DESCRIPTION.The code renames the columns and/or index labels of a DataFrame." <EXPLAINS> """CODE.df = pd.DataFrame({"A": [1, 2, 3], "B": [4, 5, 6]})
df.rename(index=str, columns={"A": "a", "B": "c"})
df.rename(index=str, columns={"A": "a", "C": "c"})
df.rename(str.lower, axis='columns')
df.rename({1: 2, 2: 4}, axis='index')
""" .

"DESCRIPTION.The code renames the columns of dataframe x to uppercase and maps the columns 'foo', 'bar', and 'baz' to 'a', 'b', and 'c', respectively." <EXPLAINS> """CODE.x.rename(str.upper)
x.rename({'foo' : 'a', 'bar' : 'b', 'baz' : 'c'})""" .

"DESCRIPTION.The code renders a template with the variable 'knights' set to the value 'that say nih'." <EXPLAINS> """CODE.template.render(knights='that say nih')
template.render({'knights': 'that say nih'})""" .

"DESCRIPTION.The code reorders the fields of two feature objects and checks if the type of the first feature object after reordering matches the type of the second feature object." <EXPLAINS> """CODE.from datasets import Features, Sequence, Value

f1 = Features({"root": Sequence({"a": Value("string"), "b": Value("string")})})
f2 = Features({"root": {"b": Sequence(Value("string")), "a": Sequence(Value("string"))}})
assert f1.type != f2.type
f1.reorder_fields_as(f2)
assert f1.reorder_fields_as(f2).type == f2.type
""" .

"DESCRIPTION.The code repeatedly trains a machine learning model using the MyTrainer class instance and exports the trained policy model to the specified directory." <EXPLAINS> """CODE.trainer = MyTrainer()
for _ in range(10):
    trainer.train()
trainer.export_policy_model("/tmp/export_dir")""" .

"DESCRIPTION.The code rescales the action space of an environment to be within the limits of a specified range specified by the variables a and b." <EXPLAINS> "CODE.RescaleAction(env, a, b).action_space == Box(a,b)" .

"DESCRIPTION.The code reshapes a 2D tensor into a 3D tensor with an additional dimension, and then performs global max pooling on the 3D tensor." <EXPLAINS> """CODE.x = tf.constant([[1., 2., 3.], [4., 5., 6.], [7., 8., 9.]])
x = tf.reshape(x, [3, 3, 1])
max_pool_1d = tf.keras.layers.GlobalMaxPooling1D()
max_pool_1d(x)""" .

"DESCRIPTION.The code reshapes a given constant tensor 'a' into a new shape of 2 rows and 6 columns using the keras backend." <EXPLAINS> """CODE.a = tf.constant([[1, 2, 3], [4, 5, 6], [7, 8, 9], [10, 11, 12]])
tf.keras.backend.reshape(a, shape=(2, 6))""" .

"DESCRIPTION.The code reshapes numpy arrays to the specified dimensions and returns the shape of the resulting array." <EXPLAINS> """CODE.reshape_nd(numpy.empty(0), 1).shape
reshape_nd(numpy.empty(1), 2).shape
reshape_nd(numpy.empty((2, 3)), 3).shape
reshape_nd(numpy.empty((3, 4, 5)), 3).shape""" .

"DESCRIPTION.The code reshapes the axes of input arrays according to the specified order of dimensions." <EXPLAINS> """CODE.reshape_axes('YXS', (219, 301, 1), (219, 301))
reshape_axes('IYX', (12, 219, 301), (3, 4, 219, 1, 301, 1)""" .

"DESCRIPTION.The code reshapes the input data into different shapes: from a 1D array of length 12 to a 2D array of shape (3, 4), then to a 2D array of shape (6, 2), and finally to a 4D array with the last two dimensions being 2 and 2." <EXPLAINS> """CODE.model = Sequential()
model.add(Reshape((3, 4), input_shape=(12,)))
model.add(Reshape((6, 2)))
model.add(Reshape((-1, 2, 2)))
""" .

"DESCRIPTION.The code reshapes the input tensor into different shapes: (3, 4), (6, 2), and (-1, 2, 2) sequentially." <EXPLAINS> """CODE.model = Sequential()
model.add(Reshape((3, 4), input_shape=(12,)))
model.add(Reshape((6, 2)))
model.add(Reshape((-1, 2, 2)))
""" .

"DESCRIPTION.The code reshuffles the input data batch randomly." <EXPLAINS> """CODE.import paddle.fluid as fluid
x = fluid.layers.data(name="x", shape=[-1, 4])
out = fluid.contrib.layers.shuffle_batch(x)""" .

"DESCRIPTION.The code resizes an input tensor of shape [None,3,6,10] to the specified output shape [12,12] using interpolation." <EXPLAINS> """CODE.import paddle
import paddle.fluid as fluid
import numpy as np
input = fluid.data(name="input", shape=[None,3,6,10])
output = fluid.layers.interpolate(input=input,out_shape=[12,12])
place = fluid.CPUPlace()
exe = fluid.Executor(place)
exe.run(fluid.default_startup_program())

input_data = np.random.rand(2,3,6,10).astype("float32")
output_data = exe.run(fluid.default_main_program(),
    feed={"input":input_data},
    fetch_list=[output],
    return_numpy=True)

print(output_data[0].shape)""" .

"DESCRIPTION.The code resizes images in a dataset to a specified size of 200x200 pixels." <EXPLAINS> """CODE.size = (200, 200)
ds = ds.map(lambda img: tf.image.resize(img, size))


size = (200, 200)
ds = ds.map(lambda img: smart_resize(img, size))
""" .

"DESCRIPTION.The code resizes images in a dataset to the specified size of (200, 200)." <EXPLAINS> """CODE.size = (200, 200)
ds = ds.map(lambda img: tf.image.resize(img, size))


size = (200, 200)
ds = ds.map(lambda img: smart_resize(img, size))
""" .

"DESCRIPTION.The code resizes images in the dataset to a specific size of 200x200 pixels." <EXPLAINS> """CODE.size = (200, 200)
ds = ds.map(lambda img: tf.image.resize(img, size))


size = (200, 200)
ds = ds.map(lambda img: smart_resize(img, size))
""" .

"DESCRIPTION.The code resolves patterns locally or by URLs for YAML files in the \"src\" directory and its subdirectories." <EXPLAINS> """CODE.from datasets.data_files import resolve_patterns_locally_or_by_urls

base_path = "."
resolve_patterns_locally_or_by_urls(base_path, ["src/**/*.yaml"])
""" .

"DESCRIPTION.The code resolves the file path pattern \"docs/**/*.py\" in the base directory \".\"." <EXPLAINS> """CODE.from datasets.data_files import resolve_pattern
base_path = "."
resolve_pattern("docs/**/*.py", base_path)
""" .

"DESCRIPTION.The code retrieves a list of 411 pending access requests for the repository \"meta-llama/Llama-2-7b\" from Hugging Face Hub. Each request includes the username, full name, email, timestamp, status, and additional fields. Then, it accepts the access request from user \"Clem\" for the same repository." <EXPLAINS> """CODE.from huggingface_hub import list_pending_access_requests, accept_access_request

# List pending requests
requests = list_pending_access_requests("meta-llama/Llama-2-7b")
len(requests)
411
requests[0]
[
    AccessRequest(
        username='clem',
        fullname='Clem ð¤',
        email='***',
        timestamp=datetime.datetime(2023, 11, 23, 18, 4, 53, 828000, tzinfo=datetime.timezone.utc),
        status='pending',
        fields=None,
    ),
    ...
]

# Accept Clem's request
accept_access_request("meta-llama/Llama-2-7b", "clem")
""" .

"DESCRIPTION.The code retrieves a list of deployed models for zero-shot classification and text generation from the Hugging Face model hub using the InferenceClient class. It allows you to list all zero-shot-classification models or filter the list for models from a specific framework, such as text-generation-inference." <EXPLAINS> """CODE.from huggingface_hub import InferenceClient
client = InferenceClient()

# Discover zero-shot-classification models currently deployed
models = client.list_deployed_models()
models["zero-shot-classification"]
['Narsil/deberta-large-mnli-zero-cls', 'facebook/bart-large-mnli', ...]

# List from only 1 framework
client.list_deployed_models("text-generation-inference")
{'text-generation': ['bigcode/starcoder', 'meta-llama/Llama-2-70b-chat-hf', ...], ...}
""" .

"DESCRIPTION.The code retrieves a list of rejected access requests for the repository \"meta-llama/Llama-2-7b\" from the Hugging Face Hub. Each access request includes information such as the username, full name, email, timestamp, status (rejected), and additional fields (if available)." <EXPLAINS> """CODE.from huggingface_hub import list_rejected_access_requests

requests = list_rejected_access_requests("meta-llama/Llama-2-7b")
len(requests)
411
requests[0]
[
    AccessRequest(
        username='clem',
        fullname='Clem ð¤',
        email='***',
        timestamp=datetime.datetime(2023, 11, 23, 18, 4, 53, 828000, tzinfo=datetime.timezone.utc),
        status='rejected',
        fields=None,
    ),
    ...
]
""" .

"DESCRIPTION.The code retrieves activation functions by name or function, and returns the corresponding function object. If an unknown activation function is provided, it raises a ValueError." <EXPLAINS> """CODE.tf.keras.activations.get('softmax')
 <function softmax at 0x1222a3d90>
tf.keras.activations.get(tf.keras.activations.softmax)
 <function softmax at 0x1222a3d90>
tf.keras.activations.get(None)
 <function linear at 0x1239596a8>
tf.keras.activations.get(abs)
 <built-in function abs>
tf.keras.activations.get('abcd')
Traceback (most recent call last):
...
ValueError: Unknown activation function:abcd""" .

"DESCRIPTION.The code retrieves all markdown files in the 'docs/catalog' directory of the 'tensorflow/datasets' GitHub repository. It also verifies that the GitHub path object contains the correct subpath, repo, and branch information." <EXPLAINS> """CODE.path = GithubPath.from_repo('tensorflow/datasets')
path = path / 'docs' / 'catalog'
assert path.is_dir()
datasets = [
    p.name for p in path.iterdir() if p.match('*.md')
]

path = GithubPath('github://tensorflow/datasets/tree/master/docs/README.md')
assert path.subpath == 'docs/README.md'
assert path.repo == 'tensorflow/datasets'
assert path.branch == 'master'""" .

"DESCRIPTION.The code retrieves an activation function from the tf.keras.activations module based on the input provided. It can fetch the 'softmax' activation function, the softmax activation function itself (tf.keras.activations.softmax), a None value, the 'abs' function, or the 'abcd' function." <EXPLAINS> """CODE.tf.keras.activations.get('softmax')
tf.keras.activations.get(tf.keras.activations.softmax)
tf.keras.activations.get(None)
tf.keras.activations.get(abs)
tf.keras.activations.get('abcd')""" .

"DESCRIPTION.The code retrieves and prints discussions from a specific repository on the Hugging Face model hub." <EXPLAINS> """CODE.from huggingface_hub import get_repo_discussions
discussions_list = list(get_repo_discussions(repo_id="bert-base-uncased"))

from huggingface_hub import get_repo_discussions
for discussion in get_repo_discussions(repo_id="bert-base-uncased"):
    print(discussion.num, discussion.title)
""" .

"DESCRIPTION.The code retrieves and prints information about the C drive on the computer using the Windows Management Instrumentation (WMI) framework." <EXPLAINS> """CODE.import win32com.client
import wmi

wmiobj = win32com.client.GetObject ("winmgmts:Win32_LogicalDisk.DeviceID='C:'")
c_drive = wmi._wmi_object (wmiobj)
print c_drive""" .

"DESCRIPTION.The code retrieves and returns the query parameters from the current URL in a Streamlit app." <EXPLAINS> "CODE.st.experimental_get_query_params()" .

"DESCRIPTION.The code retrieves different activation functions by name or object from the tf.keras.activations module." <EXPLAINS> """CODE.tf.keras.activations.get('softmax')
tf.keras.activations.get(tf.keras.activations.softmax)
tf.keras.activations.get(None)
tf.keras.activations.get(abs)
tf.keras.activations.get('abcd')""" .

"DESCRIPTION.The code retrieves features from an image using a pre-trained VisionTextDualEncoderModel and AutoFeatureExtractor without showcasing the dependencies and function calls in the code." <EXPLAINS> """CODE.from PIL import Image
import requests
from transformers import VisionTextDualEncoderModel, AutoFeatureExtractor

model = VisionTextDualEncoderModel.from_pretrained("clip-italian/clip-italian")
feature_extractor = AutoFeatureExtractor.from_pretrained("google/vit-base-patch16-224")

url = "http://images.cocodataset.org/val2017/000000039769.jpg"
image = Image.open(requests.get(url, stream=True).raw)

inputs = feature_extractor(images=image, return_tensors="pt")

image_features = model.get_image_features(**inputs)""" .

"DESCRIPTION.The code retrieves information about specific files from different repositories on the Hugging Face Hub platform." <EXPLAINS> """CODE.from huggingface_hub import list_files_info
files_info = list_files_info("lysandre/arxiv-nlp", ["README.md", "config.json"])
files_info
<generator object HfApi.list_files_info at 0x7f93b848e730>
list(files_info)
[
    RepoFile: {"blob_id": "43bd404b159de6fba7c2f4d3264347668d43af25", "lfs": None, "rfilename": "README.md", "size": 391},
    RepoFile: {"blob_id": "2f9618c3a19b9a61add74f70bfb121335aeef666", "lfs": None, "rfilename": "config.json", "size": 554},
]

from huggingface_hub import list_files_info
[info.rfilename for info in list_files_info("stabilityai/stable-diffusion-2", "vae") if info.lfs is not None]
['vae/diffusion_pytorch_model.bin', 'vae/diffusion_pytorch_model.safetensors']

from huggingface_hub import list_files_info
[info.rfilename for info in list_files_info("glue", repo_type="dataset")]
['.gitattributes', 'README.md', 'dataset_infos.json', 'glue.py']
""" .

"DESCRIPTION.The code retrieves information about specific paths (\"README.md\" and \"en\") in the dataset repository \"allenai/c4\" from the Hugging Face Hub. The information includes details such as file size, blob_id, tree_id, and last commit for each path." <EXPLAINS> """CODE.from huggingface_hub import get_paths_info
paths_info = get_paths_info("allenai/c4", ["README.md", "en"], repo_type="dataset")
paths_info
[
    RepoFile(path='README.md', size=2379, blob_id='f84cb4c97182890fc1dbdeaf1a6a468fd27b4fff', lfs=None, last_commit=None, security=None),
    RepoFolder(path='en', tree_id='dc943c4c40f53d02b31ced1defa7e5f438d5862e', last_commit=None)
]
""" .

"DESCRIPTION.The code retrieves information about the C: drive using the Windows Management Instrumentation (WMI) API and prints out the information." <EXPLAINS> """CODE.import win32com.client
import wmi

wmiobj = win32com.client.GetObject ("winmgmts:Win32_LogicalDisk.DeviceID='C:'")
c_drive = wmi._wmi_object (wmiobj)
print c_drive""" .

"DESCRIPTION.The code retrieves information about the Win32 serial port and its related settings using the WMI (Windows Management Instrumentation) in Python. It first creates a WMI object, retrieves the Win32_SerialPort object, and then prints out the references of the serial port and its settings." <EXPLAINS> """CODE.c =  wmi.WMI ()
sp = c.Win32_SerialPort ()[0]

for i in sp.references ():
  print i

for i in sp.references (wmi_class="Win32_SerialPortSetting"):
  print i""" .

"DESCRIPTION.The code retrieves information about the Win32_PortResource associated with the ParallelPort object and prints the Win32_PnPEntity objects associated with the ParallelPort object." <EXPLAINS> """CODE.c = wmi.WMI ()
pp = c.Win32_ParallelPort ()[0]

for i in pp.associators (wmi_association_class="Win32_PortResource"):
  print i

for i in pp.associators (wmi_result_class="Win32_PnPEntity"):
  print i""" .

"DESCRIPTION.The code retrieves information about the branches, tags, and commits in the specified repositories on the Hugging Face Model Hub using the Hugging Face API." <EXPLAINS> """CODE.from huggingface_hub import HfApi
api = HfApi()
api.list_repo_refs("gpt2")
GitRefs(branches=[GitRefInfo(name='main', ref='refs/heads/main', target_commit='e7da7f221d5bf496a48136c0cd264e630fe9fcc8')], converts=[], tags=[])

api.list_repo_refs("bigcode/the-stack", repo_type='dataset')
GitRefs(
    branches=[
        GitRefInfo(name='main', ref='refs/heads/main', target_commit='18edc1591d9ce72aa82f56c4431b3c969b210ae3'),
        GitRefInfo(name='v1.1.a1', ref='refs/heads/v1.1.a1', target_commit='f9826b862d1567f3822d3d25649b0d6d22ace714')
    ],
    converts=[],
    tags=[
        GitRefInfo(name='v1.0', ref='refs/tags/v1.0', target_commit='c37a8cd1e382064d8aced5e05543c5f7753834da')
    ]
)
""" .

"DESCRIPTION.The code retrieves information about the first parallel port on a Windows machine and prints out its derived classes in a concatenated string separated by ' <- '." <EXPLAINS> """CODE.pp0 = wmi.WMI().Win32_ParallelPort()[0]
print(' <- '.join(pp0.derivation()))""" .

"DESCRIPTION.The code retrieves information about the first parallel port on a Windows system and prints its relative path." <EXPLAINS> """CODE.pp0 = wmi.WMI ().Win32_ParallelPort ()[0]
print pp0.path ().RelPath""" .

"DESCRIPTION.The code retrieves information about the logical disks on a Windows system using the Windows Management Instrumentation (WMI) interface." <EXPLAINS> """CODE.wmi.WMI ().instances ("Win32_LogicalDisk")
wmi.WMI ().Win32_LogicalDisk ()""" .

"DESCRIPTION.The code retrieves information about the parallel port associated with Win32_PortResource and Win32_PnPEntity classes using Windows Management Instrumentation (WMI)." <EXPLAINS> """CODE.c = wmi.WMI ()
pp = c.Win32_ParallelPort ()[0]

for i in pp.associators (wmi_association_class="Win32_PortResource"):
  print i

for i in pp.associators (wmi_result_class="Win32_PnPEntity"):
  print i""" .

"DESCRIPTION.The code retrieves information about the serial ports on the system using the Windows Management Instrumentation (WMI) API. It first obtains the list of serial ports and then prints out the settings associated with each serial port." <EXPLAINS> """CODE.c =  wmi.WMI ()
sp = c.Win32_SerialPort ()[0]

for i in sp.references ():
  print i

for i in sp.references (wmi_class="Win32_SerialPortSetting"):
  print i""" .

"DESCRIPTION.The code retrieves instances of Win32 logical disks using Windows Management Instrumentation (WMI) in Python." <EXPLAINS> """CODE.wmi.WMI().instances("Win32_LogicalDisk")
wmi.WMI().Win32_LogicalDisk()""" .

"DESCRIPTION.The code retrieves option data for AAPL stock, including calls and puts for a specified month and year. It then calculates calls near the stock price with a specified range. Finally, it gets forward data for calls and puts with a specified month." <EXPLAINS> """CODE.aapl = Options('aapl', 'yahoo')
calls = aapl.get_calls(9, 2012)
puts = aapl.get_puts(9, 2012)
cut_calls = aapl.get_near_stock_price(calls, above_below=3)
forward_calls, forward_puts = aapl.get_forward_data(8, call=True, put=True)""" .

"DESCRIPTION.The code retrieves specific values from the frozen dictionary 'transaction'. It attempts to get the value at specific keys like 'purchase', 'name', 'items', 'total' and 'apple'. It also tries to retrieve values at non-existent keys like 'y' and out of range index like 10. Some of the attempts include specifying a default value or setting a flag for non-existent keys." <EXPLAINS> """CODE.from pyrsistent import freeze
transaction = freeze({'name': 'Alice',
                      'purchase': {'items': ['Apple', 'Orange'],
                                   'costs': [0.50, 1.25]},
                      'credit card': '5555-1234-1234-1234'})
get_in(['purchase', 'items', 0], transaction)
get_in(['name'], transaction)
get_in(['purchase', 'total'], transaction)
get_in(['purchase', 'items', 'apple'], transaction)
get_in(['purchase', 'items', 10], transaction)
get_in(['purchase', 'total'], transaction, 0)
get_in(['y'], {}, no_default=True)""" .

"DESCRIPTION.The code retrieves text features from the AltCLIP model for the input text descriptions of a cat and a dog." <EXPLAINS> """CODE.from transformers import AutoProcessor, AltCLIPModel
model = AltCLIPModel.from_pretrained("BAAI/AltCLIP")
processor = AutoProcessor.from_pretrained("BAAI/AltCLIP")
inputs = processor(text=["a photo of a cat", "a photo of a dog"], padding=True, return_tensors="pt")
text_features = model.get_text_features(**inputs)
""" .

"DESCRIPTION.The code retrieves the action distribution class and dimension from the ModelCatalog based on the environment's action space, then gets a model from the ModelCatalog which will be used with the specified input and distribution dimension. Finally, it creates an instance of the action distribution class using the model's outputs, and samples an action from this distribution." <EXPLAINS> """CODE.dist_class, dist_dim = ModelCatalog.get_action_dist(env.action_space)
model = ModelCatalog.get_model(inputs, dist_dim)
dist = dist_class(model.outputs)
action_op = dist.sample()""" .

"DESCRIPTION.The code retrieves the activation function based on the input provided. If a valid activation function name or function is given, it returns the corresponding function. If None is given, it returns the linear activation function. If an unknown input is provided, it raises a ValueError indicating the function is unknown." <EXPLAINS> """CODE.tf.keras.activations.get('softmax')
 <function softmax at 0x1222a3d90>
tf.keras.activations.get(tf.keras.activations.softmax)
 <function softmax at 0x1222a3d90>
tf.keras.activations.get(None)
 <function linear at 0x1239596a8>
tf.keras.activations.get(abs)
 <built-in function abs>
tf.keras.activations.get('abcd')
Traceback (most recent call last):
...
ValueError: Unknown activation function:abcd""" .

"DESCRIPTION.The code retrieves the activation function specified in the argument passed to the tf.keras.activations.get() function. It can return predefined activation functions like 'softmax', 'abs', or custom activation functions specified by the user. If the argument passed is None or does not match any predefined activation function, it will return an error." <EXPLAINS> """CODE.tf.keras.activations.get('softmax')
tf.keras.activations.get(tf.keras.activations.softmax)
tf.keras.activations.get(None)
tf.keras.activations.get(abs)
tf.keras.activations.get('abcd')""" .

"DESCRIPTION.The code retrieves the base version number from a given version string, ignoring any additional information such as build numbers or labels." <EXPLAINS> """CODE.Version("1.2.3").base_version
Version("1.2.3+abc").base_version
Version("1!1.2.3+abc.dev1").base_version""" .

"DESCRIPTION.The code retrieves the documentation for the pandas DataFrame class and for a function called my_poorly_documented_function." <EXPLAINS> """CODE.st.help(pandas.DataFrame)
x = my_poorly_documented_function()
st.help(x)""" .

"DESCRIPTION.The code retrieves the information about the initial commit of the \"gpt2\" repository from the Hugging Face model hub, which is a system commit containing the `.gitattributes` file. It then creates a new empty branch named \"new_empty_branch\" based on the initial commit." <EXPLAINS> """CODE.from huggingface_hub import HfApi
api = HfApi()

# Commits are sorted by date (last commit first)
initial_commit = api.list_repo_commits("gpt2")[-1]

# Initial commit is always a system commit containing the `.gitattributes` file.
initial_commit
GitCommitInfo(
    commit_id='9b865efde13a30c13e0a33e536cf3e4a5a9d71d8',
    authors=['system'],
    created_at=datetime.datetime(2019, 2, 18, 10, 36, 15, tzinfo=datetime.timezone.utc),
    title='initial commit',
    message='',
    formatted_title=None,
    formatted_message=None
)

# Create an empty branch by deriving from initial commit
api.create_branch("gpt2", "new_empty_branch", revision=initial_commit.commit_id)
""" .

"DESCRIPTION.The code retrieves the information of a dataset named \"demo1\" from the Hugging Face model hub, then resolves file patterns in the dataset repository for CSV files located at the \"data\" directory." <EXPLAINS> """CODE.import huggingface_hub
from datasets.data_files import resolve_patterns_in_dataset_repository
dataset_info = huggingface_hub.HfApi().dataset_info("lhoestq/demo1")
resolve_patterns_in_dataset_repository(dataset_info, ["data/*.csv"])
""" .

"DESCRIPTION.The code retrieves the pre-release identifier (pre) of different versions of a software release." <EXPLAINS> """CODE.print(Version("1.2.3").pre)
Version("1.2.3a1").pre
Version("1.2.3b1").pre
Version("1.2.3rc1").pre""" .

"DESCRIPTION.The code retrieves the reference model using an accelerator and performs a training step on the reference model." <EXPLAINS> """CODE.ref_model = accelerator.get_reference_model(model)
ref_model.training_step(...)""" .

"DESCRIPTION.The code retrieves the samplerate of an audio file using the librosa library." <EXPLAINS> """CODE.path = librosa.util.example_audio_file()
librosa.get_samplerate(path)
44100""" .

"DESCRIPTION.The code retrieves the string representation of a specific resolution constant from the Resolution class." <EXPLAINS> "CODE.Resolution.get_str(Resolution.RESO_SEC)" .

"DESCRIPTION.The code retrieves the values of two specific flags, FLAGS_eager_delete_tensor_gb and FLAGS_check_nan_inf, from the paddle.fluid module and prints the values." <EXPLAINS> """CODE.import paddle.fluid as fluid

flags = ['FLAGS_eager_delete_tensor_gb', 'FLAGS_check_nan_inf']
res = fluid.get_flags(flags)
print(res)""" .

"DESCRIPTION.The code retrieves the version number \"1.2.3\" from the Specifier object." <EXPLAINS> "CODE.Specifier(\"==1.2.3\").version" .

"DESCRIPTION.The code retrieves time steps for a model by processing audio data through a pre-trained wav2vec2 model, extracting features, and decoding the predicted transcription into word offsets with start and end times in seconds." <EXPLAINS> """CODE.# Let's see how to retrieve time steps for a model
from transformers import AutoTokenizer, AutoFeatureExtractor, AutoModelForCTC
from datasets import load_dataset
import datasets
import torch

# import model, feature extractor, tokenizer
model = AutoModelForCTC.from_pretrained("facebook/wav2vec2-base-960h")
tokenizer = AutoTokenizer.from_pretrained("facebook/wav2vec2-base-960h")
feature_extractor = AutoFeatureExtractor.from_pretrained("facebook/wav2vec2-base-960h")

# load first sample of English common_voice
dataset = load_dataset("common_voice", "en", split="train", streaming=True)
dataset = dataset.cast_column("audio", datasets.Audio(sampling_rate=16_000))
dataset_iter = iter(dataset)
sample = next(dataset_iter)

# forward sample through model to get greedily predicted transcription ids
input_values = feature_extractor(sample["audio"]["array"], return_tensors="pt").input_values
logits = model(input_values).logits[0]
pred_ids = torch.argmax(logits, axis=-1)

# retrieve word stamps (analogous commands for `output_char_offsets`)
outputs = tokenizer.decode(pred_ids, output_word_offsets=True)
# compute `time_offset` in seconds as product of downsampling ratio and sampling_rate
time_offset = model.config.inputs_to_logits_ratio / feature_extractor.sampling_rate

word_offsets = [
...     {
...         "word": d["word"],
...         "start_time": round(d["start_offset"] * time_offset, 2),
...         "end_time": round(d["end_offset"] * time_offset, 2),
...     }
...     for d in outputs.word_offsets
... ]
# compare word offsets with audio `common_voice_en_100038.mp3` online on the dataset viewer:
# https://huggingface.co/datasets/common_voice/viewer/en/train
word_offsets[:3]
[{'word': 'WHY', 'start_time': 1.42, 'end_time': 1.54}, {'word': 'DOES', 'start_time': 1.64, 'end_time': 1.9}, {'word': 'MILISANDRA', 'start_time': 2.26, 'end_time': 2.9}]
""" .

"DESCRIPTION.The code retrieves training sequences from the IMDB dataset, creates a mapping of words to indices, reverses the word index to obtain a mapping of indices to words, and decodes the first sequence in the dataset." <EXPLAINS> """CODE.# Retrieve the training sequences.
(x_train, _), _ = keras.datasets.imdb.load_data()
# Retrieve the word index file mapping words to indices
word_index = keras.datasets.imdb.get_word_index()
# Reverse the word index to obtain a dict mapping indices to words
inverted_word_index = dict((i, word) for (word, i) in word_index.items())
# Decode the first sequence in the dataset
decoded_sequence = " ".join(inverted_word_index[i] for i in x_train[0])
""" .

"DESCRIPTION.The code retrieves training sequences from the IMDB dataset, obtains a word index mapping words to indices, reverses the word index to obtain a dictionary mapping indices to words, and decodes the first sequence in the dataset using the inverted word index." <EXPLAINS> """CODE.# Retrieve the training sequences.
(x_train, _), _ = keras.datasets.imdb.load_data()
# Retrieve the word index file mapping words to indices
word_index = keras.datasets.imdb.get_word_index()
# Reverse the word index to obtain a dict mapping indices to words
inverted_word_index = dict((i, word) for (word, i) in word_index.items())
# Decode the first sequence in the dataset
decoded_sequence = " ".join(inverted_word_index[i] for i in x_train[0])
""" .

"DESCRIPTION.The code returns a tensor with the value 0.25." <EXPLAINS> """CODE.mae(x, y)
    tensor(0.2500)""" .

"DESCRIPTION.The code returns the image data format used by Keras as 'channels_first'." <EXPLAINS> """CODE.keras.backend.image_data_format()
'channels_first'
""" .

"DESCRIPTION.The code returns the resolution of time in seconds." <EXPLAINS> """CODE.Resolution.get_reso('second')
Resolution.get_reso('second') == Resolution.RESO_SEC""" .

"DESCRIPTION.The code returns the value of the epsilon constant used in Keras backend." <EXPLAINS> """CODE.keras.backend.epsilon()
1e-07
""" .

"DESCRIPTION.The code reverses the list [1, 2, 3]." <EXPLAINS> """CODE.plist([1, 2, 3]).reverse()
reversed(plist([1, 2, 3]))""" .

"DESCRIPTION.The code reverses the order of elements in a deque." <EXPLAINS> """CODE.pdeque([1, 2, 3]).reverse()
reversed(pdeque([1, 2, 3]))""" .

"DESCRIPTION.The code rounds the numbers up to the nearest integer." <EXPLAINS> "CODE.tf.math.ceil([-1.7, -1.5, -0.2, 0.2, 1.5, 1.7, 2.0])" .

"DESCRIPTION.The code runs a HTTP server using uvicorn with specified handlers for key-value store operations and routing." <EXPLAINS> """CODE.import uvicorn
uvicorn.run(HTTPProxy(kv_store_actor_handle, router_handle))""" .

"DESCRIPTION.The code runs a function `run_me` for 100 iterations, where it sleeps for 1 second in each iteration and logs the values \"hello\" as \"world\" and \"ray\" as \"tune\" using the `track.log` function. The function is executed using Ray Tune for analysis." <EXPLAINS> """CODE.import time
from ray import tune

def run_me(config):
    for iter in range(100):
        time.sleep(1)
        tune.report(hello="world", ray="tune")

analysis = tune.run(run_me)""",
        """CODE.import time
from ray import tune
from ray.tune import track

def run_me(config):
    for iter in range(100):
        time.sleep(1)
        track.log(hello="world", ray="tune")

analysis = tune.run(run_me)""" .

"DESCRIPTION.The code runs a hyperparameter tuning optimization using Ray Tune. It defines a search space for two parameters, \"parameter_1\" and \"parameter_2\", and uses the WandbLoggerCallback to log the optimization results to Weights and Biases platform." <EXPLAINS> """CODE.from ray.tune.logger import DEFAULT_LOGGERS
from ray.tune.integration.wandb import WandbLoggerCallback
tune.run(
    train_fn,
    config={
        # define search space here
        "parameter_1": tune.choice([1, 2, 3]),
        "parameter_2": tune.choice([4, 5, 6]),
    },
    callbacks=[WandbLoggerCallback(
        project="Optimization_Project",
        api_key_file="/path/to/file",
        log_config=True)])""" .

"DESCRIPTION.The code runs a query using a mock DAO object with a condition that the specified string contains 'IN (1, 2, 4, 5)', and returns a mock result from the query." <EXPLAINS> "CODE.mock_dao.RunQuery(StrContains('IN (1, 2, 4, 5)')).AndReturn(mock_result)" .

"DESCRIPTION.The code runs a training experiment called \"my_exp\" on a trainable model and saves the results to a local directory. It then analyzes the results of the experiment stored in the specified path." <EXPLAINS> """CODE.tune.run(my_trainable, name="my_exp", local_dir="~/tune_results")
analysis = ExperimentAnalysis(
    experiment_path="~/tune_results/my_exp")""" .

"DESCRIPTION.The code runs the 'python --version' command using the operating system." <EXPLAINS> "CODE.DocBuilder()._run_os('python', '--version')" .

"DESCRIPTION.The code samples involve creating instances of RelaxedBernoulli distributions with different temperatures and parameters, and then sampling from these distributions. The RelaxBernoulli distribution is a relaxed version of the Bernoulli distribution which allows for sampling from a continuous distribution that approximates the Bernoulli distribution." <EXPLAINS> """CODE.temperature = 0.5
p = [0.1, 0.5, 0.4]
dist = RelaxedBernoulli(temperature, probs=p)


temperature = 0.5
logits = [-2, 2, 0]
dist = RelaxedBernoulli(temperature, logits=logits)


temperature = 0.5
logits = [-2, 2, 0]
dist = Logistic(logits/temperature, 1./temperature)
samples = dist.sample()
sigmoid_samples = tf.sigmoid(samples)
# sigmoid_samples has the same distribution as samples from
# RelaxedBernoulli(temperature, logits=logits)


temperature = 1e-5
logits = [-2, 2, 0]
dist = RelaxedBernoulli(temperature, logits=logits)


temperature = 100
logits = [-2, 2, 0]
dist = RelaxedBernoulli(temperature, logits=logits)
""" .

"DESCRIPTION.The code saves a Keras model to a file, deletes the existing model, and then loads the saved model back into memory." <EXPLAINS> """CODE.from keras.models import load_model

model.save('my_model.h5')  # creates a HDF5 file 'my_model.h5'
del model  # deletes the existing model

# returns a compiled model
# identical to the previous one
model = load_model('my_model.h5')
""" .

"DESCRIPTION.The code saves a feature space model to a file named \"myfeaturespace.keras\" and then reloads the saved model back into memory." <EXPLAINS> """CODE.feature_space.save("myfeaturespace.keras")
reloaded_feature_space = keras.models.load_model("myfeaturespace.keras")
""" .

"DESCRIPTION.The code saves a list of beat times to a CSV file named 'beat_times.csv'." <EXPLAINS> "CODE.librosa.output.times_csv('beat_times.csv', beats)" .

"DESCRIPTION.The code saves a machine learning model to a HDF5 file, deletes the existing model, and then loads the saved model back into memory." <EXPLAINS> """CODE.from keras.models import load_model

model.save('my_model.h5')  # creates a HDF5 file 'my_model.h5'
del model  # deletes the existing model

# returns a compiled model
# identical to the previous one
model = load_model('my_model.h5')
""" .

"DESCRIPTION.The code saves a model to a HDF5 file, deletes the existing model, and then loads the saved model from the file." <EXPLAINS> """CODE.from keras.models import load_model

model.save('my_model.h5')  # creates a HDF5 file 'my_model.h5'
del model  # deletes the existing model

# returns a compiled model
# identical to the previous one
model = load_model('my_model.h5')
""" .

"DESCRIPTION.The code saves a model to a specified path, reloads the model from the saved artifact, and then makes predictions using the reloaded model on the given inputs." <EXPLAINS> """CODE.model.export("path/to/artifact")
reloaded_layer = TFSMLayer("path/to/artifact")
outputs = reloaded_layer(inputs)
""" .

"DESCRIPTION.The code saves a neural network model to a HDF5 file named 'my_model.h5', deletes the existing model from memory, and then loads the saved model back into memory." <EXPLAINS> """CODE.from keras.models import load_model

model.save('my_model.h5')  # creates a HDF5 file 'my_model.h5'
del model  # deletes the existing model

# returns a compiled model
# identical to the previous one
model = load_model('my_model.h5')
""" .

"DESCRIPTION.The code saves a reduction object using the pickle module." <EXPLAINS> """CODE.pickler.save(Reduce(*reduction))
pickler.save_reduce(*reduction, obj=reduction)""" .

"DESCRIPTION.The code saves a trained Keras model to a file, deletes the existing model, and then loads the saved model back into memory." <EXPLAINS> """CODE.from keras.models import load_model

model.save('my_model.h5')  # creates a HDF5 file 'my_model.h5'
del model  # deletes the existing model

# returns a compiled model
# identical to the previous one
model = load_model('my_model.h5')
""" .

"DESCRIPTION.The code saves a trained model to a HDF5 file ('my_model.h5'), then deletes the existing model. Finally, it loads the saved model from the HDF5 file." <EXPLAINS> """CODE.from keras.models import load_model

model.save('my_model.h5')  # creates a HDF5 file 'my_model.h5'
del model  # deletes the existing model

# returns a compiled model
# identical to the previous one
model = load_model('my_model.h5')
""" .

"DESCRIPTION.The code saves a trained model to a HDF5 file named 'my_model.h5', deletes the existing model from memory, and then loads the saved model back into memory." <EXPLAINS> """CODE.from keras.models import load_model

model.save('my_model.h5')  # creates a HDF5 file 'my_model.h5'
del model  # deletes the existing model

# returns a compiled model
# identical to the previous one
model = load_model('my_model.h5')
""" .

"DESCRIPTION.The code saves a trained model to a HDF5 file named 'my_model.h5', deletes the existing model, and then loads the saved model back into memory." <EXPLAINS> """CODE.model.save('my_model.h5')  # creates a HDF5 file 'my_model.h5'
del model  # deletes the existing model

model = load_model('my_model.h5')
""" .

"DESCRIPTION.The code saves a trained model to a file in HDF5 format, deletes the existing model from memory, and then loads the saved model back into memory, returning a compiled model identical to the previous one." <EXPLAINS> """CODE.from keras.models import load_model

model.save('my_model.h5')  # creates a HDF5 file 'my_model.h5'
del model  # deletes the existing model

# returns a compiled model
# identical to the previous one
model = load_model('my_model.h5')
""" .

"DESCRIPTION.The code saves a video of the current environment rendering with a specified frames per second and starting index for steps, then updates the step and episode indices, and finally resets the environment." <EXPLAINS> """CODE.save_video(
    env.render(),
    "videos",
    fps=env.metadata["render_fps"],
    step_starting_index=step_starting_index,
    episode_index=episode_index
)
step_starting_index = step_index + 1
episode_index += 1
env.reset()""" .

"DESCRIPTION.The code saves and loads the state dictionary of an embedding layer in a parallel context using PaddlePaddle's dygraph module." <EXPLAINS> """CODE.import paddle.fluid as fluid
with fluid.dygraph.guard():
    strategy=dygraph.parallel.prepare_context()
    emb = fluid.dygraph.Embedding([10, 10])
    emb = dygraph.parallel.DataParallel(emb, strategy)

    state_dict = emb.state_dict()
    fluid.save_dygraph( state_dict, "paddle_dy")

    para_state_dict, _ = fluid.load_dygraph( "paddle_dy")

    emb.set_dict( para_state_dict )""" .

"DESCRIPTION.The code saves and restores checkpoint files for a model during training using TensorFlow's eager execution mode." <EXPLAINS> """CODE.import tensorflow as tf
import tensorflow.contrib.eager as tfe
import os

checkpoint_directory = "/tmp/training_checkpoints"
checkpoint_prefix = os.path.join(checkpoint_directory, "ckpt")

root = tfe.Checkpoint(optimizer=optimizer, model=model)
root.restore(tf.train.latest_checkpoint(checkpoint_directory))
for _ in range(num_training_steps):
  optimizer.minimize( ... )
root.save(file_prefix=checkpoint_prefix)
""" .

"DESCRIPTION.The code saves checkpoints and summaries during training with the SingularMonitoredSession, running the training operation until the session should stop." <EXPLAINS> """CODE.saver_hook = CheckpointSaverHook(...)
summary_hook = SummaryHook(...)
with SingularMonitoredSession(hooks=[saver_hook, summary_hook]) as sess:
  while not sess.should_stop():
    sess.run(train_op)
""" .

"DESCRIPTION.The code saves the session into a file using the dill library, traces the execution of functions, dumps a dictionary and a lambda function, and logs the output to a file named 'output.txt'." <EXPLAINS> """CODE.import dill
dill.detect.trace(True)
dill.dump_session()
from dill import detect
D = {'a': 42, 'b': {'x': None}}
with detect.trace():
    dumps(D)
squared = lambda x: x**2
with detect.trace('output.txt', mode='w') as log:
    log("> D = %r", D)
    dumps(D)
    log("> squared = %r", squared)
    dumps(squared)
""" .

"DESCRIPTION.The code saves the training metrics such as loss and accuracy in a CSV file during the training process of a machine learning model." <EXPLAINS> """CODE.csv_logger = CSVLogger('training.log')
model.fit(X_train, Y_train, callbacks=[csv_logger])
""" .

"DESCRIPTION.The code saves the variable \"step\" to a checkpoint file at the specified location \"/tmp/ckpt\" using TensorFlow." <EXPLAINS> """CODE.step = tf.Variable(0, name="step")
checkpoint = tf.Checkpoint(step=step)
options = tf.CheckpointOptions(experimental_io_device="/job:localhost")
checkpoint.save("/tmp/ckpt", options=options)""" .

"DESCRIPTION.The code scans a table for rows with a specific prefix ('row_prefix') and retrieves values from specified columns ('c1', 'c2' from 'cfa' family and 'c3' from 'cfb' family). It retrieves data using two different methods, one using columns parameter and the other using separate parameters for cfa and cfb columns." <EXPLAINS> """CODE.
ds1 = table.scan_prefix("row_prefix", columns=[("cfa", "c1"),
                                               ("cfa", "c2"),
                                               ("cfb", "c3")])
ds2 = table.scan_prefix("row_prefix", cfa=["c1", "c2"], cfb="c3")
""" .

"DESCRIPTION.The code scans a table in parallel for rows that have a specific prefix and retrieves specific columns from those rows. Two datasets (ds1 and ds2) are created with different column specifications." <EXPLAINS> """CODE.table = # ...
ds1 = table.parallel_scan_prefix("row_prefix", columns=[("cfa", "c1"),
                                                        ("cfa", "c2"),
                                                        ("cfb", "c3")])
ds2 = table.parallel_scan_prefix("row_prefix", cfa=["c1", "c2"], cfb="c3")
""" .

"DESCRIPTION.The code scans the cache directory for information related to huggingface models and datasets." <EXPLAINS> """CODE.from huggingface_hub import scan_cache_dir

hf_cache_info = scan_cache_dir()
""" .

"DESCRIPTION.The code schedules commits every 10 minutes for a dataset repository named \"test_scheduler\" using the Hugging Face Hub. It opens a CSV file located in a watched folder, writes \"first line\" to the file, and later writes \"second line\" to the same file." <EXPLAINS> """CODE.from pathlib import Path
from huggingface_hub import CommitScheduler

# Scheduler uploads every 10 minutes
csv_path = Path("watched_folder/data.csv")
CommitScheduler(repo_id="test_scheduler", repo_type="dataset", folder_path=csv_path.parent, every=10)

with csv_path.open("a") as f:
...     f.write("first line")

# Some time later (...)
with csv_path.open("a") as f:
...     f.write("second line")
""" .

"DESCRIPTION.The code searches for occurrences of alphabetic words in a given string and prints each match found." <EXPLAINS> """CODE.wd = Word(alphas)
for match in locatedExpr(wd).searchString("ljsdf123lksdjjf123lkkjj1222"):
    print(match)""" .

"DESCRIPTION.The code segment is used for measuring the time taken to load training data." <EXPLAINS> """CODE.with self.profile('load training data'):
    # load training data code""" .

"DESCRIPTION.The code segment is used to load training data and the process is being timed using a profiling mechanism." <EXPLAINS> """CODE.with self.profile('load training data'):
    # load training data code""" .

"DESCRIPTION.The code segment performs groupby operation on the 'gender' column of the DataFrame 'df' and computes the count of unique values in the grouped data." <EXPLAINS> """CODE.df = pd.DataFrame({
    'gender': ['male', 'male', 'female', 'male', 'female', 'male'],
    'education': ['low', 'medium', 'high', 'low', 'high', 'low'],
    'country': ['US', 'FR', 'US', 'FR', 'FR', 'FR']
})

df.groupby('gender').value_counts()
df.groupby('gender').value_counts(ascending=True)
df.groupby('gender').value_counts(normalize=True)
df.groupby('gender', as_index=False).value_counts()
df.groupby('gender', as_index=False).value_counts(normalize=True)""" .

"DESCRIPTION.The code segment performs time lag filtering on a given dataset by converting it to lag domain, applying a filtering function, and then converting it back to recurrence domain. Additionally, it applies a diagonal median filtering with specific parameters to filter a given dataset." <EXPLAINS> """CODE.data_tl = librosa.segment.recurrence_to_lag(data)
data_filtered_tl = function(data_tl)
data_filtered = librosa.segment.lag_to_recurrence(data_filtered_tl)

diagonal_median = librosa.segment.timelag_filter(median_filter)
rec_filtered = diagonal_median(rec, size=(1, 5), mode='mirror')""" .

"DESCRIPTION.The code segment utilizes the GPTSw3Tokenizer to tokenize the input text (\"Svenska Ã¤r kul!\") and returns the corresponding input IDs." <EXPLAINS> """CODE.
from transformers import GPTSw3Tokenizer
tokenizer = GPTSw3Tokenizer.from_pretrained("AI-Sweden/gpt-sw3-126m")
tokenizer("Svenska Ã¤r kul!")['input_ids']
[1814, 377, 3617, 63504]
""" .

"DESCRIPTION.The code segments are performing the operation of binning the input array into discrete intervals based on specified criteria, such as number of bins or labels output." <EXPLAINS> """CODE.cut(np.array([.2, 1.4, 2.5, 6.2, 9.7, 2.1]), 3, retbins=True)
cut(np.ones(5), 4, labels=False)""" .

"DESCRIPTION.The code segments audio data into 10 segments and creates annotations for each segment." <EXPLAINS> """CODE.
boundaries = librosa.segment.agglomerative(data, k=10)
boundary_times = librosa.frames_to_time(boundaries, sr=sr, hop_length=hop_length)
time_start, time_end = boundaries[:-1], boundaries[1:]
labels = ['Segment #%03d' % i for i in range(len(time_start))]
librosa.output.annotation('segments.csv', time_start, time_end, annotations=annotations)
""" .

"DESCRIPTION.The code segments create RowPartition objects with different row partitioning methods based on the provided input values." <EXPLAINS> """CODE.p1 = RowPartition.from_row_lengths([4, 0, 3, 1, 0])
p2 = RowPartition.from_row_splits([0, 4, 4, 7, 8, 8])
p3 = RowPartition.from_row_starts([0, 4, 4, 7, 8], nvals=8)
p4 = RowPartition.from_row_limits([4, 4, 7, 8, 8])
p5 = RowPartition.from_value_rowids([0, 0, 0, 0, 2, 2, 2, 3], nrows=5)
""" .

"DESCRIPTION.The code selects and executes a specific code block based on the platform. It passes arguments to the selected code block and executes it." <EXPLAINS> """CODE.def cpu_code(*args): ...
def tpu_code(*args): ...
def other_platforms_code(*args): ...
res = platform_dependent(*args, cpu=cpu_code, tpu=tpu_code,
                         default=other_platforms_code)
""" .

"DESCRIPTION.The code selects elements from the input array based on the specified indices and returns the selected elements in a new array. The first output array contains the rows selected based on the indices provided in data_index, while the second output array contains the elements selected along the specified dimension (dim=1) based on the indices." <EXPLAINS> """CODE.import paddle
import paddle.fluid as fluid
import numpy as np

data = np.array([[1.0, 2.0, 3.0, 4.0],
                 [5.0, 6.0, 7.0, 8.0],
                 [9.0, 10.0, 11.0, 12.0]])
data_index = np.array([0, 1, 1]).astype('int32')

with fluid.dygraph.guard():
    x = fluid.dygraph.to_variable(data)
    index = fluid.dygraph.to_variable(data_index)
    out_z1 = fluid.layers.index_select(x, index)
    print(out_z1.numpy())
    #[[1. 2. 3. 4.]
    # [5. 6. 7. 8.]
    # [5. 6. 7. 8.]]
    out_z2 = fluid.layers.index_select(x, index, dim=1)
    print(out_z2.numpy())
    #[[ 1.  2.  2.]
    # [ 5.  6.  6.]
    # [ 9. 10. 10.]]""" .

"DESCRIPTION.The code sends a JSON response with a key-value pair \"k\": \"v\" using the provided scope, receive, and send functions." <EXPLAINS> "CODE.await JSONResponse({\"k\": \"v\"})(scope, receive, send)" .

"DESCRIPTION.The code sends a POST request using the requests library and checks the response for any errors. If an error is encountered, it raises an HfHubHTTPError and prints the formatted error message along with additional details returned by the server. It appends a specific message to the error and raises it again." <EXPLAINS> """CODE.        import requests
        from huggingface_hub.utils import hf_raise_for_status, HfHubHTTPError

        response = requests.post(...)
        try:
            hf_raise_for_status(response)
        except HfHubHTTPError as e:
            print(str(e)) # formatted message
            e.request_id, e.server_message # details returned by server

            # Complete the error message with additional information once it's raised
            e.append_to_message("
`create_commit` expects the repository to exist.")
            raise
""" .

"DESCRIPTION.The code sends and receives data between parallel processes using PaddlePaddle. If the rank of the current process is 0, it sends a tensor [7, 8, 9] to process 1. If the rank is not 0, it receives a tensor from process 0 and stores it in a variable. Finally, it converts the received tensor to a numpy array and stores it in the variable 'out'." <EXPLAINS> """CODE.import paddle
from paddle.distributed import init_parallel_env
init_parallel_env()
if paddle.distributed.ParallelEnv().rank == 0:
    data = paddle.to_tensor([7, 8, 9])
    paddle.distributed.send(data, dst=1)
else:
    data = paddle.to_tensor([1,2,3])
    paddle.distributed.recv(data, src=0)
out = data.numpy()""" .

"DESCRIPTION.The code sends telemetry data using the Hugging Face Hub library. It includes functionality to send telemetry with and without library information, to different topics, and with additional data such as user agents." <EXPLAINS> """CODE.from huggingface_hub.utils import send_telemetry

# Send telemetry without library information
send_telemetry("ping")

# Send telemetry to subtopic with library information
send_telemetry("gradio/local_link", library_name="gradio", library_version="3.22.1")

# Send telemetry with additional data
send_telemetry(
...     topic="examples",
...     library_name="transformers",
...     library_version="4.26.0",
...     user_agent={"pipeline": "text_classification", "framework": "flax"},
... )
""" .

"DESCRIPTION.The code separates the input string into multiple words and stores them in a list." <EXPLAINS> """CODE.patt = OneOrMore(Word(alphas))
result = patt.parseString("sldkj lsdkj sldkj")
print(type(result), result)
result_list = result.asList()
print(type(result_list), result_list)""" .

"DESCRIPTION.The code serializes activation functions tanh and sigmoid from tf.keras.activations and also attempts to serialize the string 'abcd'." <EXPLAINS> """CODE.tf.keras.activations.serialize(tf.keras.activations.tanh)
tf.keras.activations.serialize(tf.keras.activations.sigmoid)
tf.keras.activations.serialize('abcd')""" .

"DESCRIPTION.The code serializes activation functions, such as tanh and sigmoid, using the tf.keras.activations.serialize function." <EXPLAINS> """CODE.tf.keras.activations.serialize(tf.keras.activations.tanh)
tf.keras.activations.serialize(tf.keras.activations.sigmoid)
tf.keras.activations.serialize('abcd')""" .

"DESCRIPTION.The code serializes an ONNX program generated from a PyTorch model and saves it to a destination file in the ONNX format using Protobuf serialization." <EXPLAINS> """CODE.class ProtobufONNXProgramSerializer:
    def serialize(
        self, onnx_program: torch.onnx.ONNXProgram, destination: io.BufferedIOBase
    ) -> None:
        destination.write(onnx_program.model_proto.SerializeToString())

model = MyModel()
arg1 = torch.randn(2, 2, 2)  # positional input 1
torch.onnx.dynamo_export(model, arg1).save(
    destination="exported_model.onnx",
    serializer=ProtobufONNXProgramSerializer(),
)""" .

"DESCRIPTION.The code serializes different activation functions in TensorFlow Keras." <EXPLAINS> """CODE.tf.keras.activations.serialize(tf.keras.activations.tanh)
tf.keras.activations.serialize(tf.keras.activations.sigmoid)
tf.keras.activations.serialize('abcd')""" .

"DESCRIPTION.The code serializes different activation functions provided by TensorFlow's Keras, including tanh, sigmoid, and 'abcd'." <EXPLAINS> """CODE.tf.keras.activations.serialize(tf.keras.activations.tanh)
tf.keras.activations.serialize(tf.keras.activations.sigmoid)
tf.keras.activations.serialize('abcd')""" .

"DESCRIPTION.The code sets a connection timeout for 2 seconds for connecting to a specified URL like `www.google.com` and a read timeout of 7 seconds. It then creates an HTTP connection pool with the specified URL and timeouts, and makes a request using this pool." <EXPLAINS> """CODE.timeout = urllib3.util.Timeout(connect=2.0, read=7.0)
pool = HTTPConnectionPool('www.google.com', 80, timeout=timeout)
pool.request(...)
""" .

"DESCRIPTION.The code sets a flag related to eager deletion of tensors in the PaddlePaddle deep learning framework to 1.0 GB." <EXPLAINS> """CODE.import paddle.fluid as fluid
fluid.set_flags({'FLAGS_eager_delete_tensor_gb': 1.0})""" .

"DESCRIPTION.The code sets a private attribute called '_my_attribute' with the specified line number." <EXPLAINS> "CODE.self.attr('_my_attribute', lineno=lineno)" .

"DESCRIPTION.The code sets a specific XLA environment flag '--xla_gpu_enable_fast_min_max' to false for a function named testFoo." <EXPLAINS> """CODE.@set_xla_env_flag(flag='--xla_gpu_enable_fast_min_max=false')
def testFoo(self):
    ...""" .

"DESCRIPTION.The code sets a timeout value of nearly 3.9 seconds for the mock data access object." <EXPLAINS> "CODE.mock_dao.SetTimeout((IsAlmost(3.9)))" .

"DESCRIPTION.The code sets an environment variable \"FOO\" to \"bar\", prints out the current environment variables, changes the value of \"FOO\" to \"new_bar\" within a cleared environment, and prints out the updated value of \"FOO\"." <EXPLAINS> """CODE.import os
from accelerate.utils import clear_environment

os.environ["FOO"] = "bar"
with clear_environment():
    print(os.environ)
    os.environ["FOO"] = "new_bar"
    print(os.environ["FOO"])

""" .

"DESCRIPTION.The code sets and prints the random seed value in a PaddlePaddle program. The default random seed is set to 0, but it can be modified using the `global_seed` method to control the randomness of the program's operations." <EXPLAINS> """CODE.import paddle.fluid as fluid
prog = fluid.default_main_program()
print(prog.random_seed)
## 0
## the default random seed is 0

prog.global_seed(102)
prog1 = fluid.default_main_program()
print(prog1.random_seed)
## 102
## the random seed is 102""" .

"DESCRIPTION.The code sets and retrieves the value of the epsilon parameter used in the Keras backend." <EXPLAINS> """CODE.    from keras import backend as K
    K.epsilon()
    K.set_epsilon(1e-05)
    K.epsilon()
""" .

"DESCRIPTION.The code sets environment variables for the PL Trainer with the number of GPUs as 42 and a specific value for another variable. It then parses the environment variables for the Trainer and returns a Namespace object with the specified GPU value." <EXPLAINS> """CODE.import os
os.environ["PL_TRAINER_DEVICES"] = '42'
os.environ["PL_TRAINER_BLABLABLA"] = '1.23'
_parse_env_variables(Trainer)
Namespace(devices=42)""",
        """CODE.import os
os.environ["PL_TRAINER_GPUS"] = '42'
os.environ["PL_TRAINER_BLABLABLA"] = '1.23'
parse_env_variables(Trainer)
Namespace(gpus=42)""" .

"DESCRIPTION.The code sets environment variables for the PyTorch Lightning Trainer class and parses them to create a Namespace object with specified attributes." <EXPLAINS> """CODE.from pytorch_lightning import Trainer
parse_env_variables(Trainer)
Namespace()
import os
os.environ["PL_TRAINER_GPUS"] = '42'
os.environ["PL_TRAINER_BLABLABLA"] = '1.23'
parse_env_variables(Trainer)
Namespace(gpus=42)
del os.environ["PL_TRAINER_GPUS"]""" .

"DESCRIPTION.The code sets the Fast Fourier Transform (FFT) library used by the librosa library to PyFFTW, a Python wrapper for the FFTW library." <EXPLAINS> """CODE.import pyfftw
librosa.set_fftlib(pyfftw.interfaces.numpy_fft)
librosa.set_fftlib()""" .

"DESCRIPTION.The code sets the data directory for librosa library, then loads two audio files named 'brahms' and 'vibeace' using librosa, with the 'hq' option set to True for the second file." <EXPLAINS> """CODE.import os
os.environ['LIBROSA_DATA_DIR'] = '/path/to/store/data'
import librosa
y, sr = librosa.load(librosa.example('brahms'))
y, sr = librosa.load(librosa.example('vibeace', hq=True))""" .

"DESCRIPTION.The code sets the data type for floating point numbers to 'float16' in the Keras backend." <EXPLAINS> """CODE.from keras import backend as K
K.set_floatx('float16')
""" .

"DESCRIPTION.The code sets the default data type for floating point numbers to be 'float16' in the Keras backend." <EXPLAINS> """CODE.from keras import backend as K
K.set_floatx('float16')
""" .

"DESCRIPTION.The code sets the default floating point precision for keras backend to float16, float32, and float64 respectively." <EXPLAINS> """CODE.tf.keras.backend.set_floatx('float16')
tf.keras.backend.set_floatx('float32')
tf.keras.backend.set_floatx('float64')""" .

"DESCRIPTION.The code sets the device to a custom CPU, creates a stream object, and synchronizes the stream." <EXPLAINS> """CODE.import paddle

paddle.set_device('custom_cpu')
s = paddle.device.Stream()
s.synchronize()""" .

"DESCRIPTION.The code sets the environment variable FOO to 'test' and then expands the path string 'variable FOO is $FOO'." <EXPLAINS> """CODE.os.environ['FOO']='test'
expand_path('variable FOO is $FOO')""" .

"DESCRIPTION.The code sets the epsilon value to 1e-05 in the Keras backend." <EXPLAINS> """CODE.    from keras import backend as K
    K.set_epsilon(1e-05)
""" .

"DESCRIPTION.The code sets the epsilon value to 1e-7 for the backend of the TensorFlow Keras library." <EXPLAINS> """CODE.tf.keras.backend.set_epsilon(1e-5)
tf.keras.backend.set_epsilon(1e-7)""" .

"DESCRIPTION.The code sets the global dtype policy to mixed precision and specifies a dense layer with output size 10." <EXPLAINS> """CODE.tf.keras.mixed_precision.global_policy()
tf.keras.layers.Dense(10).dtype_policy""" .

"DESCRIPTION.The code sets the global policy for mixed precision to 'mixed_float16', then retrieves the global policy. It creates a Dense layer with a default dtype policy. It creates another Dense layer with dtype 'float64' policy. Finally, it changes the global policy to 'float32'." <EXPLAINS> """CODE.tf.keras.mixed_precision.set_global_policy('mixed_float16')
tf.keras.mixed_precision.global_policy()
tf.keras.layers.Dense(10).dtype_policy
tf.keras.layers.Dense(10, dtype='float64').dtype_policy
tf.keras.mixed_precision.set_global_policy('float32')
""" .

"DESCRIPTION.The code sets the global policy for mixed precision training to 'mixed_float16', checks the global policy, creates a dense layer with a dtype policy of mixed float16, creates another dense layer with a dtype policy of float64, and finally changes the global policy to 'float32'." <EXPLAINS> """CODE.tf.keras.mixed_precision.set_global_policy('mixed_float16')
tf.keras.mixed_precision.global_policy()
tf.keras.layers.Dense(10).dtype_policy
tf.keras.layers.Dense(10, dtype='float64').dtype_policy
tf.keras.mixed_precision.set_global_policy('float32')""" .

"DESCRIPTION.The code sets the image data format for the TensorFlow Keras backend to 'channels_first' and then to 'channels_last'." <EXPLAINS> """CODE.tf.keras.backend.set_image_data_format('channels_first')
tf.keras.backend.set_image_data_format('channels_last')""" .

"DESCRIPTION.The code sets the image data format to 'channels_last' for the Keras backend." <EXPLAINS> """CODE.from keras import backend as K
K.set_image_data_format('channels_last')
""" .

"DESCRIPTION.The code sets the mode of the Request object to \"rI\" for reading and \"wv\" for writing." <EXPLAINS> """CODE.Request.Mode("rI")
Request.Mode("wv")""" .

"DESCRIPTION.The code sets the noise alpha value of the embed tokens in a model to 0.1 and registers a forward hook function called neftune_post_forward_hook." <EXPLAINS> """CODE.model = ...
model.embed_tokens.neftune_noise_alpha = 0.1
model.embed_tokens.register_forward_hook(neftune_post_forward_hook)
""" .

"DESCRIPTION.The code sets the number of jobs for parallel processing tasks based on the number of available CPU cores, ensuring that at least one core is reserved for the system." <EXPLAINS> """CODE.from sklearn.utils import _get_n_jobs
_get_n_jobs(4)
jobs = _get_n_jobs(-2)
assert jobs == max(cpu_count() - 1, 1)
_get_n_jobs(0)
""" .

"DESCRIPTION.The code sets the page configuration for a streamlit web application with the title \"Ex-stream-ly Cool App\", an icon of ð§, a wide layout, and an expanded initial sidebar state." <EXPLAINS> """CODE.st.set_page_config(
    page_title="Ex-stream-ly Cool App",
    page_icon="ð§",
    layout="wide",
    initial_sidebar_state="expanded",
)""" .

"DESCRIPTION.The code sets the style properties of a pandas DataFrame to have white text color and right alignment." <EXPLAINS> "CODE.df.style.set_properties(color=\"white\", align=\"right\")" .

"DESCRIPTION.The code sets the title of a visual element to \"This is a title\"." <EXPLAINS> "CODE.st.title('This is a title')" .

"DESCRIPTION.The code sets the traffic distribution for a service named \"service-name\" between two backends, with \"backend:v1\" receiving 50% of the traffic and \"backend:v2\" receiving 50% of the traffic." <EXPLAINS> """CODE.serve.set_traffic("service-name", {
    "backend:v1": 0.5,
    "backend:v2": 0.5
})""" .

"DESCRIPTION.The code sets the value at index 1 of property 'c' in object m1 to 17." <EXPLAINS> """CODE.m1 = m(a=5, b=6, c=v(1, 2))
m1.set_in(('c', 1), 17)""" .

"DESCRIPTION.The code sets up a Jinja2 environment with autoescaping enabled for HTML and XML files by default." <EXPLAINS> """CODE.from jinja2 import Environment, select_autoescape
env = Environment(autoescape=select_autoescape(
    enabled_extensions=('html', 'xml'),
    default_for_string=True,
))

from jinja2 import Environment, select_autoescape
env = Environment(autoescape=select_autoescape(
    disabled_extensions=('txt',),
    default_for_string=True,
    default=True,
))
""" .

"DESCRIPTION.The code sets up a PB2 (Population Based Training) instance with specified parameters for hyperparameter optimization. It then uses the PB2 scheduler to run a hyperparameter tuning experiment with 8 samples." <EXPLAINS> """CODE.pb2 = PB2(
    time_attr="timesteps_total",
    metric="episode_reward_mean",
    mode="max",
    perturbation_interval=10000,
    hyperparam_mutations={
        # These must be continuous, currently a limitation.
        "factor_1": lambda: random.uniform(0.0, 20.0),
    })
tune.run({...}, num_samples=8, scheduler=pb2)""" .

"DESCRIPTION.The code sets up a PGTrainer object for training an agent in the CartPole environment. It initializes the trainer with a PolicyServerInput and runs the training loop indefinitely. It then creates a PolicyClient for interacting with the trained agent, starting an episode, getting actions based on observations, and logging rewards during training." <EXPLAINS> """CODE.pg = PGTrainer(
...     env="CartPole-v0", config={
...         "input": lambda ioctx:
...             PolicyServerInput(ioctx, addr, port),
...         "num_workers": 0,  # Run just 1 server, in the trainer.
...     }
while True:
        pg.train()

client = PolicyClient("localhost:9900", inference_mode="local")
eps_id = client.start_episode()
action = client.get_action(eps_id, obs)
...
client.log_returns(eps_id, reward)
...
client.log_returns(eps_id, reward)""" .

"DESCRIPTION.The code sets up a PoolManager for making HTTP requests with different retry configurations, including setting up retries for connection, reading, and redirecting. The code then makes three HTTP GET requests to 'http://example.com/' with different retry configurations set." <EXPLAINS> """CODE.retries = Retry(connect=5, read=2, redirect=5)
http = PoolManager(retries=retries)
response = http.request('GET', 'http://example.com/')

response = http.request('GET', 'http://example.com/', retries=Retry(10))

response = http.request('GET', 'http://example.com/', retries=False)
""" .

"DESCRIPTION.The code sets up a TensorFlow cluster with workers and parameter servers using the TF_CONFIG environment variable. It then initializes a MultiWorkerMirroredStrategy for distributed training. Depending on the task type (worker or ps), the code performs different actions specific to each type within the if-else blocks." <EXPLAINS> """CODE.os.environ['TF_CONFIG'] = json.dumps({
  'cluster': {
      'worker': ["localhost:12345", "localhost:23456"],
      'ps': ["localhost:34567"]
  },
  'task': {'type': 'worker', 'index': 0}
})

# This implicitly uses TF_CONFIG for the cluster and current task info.
strategy = tf.distribute.experimental.MultiWorkerMirroredStrategy()

...

if strategy.cluster_resolver.task_type == 'worker':
  # Perform something that's only applicable on workers. Since we set this
  # as a worker above, this block will run on this particular instance.
elif strategy.cluster_resolver.task_type == 'ps':
  # Perform something that's only applicable on parameter servers. Since we
  # set this as a worker above, this block will not run on this particular
  # instance.
""" .

"DESCRIPTION.The code sets up a URIBuilder object and adds credentials for a user 'root' with a password 's3crete' and then removes the password for the same user 'root'." <EXPLAINS> """CODE.URIBuilder().add_credentials('root', 's3crete')
URIBuilder().add_credentials('root', None)""" .

"DESCRIPTION.The code sets up a URIBuilder object and adds credentials with the username 'root' and password 's3crete'. Then it sets up another URIBuilder object and adds credentials with the username 'root' but with no password provided." <EXPLAINS> """CODE.URIBuilder().add_credentials('root', 's3crete')
URIBuilder().add_credentials('root', None)""" .

"DESCRIPTION.The code sets up a custom CPU device using PaddlePaddle framework and creates a stream on that device to perform queries." <EXPLAINS> """CODE.import paddle
paddle.set_device('custom_cpu')
s = paddle.device.Stream()
s.query()""" .

"DESCRIPTION.The code sets up a custom CPU device using PaddlePaddle, creates a stream, records an event e1 in the stream, and creates and records another event e2 using a separate event object." <EXPLAINS> """CODE.import paddle
paddle.set_device('custom_cpu')
s = paddle.device.Stream()
e1 = s.record_event()

e2 = paddle.device.Event()
s.record_event(e2)""" .

"DESCRIPTION.The code sets up a dispatch server and a worker server using TensorFlow's experimental service module. It creates a dataset with range 10 and distributes it using the dispatch server. Finally, it prints the dataset as a list after iterating over it using numpy iterator. The dispatch server is also set up to join at port 5050." <EXPLAINS> """CODE.dispatcher = tf.data.experimental.service.DispatchServer(port=0)
dispatcher_address = dispatcher.target.split("://")[1]
worker = tf.data.experimental.service.WorkerServer(
    port=0, dispatcher_address=dispatcher_address)
dataset = tf.data.Dataset.range(10)
dataset = dataset.apply(tf.data.experimental.service.distribute(
    processing_mode="parallel_epochs", service=dispatcher.target))
print(list(dataset.as_numpy_iterator()))

dispatcher = tf.data.experimental.service.DispatchServer(port=5050)
dispatcher.join()""" .

"DESCRIPTION.The code sets up a distributed RPC framework using PyTorch to create a worker1 and worker0 environment with a world size of 2. Then, it defines a remote linear module with input dimensions of 20 and output dimensions of 30. It sends a random tensor of size 128x20 to the remote linear module for processing and receives the processed result. Finally, it shuts down the RPC connections for both worker1 and worker0." <EXPLAINS> """CODE.class MyModule(nn.Module):
    def forward(input):
        return input + 1

module_cls = MyModule

import torch
import torch.distributed.rpc as rpc
from torch import nn, Tensor
from torch.distributed.nn.api.remote_module import RemoteModule

rpc.init_rpc("worker0", rank=0, world_size=2)
remote_linear_module = RemoteModule(
    "worker1", nn.Linear, args=(20, 30),
)
input = torch.randn(128, 20)
ret_fut = remote_linear_module.forward_async(input)
ret = ret_fut.wait()
rpc.shutdown()

import torch
import torch.distributed.rpc as rpc

rpc.init_rpc("worker1", rank=1, world_size=2)
rpc.shutdown()""" .

"DESCRIPTION.The code sets up a distributed training environment using data parallelism and model parallelism on 8 devices. It creates a mesh with 2 devices for data parallelism and 4 devices for model parallelism. The code also specifies how the variables of the `Dense` and `Conv2D` layers should be sharded across the devices based on the layout map. Finally, it creates a model, compiles it, and fits the data using the specified distributed training setup." <EXPLAINS> """CODE.devices = list_devices()    # Assume there are 8 devices.

# Create a mesh with 2 devices for data parallelism and 4 devices for
# model parallelism.
device_mesh = DeviceMesh(shape=(2, 4), axis_names=('batch', 'model'),
                         devices=devices)
# Create a layout map that shard the `Dense` layer and `Conv2D`
# layer variables on the last dimension.
# Based on the `device_mesh`, this means the variables
# will be split across 4 devices. Any other variable that doesn't
# match any key in the layout map will be fully replicated.
layout_map = LayoutMap(device_mesh)
layout_map['dense.*kernel'] = (None, 'model')
layout_map['dense.*bias'] = ('model',)
layout_map['conv2d.*kernel'] = (None, None, None, 'model')
layout_map['conv2d.*bias'] = ('model',)

distribution = ModelParallel(device_mesh=device_mesh,
                             layout_map=layout_map,
                             batch_dim_name='batch')
# Set the global distribution, or via `with distribution.scope():`
set_distribution(distribution)

model = model_creation()
model.compile()
model.fit(data)



# With only the shape change for the device mesh, the variables will be
# sharded across 8 devices instead of 4, which further reduces the memory
# footprint of variables on each of the device.
device_mesh = DeviceMesh(shape=(1, 8), axis_names=('batch', 'model'),
                         devices=devices)



model = create_model()
for v in model.variables:
    print(v.path)
""" .

"DESCRIPTION.The code sets up a factory function that creates a Requests Session with configured proxies for HTTP and HTTPS connections. It then sets this factory function as the default session factory using the huggingface_hub library. Finally, it obtains a session object using the configured factory function." <EXPLAINS> """CODE.import requests
from huggingface_hub import configure_http_backend, get_session

# Create a factory function that returns a Session with configured proxies
def backend_factory() -> requests.Session:
    session = requests.Session()
    session.proxies = {"http": "http://10.10.1.10:3128", "https": "https://10.10.1.11:1080"}
    return session

# Set it as the default session factory
configure_http_backend(backend_factory=backend_factory)

# In practice, this is mostly done internally in `huggingface_hub`
session = get_session()
""" .

"DESCRIPTION.The code sets up a learning rate scheduler that decreases the learning rate by a factor of 0.5 every 4 iterations. It then runs a training loop for 100 epochs, where it trains the model and validates the model's performance while updating the learning rate at each iteration." <EXPLAINS> """CODE.scheduler = ConstantLR(optimizer=self.opt, factor=0.5, total_iters=4)
for epoch in range(100):
    train(...)
    validate(...)
    scheduler.step()""" .

"DESCRIPTION.The code sets up a logger to log experiment statistics and results to a CSV file named \"logs\" with the experiment name \"my_exp_name\" using PyTorch Lightning. It then creates a trainer object with the logger for training PyTorch models." <EXPLAINS> """CODE.from pytorch_lightning import Trainer
from pytorch_lightning.loggers import CSVLogger
logger = CSVLogger("logs", name="my_exp_name")
trainer = Trainer(logger=logger)""" .

"DESCRIPTION.The code sets up a mock function that returns True for a specific type, and then uses that mock function to check if a given type matches the specified type \"foo.bar.Baz\"." <EXPLAINS> """CODE.mock_is_type.side_effect = make_is_type_mock("foo.bar.Baz")
mock_is_type(my_type, "foo.bar.Baz")""" .

"DESCRIPTION.The code sets up a model and trainer, runs a learning rate finder, inspects the results, determines the suggested learning rate, overwrites the learning rate, creates a new model with the suggested learning rate, and then trains the model with the new learning rate." <EXPLAINS> """CODE.# Setup model and trainer
model = MyModelClass(hparams)
trainer = pl.Trainer()

# Run lr finder
lr_finder = trainer.lr_find(model, ...)

# Inspect results
fig = lr_finder.plot(); fig.show()
suggested_lr = lr_finder.suggestion()

# Overwrite lr and create new model
hparams.lr = suggested_lr
model = MyModelClass(hparams)

# Ready to train with new learning rate
trainer.fit(model)
""" .

"DESCRIPTION.The code sets up a model training process where the optimizer uses Exponential Moving Average (EMA) weights for computing metrics. The training process includes swapping EMA weights during training and optionally saving model checkpoints with EMA weights." <EXPLAINS> """CODE.# Remember to set `use_ema=True` in the optimizer
optimizer = SGD(use_ema=True)
model.compile(optimizer=optimizer, loss=..., metrics=...)

# Metrics will be computed with EMA weights
model.fit(X_train, Y_train, callbacks=[SwapEMAWeights()])

# If you want to save model checkpoint with EMA weights, you can set
# `swap_on_epoch=True` and place ModelCheckpoint after SwapEMAWeights.
model.fit(
    X_train,
    Y_train,
    callbacks=[SwapEMAWeights(swap_on_epoch=True), ModelCheckpoint(...)]
)
""" .

"DESCRIPTION.The code sets up a multi-process runner for executing tasks. It defines a function proc_func that includes user code to be run and sets a breakpoint for debugging. Another function follow_ups is defined to sleep for 5 seconds before starting a single process for evaluation. The main portion of the code initiates a multi-process runner with specified parameters, starts a thread for follow-up tasks, and then starts the chief task in the process. The code finally waits for all processes to join before completion." <EXPLAINS> """CODE.def proc_func():
  # user code to be run
  import pdb; pdb.set_trace()

def follow_ups():
  time.sleep(5)
  mpr.start_single_process(
      task_type='evaluator',
      task_id=0)

mpr = multi_process_runner.MultiProcessRunner(
    proc_func,
    multi_worker_test_base.create_cluster_spec(
        has_chief=True, num_workers=1))
threading.Thread(target=follow_ups).start()
mpr.start_in_process_as(as_task_type='chief', as_task_id=0)
mpr.join()
""" .

"DESCRIPTION.The code sets up a neural network model using PaddlePaddle's static graph mode and IPU (Intel Processor Unit) strategy. It creates data input nodes, performs a basic operation on the data, configures the IPU strategy for training, sets up the graph configuration, and compiles the program for execution on the IPU." <EXPLAINS> """CODE.import paddle
import paddle.static as static

paddle.enable_static()

a = static.data(name='data', shape=[None, 1], dtype='int32')
b = a + 1
main_prog = static.default_main_program()

ipu_strategy = static.IpuStrategy()
ipu_strategy.set_graph_config(num_ipus=1, is_training=True, micro_batch_size=1)
ipu_strategy.set_pipelining_config(enable_pipelining=False, batches_per_step=1, enable_gradient_accumulation=False, accumulation_factor=1)
ipu_strategy.set_precision_config(enable_fp16=False)

ipu_compiled_program = static.IpuCompiledProgram(
    main_prog,
    ipu_strategy=ipu_strategy)""" .

"DESCRIPTION.The code sets up a storage context for managing file storage paths and configurations. It defines paths for storing experiment data, trial data, and checkpoints, and provides functionality for copying files to specific locations. It also includes functionality for automatically resolving storage paths." <EXPLAINS> """CODE.from ray.train._internal.storage import StorageContext
import os
os.environ["RAY_AIR_LOCAL_CACHE_DIR"] = "/tmp/ray_results"
storage = StorageContext(
    storage_path="mock://netloc/bucket/path?param=1",
    experiment_dir_name="exp_name",
)
storage.storage_filesystem   # Auto-resolved  # doctest: +ELLIPSIS
<pyarrow._fs._MockFileSystem object...
storage.experiment_fs_path
'bucket/path/exp_name'
storage.experiment_local_path
'/tmp/ray_results/exp_name'
storage.trial_dir_name = "trial_dir"
storage.trial_fs_path
'bucket/path/exp_name/trial_dir'
storage.trial_local_path
'/tmp/ray_results/exp_name/trial_dir'
storage.current_checkpoint_index = 1
storage.checkpoint_fs_path
'bucket/path/exp_name/trial_dir/checkpoint_000001'

from ray.train._internal.storage import StorageContext
import os
os.environ["RAY_AIR_LOCAL_CACHE_DIR"] = "/tmp/ray_results"
storage = StorageContext(
    storage_path=None,
    experiment_dir_name="exp_name",
)
storage.storage_path  # Auto-resolved
'/tmp/ray_results'
storage.storage_local_path
'/tmp/ray_results'
storage.experiment_local_path
'/tmp/ray_results/exp_name'
storage.experiment_fs_path
'/tmp/ray_results/exp_name'
storage.syncer is None
True
storage.storage_filesystem   # Auto-resolved  # doctest: +ELLIPSIS
<pyarrow._fs.LocalFileSystem object...

pyarrow.fs.copy_files(
    local_dir,
    os.path.join(storage.trial_fs_path, "subdir"),
    destination_filesystem=storage.filesystem
)
""" .

"DESCRIPTION.The code sets up a testing environment by stubbing out the 'exists' function in the 'os.path' module with a lambda function that always returns 1. It then clears all stubs that were set up." <EXPLAINS> """CODE.stubs = StubOutForTesting()
stubs.Set(os.path, 'exists', lambda x: 1)
stubs.UnsetAll()""" .

"DESCRIPTION.The code sets up a timer to stop training after a specific duration of time, which can be specified in different ways, such as using a string in the format \"days:hours:minutes:seconds\", a datetime.timedelta object, or a dictionary with weeks and days. The Trainer class from pytorch_lightning is used along with the Timer callback to force training to stop after the given time limit. The Timer object can also be used to query the training/validation/test time in seconds by specifying the type of time to be queried." <EXPLAINS> """CODE.from pytorch_lightning import Trainer
from pytorch_lightning.callbacks import Timer

# stop training after 12 hours
timer = Timer(duration="00:12:00:00")

# or provide a datetime.timedelta
from datetime import timedelta
timer = Timer(duration=timedelta(weeks=1))

# or provide a dictionary
timer = Timer(duration=dict(weeks=4, days=2))

# force training to stop after given time limit
trainer = Trainer(callbacks=[timer])

# query training/validation/test time (in seconds)
timer.time_elapsed("train")
timer.start_time("validate")
timer.end_time("test")""" .

"DESCRIPTION.The code sets up an IPU (Intelligent Processing Unit) strategy for executing operations in a static graph using PaddlePaddle." <EXPLAINS> """CODE.import paddle
import paddle.static as static

paddle.enable_static()

ipu_strategy = static.IpuStrategy()""" .

"DESCRIPTION.The code sets up an argument parser object and uses it to add arguments specific to a LightningDataModule. It then creates a LightningDataModule object based on the input arguments." <EXPLAINS> """CODE.parser = ArgumentParser(add_help=False)
parser = LightningDataModule.add_argparse_args(parser)
module = LightningDataModule.from_argparse_args(args)""" .

"DESCRIPTION.The code sets up an in-memory dataset for distributed training with PaddlePaddle framework and configures the send sleep seconds to 2 for the dataset." <EXPLAINS> """CODE.import paddle
paddle.enable_static()
dataset = paddle.distributed.InMemoryDataset()
dataset._set_fleet_send_sleep_seconds(2)""",
        """CODE.import paddle.base as base
dataset = base.DatasetFactory().create_dataset("InMemoryDataset")
dataset.set_fleet_send_sleep_seconds(2)""" .

"DESCRIPTION.The code sets up an in-memory dataset for distributed training with a batch size of 800." <EXPLAINS> """CODE.dataset = base.DatasetFactory().create_dataset("InMemoryDataset")
dataset.set_fleet_send_batch_size(800)""",
        "CODE.dataset.set_fleet_send_batch_size(800)",
        """CODE.import paddle
paddle.enable_static()
dataset = paddle.distributed.InMemoryDataset()
dataset._set_fleet_send_batch_size(800)""" .

"DESCRIPTION.The code sets up an in-memory dataset for distributed training with a sleep time of 2 seconds for sending data between nodes." <EXPLAINS> """CODE.import paddle
paddle.enable_static()
dataset = paddle.distributed.InMemoryDataset()
dataset._set_fleet_send_sleep_seconds(2)""",
        """CODE.import paddle.base as base
dataset = base.DatasetFactory().create_dataset("InMemoryDataset")
dataset.set_fleet_send_sleep_seconds(2)""" .

"DESCRIPTION.The code sets up an inference endpoint for text generation using the GPT-2 model on a PyTorch framework. The endpoint is created with specific settings such as using a CPU accelerator, AWS vendor, protected type, medium instance size, and c6i instance type in the us-east-1 region. The endpoint is currently in a 'pending' status before running text generation on it." <EXPLAINS> """CODE.from huggingface_hub import HfApi
api = HfApi()
create_inference_endpoint(
    "my-endpoint-name",
    repository="gpt2",
    framework="pytorch",
    task="text-generation",
    accelerator="cpu",
    vendor="aws",
    region="us-east-1",
    type="protected",
    instance_size="medium",
    instance_type="c6i"
)
endpoint
InferenceEndpoint(name='my-endpoint-name', status="pending",...)

# Run inference on the endpoint
endpoint.client.text_generation(...)
"..."
""" .

"DESCRIPTION.The code sets up configuration for JAX using absl library and runs the main function when the script is executed." <EXPLAINS> """CODE.from absl import app
import jax
...

if __name__ == '__main__':
  jax.config.config_with_absl()
  app.run(main)
""" .

"DESCRIPTION.The code sets up the folder containing checksums for a dataset, adds the checksum directory for when the dataset is imported, and defines a class for a custom dataset." <EXPLAINS> """CODE.
# Set-up the folder containing the 'my_dataset.txt' checksums.
checksum_dir = os.path.join(os.path.dirname(__file__), 'checksums/')
checksum_dir = os.path.normpath(checksum_dir)

# Add the checksum dir (will be executed when the user import your dataset)
tfds.download.add_checksums_dir(checksum_dir)

class MyDataset(tfds.core.DatasetBuilder):
  ...
""" .

"DESCRIPTION.The code shuffles the channels of the input tensor in groups of 2." <EXPLAINS> """CODE.channel_shuffle = nn.ChannelShuffle(2)
input = torch.randn(1, 4, 2, 2)
output = channel_shuffle(input)""" .

"DESCRIPTION.The code slices a tensor array `complete_tensor` using specific indices, and then retrieves the sliced index based on the rank, shape, dimensions mapping, process shape, and process group." <EXPLAINS> """CODE.import numpy as np
complete_tensor = np.array([[[1.11, 1.12, 1.13, 1.14, 1.15, 1.16]]])
rank = 2
complete_shape = [1, 1, 6]
dims_mapping = [-1, -1, 0]
process_shape = [3]
process_group = [0, 1, 2]

slice_tensor = _slice_tensor(complete_tensor, [[], [], [2, 4]], 3)

index = _get_sliced_index(rank, complete_shape, dims_mapping, process_shape, process_group)""" .

"DESCRIPTION.The code snippet calculates the length of the string representation of each element in a DataFrame, creates a copy of the DataFrame, sets the value at the first row and first column to a pd.NA value in the copy, then calculates the length of the string representation of each element in the copy DataFrame while ignoring the NA values. Finally, it squares each element in the original DataFrame." <EXPLAINS> """CODE.df.map(lambda x: len(str(x)))
df_copy = df.copy()
df_copy.iloc[0, 0] = pd.NA
df_copy.map(lambda x: len(str(x)), na_action='ignore')
df.map(lambda x: x**2)""" .

"DESCRIPTION.The code snippet calculates the mean absolute error loss between two sets of values, creates a machine learning model, and compiles the model using stochastic gradient descent optimizer and mean absolute error loss function." <EXPLAINS> """CODE.mae = keras.losses.MeanAbsoluteError()
loss = mae([0., 0., 1., 1.], [1., 1., 1., 0.])
model = keras.Model(inputs, outputs)
model.compile('sgd', loss=keras.losses.MeanAbsoluteError())""" .

"DESCRIPTION.The code snippet converts the data type from real to complex for the given input data types (np.float32, np.int16, np.complex128)." <EXPLAINS> """CODE.librosa.util.dtype_r2c(np.float32)
librosa.util.dtype_r2c(np.int16)
librosa.util.dtype_r2c(np.complex128)""" .

"DESCRIPTION.The code snippet creates a CategoryEncoding layer in TensorFlow Keras with a specified maximum number of tokens and output mode. The layer is then applied to input data to encode the categories into a count representation." <EXPLAINS> """CODE.layer = tf.keras.layers.experimental.preprocessing.CategoryEncoding(
          max_tokens=4, output_mode="count")
layer([[0, 1], [0, 0], [1, 2], [3, 1]])

layer = tf.keras.layers.experimental.preprocessing.CategoryEncoding(
          max_tokens=4, output_mode="count")
count_weights = np.array([[.1, .2], [.1, .1], [.2, .3], [.4, .2]])
layer([[0, 1], [0, 0], [1, 2], [3, 1]], count_weights=count_weights)
""" .

"DESCRIPTION.The code snippet creates a URIBuilder object and adds a scheme with the value 'HTTPS' to the URI being built." <EXPLAINS> """CODE... code-block:: python

    URIBuilder().add_scheme('HTTPS')""" .

"DESCRIPTION.The code snippet creates a dictionary containing predictions for classes 'cat' and 'dog' along with corresponding ids, then writes this dictionary to a file or another data storage using the method `write_dict` of an object named `result`." <EXPLAINS> """CODE.predictions_to_write = {'preds': ['cat', 'dog'], 'ids': tensor([0, 1])}
result.write_dict(predictions_to_write)""" .

"DESCRIPTION.The code snippet creates a sequential model consisting of two bidirectional LSTM layers, followed by a dense layer with 5 units and a softmax activation function. It then compiles the model using categorical crossentropy loss and the RMSprop optimizer." <EXPLAINS> """CODE.model = Sequential()
model.add(Bidirectional(LSTM(10, return_sequences=True), input_shape=(5, 10)))
model.add(Bidirectional(LSTM(10)))
model.add(Dense(5))
model.add(Activation('softmax'))
model.compile(loss='categorical_crossentropy', optimizer='rmsprop')""" .

"DESCRIPTION.The code snippet demonstrates creating a pandas Series of datetime objects and converting them to Python datetime objects. The code also shows how nanosecond precision is truncated to microseconds in pandas." <EXPLAINS> """CODE.s = pd.Series(pd.date_range('20180310', periods=2))
s
0   2018-03-10
1   2018-03-11
dtype: datetime64[ns]

s.dt.to_pydatetime()
array([datetime.datetime(2018, 3, 10, 0, 0),
       datetime.datetime(2018, 3, 11, 0, 0)], dtype=object)

pandas' nanosecond precision is truncated to microseconds.

s = pd.Series(pd.date_range('20180310', periods=2, freq='ns'))
s
0   2018-03-10 00:00:00.000000000
1   2018-03-10 00:00:00.000000001
dtype: datetime64[ns]

s.dt.to_pydatetime()
array([datetime.datetime(2018, 3, 10, 0, 0),
       datetime.datetime(2018, 3, 10, 0, 0)], dtype=object)""" .

"DESCRIPTION.The code snippet encodes the input text using the FlaxBart model to generate encoder outputs." <EXPLAINS> """CODE.from transformers import BartTokenizer, FlaxBartForConditionalGeneration
model = FlaxBartForConditionalGeneration.from_pretrained('facebook/bart-large-cnn')
tokenizer = BartTokenizer.from_pretrained('facebook/bart-large-cnn')
text = "My friends are cool but they eat too many carbs."
inputs = tokenizer(text, max_length=1024, return_tensors='jax')
encoder_outputs = model.encode(**inputs)""" .

"DESCRIPTION.The code snippet generates a unique identifier (uid) for the string 'dense' in Keras backend." <EXPLAINS> """CODE.keras.backend.get_uid('dense')
1
keras.backend.get_uid('dense')
2""" .

"DESCRIPTION.The code snippet initializes a universal detector for character encoding, feeds it some bytes of data, closes the detector, and retrieves the detected character encoding." <EXPLAINS> """CODE.        u = UniversalDetector()
        u.feed(some_bytes)
        u.close()
        detected = u.result""" .

"DESCRIPTION.The code snippet normalizes the data format in Keras backend to ensure consistency either by setting it to None or 'channels_last'." <EXPLAINS> """CODE.from keras import backend as K
K.normalize_data_format(None)
K.normalize_data_format('channels_last')
""" .

"DESCRIPTION.The code snippet reduces the learning rate when the validation loss does not improve for a certain number of epochs during training a machine learning model." <EXPLAINS> """CODE.reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2,
                              patience=5, min_lr=0.001)
model.fit(X_train, Y_train, callbacks=[reduce_lr])
""" .

"DESCRIPTION.The code snippet runs a training loop for a machine learning model using JAX library, where the code uses JAX profiler to trace the execution steps of the training process." <EXPLAINS> """CODE.import jax

while global_step < NUM_STEPS:
    with jax.profiler.StepTraceContext("train", step_num=global_step):
        train_step()
        global_step += 1
""" .

"DESCRIPTION.The code snippets demonstrate how to use TensorBoard callbacks in TensorFlow to log training metrics and visualize model performance during training. The callbacks are added to the `model.fit()` method to log various metrics and summaries to the specified log directory. Additionally, the profile_batch parameter can be used to profile specific batches or a range of batches during training." <EXPLAINS> """CODE.tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir="./logs")
model.fit(x_train, y_train, epochs=2, callbacks=[tensorboard_callback])
# Then run the tensorboard command to view the visualizations.


class MyModel(tf.keras.Model):

  def build(self, _):
    self.dense = tf.keras.layers.Dense(10)

  def call(self, x):
    outputs = self.dense(x)
    tf.summary.histogram('outputs', outputs)
    return outputs

model = MyModel()
model.compile('sgd', 'mse')

# Make sure to set `update_freq=N` to log a batch-level summary every N batches.
# In addition to any `tf.summary` contained in `Model.call`, metrics added in
# `Model.compile` will be logged every N batches.
tb_callback = tf.keras.callbacks.TensorBoard('./logs', update_freq=1)
model.fit(x_train, y_train, callbacks=[tb_callback])


def my_summary(x):
  tf.summary.histogram('x', x)
  return x

inputs = tf.keras.Input(10)
x = tf.keras.layers.Dense(10)(inputs)
outputs = tf.keras.layers.Lambda(my_summary)(x)
model = tf.keras.Model(inputs, outputs)
model.compile('sgd', 'mse')

# Make sure to set `update_freq=N` to log a batch-level summary every N batches.
# In addition to any `tf.summary` contained in `Model.call`, metrics added in
# `Model.compile` will be logged every N batches.
tb_callback = tf.keras.callbacks.TensorBoard('./logs', update_freq=1)
model.fit(x_train, y_train, callbacks=[tb_callback])


# Profile a single batch, e.g. the 5th batch.
tensorboard_callback = tf.keras.callbacks.TensorBoard(
    log_dir='./logs', profile_batch=5)
model.fit(x_train, y_train, epochs=2, callbacks=[tensorboard_callback])

# Profile a range of batches, e.g. from 10 to 20.
tensorboard_callback = tf.keras.callbacks.TensorBoard(
    log_dir='./logs', profile_batch=(10,20))
model.fit(x_train, y_train, epochs=2, callbacks=[tensorboard_callback])
""" .

"DESCRIPTION.The code sorts a pandas Series based on its index values in ascending and descending order, with an option to sort the Series in place. It also demonstrates sorting a Series with MultiIndex based on a specific level, with an option to control whether to sort the remaining levels." <EXPLAINS> """CODE.s = pd.Series(['a', 'b', 'c', 'd'], index=[3, 2, 1, 4])
s.sort_index()
1    c
2    b
3    a
4    d
dtype: object

s.sort_index(ascending=False)
4    d
3    a
2    b
1    c
dtype: object

s.sort_index(inplace=True)
s
1    c
2    b
3    a
4    d
dtype: object

s = pd.Series(['a', 'b', 'c', 'd'], index=[3, 2, 1, np.nan])
s.sort_index(na_position='first')
NaN     d
 1.0    c
 2.0    b
 3.0    a
dtype: object

arrays = [np.array(['qux', 'qux', 'foo', 'foo',
...                     'baz', 'baz', 'bar', 'bar']),
...           np.array(['two', 'one', 'two', 'one',
...                     'two', 'one', 'two', 'one'])]
s = pd.Series([1, 2, 3, 4, 5, 6, 7, 8], index=arrays)
s.sort_index(level=1)
bar  one    8
baz  one    6
foo  one    4
qux  one    2
bar  two    7
baz  two    5
foo  two    3
qux  two    1
dtype: int64

s.sort_index(level=1, sort_remaining=False)
qux  one    2
foo  one    4
baz  one    6
bar  one    8
qux  two    1
foo  two    3
baz  two    5
bar  two    7
dtype: int64""" .

"DESCRIPTION.The code sorts a pandas Series in ascending and descending order based on the values in the Series. It also allows for sorting with missing values placed at the beginning or end of the Series." <EXPLAINS> """CODE.s = pd.Series([np.nan, 1, 3, 10, 5])
s
0     NaN
1     1.0
2     3.0
3     10.0
4     5.0
dtype: float64

s.sort_values(ascending=True)
1     1.0
2     3.0
4     5.0
3    10.0
0     NaN
dtype: float64

s.sort_values(ascending=False)
3    10.0
4     5.0
2     3.0
1     1.0
0     NaN
dtype: float64

s.sort_values(ascending=False, inplace=True)
s
3    10.0
4     5.0
2     3.0
1     1.0
0     NaN
dtype: float64

s.sort_values(na_position='first')
0     NaN
1     1.0
2     3.0
4     5.0
3    10.0
dtype: float64

s = pd.Series(['z', 'b', 'd', 'a', 'c'])
s
0    z
1    b
2    d
3    a
4    c
dtype: object

s.sort_values()
3    a
1    b
4    c
2    d
0    z
dtype: object""" .

"DESCRIPTION.The code sorts the DataFrame based on the values in the specified column(s) in ascending or descending order. It also allows positioning NaN values first or last, and provides the option to use a key function for customized sorting." <EXPLAINS> """CODE.df = pd.DataFrame({
...     'col1': ['A', 'A', 'B', np.nan, 'D', 'C'],
...     'col2': [2, 1, 9, 8, 7, 4],
...     'col3': [0, 1, 9, 4, 2, 3],
...     'col4': ['a', 'B', 'c', 'D', 'e', 'F']
... })
df
  col1  col2  col3 col4
0    A     2     0    a
1    A     1     1    B
2    B     9     9    c
3  NaN     8     4    D
4    D     7     2    e
5    C     4     3    F

Sort by col1

df.sort_values(by=['col1'])
  col1  col2  col3 col4
0    A     2     0    a
1    A     1     1    B
2    B     9     9    c
5    C     4     3    F
4    D     7     2    e
3  NaN     8     4    D

Sort by multiple columns

df.sort_values(by=['col1', 'col2'])
  col1  col2  col3 col4
1    A     1     1    B
0    A     2     0    a
2    B     9     9    c
5    C     4     3    F
4    D     7     2    e
3  NaN     8     4    D

Sort Descending

df.sort_values(by='col1', ascending=False)
  col1  col2  col3 col4
4    D     7     2    e
5    C     4     3    F
2    B     9     9    c
0    A     2     0    a
1    A     1     1    B
3  NaN     8     4    D

Putting NAs first

df.sort_values(by='col1', ascending=False, na_position='first')
  col1  col2  col3 col4
3  NaN     8     4    D
4    D     7     2    e
5    C     4     3    F
2    B     9     9    c
0    A     2     0    a
1    A     1     1    B

Sorting with a key function

df.sort_values(by='col4', key=lambda col: col.str.lower())
   col1  col2  col3 col4
0    A     2     0    a
1    A     1     1    B
2    B     9     9    c
3  NaN     8     4    D
4    D     7     2    e
5    C     4     3    F

Natural sort with the key argument,
using the `natsort <https://github.com/SethMMorton/natsort>` package.

df = pd.DataFrame({
...    "time": ['0hr', '128hr', '72hr', '48hr', '96hr'],
...    "value": [10, 20, 30, 40, 50]
... })
df
    time  value
0    0hr     10
1  128hr     20
2   72hr     30
3   48hr     40
4   96hr     50
from natsort import index_natsorted
df.sort_values(
...     by="time",
...     key=lambda x: np.argsort(index_natsorted(df["time"]))
... )
    time  value
0    0hr     10
3   48hr     40
2   72hr     30
4   96hr     50
1  """ .

"DESCRIPTION.The code sorts the list ['f1', 'f2', 'f10'] in a natural order." <EXPLAINS> "CODE.natural_sorted(['f1', 'f2', 'f10'])" .

"DESCRIPTION.The code sorts the series 's' in descending order based on its index." <EXPLAINS> """CODE.result1 = s.sort_index(ascending=False)
result2 = s.sort_index(ascending=[1, 0])""" .

"DESCRIPTION.The code splits a Ray data range of 10 elements into three separate data sets at indices 2 and 5, then outputs the elements within each data set separately." <EXPLAINS> """CODE.d1, d2, d3 = ray.data.range(10).split_at_indices([2, 5])
d1.take()
d2.take()
d3.take()""" .

"DESCRIPTION.The code splits a dataset into even parts for the purpose of processing with JAX, based on the number of processes and the index of the current process." <EXPLAINS> """CODE.tfds.load(..., split=tfds.split_for_jax_process('train'))


tfds.even_splits(split, n=jax.process_count())[jax.process_index()]
""" .

"DESCRIPTION.The code splits a list of inputs between two processes, with the option to apply padding to maintain equal length lists for each process." <EXPLAINS> """CODE.# Assume there are two processes
from accelerate.state import AcceleratorState

state = AcceleratorState()
with state.split_between_processes(["A", "B", "C"]) as inputs:
    print(inputs)
# Process 0
["A", "B"]
# Process 1
["C"]

with state.split_between_processes(["A", "B", "C"], apply_padding=True) as inputs:
    print(inputs)
# Process 0
["A", "B"]
# Process 1
["C", "C"]
""" .

"DESCRIPTION.The code splits a range of integers from 0 to 9 into three separate datasets based on the given indices (2 and 5), and then takes and outputs the values of each dataset separately." <EXPLAINS> """CODE.d1, d2, d3 = ray.data.range(10).split_at_indices([2, 5])
d1.take()
d2.take()
d3.take()""" .

"DESCRIPTION.The code splits the data into training and testing sets using Leave-One-Label-Out cross-validation technique. It assigns data points with the same label to the same group, and then iterates through these groups to obtain training and testing sets for each iteration. Finally, it prints out the training and testing sets for each iteration." <EXPLAINS> """CODE.from sklearn import cross_validation
import numpy as np

X = np.array([[1, 2], [3, 4], [5, 6], [7, 8])
y = np.array([1, 2, 1, 2])
labels = np.array([1, 1, 2, 2])
lol = cross_validation.LeaveOneLabelOut(labels)

for train_index, test_index in lol:
    X_train, X_test = X[train_index], X[test_index]
    y_train, y_test = y[train_index], y[test_index]
    print(X_train, X_test, y_train, y_test)""" .

"DESCRIPTION.The code splits the input tensor into three parts along the second dimension, with sizes of [4, 15, 11]. It then retrieves the shapes of each split tensor." <EXPLAINS> """CODE.split0, split1, split2 = tf.split_v(1, [4, 15, 11], value)
tf.shape(split0) ==> [5, 4]
tf.shape(split1) ==> [5, 15]
tf.shape(split2) ==> [5, 11]
split0, split1, split2 = tf.split(value, 3, 1)
tf.shape(split0) ==> [5, 10]
""" .

"DESCRIPTION.The code standardizes each group of data by subtracting the mean and dividing by the standard deviation." <EXPLAINS> "CODE.grouped.transform(lambda x: (x - x.mean()) / x.std())" .

"DESCRIPTION.The code standardizes numerical columns and encodes categorical columns in a DataFrame X." <EXPLAINS> """CODE.from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import make_column_transformer
from sklearn.compose import make_column_selector
import pandas as pd
X = pd.DataFrame({'city': ['London', 'London', 'Paris', 'Sallisaw'],
...                   'rating': [5, 3, 4, 5]})
ct = make_column_transformer(
...       (StandardScaler(),
...        make_column_selector(dtype_include=np.number)),
...       (OneHotEncoder(),
...        make_column_selector(dtype_include=object)))
ct.fit_transform(X)  """ .

"DESCRIPTION.The code starts a sequence enqueuer, retrieves data in batches from the enqueuer, processes the data (such as training, evaluating, predicting), and eventually stops the process before closing the enqueuer." <EXPLAINS> """CODE.enqueuer = SequenceEnqueuer(...)
enqueuer.start()
datas = enqueuer.get()
for data in datas:
    # Use the inputs; training, evaluating, predicting.
    # ... stop sometime.
enqueuer.close()
""" .

"DESCRIPTION.The code substitutes specific patterns in the sentences with other specified patterns." <EXPLAINS> """CODE.import jiwer

sentences = ["is the world doomed or loved?", "edibles are allegedly cultivated"]

print(jiwer.SubstituteRegexes({r"doom": r"sacr", r"(\\w+)ed": r""})(sentences))""" .

"DESCRIPTION.The code subtracts the occurrences of characters in the string 'witch' and the Counter object {'w': 1, 'a': 1, 't': 1, 'c': 1, 'h': 1} from the Counter object created from the string 'which'. It then retrieves the counts of the characters 'h' and 'w' from the Counter object." <EXPLAINS> """CODE.c = Counter('which')
c.subtract('witch')
c.subtract(Counter('watch'))
c['h']
c['w']""" .

"DESCRIPTION.The code switches the execution mode to dygraph mode, checks if the execution mode is in dygraph mode, switches the execution mode back to imperative mode, and then checks if the execution mode is in dygraph mode." <EXPLAINS> """CODE.import paddle.fluid as fluid

fluid.enable_dygraph()  # Now we are in dygragh mode
print(fluid.in_dygraph_mode())  # True
fluid.disable_dygraph()
print(fluid.in_dygraph_mode())  # False""" .

"DESCRIPTION.The code takes a URL as input and returns the host or domain name extracted from the URL." <EXPLAINS> """CODE.get_host('http://google.com/mail/')
get_host('google.com:80')""" .

"DESCRIPTION.The code takes a dictionary \"tree\" with key \"a\" and a list of integers [1, 2, 3]. It then checks if all the leaves of the tree (individual elements of the nested tree structure) are present using the function all_leaves. The first assert statement checks if all the leaves are present, and the second assert statement checks if the list [tree] does not contain all the leaves." <EXPLAINS> """CODE.tree = {"a": [1, 2, 3]}
assert all_leaves(jax.tree_leaves(tree))
assert not all_leaves([tree])""" .

"DESCRIPTION.The code takes a dictionary where the keys are strings in a specific format with forward slashes, and the values are integers. It then organizes these key-value pairs into a nested dictionary structure where each level corresponds to a part of the key separated by the forward slash. The final result is a nested dictionary representing the original data." <EXPLAINS> """CODE.def unflatten_list_dict(dt):
    result = {}
    for key, value in dt.items():
        keys = key.split('/')
        current = result
        for k in keys[:-1]:
            if k not in current:
                current[k] = {}
            current = current[k]
        current[keys[-1]] = value
    return result

dt = {"aaa/0/bb": 12, "aaa/1/cc": 56, "aaa/1/dd": 92}
print(unflatten_list_dict(dt))
""" .

"DESCRIPTION.The code takes a list as input and returns a flattened version of the list, where all nested lists are expanded into individual elements." <EXPLAINS> """CODE.serve.utils.expand([1,2,[3,4,5],6])
serve.utils.expand(["a", ["b", "c"], "d", ["e", "f"]])""" .

"DESCRIPTION.The code takes a list containing integers, strings, and None, and passes it as an argument to a function `subprocess_pickle_echo`." <EXPLAINS> "CODE.subprocess_pickle_echo([1, 'a', None])" .

"DESCRIPTION.The code takes a list of filters in the form of tuples, where each tuple contains a field name, a comparison operator, and a value, and converts them into a logical expression." <EXPLAINS> "CODE.filters_to_expression([('foo', '==', 'bar')])" .

"DESCRIPTION.The code takes a list of integers as input and splits it into ranges of consecutive numbers." <EXPLAINS> "CODE.list(split_ranges([1,0,0,1,0]))" .

"DESCRIPTION.The code takes a list of sentences, splits each sentence into a list of characters, and returns a list of lists where each inner list represents the characters of a sentence." <EXPLAINS> """CODE.import jiwer

sentences = ["hi", "this is an example"]

print(jiwer.ReduceToListOfListOfChars()(sentences))
# prints: [['h', 'i'], ['t', 'h', 'i', 's', ' ', 'i', 's', ' ', 'a', 'n', ' ', 'e', 'x', 'a', 'm', 'p', 'l', 'e']]
""" .

"DESCRIPTION.The code takes a picture from the camera input using the Streamlit library and displays the image on the screen." <EXPLAINS> """CODE.import streamlit as st

picture = st.camera_input("Take a picture")

if picture:
    st.image(picture)""" .

"DESCRIPTION.The code takes a question and text as input, tokenizes them using AlbertTokenizer, passes the input through AlbertForQuestionAnswering model to get start and end scores for the answer span, and finally prints out the answer tokens by decoding the token ids." <EXPLAINS> """CODE.tokenizer = AlbertTokenizer.from_pretrained('albert-base-v2')
model = AlbertForQuestionAnswering.from_pretrained('albert-base-v2')
question, text = "Who was Jim Henson?", "Jim Henson was a nice puppet"
input_text = "[CLS] " + question + " [SEP] " + text + " [SEP]"
input_ids = tokenizer.encode(input_text)
token_type_ids = [0 if i <= input_ids.index(102) else 1 for i in range(len(input_ids))]
start_scores, end_scores = model(torch.tensor([input_ids]), token_type_ids=torch.tensor([token_type_ids]))
all_tokens = tokenizer.convert_ids_to_tokens(input_ids)
print(' '.join(all_tokens[torch.argmax(start_scores) : torch.argmax(end_scores)+1]))""" .

"DESCRIPTION.The code takes a series of data points X and Y, splits them into sequences with a specified sample length, and creates input and target datasets using the `timeseries_dataset_from_array` function from the TensorFlow Keras preprocessing module. It then iterates through the datasets, retrieves batches of inputs and targets, and asserts that the first input sequence is equal to the first 20 elements of X, while the second target sequence is equal to the output timestamps 20-40 of Y." <EXPLAINS> """CODE.input_data = data[:-10]
targets = data[10:]
dataset = tf.keras.preprocessing.timeseries_dataset_from_array(
    input_data, targets, sequence_length=10)
for batch in dataset:
  inputs, targets = batch
  assert np.array_equal(inputs[0], data[:10])  # First sequence: steps [0-9]
  assert np.array_equal(targets[0], data[10])  # Corresponding target: step 10
  break


X = np.arange(100)
Y = X*2

sample_length = 20
input_dataset = tf.keras.preprocessing.timeseries_dataset_from_array(
  X, None, sequence_length=sample_length, sequence_stride=sample_length)
target_dataset = tf.keras.preprocessing.timeseries_dataset_from_array(
  Y, None, sequence_length=sample_length, sequence_stride=sample_length)

for batch in zip(input_dataset, target_dataset):
  inputs, targets = batch
  assert np.array_equal(inputs[0], X[:sample_length])

  # second sample equals output timestamps 20-40
  assert np.array_equal(targets[1], Y[sample_length:2*sample_length])
  break
""" .

"DESCRIPTION.The code takes a string as input and converts it to a boolean value. It returns True if the input string is 'YES' and False if the input string is 'FALSE'." <EXPLAINS> """CODE.strtobool('YES')
strtobool('FALSE')""" .

"DESCRIPTION.The code takes a string input in the format \"year/month/day\", parses it into individual integers, and then converts it into a dictionary with keys \"year\", \"month\", and \"day\" with corresponding integer values." <EXPLAINS> """CODE.integer = Word(nums)
date_str = integer("year") + '/' + integer("month") + '/' + integer("day")
result = date_str.parseString('12/31/1999')
result_dict = result.asDict()""" .

"DESCRIPTION.The code takes in categorical column data for 'states' with a vocabulary file, creates columns based on this data, parses the input features, makes linear predictions using a linear model, and then creates embedding columns for the states data with a specified embedding size." <EXPLAINS> """CODE.states = categorical_column_with_vocabulary_file(
    key='states', vocabulary_file='/us/states.txt', vocabulary_size=50,
    num_oov_buckets=5)
columns = [states, ...]
features = tf.parse_example(..., features=make_parse_example_spec(columns))
linear_prediction = linear_model(features, columns)


states = categorical_column_with_vocabulary_file(
    key='states', vocabulary_file='/us/states.txt', vocabulary_size=51,
    default_value=0)
columns = [states, ...]
features = tf.parse_example(..., features=make_parse_example_spec(columns))
linear_prediction, _, _ = linear_model(features, columns)


columns = [embedding_column(states, 3),...]
features = tf.parse_example(..., features=make_parse_example_spec(columns))
dense_tensor = input_layer(features, columns)
""" .

"DESCRIPTION.The code takes in input parameters and model state, converts and organizes them into a dictionary of 'params' and 'batch_stats'." <EXPLAINS> """CODE.params = convert_pre_linen(pre_linen_params)

batch_stats = convert_pre_linen(flax.traverse_util.unflatten_dict({
    tuple(k.split('/')[1:]): v
    for k, v in pre_linen_model_state.as_dict().items()
}))

variables = {'params': params, 'batch_stats': batch_stats}
""" .

"DESCRIPTION.The code takes in two iterators, either normal iterators or lambda functions returning iterators, and returns a list containing elements from both iterators concatenated together." <EXPLAINS> """CODE.from_iterators([range(100), range(100)])
from_iterators([lambda: range(100), lambda: range(100)])""" .

"DESCRIPTION.The code takes two input tensors of shape (32,) and concatenates them along axis 1 to create a single merged tensor." <EXPLAINS> """CODE.tensor_a = Input(shape=(32,))
tensor_b = Input(shape=(32,))
merged_tensor = merge([tensor_a, tensor_b], mode='concat', concat_axis=1)
""" .

"DESCRIPTION.The code takes user input for a movie title using a text input widget and displays the inputted movie title using the 'st.write' function." <EXPLAINS> """CODE.title = st.text_input('Movie title', 'Life of Brian')
st.write('The current movie title is', title)""" .

"DESCRIPTION.The code temporarily disables x64 support and then prints the data type of a NumPy array containing numbers from 0 to 9." <EXPLAINS> """CODE.with disable_x64():
  ...   print(jnp.arange(10.0).dtype)""" .

"DESCRIPTION.The code terminates multiple processes, waits for them to finish within a time limit, prints a message when each process is terminated, and kills any processes that are still alive after the time limit." <EXPLAINS> """CODE.def on_terminate(proc):
    print("process {} terminated".format(proc))

for p in procs:
    p.terminate()

gone, still_alive = wait_procs(procs, 3, callback=on_terminate)
for p in still_alive:
    p.kill()""" .

"DESCRIPTION.The code tokenizes a given input text using the CTRL tokenizer and passes it through a pre-trained CTRL model to obtain the last hidden states of the model." <EXPLAINS> """CODE.import tensorflow as tf
from transformers import CTRLTokenizer, TFCTRLModel

tokenizer = CTRLTokenizer.from_pretrained('ctrl')
model = TFCTRLModel.from_pretrained('ctrl')
input_ids = tf.constant(tokenizer.encode("Hello, my dog is cute"))[None, :]  # Batch size 1
outputs = model(input_ids)
last_hidden_states = outputs[0]  # The last hidden-state is the first element of the output tuple""" .

"DESCRIPTION.The code tokenizes a given input text using the XLMProphetNetTokenizer and then encodes the tokenized input using the XLMProphetNetEncoder model to generate last hidden states for each token in the input text." <EXPLAINS> """CODE.from transformers import XLMProphetNetTokenizer, XLMProphetNetEncoder
import torch

tokenizer = XLMProphetNetTokenizer.from_pretrained('microsoft/xprophetnet-large-wiki100-cased')
model = XLMProphetNetEncoder.from_pretrained('patrickvonplaten/xprophetnet-large-uncased-standalone', return_dict=True)
assert model.config.is_decoder, f"{model.__class__} has to be configured as a decoder."
inputs = tokenizer("Hello, my dog is cute", return_tensors="pt")
outputs = model(**inputs)

last_hidden_states = outputs.last_hidden_state""" .

"DESCRIPTION.The code tokenizes a list of text sequences into tokens using the RealmTokenizer model with a batch size of 2 and 2 candidates per sequence, obtained from a pretrained encoder model." <EXPLAINS> """CODE.from transformers import RealmTokenizer

# batch_size = 2, num_candidates = 2
text = [["Hello world!", "Nice to meet you!"], ["The cute cat.", "The adorable dog."]]

tokenizer = RealmTokenizer.from_pretrained("qqaatw/realm-cc-news-pretrained-encoder")
tokenized_text = tokenizer.batch_encode_candidates(text, max_length=10, return_tensors="pt")
""" .

"DESCRIPTION.The code tokenizes a question and text, generates input tokens with special tokens for classification and separation, converts the input tokens to IDs using the tokenizer, assigns token types based on the position of the [SEP] token, uses a question-answering model to predict start and end scores for the answer span, converts the predicted tokens back to words, and prints the answer span as a string." <EXPLAINS> """CODE.tokenizer = XxxTokenizer.from_pretrained('xxx-base-uncased')
model = XxxForQuestionAnswering.from_pretrained('xxx-large-uncased-whole-word-masking-finetuned-squad')
question, text = "Who was Jim Henson?", "Jim Henson was a nice puppet"
input_text = "[CLS] " + question + " [SEP] " + text + " [SEP]"
input_ids = tokenizer.encode(input_text)
token_type_ids = [0 if i <= input_ids.index(102) else 1 for i in range(len(input_ids))]
start_scores, end_scores = model(torch.tensor([input_ids]), token_type_ids=torch.tensor([token_type_ids]))
all_tokens = tokenizer.convert_ids_to_tokens(input_ids)
print(' '.join(all_tokens[torch.argmax(start_scores) : torch.argmax(end_scores)+1]))""" .

"DESCRIPTION.The code tokenizes input text using the BloomTokenizerFast model from the \"bigscience/bloom\" pretrained model, providing the input ids for the tokenized text." <EXPLAINS> """CODE.from transformers import BloomTokenizerFast
tokenizer = BloomTokenizerFast.from_pretrained("bigscience/bloom")
tokenizer("Hello world")['input_ids']
[15496, 995]
tokenizer(" Hello world")['input_ids']
[18435, 995]
""" .

"DESCRIPTION.The code tokenizes multiple text sequences into tokenized texts using a pretrained RealmTokenizerFast model with specific batch size and number of candidates." <EXPLAINS> """CODE.from transformers import RealmTokenizerFast

# batch_size = 2, num_candidates = 2
text = [["Hello world!", "Nice to meet you!"], ["The cute cat.", "The adorable dog."]]

tokenizer = RealmTokenizerFast.from_pretrained("qqaatw/realm-cc-news-pretrained-encoder")
tokenized_text = tokenizer.batch_encode_candidates(text, max_length=10, return_tensors="pt")
""" .

"DESCRIPTION.The code tokenizes the input sentence into individual words and punctuation marks." <EXPLAINS> """CODE.tokenize('Bob dropped the apple. Where is the apple?')
['Bob', 'dropped', 'the', 'apple', '.', 'Where', 'is', 'the', 'apple', '?']""" .

"DESCRIPTION.The code tokenizes the input text \"Hello world!\" using a GPT2 tokenizer and then prints the tokens generated." <EXPLAINS> """CODE.tokenizer = GPT2Tokenizer()
text = "Hello world!"
tokens = tokenizer.tokenize(text)
print(tokens)
tokenizer.decode(tokens)""" .

"DESCRIPTION.The code tokenizes the input text \"Hello world!\" using the GPT2 tokenizer and prints the tokens resulting from the tokenization process." <EXPLAINS> """CODE.tokenizer = GPT2Tokenizer()
text = "Hello world!"
tokens = tokenizer.tokenize(text)
print(tokens)""" .

"DESCRIPTION.The code tokenizes the input text \"Hello world!\" using the GPT2 tokenizer and then decodes the tokens back into human-readable text." <EXPLAINS> """CODE.tokenizer = GPT2Tokenizer()
text = "Hello world!"
tokens = tokenizer.tokenize(text)
print(tokens)
tokenizer.decode(tokens)""" .

"DESCRIPTION.The code tokenizes the input text \"Hello world!\" using the GPT2Tokenizer." <EXPLAINS> """CODE.tokenizer = GPT2Tokenizer()
text = "Hello world!"
tokens = tokenizer.tokenize(text)
print(tokens)""" .

"DESCRIPTION.The code trains a Gradient Boosting Classifier model on the provided samples and labels data, and then makes a prediction on a new input data point [0.5, 0, 0] using the trained model." <EXPLAINS> """CODE.from sklearn.ensemble import GradientBoostingClassifier
gb = GradientBoostingClassifier().fit(samples, labels)
print gb.predict([[0.5, 0, 0]])
""" .

"DESCRIPTION.The code trains a Random Forest Regressor model on a dataset with features labeled as 'x' and target variable labeled as 'y'." <EXPLAINS> """CODE.import ray

from ray.ml.train.integrations.sklearn import SklearnTrainer
from sklearn.ensemble import RandomForestRegressor

train_dataset = ray.data.from_items(
    [{"x": x, "y": x + 1} for x in range(32)])
trainer = SklearnTrainer(
    sklearn_estimator=RandomForestRegressor,
    label_column="y",
    scaling_config={
        "trainer_resources": {"CPU": 4}
    },
    datasets={"train": train_dataset}
)
result = trainer.fit()
""" .

"DESCRIPTION.The code trains a TensorFlow Estimator model continuously until a stopping condition is met, evaluating the model's performance on an evaluation dataset after each training iteration. If a custom stopping condition is met, the training loop terminates." <EXPLAINS> """CODE.est = tf.estimator.Estimator(model_fn)
while True:
  est.train(
      train_input_fn,
      hooks=[tf.contrib.data.CheckpointInputPipelineHook(est)],
      steps=train_steps_per_eval)
  # Note: We do not pass the hook here.
  metrics = est.evaluate(eval_input_fn)
  if should_stop_the_training(metrics):
    break
""" .

"DESCRIPTION.The code trains a categorical Naive Bayes classifier (CategoricalNB) using the data X and labels y, then predicts the label of the sample X[2:3] and prints the prediction." <EXPLAINS> """CODE.from sklearn.naive_bayes import CategoricalNB
clf = CategoricalNB()
clf.fit(X, y)
print(clf.predict(X[2:3]))""" .

"DESCRIPTION.The code trains a language model using the GPT-2 model architecture on the WikiText dataset. It tokenizes the text data, groups the texts into chunks of a specific block size, initializes the trainer with model configuration and training arguments, and then trains the model using distributed training with the specified number of workers." <EXPLAINS> """CODE.# Based on
# huggingface/notebooks/examples/language_modeling_from_scratch.ipynb

# Hugging Face imports
from datasets import load_dataset
import transformers
from transformers import AutoConfig, AutoModelForCausalLM, AutoTokenizer

import ray
from ray.train.huggingface import TransformersTrainer
from ray.air.config import ScalingConfig

# If using GPUs, set this to True.
use_gpu = False

model_checkpoint = "gpt2"
tokenizer_checkpoint = "sgugger/gpt2-like-tokenizer"
block_size = 128

datasets = load_dataset("wikitext", "wikitext-2-raw-v1")
tokenizer = AutoTokenizer.from_pretrained(tokenizer_checkpoint)

def tokenize_function(examples):
    return tokenizer(examples["text"])

tokenized_datasets = datasets.map(
    tokenize_function, batched=True, num_proc=1, remove_columns=["text"]
)

def group_texts(examples):
    # Concatenate all texts.
    concatenated_examples = {
        k: sum(examples[k], []) for k in examples.keys()
    }
    total_length = len(concatenated_examples[list(examples.keys())[0]])
    # We drop the small remainder, we could add padding if the model
    # supported it.
    # instead of this drop, you can customize this part to your needs.
    total_length = (total_length // block_size) * block_size
    # Split by chunks of max_len.
    result = {
        k: [
            t[i : i + block_size]
            for i in range(0, total_length, block_size)
        ]
        for k, t in concatenated_examples.items()
    }
    result["labels"] = result["input_ids"].copy()
    return result

lm_datasets = tokenized_datasets.map(
    group_texts,
    batched=True,
    batch_size=1000,
    num_proc=1,
)
ray_train_ds = ray.data.from_huggingface(lm_datasets["train"])
ray_evaluation_ds = ray.data.from_huggingface(
    lm_datasets["validation"]
)

def trainer_init_per_worker(train_dataset, eval_dataset, **config):
    model_config = AutoConfig.from_pretrained(model_checkpoint)
    model = AutoModelForCausalLM.from_config(model_config)
    args = transformers.TrainingArguments(
        output_dir=f"{model_checkpoint}-wikitext2",
        evaluation_strategy="epoch",
        save_strategy="epoch",
        logging_strategy="epoch",
        learning_rate=2e-5,
        weight_decay=0.01,
        no_cuda=(not use_gpu),
    )
    return transformers.Trainer(
        model=model,
        args=args,
        train_dataset=train_dataset,
        eval_dataset=eval_dataset,
    )

scaling_config = ScalingConfig(num_workers=3, use_gpu=use_gpu)
trainer = TransformersTrainer(
    trainer_init_per_worker=trainer_init_per_worker,
    scaling_config=scaling_config,
    datasets={"train": ray_train_ds, "evaluation": ray_evaluation_ds},
)
result = trainer.fit()
""" .

"DESCRIPTION.The code trains a linear classifier using age and language features, then exports the trained model to be used for serving, analysis with TFMA, or direct loading." <EXPLAINS> """CODE.classifier = tf.estimator.LinearClassifier(
    feature_columns=[age, language])
classifier.train(input_fn=input_fn)

feature_spec = {
    'age': tf.placeholder(dtype=tf.int64),
    'language': array_ops.placeholder(dtype=tf.string)
}
label_spec = tf.placeholder(dtype=dtypes.int64)

train_rcvr_fn = tf.contrib.estimator.build_raw_supervised_input_receiver_fn(
    feature_spec, label_spec)

serve_rcvr_fn = tf.estimator.export.build_parsing_serving_input_receiver_fn(
    feature_spec)

rcvr_fn_map = {
    model_fn_lib.ModeKeys.TRAIN: train_rcvr_fn,
    model_fn_lib.ModeKeys.PREDICT: serve_rcvr_fn,
}

export_dir = tf.contrib.estimator.export_all_saved_models(
    classifier,
    export_dir_base='my_model/',
    input_receiver_fn_map=rcvr_fn_map)

# export_dirs is a dict of directories with SavedModels, which
# can be used for serving, analysis with TFMA, or directly loaded in.
with ops.Graph().as_default() as graph:
  with session.Session(graph=graph) as sess:
    loader.load(sess, [tag_constants.TRAINING], export_dir)
    weights = graph.get_tensor_by_name('linear/linear_model/age/weights')
    ...
""" .

"DESCRIPTION.The code trains a machine learning model on a batch of samples." <EXPLAINS> "CODE.ev.learn_on_batch(samples)" .

"DESCRIPTION.The code trains a machine learning model using Distributed SGD (Stochastic Gradient Descent) with 100 iterations. It uses Torch as the backend and runs the training function, printing \"Worker 0\" if the world rank is 0." <EXPLAINS> """CODE.import time
from ray.util import sgd

def train_func():
    for iter in range(100):
        time.sleep(1)
        if sgd.world_rank() == 0:
            print("Worker 0")

trainer = Trainer(backend="torch")
trainer.start()
trainer.run(train_func)
trainer.shutdown()""" .

"DESCRIPTION.The code trains a model using the MultiWorkerMirroredStrategy from TensorFlow, fitting the model on a dataset shard with a custom callback called ReportCheckpointCallback." <EXPLAINS> """CODE.from ray.air.integrations.keras import ReportCheckpointCallback
def train_loop_per_worker():
    strategy = tf.distribute.MultiWorkerMirroredStrategy()
    with strategy.scope():
        model = build_model()

    model.fit(dataset_shard, callbacks=[ReportCheckpointCallback()])""" .

"DESCRIPTION.The code trains a model using the Torch backend with GPU support if available, setting the GPU device and then running the training function provided." <EXPLAINS> """CODE.import time
from ray.util import sgd

def train_func():
    if torch.cuda.is_available():
        torch.cuda.set_device(sgd.local_rank())
    ...

trainer = Trainer(backend="torch", use_gpu=True)
trainer.start()
trainer.run(train_func)
trainer.shutdown()""" .

"DESCRIPTION.The code trains a model using the specified configuration for a certain number of epochs, generates metrics during training and validation, reports the metrics to Ray SGD, and returns the trained model. Additionally, the code sets up an iterator to run the training function with the given configuration, processes the results, retrieves the latest checkpoint, checks if the iteration is finished, and retrieves the final model." <EXPLAINS> """CODE.def train_func(config):
    ...
    for _ in config["epochs"]:
        metrics = train()
        metrics = validate(...)
        ray.sgd.report(**metrics)
    return model

iterator = trainer.run_iterator(train_func, config=config)

for result in iterator:
    do_stuff(result)
    latest_ckpt = trainer.get_latest_checkpoint()

assert iterator.is_finished()
model = iterator.get_fin()[0]
""" .

"DESCRIPTION.The code trains a stacking ensemble model using RidgeCV and LinearSVR as base estimators, and RandomForestRegressor as a final estimator. It then splits the data into training and testing sets, fits the model on the training data, and calculates the R^2 score on the testing data." <EXPLAINS> """CODE.from sklearn.datasets import load_diabetes
from sklearn.linear_model import RidgeCV
from sklearn.svm import LinearSVR
from sklearn.ensemble import RandomForestRegressor
from sklearn.ensemble import StackingRegressor
X, y = load_diabetes(return_X_y=True)
estimators = [
...     ('lr', RidgeCV()),
...     ('svr', LinearSVR(random_state=42))
... ]
reg = StackingRegressor(
...     estimators=estimators,
...     final_estimator=RandomForestRegressor(n_estimators=10,
...                                           random_state=42)
... )
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(
...     X, y, random_state=42
... )
reg.fit(X_train, y_train).score(X_test, y_test)
0.3...""" .

"DESCRIPTION.The code transforms a diagonal matrix by taking the exponential of its elements." <EXPLAINS> """CODE.b = tfb.TransformDiagonal(diag_bijector=tfb.Exp())

b.forward([[1., 0.],
           [0., 1.]])""" .

"DESCRIPTION.The code transforms the content of articles in the 'news_paper' object by truncating them to 25 characters if they are longer than 25 characters, and to 15 characters if they are longer than 15 characters." <EXPLAINS> """CODE.news_paper = freeze({'articles': [{'author': 'Sara', 'content': 'A short article'},
                                  {'author': 'Steve', 'content': 'A slightly longer article'}],
                     'weather': {'temperature': '11C', 'wind': '5m/s'}})
short_news = news_paper.transform(['articles', ny, 'content'], lambda c: c[:25] + '...' if len(c) > 25 else c)
very_short_news = news_paper.transform(['articles', ny, 'content'], lambda c: c[:15] + '...' if len(c) > 15 else c)""" .

"DESCRIPTION.The code transforms the given file paths into a dotted format, compares a specific path ('/foo/bar/baz.py') with the transformed paths, and returns a tuple containing the common path elements ('bar', 'baz') and a boolean indicating if the comparison was successful." <EXPLAINS> """CODE.from os.path import abspath
transform_path_to_dotted([abspath("/foo")], abspath('/foo/bar/baz.py'))
(('bar', 'baz'), False)""" .

"DESCRIPTION.The code transposes a 2x3 matrix using the PaddlePaddle library and prints the shape of the transposed matrix." <EXPLAINS> """CODE.import paddle
import paddle.fluid as fluid
x = fluid.data(name='x', shape=[2, 3], dtype='float32')
x_transposed = fluid.layers.t(x)
print x_transposed.shape""" .

"DESCRIPTION.The code transposes the axes of a 4-dimensional array with shape (2, 3, 4, 5) based on the specified order ('TYXC') and returns the shape of the transposed array which has 5 dimensions." <EXPLAINS> "CODE.transpose_axes(numpy.zeros((2, 3, 4, 5)), 'TYXC', asaxes='CTZYX').shape" .

"DESCRIPTION.The code transposes the shape of given input data according to the specified data format and spatial axes." <EXPLAINS> """CODE.from keras.utils.generic_utils import transpose_shape
transpose_shape((16, 128, 128, 32),'channels_first', spatial_axes=(1, 2))
transpose_shape((16, 128, 128, 32), 'channels_last', spatial_axes=(1, 2))
transpose_shape((128, 128, 32), 'channels_first', spatial_axes=(0, 1))
""" .

"DESCRIPTION.The code transposes the shape of input data according to the specified data format ('channels_first' or 'channels_last') and spatial axes." <EXPLAINS> """CODE.from keras.utils.generic_utils import transpose_shape
transpose_shape((16, 128, 128, 32),'channels_first', spatial_axes=(1, 2))
transpose_shape((16, 128, 128, 32), 'channels_last', spatial_axes=(1, 2))
transpose_shape((128, 128, 32), 'channels_first', spatial_axes=(0, 1))
""" .

"DESCRIPTION.The code unboxes a scalar value from a Timedelta object representing a duration of 10 seconds." <EXPLAINS> "CODE.self._unbox_scalar(Timedelta('10s'))" .

"DESCRIPTION.The code unescapes HTML entities in the given string." <EXPLAINS> "CODE.Markup(\"Main &raquo; <em>About</em>\").unescape()" .

"DESCRIPTION.The code unpacks the arguments passed in the list and applies them to the function call using range(6) as the iterable." <EXPLAINS> """CODE.unpack_args(range(6), [1, 2, 1, -1])
unpack_args(range(6), [1, 2, 1])
unpack_args(range(6), [-1])
unpack_args(range(6), [1, 1])
unpack_args(range(6), [-1,1,1,1,1])""" .

"DESCRIPTION.The code unstacks the data in a pandas Series based on the specified level, either the last level or the first level." <EXPLAINS> """CODE.s.unstack(level=-1)
s.unstack(level=0)""" .

"DESCRIPTION.The code unstacks the data in the Series 's' along the specified level, either the last level or the first level." <EXPLAINS> """CODE.s.unstack(level=-1)
s.unstack(level=0)""" .

"DESCRIPTION.The code unstacks the given pandas Series 's' by the specified level, either using the default level (-1) or level 0." <EXPLAINS> """CODE.s.unstack(level=-1)
s.unstack(level=0)""" .

"DESCRIPTION.The code unstacks the hierarchical index of the Series 's' along the specified level, either the last level (-1) or the first level (0)." <EXPLAINS> """CODE.s.unstack(level=-1)
s.unstack(level=0)""" .

"DESCRIPTION.The code updates every other element in the 4th column of the array x with a value of 6." <EXPLAINS> "CODE.jax.ops.index_update(x, jnp.index_exp[::2, 3:], 6.)" .

"DESCRIPTION.The code updates the 'hobbies' key in the dictionary t with a new list containing both the original hobbies and the new hobby 'gaming'." <EXPLAINS> """CODE.t = {'name': 'Ferry', 'hobbies': ['programming', 'sci-fi']}
deepupdate(t, {'hobbies': ['gaming']})
print(t)
{'name': 'Ferry', 'hobbies': ['programming', 'sci-fi', 'gaming']}""" .

"DESCRIPTION.The code updates the counter value in the state of the app and prints a message displaying the updated counter value." <EXPLAINS> """CODE.import param
app = AppStateWatcher()
app.state.counter = 1

@param.depends(app.param.state, watch=True)
def update(state):
    print(f"The counter was updated to {state.counter}")

app.state.counter += 1""" .

"DESCRIPTION.The code updates the metadata of a collection on the Hugging Face platform by setting the title, description, privacy status, and theme of the collection. It then retrieves and prints the slug of the updated collection." <EXPLAINS> """CODE.from huggingface_hub import update_collection_metadata
collection = update_collection_metadata(
    collection_slug="username/iccv-2023-64f9a55bb3115b4f513ec026",
    title="ICCV Oct. 2023"
    description="Portfolio of models, datasets, papers and demos I presented at ICCV Oct. 2023",
    private=False,
    theme="pink",
)
collection.slug
""" .

"DESCRIPTION.The code updates the method names in a saved model located at export_dir. It replaces the method name for the signature key \"foo\" with \"regress\" and the method name for the signature key \"bar\" with \"classify\" for serving. The updated model is then saved to a new export directory, new_export_dir." <EXPLAINS> """CODE.updater = tf.compat.v1.saved_model.MethodNameUpdater(export_dir)
updater.replace_method_name(signature_key="foo", method_name="regress")
updater.replace_method_name(signature_key="bar", method_name="classify", tags="serve")
updater.save(new_export_dir)
""" .

"DESCRIPTION.The code updates the model parameters using the optimizer while training for 300 iterations. After the 160th iteration, it switches to using SWA (Stochastic Weight Averaging) to update the model parameters and adjusts the learning rate accordingly. Finally, it updates the batch normalization statistics for the SWA model." <EXPLAINS> """CODE.loader, optimizer, model, loss_fn = ...
swa_model = torch.optim.swa_utils.AveragedModel(model)
scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer,
                                    T_max=300)
swa_start = 160
swa_scheduler = SWALR(optimizer, swa_lr=0.05)
for i in range(300):
     for input, target in loader:
         optimizer.zero_grad()
         loss_fn(model(input), target).backward()
         optimizer.step()
     if i > swa_start:
         swa_model.update_parameters(model)
         swa_scheduler.step()
     else:
         scheduler.step()

# Update bn statistics for the swa_model at the end
torch.optim.swa_utils.update_bn(loader, swa_model)""" .

"DESCRIPTION.The code updates the value at index `idx` in array `x` to be the minimum value between the current value at that index and the value `y`." <EXPLAINS> """CODE.``x[idx] = minimum(x[idx], y)``
``x.at[idx].min(y)`` is syntactic sugar for
``jax.ops.index_min(x, jax.ops.index[idx], y)``, and
returns the value of ``x`` that would result from the NumPy-style
:mod:indexed assignment <numpy.doc.indexing>`
``x[idx] = minimum(x[idx], y)``.""" .

"DESCRIPTION.The code updates the values of 'a' and 'c' in the hparams dictionary and then retrieves and prints the updated values of 'b' and 'c'." <EXPLAINS> """CODE.hparams = {'c': 4}
update_hparams(hparams, {'a': {'b': 2}, 'c': 1})
hparams['a']['b'], hparams['c']
update_hparams(hparams, {'a': {'b': 4}, 'c': 7})
hparams['a']['b'], hparams['c']""" .

"DESCRIPTION.The code uploads a file from a local directory to a specified remote directory in a specific repository on Hugging Face. The first block of code uploads a file to a dataset repository, while the second block of code uploads a file to a model repository." <EXPLAINS> """CODE.with open("./local/filepath", "rb") as fobj:
...     upload_file(
...         path_or_fileobj=fileobj,
...         path_in_repo="remote/file/path.h5",
...         repo_id="username/my-dataset",
...         repo_type="datasets",
...         token="my_token",
...    )
"https://huggingface.co/datasets/username/my-dataset/blob/main/remote/file/path.h5"

upload_file(
...     path_or_fileobj=".\\\\local\\\\file\\\\path",
...     path_in_repo="remote/file/path.h5",
...     repo_id="username/my-model",
...     token="my_token",
... )
"https://huggingface.co/username/my-model/blob/main/remote/file/path.h5"
""" .

"DESCRIPTION.The code uses Gradient Descent Optimizer with SyncReplicasOptimizerV2 to minimize the total loss. It also includes operations for initializing tokens and queue runners, as well as supervisor for managing training sessions. If the chief worker and sync replicas flag are enabled, the code starts queue runners and initializes tokens." <EXPLAINS> """CODE.opt = GradientDescentOptimizer(learning_rate=0.1)
opt = tf.SyncReplicasOptimizerV2(opt, replicas_to_aggregate=50,
                                 total_num_replicas=50)
grads = opt.minimize(total_loss, global_step=self.global_step)
init_token_op = opt.get_init_tokens_op()
chief_queue_runner = opt.get_chief_queue_runner()

if is_chief:
  local_init_op = opt.chief_init_op
else:
  local_init_op = opt.local_step_init_op
ready_for_local_init_op = opt.ready_for_local_init_op
sv = tf.Supervisor(graph=g,
                   is_chief=is_chief,
                   local_init_op=local_init_op,
                   ready_for_local_init_op=ready_for_local_init_op,
                   saver=model.saver)

if is_chief and FLAGS.sync_replicas:
  sv.start_queue_runners(sess, [chief_queue_runner])
  sess.run(init_token_op)
""" .

"DESCRIPTION.The code uses TensorFlow and Keras to create an Xception model for image classification. It replicates the model on 8 GPUs for parallel processing, compiles the model with categorical crossentropy loss and RMSprop optimizer, generates dummy data for training, fits the model on the data for 20 epochs with a batch size of 256 on multiple GPUs, and then saves the model as 'my_model.h5'." <EXPLAINS> """CODE.    import tensorflow as tf
    from keras.applications import Xception
    from keras.utils import multi_gpu_model
    import numpy as np

    num_samples = 1000
    height = 224
    width = 224
    num_classes = 1000

    # Instantiate the base model (or "template" model).
    # We recommend doing this with under a CPU device scope,
    # so that the model's weights are hosted on CPU memory.
    # Otherwise they may end up hosted on a GPU, which would
    # complicate weight sharing.
    with tf.device('/cpu:0'):
        model = Xception(weights=None,
                         input_shape=(height, width, 3),
                         classes=num_classes)

    # Replicates the model on 8 GPUs.
    # This assumes that your machine has 8 available GPUs.
    parallel_model = multi_gpu_model(model, gpus=8)
    parallel_model.compile(loss='categorical_crossentropy',
                           optimizer='rmsprop')

    # Generate dummy data.
    x = np.random.random((num_samples, height, width, 3))
    y = np.random.random((num_samples, num_classes))

    # This `fit` call will be distributed on 8 GPUs.
    # Since the batch size is 256, each GPU will process 32 samples.
    parallel_model.fit(x, y, epochs=20, batch_size=256)

    # Save model via the template model (which shares the same weights):
    model.save('my_model.h5')


     ..
     # Not needed to change the device scope for model definition:
     model = Xception(weights=None, ..)

     try:
         model = multi_gpu_model(model, cpu_relocation=True)
         print("Training using multiple GPUs..")
     except:
         print("Training using single GPU or CPU..")

     model.compile(..)
     ..


     ..
     # Not needed to change the device scope for model definition:
     model = Xception(weights=None, ..)

     try:
         model = multi_gpu_model(model, cpu_merge=False)
         print("Training using multiple GPUs..")
     except:
         print("Training using single GPU or CPU..")
     model.compile(..)
     ..
""" .

"DESCRIPTION.The code uses a ReLU activation function to transform input values. It sets negative values to 0 while leaving non-negative values unchanged." <EXPLAINS> """CODE.layer = tf.keras.layers.Activation('relu')
output = layer([-3.0, -1.0, 0.0, 2.0])
list(output.numpy())
[0.0, 0.0, 0.0, 2.0]
layer = tf.keras.layers.Activation(tf.nn.relu)
output = layer([-3.0, -1.0, 0.0, 2.0])
list(output.numpy())
[0.0, 0.0, 0.0, 2.0]""" .

"DESCRIPTION.The code uses a function called tree_map to apply a given lambda function to all values in a dictionary and returns a new dictionary with the updated values." <EXPLAINS> """CODE.tree_map(lambda x: x + 1, {'x': 7, 'y': (42, 64)})
{'x': 8, 'y': (43, 65)}
tree_map(lambda x: x is None, {'x': 7, 'y': (42, 64), 'z': None})
{'x': False, 'y': (False, False), 'z': True}
""" .

"DESCRIPTION.The code uses a pandas Series to check if each element contains the letter 'a'. It performs three different checks: one that is case-sensitive, one that ignores case, and one that ignores NaN values." <EXPLAINS> """CODE.s = pd.Series(['Antelope', 'Lion', 'Zebra', np.nan])
s.str.contains(pat='a')
s.str.contains(pat='a', case=False)
s.str.contains(pat='a', na=False)
""" .

"DESCRIPTION.The code uses a pre-trained XLMProphetNet model to generate hidden states of the input text \"Hello, my dog is cute\"." <EXPLAINS> """CODE.from transformers import XLMProphetNetTokenizer, XLMProphetNetDecoder
import torch

tokenizer = XLMProphetNetTokenizer.from_pretrained('microsoft/xprophetnet-large-wiki100-cased')
model = XLMProphetNetDecoder.from_pretrained('patrickvonplaten/xprophetnet-large-uncased-standalone', add_cross_attention=False, return_dict=True)
assert model.config.is_decoder, f"{model.__class__} has to be configured as a decoder."
inputs = tokenizer("Hello, my dog is cute", return_tensors="pt")
outputs = model(**inputs)

last_hidden_states = outputs.last_hidden_state
""" .

"DESCRIPTION.The code uses a pre-trained model for object detection to detect similar objects in an image based on a query image. It makes predictions on the input image and provides bounding box coordinates and confidence scores for the detected objects. The threshold for object detection and non-maximum suppression is set to 0.9 and 0.3 respectively. Finally, it prints out the confidence score and location of the detected objects in the image." <EXPLAINS> """CODE.import requests
from PIL import Image
import torch
from transformers import AutoProcessor, Owlv2ForObjectDetection

processor = AutoProcessor.from_pretrained("google/owlv2-base-patch16-ensemble")
model = Owlv2ForObjectDetection.from_pretrained("google/owlv2-base-patch16-ensemble")
url = "http://images.cocodataset.org/val2017/000000039769.jpg"
image = Image.open(requests.get(url, stream=True).raw)
query_url = "http://images.cocodataset.org/val2017/000000001675.jpg"
query_image = Image.open(requests.get(query_url, stream=True).raw)
inputs = processor(images=image, query_images=query_image, return_tensors="pt")
with torch.no_grad():
    outputs = model.image_guided_detection(**inputs)
# Target image sizes (height, width) to rescale box predictions [batch_size, 2]
target_sizes = torch.Tensor([image.size[::-1]])
# Convert outputs (bounding boxes and class logits) to COCO API
results = processor.post_process_image_guided_detection(
    outputs=outputs, threshold=0.9, nms_threshold=0.3, target_sizes=target_sizes
)
i = 0  # Retrieve predictions for the first image
boxes, scores = results[i]["boxes"], results[i]["scores"]
for box, score in zip(boxes, scores):
    box = [round(i, 2) for i in box.tolist()]
    print(f"Detected similar object with confidence {round(score.item(), 3)} at location {box}")""" .

"DESCRIPTION.The code uses a sliding window of size 3 with a stride of 2 on a dataset 'a' to create batches of consecutive elements." <EXPLAINS> """CODE.a.apply(tf.contrib.data.sliding_window_batch(window_size=3, stride=2)) ==
{
    [[1], [2], [3]],
    [[3], [4], [5]],
}
""" .

"DESCRIPTION.The code uses a tokenizer from the Qwen2TokenizerFast class to convert input text into tokenized input_ids for \"Hello world\" and \" Hello world\"." <EXPLAINS> """CODE.from transformers import Qwen2TokenizerFast

tokenizer = Qwen2TokenizerFast.from_pretrained("Qwen/Qwen-tokenizer")
tokenizer("Hello world")["input_ids"]
[9707, 1879]

tokenizer(" Hello world")["input_ids"]
[21927, 1879]
""" .

"DESCRIPTION.The code uses an asyncio event loop to run an asynchronous application until it completes." <EXPLAINS> """CODE.from prompt_toolkit.eventloop import use_asyncio_event_loop
from asyncio import get_event_loop

use_asyncio_event_loop()
get_event_loop().run_until_complete(
    application.run_async().to_asyncio_future())
""" .

"DESCRIPTION.The code uses t-SNE (t-distributed Stochastic Neighbor Embedding) algorithm to reduce the dimensionality of the input array X from 3D to 2D." <EXPLAINS> """CODE.import numpy as np
from sklearn.manifold import TSNE
X = np.array([[0, 0, 0], [0, 1, 1], [1, 0, 1], [1, 1, 1]])
model = TSNE(n_components=2, random_state=0)
model.fit_transform(X) # doctest: +ELLIPSIS, +NORMALIZE_WHITESPACE
array([[  887.28...,   238.61...],
       [ -714.79...,  3243.34...],
       [  957.30..., -2505.78...],
       [-1130.28...,  -974.78...])""" .

"DESCRIPTION.The code uses the GPT2 tokenizer to tokenize the input text \"Hello world\" and \" Hello world\", returning their corresponding input ids." <EXPLAINS> """CODE.from transformers import GPTNeoXTokenizerFast
tokenizer = GPTNeoXTokenizerFast.from_pretrained("gpt2")
tokenizer("Hello world")['input_ids']
[15496, 995]
tokenizer(" Hello world")['input_ids']
[18435, 995]
""" .

"DESCRIPTION.The code uses the Jiwer library to create a composition that removes multiple spaces and reduces the text to a list of lists of words." <EXPLAINS> """CODE.python3
import jiwer

jiwer.Compose([
    jiwer.RemoveMultipleSpaces(),
    jiwer.ReduceToListOfListOfWords()
])
""" .

"DESCRIPTION.The code uses the LabelEncoder class from the sklearn.preprocessing module to encode and decode categorical labels." <EXPLAINS> """CODE.from sklearn import preprocessing
le = preprocessing.LabelEncoder()
le.fit([1, 2, 2, 6])
le.classes_
le.transform([1, 1, 2, 6])
le.inverse_transform([0, 0, 1, 2])

le = preprocessing.LabelEncoder()
le.fit(["paris", "paris", "tokyo", "amsterdam"])
list(le.classes_)
le.transform(["tokyo", "tokyo", "paris"])
list(le.inverse_transform([2, 2, 1]))""" .

"DESCRIPTION.The code uses the NearestCentroid classifier from the scikit-learn library to fit a model to the input data X and corresponding labels y. It then predicts the class label for a new input data point and outputs the predicted class label." <EXPLAINS> """CODE.from sklearn.neighbors.nearest_centroid import NearestCentroid
import numpy as np
X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2])
y = np.array([1, 1, 1, 2, 2, 2])
clf = NearestCentroid()
clf.fit(X, y)
NearestCentroid(metric='euclidean', shrink_threshold=None)
print clf.predict([[-0.8, -1]])
[1]""" .

"DESCRIPTION.The code uses the XLMProphetNet model and tokenizer to generate hidden states for input and output sequences." <EXPLAINS> """CODE.from transformers import XLMProphetNetTokenizer, XLMProphetNetModel
tokenizer = XLMProphetNetTokenizer.from_pretrained('microsoft/xprophetnet-large-wiki100-cased')
model = XLMProphetNetModel.from_pretrained('microsoft/xprophetnet-large-wiki100-cased')
input_ids = tokenizer("Studies have been shown that owning a dog is good for you", return_tensors="pt").input_ids
decoder_input_ids = tokenizer("Studies show that", return_tensors="pt").input_ids
outputs = model(input_ids=input_ids, decoder_input_ids=decoder_input_ids, return_dict=True)
last_hidden_states = outputs.last_hidden_state
last_hidden_states_ngram = outputs.last_hidden_state_ngram""" .

"DESCRIPTION.The code uses the XLMProphetNet model to generate predictions based on input text. It tokenizes the input text, generates decoder input tokens, and then uses the model to predict the next token and n-gram tokens in the sequence." <EXPLAINS> """CODE.from transformers import XLMProphetNetTokenizer, XLMProphetNetForConditionalGeneration
tokenizer = XLMProphetNetTokenizer.from_pretrained('microsoft/xprophetnet-large-wiki100-cased')
model =  XLMProphetNetForConditionalGeneration.from_pretrained('microsoft/xprophetnet-large-wiki100-cased')
input_ids = tokenizer("Studies have been shown that owning a dog is good for you", return_tensors="pt").input_ids
decoder_input_ids = tokenizer("Studies show that", return_tensors="pt").input_ids
outputs = model(input_ids=input_ids, decoder_input_ids=decoder_input_ids, return_dict=True)
logits_next_token = outputs.logits
logits_ngram_next_tokens = outputs.logits_ngram""" .

"DESCRIPTION.The code uses the `tf.keras.activations.get` function to retrieve various activation functions by passing in different arguments: the string 'softmax', the softmax activation function itself, None, the abs function, and the string 'abcd'." <EXPLAINS> """CODE.tf.keras.activations.get('softmax')
tf.keras.activations.get(tf.keras.activations.softmax)
tf.keras.activations.get(None)
tf.keras.activations.get(abs)
tf.keras.activations.get('abcd')
""" .

"DESCRIPTION.The code uses the bytescale function to rescale the pixel values in the input image array to fit within a specified range (default is 0-255). The function returns a new array with the rescaled pixel values in the uint8 data type. The function can also take additional parameters to specify the range of input values (cmin and cmax) or the range of output values (high and low)." <EXPLAINS> """CODE.from scipy.misc import bytescale
img = np.array([[ 91.06794177,   3.39058326,  84.4221549 ],
...                 [ 73.88003259,  80.91433048,   4.88878881],
...                 [ 51.53875334,  34.45808177,  27.5873488 ]])
bytescale(img)
array([[255,   0, 236],
       [205, 225,   4],
       [140,  90,  70]], dtype=uint8)
bytescale(img, high=200, low=100)
array([[200, 100, 192],
       [180, 188, 102],
       [155, 135, 128]], dtype=uint8)
bytescale(img, cmin=0, cmax=255)
array([[91,  3, 84],
       [74, 81,  5],
       [52, 34, 28]], dtype=uint8)""" .

"DESCRIPTION.The code uses the pandas DataFrame method iloc to access and retrieve specific rows and columns from the DataFrame." <EXPLAINS> """CODE.mydict = [{'a': 1, 'b': 2, 'c': 3, 'd': 4},
          {'a': 100, 'b': 200, 'c': 300, 'd': 400},
          {'a': 1000, 'b': 2000, 'c': 3000, 'd': 4000 }]
df = pd.DataFrame(mydict)
df


type(df.iloc[0])
<class 'pandas.core.series.Series'>
df.iloc[0]
a    1
b    2
c    3
d    4
Name: 0, dtype: int64


df.iloc[[0]]
   a  b  c  d
0  1  2  3  4
type(df.iloc[[0]])
<class 'pandas.core.frame.DataFrame'>


df.iloc[[0, 1]]
     a    b    c    d
0    1    2    3    4
1  100  200  300  400


df.iloc[:3]
      a     b     c     d
0     1     2     3     4
1   100   200   300   400
2  1000  2000  3000  4000


df.iloc[[True, False, True]]
      a     b     c     d
0     1     2     3     4
2  1000  2000  3000  4000


df.iloc[lambda x: x.index % 2 == 0]
      a     b     c     d
0     1     2     3     4
2  1000  2000  3000  4000


df.iloc[0, 1]
2


df.iloc[[0, 2], [1, 3]]
      b     d
0     2     4
2  2000  4000


df.iloc[1:3, 0:3]
      a     b     c
1   100   200   300
2  1000  2000  3000


df.iloc[:, [True, False, True, False]]
      a     c
0     1     3
1   100   300
2  1000  3000


df.iloc[:, lambda df: [0, 2]]
      a     c
0     1     3
1   100   300
2  1000  3000
""" .

"DESCRIPTION.The code uses the tf.keras.activations.get() function to retrieve the activation function specified as a parameter. It tries to retrieve the 'softmax' activation function, the softmax activation function itself, None, the abs function, and 'abcd', respectively." <EXPLAINS> """CODE.tf.keras.activations.get('softmax')
tf.keras.activations.get(tf.keras.activations.softmax)
tf.keras.activations.get(None)
tf.keras.activations.get(abs)
tf.keras.activations.get('abcd')""" .

"DESCRIPTION.The code uses the torch library in Python to find unique elements in a given tensor. It returns the unique elements in the tensor and, if specified, also returns the indices of the original tensor that correspond to the unique elements." <EXPLAINS> """CODE.output = torch.unique(torch.tensor([1, 3, 2, 3], dtype=torch.long))
    output
    tensor([ 2,  3,  1])

    output, inverse_indices = torch.unique(
            torch.tensor([1, 3, 2, 3], dtype=torch.long), sorted=True, return_inverse=True)
    output
    tensor([ 1,  2,  3])
    inverse_indices
    tensor([ 0,  2,  1,  2])

    output, inverse_indices = torch.unique(
            torch.tensor([[1, 3], [2, 3]], dtype=torch.long), sorted=True, return_inverse=True)
    output
    tensor([ 1,  2,  3])
    inverse_indices
    tensor([[ 0,  2],
            [ 1,  2]])""" .

"DESCRIPTION.The code uses the torch.unravel_index function to convert flat indices to multi-dimensional indices in a given shape. It handles single and multiple input indices, and can work with different shapes such as 2D, 3D, and 4D." <EXPLAINS> """CODE.import torch
torch.unravel_index(torch.tensor(4), (3, 2))
torch.unravel_index(torch.tensor([4, 1]), (3, 2))
torch.unravel_index(torch.tensor([0, 1, 2, 3, 4, 5]), (3, 2))
torch.unravel_index(torch.tensor([1234, 5678]), (10, 10, 10, 10))
torch.unravel_index(torch.tensor([[1234], [5678]]), (10, 10, 10, 10))
torch.unravel_index(torch.tensor([[1234], [5678]]), (100, 100))
""" .

"DESCRIPTION.The code utilizes a pre-trained CLIP model to extract text features from the provided input text descriptions of images." <EXPLAINS> """CODE.from transformers import CLIPTokenizer, TFCLIPModel

model = TFCLIPModel.from_pretrained("openai/clip-vit-base-patch32")
tokenizer = CLIPTokenizer.from_pretrained("openai/clip-vit-base-patch32")

inputs = tokenizer(["a photo of a cat", "a photo of a dog"], padding=True, return_tensors="tf")
text_features = model.get_text_features(**inputs)
""" .

"DESCRIPTION.The code utilizes a pre-trained transformer model to tokenize and encode the input text \"Hello, my dog is cute\". The model then generates the last hidden states based on the input, which can be further utilized for downstream natural language processing tasks." <EXPLAINS> """CODE.import tensorflow as tf
from transformers import XxxTokenizer, TFXxxModel

tokenizer = XxxTokenizer.from_pretrained('xxx-base-uncased')
model = TFXxxModel.from_pretrained('xxx-base-uncased')
input_ids = tf.constant(tokenizer.encode("Hello, my dog is cute"))[None, :]  # Batch size 1
outputs = model(input_ids)
last_hidden_states = outputs[0]  # The last hidden-state is the first element of the output tuple""" .

"DESCRIPTION.The code utilizes the Bernoulli distribution from PaddlePaddle to generate samples with different probabilities. It demonstrates sampling from a Bernoulli distribution with various parameters, reshaping the samples, applying the sigmoid function, and calculating the sum of the sigmoid outputs." <EXPLAINS> """CODE.import paddle
from paddle.distribution import Bernoulli

paddle.seed(2023)

rv = Bernoulli(paddle.full((), 0.3))
print(rv.sample([100]).shape)

rv = Bernoulli(0.3)
print(rv.rsample([100]).shape)

rv = Bernoulli(paddle.to_tensor([0.3, 0.5]))
print(rv.rsample([100]).shape)

rv = Bernoulli(paddle.to_tensor([0.3, 0.5]))
print(rv.rsample([100, 2]).shape)

rv = Bernoulli(0.3)
rsample = rv.rsample([3, ])
rsample_sigmoid = paddle.nn.functional.sigmoid(rsample)
print(rsample, rsample_sigmoid)

print(paddle.nn.functional.sigmoid(rv.rsample([1000, ], temperature=1.0)).sum())

print(paddle.nn.functional.sigmoid(rv.rsample([1000, ], temperature=0.1)).sum())""" .

"DESCRIPTION.The code utilizes the GPTNeoXJapaneseTokenizer to encode and decode text, ensuring that variant forms of the same word (like æ¶å¿ and æ¶æ) are correctly represented with the same token." <EXPLAINS> """CODE.from transformers import GPTNeoXJapaneseTokenizer

tokenizer = GPTNeoXJapaneseTokenizer.from_pretrained("abeja/gpt-neox-japanese-2.7b")
# You can confirm both æ¶å¿ and æ¶æ are encoded to 17749
tokenizer("å¾è¼©ã¯ç«ã§ããð¯ãå®ã¯æ¶å¿(æ¶æ)å¤§å­¦åºèº«")["input_ids"]
[30014, 26883, 26638, 27228, 25, 26650, 31732, 31679, 27809, 26638, 17749, 31592, 17749, 31593, 321, 1281]

# Both æ¶å¿ and æ¶æ are decoded to æ¶å¿
tokenizer.decode(tokenizer("å¾è¼©ã¯ç«ã§ããð¯ãå®ã¯æ¶å¿(æ¶æ)å¤§å­¦åºèº«")["input_ids"])
'å¾è¼©ã¯ç«ã§ããð¯ãå®ã¯æ¶å¿(æ¶å¿)å¤§å­¦åºèº«'
""" .

"DESCRIPTION.The code utilizes the HorovodRayAccelerator to distribute the training of a MNIST classifier across 2 nodes with 4 workers per node, each worker using 1 CPU and 1 GPU. It sets up the Trainer with 1 GPU and the specified accelerator to train the ptl_model on the MNIST dataset." <EXPLAINS> """CODE.import pytorch_lightning as ptl
from ray.util.lightning_accelerators import HorovodRayAccelerator

ptl_model = MNISTClassifier(...)
# 2 nodes, 4 workers per node, each using 1 CPU and 1 GPU.
accelerator = HorovodRayAccelerator(num_hosts=2, num_slots=4,
    use_gpu=True)

# If using GPUs, set the ``gpus`` arg to a value > 0.
# The actual number of GPUs is determined by ``num_slots``.
trainer = pl.Trainer(..., gpus=1, accelerator=accelerator)
trainer.fit(ptl_model)""" .

"DESCRIPTION.The code utilizes the HorovodRayAccelerator to distribute the training of a MNIST classifier model across 2 nodes with 4 workers per node, each utilizing 1 CPU and 1 GPU. The Trainer object is then created with 1 GPU specified, and the model is trained using this configuration." <EXPLAINS> """CODE.import pytorch_lightning as ptl
from ray.util.lightning_accelerators import HorovodRayAccelerator

ptl_model = MNISTClassifier(...)
# 2 nodes, 4 workers per node, each using 1 CPU and 1 GPU.
accelerator = HorovodRayAccelerator(num_hosts=2, num_slots=4,
    use_gpu=True)

# If using GPUs, set the ``gpus`` arg to a value > 0.
# The actual number of GPUs is determined by ``num_slots``.
trainer = pl.Trainer(..., gpus=1, accelerator=accelerator)
trainer.fit(ptl_model)""" .

"DESCRIPTION.The code utilizes the InferenceClient class from the huggingface_hub library to generate conversational responses. It first asks \"Hi, who are you?\" and receives a generated response \"I am the one who knocks.\" It then continues the conversation by responding to \"Wow, that's scary!\" using the previously generated response." <EXPLAINS> """CODE.from huggingface_hub import InferenceClient
client = InferenceClient()
output = client.conversational("Hi, who are you?")
output
{'generated_text': 'I am the one who knocks.', 'conversation': {'generated_responses': ['I am the one who knocks.'], 'past_user_inputs': ['Hi, who are you?']}, 'warnings': ['Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.']}
client.conversational(
...     "Wow, that's scary!",
...     generated_responses=output["conversation"]["generated_responses"],
...     past_user_inputs=output["conversation"]["past_user_inputs"],
... )
""" .

"DESCRIPTION.The code utilizes the InferenceClient from the Hugging Face Hub library to perform automatic speech recognition on the \"hello_world.wav\" file, returning the recognized text as \"hello world\"." <EXPLAINS> """CODE.from huggingface_hub import InferenceClient
client = InferenceClient()
client.automatic_speech_recognition("hello_world.wav")
"hello world"
""" .

"DESCRIPTION.The code utilizes the torch.unique function in PyTorch to return unique elements from input tensors along with optional outputs such as sorted output and inverse indices. The function can handle 1-dimensional and 2-dimensional input tensors and returns unique elements along with their respective inverse indices if specified." <EXPLAINS> """CODE.output = torch.unique(torch.tensor([1, 3, 2, 3], dtype=torch.long))
output
tensor([ 2,  3,  1])

output, inverse_indices = torch.unique(
        torch.tensor([1, 3, 2, 3], dtype=torch.long), sorted=True, return_inverse=True)
output
tensor([ 1,  2,  3])
inverse_indices
tensor([ 0,  2,  1,  2])

output, inverse_indices = torch.unique(
        torch.tensor([[1, 3], [2, 3]], dtype=torch.long), sorted=True, return_inverse=True)
output
tensor([ 1,  2,  3])
inverse_indices
tensor([[ 0,  2],
        [ 1,  2]])
""" .

"DESCRIPTION.The code validates the repository IDs \"valid_repo_id\" and \"other..repo..id\"." <EXPLAINS> """CODE.from huggingface_hub.utils import validate_repo_id
validate_repo_id(repo_id="valid_repo_id")
validate_repo_id(repo_id="other..repo..id")
""" .

"DESCRIPTION.The code validates traditional ZIP codes, ZIP+4 forms, and identifies invalid ZIP codes." <EXPLAINS> """CODE.zip = Combine(Word(nums, exact=5) + Optional('-' + Word(nums, exact=4))
zip.runTests('''
    # traditional ZIP code
    12345

    # ZIP+4 form
    12101-0001

    # invalid ZIP
    98765-
    ''')""" .

"DESCRIPTION.The code validates whether the input array meets the schema requirements, including being an array with items that are in the enum list [1, 2, 3] and having a maximum length of 2. If the input does not meet these requirements, it will output the corresponding error messages." <EXPLAINS> """CODE.schema = {
...     "type" : "array",
...     "items" : {"enum" : [1, 2, 3]},
...     "maxItems" : 2,
... }
v = Draft3Validator(schema)
for error in sorted(v.iter_errors([2, 3, 4]), key=str):
...     print(error.message)
4 is not one of [1, 2, 3]
[2, 3, 4] is too long""" .

"DESCRIPTION.The code verifies whether the input string \"100\" is made up of numerical digits." <EXPLAINS> """CODE.expr = Word(nums)
assert expr.matches("100")""" .

"DESCRIPTION.The code visualizes the feature importances of the model based on the specified features: 'width', 'height', and 'length'." <EXPLAINS> "CODE.wandb.sklearn.plot_feature_importances(model, ['width', 'height, 'length'])" .

"""DESCRIPTION.The code wraps the `tf.keras.layers.Conv2D` function with Spectral Normalization and applies it to a random input `x` with shape (1, 10, 10, 1), resulting in an output `y` with shape (1, 9, 9, 2).

The code also wraps the `tf.keras.layers.Dense` function with Spectral Normalization and applies it to the same input `x`, resulting in an output `y` with shape (1, 10, 10, 10).""" <EXPLAINS> """CODE.Wrap `keras.layers.Conv2D`:
x = np.random.rand(1, 10, 10, 1)
conv2d = SpectralNormalization(tf.keras.layers.Conv2D(2, 2))
y = conv2d(x)
y.shape
TensorShape([1, 9, 9, 2])

Wrap `keras.layers.Dense`:
x = np.random.rand(1, 10, 10, 1)
dense = SpectralNormalization(tf.keras.layers.Dense(10))
y = dense(x)
y.shape
TensorShape([1, 10, 10, 10])""" .

"DESCRIPTION.The code writes a prediction tensor to a file named 'my_predictions.pt' using the function 'write_prediction' with the key 'pred'." <EXPLAINS> "CODE.self.write_prediction('pred', torch.tensor(...), filename='my_predictions.pt')" .

"DESCRIPTION.The code writes images stored in a column named \"image\" to the local path \"/tmp/images\" in different file formats such as jpeg, png, jpg, and bmp." <EXPLAINS> """CODE.ds.write_images("local:///tmp/images", column="image")
ds.write_images("local:///tmp/images", column="image", file_format="jpeg")
ds.write_images("local:///tmp/images", column="image", file_format="png")
ds.write_images("local:///tmp/images", column="image", file_format="jpg")
ds.write_images("local:///tmp/images", column="image", file_format="bmp")""" .

"DESCRIPTION.The code zips and then unzips datasets, once with tuples and once with dictionaries." <EXPLAINS> """CODE.ds1 = tf.data.Dataset.from_tensor_slices([1, 2, 3])
ds2 = tf.data.Dataset.from_tensor_slices([4, 5, 6])
ds_zipped_tuple = tf.data.Dataset.zip((ds1, ds2))
ds_unzipped_tuple = _unzip_dataset(ds_zipped_tuple)
ds_zipped_dict = tf.data.Dataset.zip({'ds1': ds1, 'ds2': ds2})
ds_unzipped_dict = _unzip_dataset(ds_zipped_dict)""" .

"""DESCRIPTION.The first block of code creates a DataFrame with columns 'A', 'B', and 'C', then converts it to an xarray.

The second block of code creates a DataFrame with columns 'A', 'B', and 'C', sets the index to be a MultiIndex using columns 'B' and 'A', and then converts it to an xarray.

The third block of code creates a Panel with item axis 'A', 'B', 'C', and 'D', major axis as a date range from '20130101' with 3 periods, and minor axis 'first' and 'second', then converts it to an xarray.""" <EXPLAINS> """CODE.df = pd.DataFrame({'A' : [1, 1, 2],
                   'B' : ['foo', 'bar', 'foo'],
                   'C' : np.arange(4.,7)})
df.to_xarray()

df = pd.DataFrame({'A' : [1, 1, 2],
                   'B' : ['foo', 'bar', 'foo'],
                   'C' : np.arange(4.,7)}
                 ).set_index(['B','A'])
df.to_xarray()

p = pd.Panel(np.arange(24).reshape(4,3,2),
             items=list('ABCD'),
             major_axis=pd.date_range('20130101', periods=3),
             minor_axis=['first', 'second'])
p.to_xarray()""" .

"DESCRIPTION.The first class `Foo` defines a neural network module that takes an input `x`, passes it through a dense layer with 4 units, saves the intermediate result 'h', and then passes it through another dense layer with 2 units before returning the final result. The second class `Foo2` also saves intermediate results 'h' for the input `x` and its double, using custom initialization and reduction functions, before returning the input itself." <EXPLAINS> """CODE.import jax
import jax.numpy as jnp
import flax.linen as nn

class Foo(nn.Module):
  @nn.compact
  def __call__(self, x):
    h = nn.Dense(4)(x)
    self.sow('intermediates', 'h', h)
    return nn.Dense(2)(h)

x = jnp.ones((16, 9))
model = Foo()
variables = model.init(jax.random.PRNGKey(0), x)
y, state = model.apply(variables, x, mutable=['intermediates'])
print(state['intermediates'])  # {'h': (...,)}

class Foo2(nn.Module):
  @nn.compact
  def __call__(self, x):
    init_fn = lambda: 0
    reduce_fn = lambda a, b: a + b
    self.sow('intermediates', 'h', x,
             init_fn=init_fn, reduce_fn=reduce_fn)
    self.sow('intermediates', 'h', x * 2,
             init_fn=init_fn, reduce_fn=reduce_fn)
    return x

model = Foo2()
variables = model.init(jax.random.PRNGKey(0), x)
y, state = model.apply(variables, jnp.ones((1, 1)), mutable=['intermediates'])
print(state['intermediates'])  # ==> {'h': [[3.]]}
""" .

"""DESCRIPTION.The first code segment defines a Keras model, compiles it, converts it to an Estimator, defines an input function, and trains the Estimator for one step.

The second code segment defines input layers, output layers, a Keras model, compiles it, defines export outputs, converts it to an Estimator with specified export outputs, defines an input function, and trains the Estimator for one step.""" <EXPLAINS> """CODE.keras_model = tf.keras.Model(...)
keras_model.compile(...)

estimator = tf.keras.estimator.model_to_estimator(keras_model)

def input_fn():
  return dataset_ops.Dataset.from_tensors(
      ({'features': features, 'sample_weights': sample_weights},
       targets))

estimator.train(input_fn, steps=1)


inputs = {'a': tf.keras.Input(..., name='a'),
          'b': tf.keras.Input(..., name='b')}
outputs = {'c': tf.keras.layers.Dense(..., name='c')(inputs['a']),
           'd': tf.keras.layers.Dense(..., name='d')(inputs['b'])}
keras_model = tf.keras.Model(inputs, outputs)
keras_model.compile(...)
export_outputs = {'c': tf.estimator.export.RegressionOutput,
                  'd': tf.estimator.export.ClassificationOutput}

estimator = tf.keras.estimator.model_to_estimator(
    keras_model, export_outputs=export_outputs)

def input_fn():
  return dataset_ops.Dataset.from_tensors(
      ({'features': features, 'sample_weights': sample_weights},
       targets))

estimator.train(input_fn, steps=1)
""" .

"""DESCRIPTION.The first code segment defines a class Foo that implements a neural network module, where the __call__ method applies two Dense layers to the input x and stores the intermediate result h in the state dictionary. The method then returns the output after applying another Dense layer to h.

The second code segment defines a class Foo that implements a neural network module, where the __call__ method performs some calculations on the input x and stores the intermediate results in the state dictionary with custom initialization and reduction functions. It then returns the input x.""" <EXPLAINS> """CODE.class Foo(nn.Module):
    @nn.compact
    def __call__(self, x):
      h = nn.Dense(4)(x)
      self.sow('intermediates', 'h', h)
      return nn.Dense(2)(h)
  y, state = Foo.apply(params, x, mutable=['intermediates'])
  print(state['intermediates'])  # {'h': (...,)}

class Foo(nn.Module):
    @nn.compact
    def __call__(self, x):
      init_fn = lambda: 0
      reduce_fn = lambda a, b: a + b
      self.sow('intermediates', x, h,
               init_fn=init_fn, reduce_fn=reduce_fn)
      self.sow('intermediates', x * 2, h,
               init_fn=init_fn, reduce_fn=reduce_fn)
      return x
  y, state = Foo.apply(params, 1, mutable=['intermediates'])
  print(state['intermediates'])  # ==> {'h': 3}
""" .

"""DESCRIPTION.The first code snippet creates a StaticWebFrontend object that serves files from a specified folder path when configuring the layout in the Work class.

The second code snippet configures a layout by defining tabs with specific content for each tab in the Work class.""" <EXPLAINS> """CODE.from lightning_app.frontend import StaticWebFrontend

class Work(LightningWork):
    def configure_layout(self):
        return StaticWebFrontend("path/to/folder/to/serve")

class Work(LightningWork):
    def configure_layout(self):
        return [
            dict(name="First Tab", content=self.child0),
            dict(name="Second Tab", content=self.child1),
            dict(name="Lightning", content="https://lightning.ai"),
        ]
""" .

"""DESCRIPTION.The first code snippet creates a custom learning rate scheduler using ExponentialDecay in the Keras library, with an initial learning rate of 1e-2, decay steps of 10000, and decay rate of 0.9. It then uses this scheduler to initialize a stochastic gradient descent (SGD) optimizer.

The second code snippet defines a custom learning rate schedule class that divides the initial learning rate by the step (plus 1) when called. It then uses this custom learning rate scheduler to initialize a SGD optimizer in TensorFlow.""" <EXPLAINS> """CODE.lr_schedule = keras.optimizers.schedules.ExponentialDecay(
    initial_learning_rate=1e-2,
    decay_steps=10000,
    decay_rate=0.9)
optimizer = keras.optimizers.SGD(learning_rate=lr_schedule)


class MyLRSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):

  def __init__(self, initial_learning_rate):
    self.initial_learning_rate = initial_learning_rate

  def __call__(self, step):
     return self.initial_learning_rate / (step + 1)

optimizer = tf.keras.optimizers.SGD(learning_rate=MyLRSchedule(0.1))
""" .

"""DESCRIPTION.The first code snippet defines a custom layer in Python using TensorFlow's Keras API. The call method of the MyLayer class calculates the absolute mean of the input data and adds it as a loss to the layer.

The second code snippet creates a neural network model in Python using TensorFlow's Keras API. It consists of two Dense layers and applies activity regularization by adding the absolute mean of the first Dense layer's outputs as a loss to the model.

The third code snippet defines another neural network model in Python using TensorFlow's Keras API. It adds weight regularization by adding the mean of the kernel weights of the first Dense layer as a loss function to the model.""" <EXPLAINS> """CODE.class MyLayer(tf.keras.layers.Layer):
  def call(self, inputs):
    self.add_loss(tf.abs(tf.reduce_mean(inputs)))
    return inputs


inputs = tf.keras.Input(shape=(10,))
x = tf.keras.layers.Dense(10)(inputs)
outputs = tf.keras.layers.Dense(1)(x)
model = tf.keras.Model(inputs, outputs)
# Activity regularization.
model.add_loss(tf.abs(tf.reduce_mean(x)))


inputs = tf.keras.Input(shape=(10,))
d = tf.keras.layers.Dense(10)
x = d(inputs)
outputs = tf.keras.layers.Dense(1)(x)
model = tf.keras.Model(inputs, outputs)
# Weight regularization.
model.add_loss(lambda: tf.reduce_mean(d.kernel))
""" .

"""DESCRIPTION.The first code snippet defines a custom layer in TensorFlow that calculates the absolute mean of the input values and adds it as a regularization loss to the layer.

The second code snippet creates a neural network model using TensorFlow with two dense layers. It adds the absolute mean of the second layer's output values as a regularization loss to the model.

The third code snippet creates a neural network model using TensorFlow with two dense layers. It adds the absolute mean of the weight values in the first dense layer as a regularization loss to the model.""" <EXPLAINS> """CODE.class MyLayer(tf.keras.layers.Layer):
  def call(self, inputs):
    self.add_loss(tf.abs(tf.reduce_mean(inputs)))
    return inputs


inputs = tf.keras.Input(shape=(10,))
x = tf.keras.layers.Dense(10)(inputs)
outputs = tf.keras.layers.Dense(1)(x)
model = tf.keras.Model(inputs, outputs)
# Activity regularization.
model.add_loss(tf.abs(tf.reduce_mean(x)))


inputs = tf.keras.Input(shape=(10,))
d = tf.keras.layers.Dense(10)
x = d(inputs)
outputs = tf.keras.layers.Dense(1)(x)
model = tf.keras.Model(inputs, outputs)
# Weight regularization.
model.add_loss(lambda: tf.reduce_mean(d.kernel))
""" .

"""DESCRIPTION.The first code snippet defines a neural network module named `Foo` that takes an input `x`, applies two Dense layers and then returns the output. It also saves the intermediate result `h` during the forward pass. The final output is the result of passing `h` through another Dense layer.

The second code snippet defines a neural network module named `Foo` that takes an input `x`, saves two different results `x` and `x*2` during the forward pass, and returns `x`. The saved results `x` and `x*2` are reduced using a custom reduce function and the final output is the result of applying the reduce function to the saved values.""" <EXPLAINS> """CODE.class Foo(nn.Module):
    @nn.compact
    def __call__(self, x):
      h = nn.Dense(4)(x)
      self.sow('intermediates', 'h', h)
      return nn.Dense(2)(h)
  y, state = Foo.apply(params, x, mutable=['intermediates'])
  print(state['intermediates'])  # {'h': (...,)}

class Foo(nn.Module):
    @nn.compact
    def __call__(self, x):
      init_fn = lambda: 0
      reduce_fn = lambda a, b: a + b
      self.sow('intermediates', x, h,
               init_fn=init_fn, reduce_fn=reduce_fn)
      self.sow('intermediates', x * 2, h,
               init_fn=init_fn, reduce_fn=reduce_fn)
      return x
  y, state = Foo.apply(params, 1, mutable=['intermediates'])
  print(state['intermediates'])  # ==> {'h': 3}
""" .

"""DESCRIPTION.The first code snippet trains a RL (Reinforcement Learning) model using the Proximal Policy Optimization (PPO) algorithm on the CartPole-v0 environment for 5 training iterations with 2 workers and no GPU. It evaluates the model with 1 worker at an interval of 1 iteration using a sampler input.

The second code snippet uses Behavioral Cloning (BC) Trainer to train a RL model on the same CartPole-v0 environment with the specified dataset from the "/tmp/data-dir", with 2 workers for parallelism and 1 CPU for the remote dataset reading. It follows a similar evaluation setup as mentioned in the first code snippet.""" <EXPLAINS> """CODE.from ray.ml.config import RunConfig
from ray.ml.train.integrations.rl import RLTrainer

trainer = RLTrainer(
    run_config=RunConfig(stop={"training_iteration": 5}),
    scaling_config={
        "num_workers": 2,
        "use_gpu": False,
    },
    algorithm="PPO",
    config={
        "env": "CartPole-v0",
        "framework": "tf",
        "evaluation_num_workers": 1,
        "evaluation_interval": 1,
        "evaluation_config": {"input": "sampler"},
    },
)
result = trainer.fit()

import ray
from ray.ml.config import RunConfig
from ray.ml.train.integrations.rl import RLTrainer
from ray.rllib.agents.marwil.bc import BCTrainer

dataset = ray.data.read_json(
    "/tmp/data-dir", parallelism=2, ray_remote_args={"num_cpus": 1}
)

trainer = RLTrainer(
    run_config=RunConfig(stop={"training_iteration": 5}),
    scaling_config={
        "num_workers": 2,
        "use_gpu": False,
    },
    datasets={"train": dataset},
    algorithm=BCTrainer,
    config={
        "env": "CartPole-v0",
        "framework": "tf",
        "evaluation_num_workers": 1,
        "evaluation_interval": 1,
        "evaluation_config": {"input": "sampler"},
    },
)
result = trainer.fit()""" .

"""DESCRIPTION.The first example code initializes an embedding layer and an Adam optimizer with a learning rate of 0.001. It then retrieves and prints the current step learning rate, which is 0.001.

The second example code generates random input, applies a linear transformation, calculates the loss, and initializes an Adam optimizer with piecewise decay learning rate schedule. The step learning rate changes at specific steps according to the provided schedule. The code then iterates through 12 steps, minimizing the loss and verifying that the current step learning rate matches the expected values based on the piecewise decay schedule.""" <EXPLAINS> """CODE.import paddle.fluid as fluid
import numpy as np

# example1: LearningRateDecay is not used, return value is all the same
with fluid.dygraph.guard():
    emb = fluid.dygraph.Embedding([10, 10])
    adam = fluid.optimizer.Adam(0.001, parameter_list = emb.parameters())
    lr = adam.current_step_lr()
    print(lr) # 0.001

# example2: PiecewiseDecay is used, return the step learning rate
with fluid.dygraph.guard():
    inp = np.random.uniform(-0.1, 0.1, [10, 10]).astype("float32")
    linear = fluid.dygraph.nn.Linear(10, 10)
    inp = fluid.dygraph.to_variable(inp)
    out = linear(inp)
    loss = fluid.layers.reduce_mean(out)

    bd = [2, 4, 6, 8]
    value = [0.2, 0.4, 0.6, 0.8, 1.0]
    adam = fluid.optimizer.Adam(fluid.dygraph.PiecewiseDecay(bd, value, 0),
                           parameter_list=linear.parameters())

    # first step: learning rate is 0.2
    np.allclose(adam.current_step_lr(), 0.2, rtol=1e-06, atol=0.0) # True

    # learning rate for different steps
    ret = [0.2, 0.2, 0.4, 0.4, 0.6, 0.6, 0.8, 0.8, 1.0, 1.0, 1.0, 1.0]
    for i in range(12):
        adam.minimize(loss)
        lr = adam.current_step_lr()
        np.allclose(lr, ret[i], rtol=1e-06, atol=0.0) # True""" .

"DESCRIPTION.The first function \"noop\" sets the result of the \"bucket.get_tensors()\" to a future object and returns the future object. The second function \"encode_and_decode\" divides each tensor in the bucket by the world size, encodes the tensors, performs allreduce on the encoded tensors, and then decodes the result to return the decoded gradients." <EXPLAINS> """CODE.def noop(state: object, bucket: dist._GradBucket) -> torch.futures.Future
    fut = torch.futures.Future()
    fut.set_result(bucket.get_tensors())
    return fut

ddp._register_comm_hook(state = None, hook = noop)

def encode_and_decode(state: object, bucket: dist._GradBucket) -> torch.futures.Future
    tensors = [t / process_group.world_size for t in bucket.get_tensors()]
    encoded_tensors = encode(tensors) # encode gradients
    fut = process_group.allreduce(encoded_tensors).get_future()
    # Define the then callback to decode.
    def decode(fut):
        decoded_tensors = decode(fut.value()) # decode gradients
        return decoded_tensors
    return fut.then(decode)

ddp._register_comm_hook(state = None, hook = encode_and_decode)""" .

"""DESCRIPTION.The first part of the code defines a neural network model (Foo) using the Flax library, which includes two fully connected layers. The model takes an input tensor, applies the first dense layer with 4 units, saves the intermediate result of the first dense layer, and then applies a second dense layer with 2 units before returning the final output tensor.

The second part of the code defines another neural network model (Foo2) using the Flax library, which directly manipulates and saves intermediate states during the computation. It initializes an intermediate state 'h' as the input tensor x, and then saves another intermediate state 'h' as the input tensor x multiplied by 2. Finally, the model returns the input tensor x without any additional computations.""" <EXPLAINS> """CODE.import jax
import jax.numpy as jnp
import flax.linen as nn

class Foo(nn.Module):
  @nn.compact
  def __call__(self, x):
    h = nn.Dense(4)(x)
    self.sow('intermediates', 'h', h)
    return nn.Dense(2)(h)

x = jnp.ones((16, 9))
model = Foo()
variables = model.init(jax.random.PRNGKey(0), x)
y, state = model.apply(variables, x, mutable=['intermediates'])
print(state['intermediates'])  # {'h': (...,)}

class Foo2(nn.Module):
  @nn.compact
  def __call__(self, x):
    init_fn = lambda: 0
    reduce_fn = lambda a, b: a + b
    self.sow('intermediates', 'h', x,
             init_fn=init_fn, reduce_fn=reduce_fn)
    self.sow('intermediates', 'h', x * 2,
             init_fn=init_fn, reduce_fn=reduce_fn)
    return x

model = Foo2()
variables = model.init(jax.random.PRNGKey(0), x)
y, state = model.apply(variables, jnp.ones((1, 1)), mutable=['intermediates'])
print(state['intermediates'])  # ==> {'h': [[3.]]}
""" .

"DESCRIPTION.The function \"_as_arg_names\" takes a string input representing a list of comma-separated variable names, and returns a list of individual variable names." <EXPLAINS> "CODE._as_arg_names(\"a, b, c\") == [\"a\", \"b\", \"c\"]" .

"DESCRIPTION.The function \"func\" simply returns the input value \"x\"." <EXPLAINS> """CODE.@Deprecated
def func(x):
    return x""" .

"DESCRIPTION.The function \"sequence\" takes a single input and returns the input wrapped in a tuple if the input is an integer, and returns the input as a list if the input is a list." <EXPLAINS> """CODE.sequence(1)
(1,)
sequence([1])
[1]""" .

"DESCRIPTION.The function \"stop_server\" is responsible for killing the process associated with the server being managed." <EXPLAINS> """CODE.def stop_server(self):
    self._process.kill()""" .

"DESCRIPTION.The function 'flatten_with_path' takes a nested dictionary as input and returns a list of tuples where each tuple contains the path to a leaf node in the dictionary and the corresponding value." <EXPLAINS> """CODE.flatten_with_path({'a': {'b': v}}) == [(('a', 'b'), v)]
""" .

"DESCRIPTION.The function 'get_uid' returns a unique identifier for the input string 'dense' each time it is called, starting from 1 and incrementing by 1 each time." <EXPLAINS> """CODE.
  get_uid('dense')
  1
  get_uid('dense')
  2
""" .

"DESCRIPTION.The function _adjoint_identity takes in a linear operator and returns the identity matrix." <EXPLAINS> """CODE.@linear_operator_algebra.RegisterAdjoint(lin_op.LinearOperatorIdentity)
def _adjoint_identity(lin_op_a):
  # Return the identity matrix.""" .

"DESCRIPTION.The function _column_name_to_strings converts input tuples into tuples of strings." <EXPLAINS> """CODE.name = 'foo'
_column_name_to_strings(name)
'foo'
name = ('foo', 'bar')
_column_name_to_strings(name)
('foo', 'bar')
import pandas as pd
name = (1, pd.Timestamp('2017-02-01 00:00:00'))
_column_name_to_strings(name)
('1', '2017-02-01 00:00:00')""" .

"DESCRIPTION.The function _get_max_shape takes a list of lists as input and returns the maximum shape, in terms of the number of sublists and elements within each sublist, among all the sublists in the input list." <EXPLAINS> "CODE._get_max_shape([[], [[1], [2]], []])" .

"DESCRIPTION.The function _matmul_identity computes the matrix product of two identity matrices and returns the resultant identity matrix." <EXPLAINS> """CODE.@linear_operator_algebra.RegisterMatmul(
  lin_op.LinearOperatorIdentity,
  lin_op.LinearOperatorIdentity)
def _matmul_identity(a, b):
  # Return the identity matrix.""" .

"DESCRIPTION.The function _tp_tn_fp_fn takes in two arrays, y_true and y_pred, and calculates the true positives, true negatives, false positives, and false negatives. It returns these values as arrays." <EXPLAINS> """CODE.from sklearn.metrics.metrics import _tp_tn_fp_fn
y_pred = [0, 1, 0, 0]
y_true = [0, 1, 0, 1]
_tp_tn_fp_fn(y_true, y_pred)
(array([2, 1]), array([1, 2]), array([1, 0]), array([0, 1]))

y_true = np.array([0, 1, 2, 0, 1, 2])
y_pred = np.array([0, 2, 1, 0, 0, 1])
_tp_tn_fp_fn(y_true, y_pred)
(array([2, 0, 0]), array([3, 2, 3]), array([1, 2, 1]), array([0, 2, 2]))

_tp_tn_fp_fn(np.array([[0.0, 1.0], [1.0, 1.0]]), np.zeros((2, 2)))
(array([0, 0]), array([1, 0]), array([0, 0]), array([1, 2]))

_tp_tn_fp_fn([(1, 2), (3, )], [(1, 2), tuple()])
(array([1, 1, 0]), array([1, 1, 1]), array([0, 0, 0]), array([0, 0, 1])""" .

"DESCRIPTION.The function _unflatten_single_dict takes a single-level dictionary as input and unflattens it into a nested dictionary where the keys are divided based on the underscores in the original keys." <EXPLAINS> """CODE._unflatten_single_dict({
  foo_bar_baz: 123,
  foo_bar_biz: 456,
  x_bonks: 'hi',
})""" .

"DESCRIPTION.The function `_add_ragged_fields` takes in a list of lists as input and a TensorInfo object. It processes the input list to create a dictionary with two keys: 'ragged_flat_values' and 'ragged_row_length_0'. The 'ragged_flat_values' key contains a flattened version of the input list with all elements concatenated in order, along with the corresponding TensorInfo object. The 'ragged_row_length_0' key contains the length of each sub-list in the input list, along with the corresponding TensorInfo object." <EXPLAINS> """CODE.example_data = [
    [1, 2, 3],
    [],
    [4, 5]
]
tensor_info = TensorInfo(shape=(None, None,), sequence_rank=2, ...)
out = _add_ragged_fields(example_data, tensor_info)
out == {
    'ragged_flat_values': ([0, 1, 2, 3, 4, 5], TensorInfo(shape=(), ...)),
    'ragged_row_length_0': ([3, 0, 2], TensorInfo(shape=(None,), ...))
}
""" .

"DESCRIPTION.The function `_column_name_to_strings` converts the input into a tuple of strings, where each element in the tuple is converted to a string." <EXPLAINS> """CODE.name = 'foo'
_column_name_to_strings(name)
'foo'
name = ('foo', 'bar')
_column_name_to_strings(name)
('foo', 'bar')
import pandas as pd
name = (1, pd.Timestamp('2017-02-01 00:00:00'))
_column_name_to_strings(name)
('1', '2017-02-01 00:00:00')""" .

"DESCRIPTION.The function `_del_nested_attr` removes the attributes 'conv' and 'weight' from the object `obj`." <EXPLAINS> "CODE._del_nested_attr(obj, ['conv', 'weight'])" .

"DESCRIPTION.The function `_recode_for_categories` recodes the input array of codes based on old and new categories. It assigns the new codes for categories in the input array based on the mapping provided by the old and new categories." <EXPLAINS> """CODE.old_cat = pd.Index(['b', 'a', 'c'])
new_cat = pd.Index(['a', 'b'])
codes = np.array([0, 1, 1, 2])
_recode_for_categories(codes, old_cat, new_cat)
array([ 1,  0,  0, -1])""" .

"DESCRIPTION.The function `_recursive_pad` pads the input list of lists with a specified fill value to ensure all inner lists have the same length." <EXPLAINS> """CODE._recursive_pad([[], [1], [2, 3], [4]], fill_value=0)  # doctest: +NORMALIZE_WHITESPACE
array([[0, 0], [1, 0], [2, 3], [4, 0]], dtype=object)""" .

"DESCRIPTION.The function `_resolve_aliases` takes a parameter `aliases` and ensures it is a list. If `aliases` is a string, it is converted into a list with that string as the only element. If `aliases` is None, an empty list is assigned to it. The function then appends the string \"latest\" to the `aliases` list and returns the modified list." <EXPLAINS> """CODE.def _resolve_aliases(aliases):
    if isinstance(aliases, str):
        aliases = [aliases]
    elif aliases is None:
        aliases = []
    aliases.append("latest")
    return aliases

aliases = _resolve_aliases(["best", "dev"])
assert aliases == ["best", "dev", "latest"]

aliases = _resolve_aliases("boom")
assert aliases == ["boom", "latest"]
""" .

"DESCRIPTION.The function `add_ngram` takes a list of sequences, a dictionary mapping n-grams to indices, and an integer ngram_range as input. It iterates through each sequence, extracts n-grams of lengths 2 to ngram_range, checks if the n-gram is in the token_indice dictionary, and appends the corresponding index to the sequence. Finally, it returns the updated sequences." <EXPLAINS> """CODE.def add_ngram(sequences, token_indice, ngram_range):
...     for i in range(len(sequences)):
...         for n in range(2, ngram_range + 1):
...             for j in range(len(sequences[i]) - n + 1):
...                 ngram = tuple(sequences[i][j:j + n])
...                 if ngram in token_indice:
...                     sequences[i].append(token_indice[ngram])
...     return sequences
...
sequences = [[1, 3, 4, 5], [1, 3, 7, 9, 2]]
token_indice = {(1, 3): 1337, (9, 2): 42, (4, 5): 2017}
add_ngram(sequences, token_indice, ngram_range=2)
[[1, 3, 4, 5, 1337, 2017], [1, 3, 7, 9, 2, 1337, 42]]

sequences = [[1, 3, 4, 5], [1, 3, 7, 9, 2]]
token_indice = {(1, 3): 1337, (9, 2): 42, (4, 5): 2017, (7, 9, 2): 2018}
add_ngram(sequences, token_indice, ngram_range=3)
[[1, 3, 4, 5, 1337], [1, 3, 7, 9, 2, 1337, 2018]]""" .

"""DESCRIPTION.The function `antirectifier` takes input tensor `x`, normalizes it along axis 1, calculates both positive and negative values based on the rectified linear activation function, and concatenates them together before returning the result.

The function `hadamard_product_sum` computes the Hadamard product of two input tensors, sums along the last axis, and returns both the element-wise product and the sum of the products.""" <EXPLAINS> """CODE.    def antirectifier(x):
        x -= K.mean(x, axis=1, keepdims=True)
        x = K.l2_normalize(x, axis=1)
        pos = K.relu(x)
        neg = K.relu(-x)
        return K.concatenate([pos, neg], axis=1)

    def antirectifier_output_shape(input_shape):
        shape = list(input_shape)
        assert len(shape) == 2  # only valid for 2D tensors
        shape[-1] *= 2
        return tuple(shape)

    model.add(Lambda(antirectifier,
                     output_shape=antirectifier_output_shape))

    def hadamard_product_sum(tensors):
        out1 = tensors[0] * tensors[1]
        out2 = K.sum(out1, axis=-1)
        return [out1, out2]

    def hadamard_product_sum_output_shape(input_shapes):
        shape1 = list(input_shapes[0])
        shape2 = list(input_shapes[1])
        assert shape1 == shape2  # else hadamard product isn't possible
        return [tuple(shape1), tuple(shape2[:-1])]

    x1 = Dense(32)(input_1)
    x2 = Dense(32)(input_2)
    layer = Lambda(hadamard_product_sum, hadamard_product_sum_output_shape)
    x_hadamard, x_sum = layer([x1, x2])
""" .

"DESCRIPTION.The function `as_dataset` generates a fake dataset containing images and labels, with each image being a 28x28x1 array of ones and each label being assigned as the remainder of the index divided by 10. The dataset is then loaded using `tfds.load` with the split specified as 'train', and the fake data examples generated by `as_dataset` are iterated through using a for loop." <EXPLAINS> """CODE.def as_dataset(self, *args, **kwargs):
  return tf.data.Dataset.from_generator(
      lambda: ({
          'image': np.ones(shape=(28, 28, 1), dtype=np.uint8),
          'label': i % 10,
      } for i in range(num_examples)),
      output_types=self.info.features.dtype,
      output_shapes=self.info.features.shape,
  )

with mock_data(as_dataset_fn=as_dataset):
  ds = tfds.load('some_dataset', split='train')

  for ex in ds:  # ds will yield the fake data example of 'as_dataset'.
    ex
""" .

"DESCRIPTION.The function `broadcast_prefix` replicates the leaves from a prefix tree to match the number of leaves in a full tree. It first checks if the prefix tree is a prefix of the full tree and then calculates the number of leaves in the full tree. The leaves in the prefix tree are then replicated to match the number of leaves in the full tree." <EXPLAINS> """CODE.def broadcast_prefix(prefix_tree, full_tree, is_leaf=None):
    # Check if prefix_tree is a prefix of full_tree
    if not is_prefix(prefix_tree, full_tree):
        raise ValueError("Prefix tree is not a prefix of full tree")

    # Get the number of leaves in full_tree
    num_leaves_full = count_leaves(full_tree)

    # Replicate leaves from prefix_tree to match the number of leaves in full_tree
    replicated_leaves = replicate_leaves(prefix_tree, num_leaves_full)

    return replicated_leaves

def is_prefix(prefix_tree, full_tree):
    # Check if prefix_tree is a prefix of full_tree
    # Implementation details omitted for brevity
    pass

def count_leaves(tree):
    # Count the number of leaves in a tree
    # Implementation details omitted for brevity
    pass

def replicate_leaves(tree, num_replicas):
    # Replicate leaves in a tree to match the specified number of replicas
    # Implementation details omitted for brevity
    pass
""" .

"DESCRIPTION.The function `compare_round_trip` performs a round trip test for discrete cosine transform (DCT) using TensorFlow. It generates a random waveform, pads it, computes the MDCT (Modified Discrete Cosine Transform), calculates the inverse MDCT, and then compares the original waveform with the reconstructed waveform to check for similarity within a specified tolerance." <EXPLAINS> """CODE.@tf.function
def compare_round_trip():
  samples = 1000
  frame_length = 400
  halflen = frame_length // 2
  waveform = tf.random.normal(dtype=tf.float32, shape=[samples])
  waveform_pad = tf.pad(waveform, [[halflen, 0],])
  mdct = tf.signal.mdct(waveform_pad, frame_length, pad_end=True,
                        window_fn=tf.signal.vorbis_window)
  inverse_mdct = tf.signal.inverse_mdct(mdct,
                                        window_fn=tf.signal.vorbis_window)
  inverse_mdct = inverse_mdct[halflen: halflen + samples]
  return waveform, inverse_mdct

waveform, inverse_mdct = compare_round_trip()
np.allclose(waveform.numpy(), inverse_mdct.numpy(), rtol=1e-3, atol=1e-4)""" .

"DESCRIPTION.The function `f` takes a scope and input `x`, then calculates the output `y` by scaling `x` based on the parameters obtained from the `learn_scale` function. It then computes the gradients of the parameters and input `x` with respect to `y` using reverse-mode automatic differentiation and returns `y`, the gradient of the parameters, and the gradient of `x`." <EXPLAINS> """CODE.def learn_scale(scope, x):
    p = scope.param('scale', nn.initializers.zeros, ())
    return p * x

def f(scope, x):
    y, bwd = lift.vjp(learn_scale, scope, x)
    params_grad, x_grad = bwd(jnp.ones(y.shape))
    return y, params_grad, x_grad""" .

"DESCRIPTION.The function `filter_repo_objects` filters a list of file paths or CommitOperationAdd objects based on allow_patterns and ignore_patterns criteria. It outputs only the elements that match the allow_patterns and do not match the ignore_patterns." <EXPLAINS> """CODE.list(filter_repo_objects(
...     ["aaa.PDF", "bbb.jpg", ".ccc.pdf", ".ddd.png"],
...     allow_patterns=["*.pdf"],
...     ignore_patterns=[".*"],
... ))
["aaa.pdf"]


list(filter_repo_objects(
... [
...     CommitOperationAdd(path_or_fileobj="/tmp/aaa.pdf", path_in_repo="aaa.pdf")
...     CommitOperationAdd(path_or_fileobj="/tmp/bbb.jpg", path_in_repo="bbb.jpg")
...     CommitOperationAdd(path_or_fileobj="/tmp/.ccc.pdf", path_in_repo=".ccc.pdf")
...     CommitOperationAdd(path_or_fileobj="/tmp/.ddd.png", path_in_repo=".ddd.png")
... ],
... allow_patterns=["*.pdf"],
... ignore_patterns=[".*"],
... key=lambda x: x.repo_in_path
... ))
[CommitOperationAdd(path_or_fileobj="/tmp/aaa.pdf", path_in_repo="aaa.pdf")]
""" .

"DESCRIPTION.The function `image_description_dict` generates a dictionary containing information about the shape of an image, with keys for \"shape\" and \"axes\"." <EXPLAINS> """CODE.image_description_dict(b'shape=(256, 256, 3)')
description = b'{"shape": [256, 256, 3], "axes": "YXS"}'
image_description_dict(description)  # doctest: +SKIP""" .

"DESCRIPTION.The function `merge_dicts` takes in a list of dictionaries, along with optional aggregation key functions and a default function. It merges the dictionaries by combining values of the same key, applying aggregation functions specified in `agg_key_funcs` if applicable, and using the default function if the key is not found in `agg_key_funcs`. The function then returns the merged dictionary." <EXPLAINS> """CODE.import numpy as np

def merge_dicts(dicts, agg_key_funcs=None, default_func=None):
    if agg_key_funcs is None:
        agg_key_funcs = {}
    merged_dict = {}
    for d in dicts:
        for key, value in d.items():
            if key in agg_key_funcs:
                if key in merged_dict:
                    merged_dict[key].append(value)
                else:
                    merged_dict[key] = [value]
            else:
                merged_dict[key] = value
    for key, values in merged_dict.items():
        if key in agg_key_funcs:
            merged_dict[key] = agg_key_funcs[key](values)
        elif default_func is not None:
            merged_dict[key] = default_func(values)
    return merged_dict

import pprint
d1 = {'a': 1.7, 'b': 2.0, 'c': 1}
d2 = {'a': 1.1, 'b': 2.2, 'v': 1}
d3 = {'a': 1.1, 'v': 2.3}
dflt_func = min
agg_funcs = {'a': np.mean, 'v': max}
pprint.pprint(merge_dicts([d1, d2, d3], agg_funcs, dflt_func)""" .

"DESCRIPTION.The function `parse_kwargs` removes specified keys from the dictionary `kwargs` and then adds the key-value pairs from `keyvals` to the dictionary before returning the updated dictionary." <EXPLAINS> """CODE.def parse_kwargs(kwargs, *keys, **keyvals):
...     for key in keys:
...         if key in kwargs:
...             del kwargs[key]
...     kwargs.update(keyvals)
...     return kwargs""" .

"DESCRIPTION.The function `repo_name()` takes two optional parameters, a string `name` and a `prefix`. If the `name` parameter is provided, it will be used in the output. If the `prefix` parameter is provided, it will be used as a prefix for the output. The function returns a formatted string with the prefix, name, and a random numerical value." <EXPLAINS> """CODE.repo_name()
repo-2fe93f-16599646671840
repo_name("my-space", prefix='space')
space-my-space-16599481979701""" .

"DESCRIPTION.The function `test_dataloader` creates a data loader for the MNIST dataset with specified transformations and parameters, and returns it. It can also potentially return multiple data loaders." <EXPLAINS> """CODE.def test_dataloader(self):
    transform = transforms.Compose([transforms.ToTensor(),
                                    transforms.Normalize((0.5,), (1.0,))])
    dataset = MNIST(root='/path/to/mnist/', train=False, transform=transform,
                    download=True)
    loader = torch.utils.data.DataLoader(
        dataset=dataset,
        batch_size=self.batch_size,
        shuffle=False
    )

    return loader

# can also return multiple dataloaders
def test_dataloader(self):
    return [loader_a, loader_b, ..., loader_n]
""" .

"DESCRIPTION.The function `tree_structure` takes in a tree data structure where each node can have a value and children, and returns a PyTreeSpec object that describes the structure of the tree. The PyTreeSpec object shows the values and children of each node in the tree, using '*' as a placeholder for values and children, and indicates that nodes without children are leaf nodes with the attribute NoneIsLeaf." <EXPLAINS> """CODE.tree = {'b': (2, [3, 4]), 'a': 1, 'c': None, 'd': 5}
tree_structure(tree)
PyTreeSpec({'a': *, 'b': (*, [*, *]), 'c': *, 'd': *}, NoneIsLeaf)
tree_structure(1)
PyTreeSpec(*, NoneIsLeaf)
tree_structure(None)
PyTreeSpec(*, NoneIsLeaf)""" .

"DESCRIPTION.The function adjusts the version constraints for the package \"arrow\" based on the specified conditions and comments, and applies the specified adjustment (none, all, or major) accordingly." <EXPLAINS> """CODE._RequirementWithComment("arrow<=1.2.2,>=1.2.0", comment="# anything").adjust("none")
_RequirementWithComment("arrow<=1.2.2,>=1.2.0", comment="# strict").adjust("none")
_RequirementWithComment("arrow<=1.2.2,>=1.2.0", comment="# my name").adjust("all")
_RequirementWithComment("arrow>=1.2.0, <=1.2.2", comment="# strict").adjust("all")
_RequirementWithComment("arrow").adjust("all")
_RequirementWithComment("arrow>=1.2.0, <=1.2.2", comment="# cool").adjust("major")
_RequirementWithComment("arrow>=1.2.0, <=1.2.2", comment="# strict").adjust("major")
_RequirementWithComment("arrow>=1.2.0").adjust("major")
_RequirementWithComment("arrow").adjust("major")""" .

"DESCRIPTION.The function allows the model to backpropagate the loss using the specified optimizer and optimizer index." <EXPLAINS> """CODE.def backward(self, trainer, loss, optimizer, optimizer_idx):
    loss.backward()""" .

"DESCRIPTION.The function calculates the Mean Squared Error (MSE) between two input tensors, pred and target. The reduction parameter determines how the MSE is calculated (either elementwise mean or sum), and the return_state parameter specifies whether to return the MSE value only or both the MSE value and the squared error values." <EXPLAINS> """CODE.def mse(pred, target, reduction='elementwise_mean', return_state=False):
    squared_error = (pred - target) ** 2
    if reduction == 'elementwise_mean':
        mse = squared_error.mean()
    elif reduction == 'sum':
        mse = squared_error.sum()
    else:
        mse = squared_error
    if return_state:
        return mse, squared_error
    return mse

x = torch.tensor([0., 1, 2, 3])
y = torch.tensor([0., 1, 2, 2])
print(mse(x, y))
""" .

"DESCRIPTION.The function calculates the average training accuracy for each epoch based on the outputs obtained during the training process, and logs this average accuracy along with the current epoch number." <EXPLAINS> """CODE.def training_epoch_end(self, outputs):
    train_acc_mean = 0
    for output in outputs:
        train_acc_mean += output['train_acc']

    train_acc_mean /= len(outputs)

    # log training accuracy at the end of an epoch
    results = {
        'log': {'train_acc': train_acc_mean.item()}
    }
    return results


def training_epoch_end(self, outputs):
    train_acc_mean = 0
    i = 0
    for dataloader_outputs in outputs:
        for output in dataloader_outputs:
            train_acc_mean += output['train_acc']
            i += 1

    train_acc_mean /= i

    # log training accuracy at the end of an epoch
    results = {
        'log': {'train_acc': train_acc_mean.item(), 'step': self.current_epoch}
    }
    return results
""" .

"DESCRIPTION.The function calculates the dot product of a matrix with its transpose and then waits for the computation to finish." <EXPLAINS> """CODE.@jax.profiler.trace_function
def f(x):
    return jnp.dot(x, x.T).block_until_ready()

@partial(jax.profiler.trace_function, name="event_name")
def f(x):
    return jnp.dot(x, x.T).block_until_ready()""" .

"DESCRIPTION.The function calculates the number of true positives, true negatives, false positives, and false negatives based on the input of predicted and true values." <EXPLAINS> """CODE.from sklearn.metrics.metrics import _tp_tn_fp_fn
y_pred = [0, 1, 0, 0]
y_true = [0, 1, 0, 1]
_tp_tn_fp_fn(y_true, y_pred)
(array([2, 1]), array([1, 2]), array([1, 0]), array([0, 1]))

y_true = np.array([0, 1, 2, 0, 1, 2])
y_pred = np.array([0, 2, 1, 0, 0, 1])
_tp_tn_fp_fn(y_true, y_pred)
(array([2, 0, 0]), array([3, 2, 3]), array([1, 2, 1]), array([0, 2, 2]))

_tp_tn_fp_fn(np.array([[0.0, 1.0], [1.0, 1.0]]), np.zeros((2, 2)))
(array([0, 0]), array([1, 0]), array([0, 0]), array([1, 2]))

_tp_tn_fp_fn([(1, 2), (3, )], [(1, 2), tuple()])
(array([1, 1, 0]), array([1, 1, 1]), array([0, 0, 0]), array([0, 0, 1])""" .

"DESCRIPTION.The function calculates the precision and recall values for a classification task with multiple classes." <EXPLAINS> """CODE.def precision_recall(pred, target, num_classes, reduction='elementwise_mean'):
    # compute precision and recall
    # code here

    return precision, recall

x = torch.tensor([0, 1, 2, 3])
y = torch.tensor([0, 1, 2, 2])
precision_recall(x, y)
""" .

"DESCRIPTION.The function checks if the input is dict-like, meaning it verifies if the input is iterable and can be treated like a dictionary." <EXPLAINS> """CODE.is_dict_like({1: 2})
is_dict_like([1, 2, 3])""" .

"DESCRIPTION.The function composes the inputs by adding the integer N to them, only if N is equal to 1." <EXPLAINS> """CODE.@composite.Composite('AddN')
def _compose_add_n(inputs, N):
    if N == 1:
      ....""" .

"DESCRIPTION.The function configures automatic mixed precision (AMP) training for the given model and optimizers at the specified AMP level." <EXPLAINS> """CODE.def configure_apex(self, amp, model, optimizers, amp_level):
    model, optimizers = amp.initialize(
        model, optimizers, opt_level=amp_level,
    )

    return model, optimizers""" .

"DESCRIPTION.The function configures the Lightning Distributed Data Parallel (DDP) for the model with the specified device IDs and enables finding unused parameters." <EXPLAINS> """CODE.def configure_ddp(self, model, device_ids):
    # Lightning DDP simply routes to test_step, val_step, etc...
    model = LightningDistributedDataParallel(
        model,
        device_ids=device_ids,
        find_unused_parameters=True
    )
    return model""" .

"DESCRIPTION.The function converts a dictionary containing loss values for specific optimizers into a list of losses, with None values for optimizers without loss information." <EXPLAINS> """CODE._convert_optim_dict({0: {"loss": 0.0}, 2: {"loss": 0.2}}, num_optimizers=3)
[{'loss': 0.0}, None, {'loss': 0.2}]""" .

"DESCRIPTION.The function converts a given number of bytes into a human-readable format, displaying the size in a more understandable unit such as kilobytes, megabytes, gigabytes, etc." <EXPLAINS> """CODE.bytes2human(10000)
bytes2human(100001221)""" .

"DESCRIPTION.The function converts a one-dimensional array of integers into a one-hot encoded tensor where each integer in the array corresponds to a 1 in the tensor at that index and 0s elsewhere." <EXPLAINS> """CODE.to_categorical(x)
    tensor([1, 0])""" .

"DESCRIPTION.The function converts an array of labels representing classes into a matrix with one-hot encoding, where each row represents a label and each column represents a class, with a value of 1 indicating the presence of that class for the respective label." <EXPLAINS> """CODE.# Consider an array of 5 labels out of a set of 3 classes {0, 1, 2}:
labels
array([0, 2, 1, 2, 0])
# `to_categorical` converts this into a matrix with as many
# columns as there are classes. The number of rows
# stays the same.
to_categorical(labels)
array([[ 1.,  0.,  0.],
       [ 0.,  0.,  1.],
       [ 0.,  1.,  0.],
       [ 0.,  0.,  1.],
       [ 1.,  0.,  0.]], dtype=float32)
""" .

"DESCRIPTION.The function correct_tables() takes a string as input and corrects the format of LaTeX tables by ensuring proper spacing and formatting within the table environment." <EXPLAINS> """CODE.correct_tables("\\begin{table} \\begin{tabular}{l l} & \\ \\end{tabular} \\end{table}")
"\\begin{table}
\\begin{tabular}{l l} & \\ \\end{tabular}
\\end{table}"
""" .

"DESCRIPTION.The function creates a data loader for the MNIST dataset without shuffling the data. It transforms the data to tensor format and normalizes it before returning the data loader. It can also return multiple data loaders for different subsets of the MNIST dataset." <EXPLAINS> """CODE.def test_dataloader(self):
    transform = transforms.Compose([transforms.ToTensor(),
                                    transforms.Normalize((0.5,), (1.0,))])
    dataset = MNIST(root='/path/to/mnist/', train=False, transform=transform,
                    download=True)
    loader = torch.utils.data.DataLoader(
        dataset=dataset,
        batch_size=self.batch_size,
        shuffle=False
    )

    return loader

# can also return multiple dataloaders
def test_dataloader(self):
    return [loader_a, loader_b, ..., loader_n]
""" .

"DESCRIPTION.The function creates n-grams of a specified size from a given list of elements." <EXPLAINS> """CODE.create_ngram_set([1, 4, 9, 4, 1, 4], ngram_value=2)
{(4, 9), (4, 1), (1, 4), (9, 4)}
create_ngram_set([1, 4, 9, 4, 1, 4], ngram_value=3)
[(1, 4, 9), (4, 9, 4), (9, 4, 1), (4, 1, 4)]""" .

"DESCRIPTION.The function defined in the code is a linear transformation where an input vector of size 32 is transformed using a Linear layer to produce an output vector of size 1. The function returns two outputs: y and z. The optimizer used is Adam with a learning rate of 0.001, and the optimization is performed based on the 'z' output. The function is called 10 times with an input vector of ones and the corresponding 'z' values are printed." <EXPLAINS> """CODE.import paddle.fluid as fluid
import numpy as np

from paddle.fluid.dygraph.nn import Linear

@fluid.dygraph.declarative
def linear_func(x):
    x = fluid.dygraph.to_variable(x)
    linear = Linear(32, 1)
    y = linear(x)
    z = linear(x)
    return y, z

prog_trans = fluid.dygraph.ProgramTranslator()

adam = fluid.optimizer.AdamOptimizer(learning_rate=0.001)
prog_trans.set_optimizer(adam,index_of_loss=1) # minimize on 'z'

for i in range(10):
    y, z_loss = linear_func(np.ones(32).astype('float32'))
    print(z_loss.numpy())""" .

"DESCRIPTION.The function defined in the code takes an input variable x, converts it into a fluid variable, and then checks if the mean value of x is greater than 0. If the mean value is greater than 0, it subtracts 1 from x; otherwise, it adds 1 to x. Finally, it returns the modified variable x_v." <EXPLAINS> """CODE.import paddle.fluid as fluid
import numpy as np

def func(x):
    x = fluid.dygraph.to_variable(x)
    if fluid.layers.mean(x) > 0:
        x_v = x - 1
    else:
        x_v = x + 1
    return x_v

prog_trans = fluid.dygraph.ProgramTranslator()

x = np.ones([1, 2])
main_prog, start_prog, inputs, outputs = prog_trans.get_program(func, x)
print([i.name for i in inputs])
# ['x_0'] the feed input variable name representing x
print([o.name for o in outputs])
# ['_generated_var_4'] the fetch output variable name representing x_v""" .

"DESCRIPTION.The function defines a unit test named test_hello_foo that mocks the functions foo from the modules 'something' and 'something_else.foo.bar' and handles any injection in the test." <EXPLAINS> """CODE.def TestHelloWorld(unittest.TestCase):

    @patch("something.foo")
    @patch("something_else.foo.bar") # order doesn't matter
    @handle_injection_in_test # after @patch calls
    def test_hello_foo(self, mock_foo: Mock) -> None:
        (...)
""" .

"DESCRIPTION.The function extract_args takes a line of input and extracts the arguments from it." <EXPLAINS> "CODE.extract_args(line)" .

"DESCRIPTION.The function extracts the revision reference from a pull request URL and returns it in the format \"refs/pr/2\"." <EXPLAINS> """CODE._parse_revision_from_pr_url("https://huggingface.co/bigscience/bloom/discussions/2")
"refs/pr/2\"""" .

"DESCRIPTION.The function f is defined using TensorFlow and accepts a single input x. It returns the input x without any modification. The code demonstrates creating a concrete function from the original function f using different methods, such as specifying input signature or using tf.TensorSpec. Finally, the concrete function f_concrete is called with a constant input of 1.0." <EXPLAINS> """CODE.@tf.function
def f(x):
  return x

f_concrete = f.get_concrete_function(tf.constant(1.0))
f_concrete = f.get_concrete_function(x=tf.constant(1.0))

@tf.function
def f(x):
  return x

f_concrete = f.get_concrete_function(tf.TensorSpec([], tf.float64))

@tf.function(input_signature=[tf.TensorSpec(None, tf.float32)])
def f(x):
  return x

f_concrete = f.get_concrete_function()

f_concrete(tf.constant(1.0))
f_concrete(x=tf.constant(1.0))""" .

"DESCRIPTION.The function f returns the indices of the current iteration along the 'i' and 'j' axes respectively, after being decorated to be parallelized along both axes using pmap." <EXPLAINS> """CODE.@partial(pmap, axis_name='i')
@partial(pmap, axis_name='j')
def f(_):
  return lax.axis_index('i'), lax.axis_index('j')""" .

"DESCRIPTION.The function f takes input x and returns it as output." <EXPLAINS> """CODE.@tf.function
def f(x):
  return x

f_concrete = f.get_concrete_function(tf.constant(1.0))
f_concrete = f.get_concrete_function(x=tf.constant(1.0))

@tf.function
def f(x):
  return x

f_concrete = f.get_concrete_function(tf.TensorSpec([], tf.float64))

@tf.function(input_signature=[tf.TensorSpec(None, tf.float32)])
def f(x):
  return x

f_concrete = f.get_concrete_function()""" .

"DESCRIPTION.The function f(x) checks if the input x is not equal to zero using the debug_check function and returns x." <EXPLAINS> """CODE.def f(x):
    checkify.debug_check(x!=0, "cannot be zero!")
    return x
""" .

"DESCRIPTION.The function fills in the lower triangular part of a matrix with the values provided in the input." <EXPLAINS> """CODE.fill_triangular_inverse(
  [[4, 0, 0],
   [6, 5, 0],
   [3, 2, 1]])

fill_triangular_inverse(
  [[1, 2, 3],
   [0, 5, 6],
   [0, 0, 4]], upper=True)
""" .

"DESCRIPTION.The function flip_sequences takes a list of lists and a list of lengths as inputs, and returns a new list of lists where each sub-list is reversed based on its corresponding length value." <EXPLAINS> """CODE.inputs = [[1, 0, 0],
          [2, 3, 0]
          [4, 5, 6]]
lengths = [1, 2, 3]
flip_sequences(inputs, lengths) = [[1, 0, 0],
                                   [3, 2, 0],
                                   [6, 5, 4]]
""" .

"DESCRIPTION.The function generates a string indicating that 7 revisions have been selected, which count for a total of 4.3G of data." <EXPLAINS> """CODE._get_expectations_str(hf_cache_info, selected_hashes)
'7 revisions selected counting for 4.3G.'""" .

"DESCRIPTION.The function generates numpy arrays of input data and labels from each line in a file, and yielding them. The arrays are then used to train a model with a specified number of samples per epoch for a defined number of epochs." <EXPLAINS> """CODE.    def generate_arrays_from_file(path):
        while 1:
            f = open(path)
            for line in f:
                # create Numpy arrays of input data
                # and labels, from each line in the file
                x, y = process_line(line)
                yield (x, y)
            f.close()

    model.fit_generator(generate_arrays_from_file('/my_file.txt'),
                        samples_per_epoch=10000, epochs=10)
""" .

"DESCRIPTION.The function initializes automatic mixed precision training for a given model and optimizers at a specified AMP level." <EXPLAINS> """CODE.def configure_apex(self, amp, model, optimizers, amp_level):
    model, optimizers = amp.initialize(
        model, optimizers, opt_level=amp_level,
    )

    return model, optimizers""" .

"DESCRIPTION.The function is used to deserialize a Keras object using a provided configuration and optional custom objects." <EXPLAINS> """CODE.def deserialize(config, custom_objects=None):
   return deserialize_keras_object(
     identifier,
     module_objects=globals(),
     custom_objects=custom_objects,
     name="MyObjectType",
   )
""" .

"DESCRIPTION.The function is_datetimelike_v_numeric checks if the input arguments are either datetime-like or numeric. It returns True if at least one input is datetime-like and the other is numeric, otherwise it returns False." <EXPLAINS> """CODE.dt = np.datetime64(pd.datetime(2017, 1, 1))
is_datetimelike_v_numeric(1, 1)
False
is_datetimelike_v_numeric(dt, dt)
False
is_datetimelike_v_numeric(1, dt)
True
is_datetimelike_v_numeric(dt, 1)  # symmetric check
True
is_datetimelike_v_numeric(np.array([dt]), 1)
True
is_datetimelike_v_numeric(np.array([1]), dt)
True
is_datetimelike_v_numeric(np.array([dt]), np.array([1]))
True
is_datetimelike_v_numeric(np.array([1]), np.array([2]))
False
is_datetimelike_v_numeric(np.array([dt]), np.array([dt]))
False""" .

"DESCRIPTION.The function is_numeric_dtype checks if the input data type is numeric." <EXPLAINS> """CODE.is_numeric_dtype(str)
is_numeric_dtype(int)
is_numeric_dtype(float)
is_numeric_dtype(np.uint64)
is_numeric_dtype(np.datetime64)
is_numeric_dtype(np.timedelta64)
is_numeric_dtype(np.array(['a', 'b']))
is_numeric_dtype(pd.Series([1, 2]))
is_numeric_dtype(pd.Index([1, 2.]))
is_numeric_dtype(np.array([], dtype=np.timedelta64)""" .

"DESCRIPTION.The function is_object_dtype determines whether the input data type is of type object. It returns True if the input data type is object, and False otherwise." <EXPLAINS> """CODE.is_object_dtype(object)
True
is_object_dtype(int)
False
is_object_dtype(np.array([], dtype=object))
True
is_object_dtype(np.array([], dtype=int))
False
is_object_dtype([1, 2, 3])
False""" .

"DESCRIPTION.The function is_string_dtype checks if the input data type is a string type (str) and returns True if it is, False if it is not." <EXPLAINS> """CODE.is_string_dtype(str)
True
is_string_dtype(object)
True
is_string_dtype(int)
False
is_string_dtype(np.array(['a', 'b']))
True
is_string_dtype(pd.Series([1, 2]))
False""" .

"DESCRIPTION.The function joins a local path or URI with another path to create a new path or URI." <EXPLAINS> """CODE._join_path_or_uri(local_path, path_to_join)
_join_path_or_uri(uri, path_to_join)""" .

"DESCRIPTION.The function make_palindrome takes a list of tokens, reverses each token and appends it to the original list, then concatenates all tokens into a single string." <EXPLAINS> """CODE.def make_palindrome(tokens):
    tokens.extend(reversed([t[::-1] for t in tokens]))
    return ''.join(tokens)

patt = OneOrMore(Word(alphas))
print(patt.addParseAction(make_palindrome).parseString("lskdj sdlkjf lksd")) # -> 'lskdjsdlkjflksddsklfjkldsjdksl'""" .

"DESCRIPTION.The function merges two dictionaries by concatenating the values of the \"libraries\" key into a single list." <EXPLAINS> """CODE.merge_flags({"libraries": ["one"]}, {"libraries": ["two"]})
{"libraries": ["one", "two"]}""" .

"DESCRIPTION.The function myParamValidator validates a parameter using advanced logic and returns True. It is then passed to the DoSomething function in the mock_dao module." <EXPLAINS> """CODE.def myParamValidator(param):
  # Advanced logic here
  return True

mock_dao.DoSomething(Func(myParamValidator), true)""" .

"DESCRIPTION.The function my_op takes a tensor a as input, converts it to a tensor named \"a\", performs some computation using the input tensor, and returns the result of the computation using the foo_op function." <EXPLAINS> """CODE.def my_op(a):
  with tf.name_scope("MyOp") as scope:
    a = tf.convert_to_tensor(a, name="a")
    # Define some computation that uses `a`.
    return foo_op(..., name=scope)""" .

"DESCRIPTION.The function of the code is to calculate the Log-cosh error metric for a given set of predicted values and true values in a machine learning model." <EXPLAINS> """CODE.logcosh = log((exp(x) + exp(-x))/2)


m = tf.keras.metrics.LogCoshError()
m.update_state([[0, 1], [0, 0]], [[1, 1], [0, 0]])
m.result().numpy()


m.reset_state()
m.update_state([[0, 1], [0, 0]], [[1, 1], [0, 0]],
               sample_weight=[1, 0])
m.result().numpy()


model.compile(optimizer='sgd',
              loss='mse',
              metrics=[tf.keras.metrics.LogCoshError()])
""" .

"DESCRIPTION.The function of the code is to load an audio file in mp3 format, then create two time-stretched versions of the audio using a factor of 2.0 for one version and 0.5 for the other." <EXPLAINS> """CODE.y, sr = librosa.load('file.mp3')
y_fast = librosa.effects.time_stretch(y, 2.0)
y_slow = librosa.effects.time_stretch(y, 0.5)""" .

"DESCRIPTION.The function of the provided code is to compile a model using Binary Focal Crossentropy as the loss function with specific parameters such as gamma value set to 2 and using logits." <EXPLAINS> """CODE.model.compile(
  loss=tf.keras.losses.BinaryFocalCrossentropy(gamma=2.0, from_logits=True),
  ....
)


# Example 1: (batch_size = 1, number of samples = 4)
y_true = [0, 1, 0, 0]
y_pred = [-18.6, 0.51, 2.94, -12.8]
loss = tf.keras.losses.BinaryFocalCrossentropy(gamma=2, from_logits=True)
loss(y_true, y_pred).numpy()
0.691

# Example 2: (batch_size = 2, number of samples = 4)
y_true = [[0, 1], [0, 0]]
y_pred = [[-18.6, 0.51], [2.94, -12.8]]
# Using default 'auto'/'sum_over_batch_size' reduction type.
loss = tf.keras.losses.BinaryFocalCrossentropy(gamma=3, from_logits=True)
loss(y_true, y_pred).numpy()
0.647

# Using 'sample_weight' attribute
loss(y_true, y_pred, sample_weight=[0.8, 0.2]).numpy()
0.133

# Using 'sum' reduction` type.
loss = tf.keras.losses.BinaryFocalCrossentropy(gamma=4, from_logits=True,
...     reduction=tf.keras.losses.Reduction.SUM)
loss(y_true, y_pred).numpy()
1.222

# Using 'none' reduction type.
loss = tf.keras.losses.BinaryFocalCrossentropy(gamma=5, from_logits=True,
...     reduction=tf.keras.losses.Reduction.NONE)
loss(y_true, y_pred).numpy()
array([0.0017 1.1561], dtype=float32)
""" .

"DESCRIPTION.The function of the provided python code is to perform average pooling on a given input tensor `x`. The code reshapes the input tensor, applies average pooling with specified parameters (pool_size, strides, padding), and returns the output tensor after the pooling operation." <EXPLAINS> """CODE.x = tf.constant([[1., 2., 3.],
...                  [4., 5., 6.],
...                  [7., 8., 9.]])
x = tf.reshape(x, [1, 3, 3, 1])
avg_pool_2d = tf.keras.layers.AveragePooling2D(pool_size=(2, 2),
...    strides=(1, 1), padding='valid')
avg_pool_2d(x)
<tf.Tensor: shape=(1, 2, 2, 1), dtype=float32, numpy=
  array([[[[3.],
           [4.]],
          [[6.],
           [7.]]]], dtype=float32)>
x = tf.constant([[1., 2., 3., 4.],
...                  [5., 6., 7., 8.],
...                  [9., 10., 11., 12.]])
x = tf.reshape(x, [1, 3, 4, 1])
avg_pool_2d = tf.keras.layers.AveragePooling2D(pool_size=(2, 2),
...    strides=(2, 2), padding='valid')
avg_pool_2d(x)
<tf.Tensor: shape=(1, 1, 2, 1), dtype=float32, numpy=
  array([[[[3.5],
           [5.5]]]], dtype=float32)>
x = tf.constant([[1., 2., 3.],
...                  [4., 5., 6.],
...                  [7., 8., 9.]])
x = tf.reshape(x, [1, 3, 3, 1])
avg_pool_2d = tf.keras.layers.AveragePooling2D(pool_size=(2, 2),
...    strides=(1, 1), padding='same')
avg_pool_2d(x)
<tf.Tensor: shape=(1, 3, 3, 1), dtype=float32, numpy=
  array([[[[3.],
           [4.],
           [4.5]],
          [[6.],
           [7.],
           [7.5]],
          [[7.5],
           [8.5],
           [9.]]]], dtype=float32)>""" .

"DESCRIPTION.The function of this code is to trim the leading and trailing silence from an audio signal using the librosa library in Python. The trim_audio_signal function takes an audio signal as input and returns the trimmed audio signal as well as the indices of the trimmed audio signal. Finally, the code loads an audio file, trims the silence from the audio signal, and prints the duration of the original and trimmed audio signals." <EXPLAINS> """CODE.# Trim leading and trailing silence from an audio signal.

import numpy as np
import librosa

def trim_audio_signal(y, top_db=60, ref=np.max, frame_length=2048, hop_length=512):
    y_trimmed, index = librosa.effects.trim(y, top_db=top_db, ref=ref, frame_length=frame_length, hop_length=hop_length)
    return y_trimmed, index

# Load some audio
y, sr = librosa.load(librosa.util.example_audio_file())
# Trim the beginning and ending silence
yt, index = trim_audio_signal(y)
# Print the durations
print(librosa.get_duration(y), librosa.get_duration(yt))
""" .

"DESCRIPTION.The function of this code is to use LambdaCallback to print the batch number at the beginning of every batch, plot the loss after every epoch, and terminate some processes after finishing model training." <EXPLAINS> """CODE.# Print the batch number at the beginning of every batch.
batch_print_callback = LambdaCallback(
    on_batch_begin=lambda batch,logs: print(batch))

# Plot the loss after every epoch.
import numpy as np
import matplotlib.pyplot as plt
plot_loss_callback = LambdaCallback(
    on_epoch_end=lambda epoch, logs: plt.plot(np.arange(epoch),
                                              logs['loss']))

# Terminate some processes after having finished model training.
processes = ...
cleanup_callback = LambdaCallback(
    on_train_end=lambda logs: [
        p.terminate() for p in processes if p.is_alive()])

model.fit(...,
          callbacks=[batch_print_callback,
                     plot_loss_callback,
                     cleanup_callback])
""" .

"DESCRIPTION.The function optimizer_zero_grad resets the gradients of the optimizer's parameters to zero at the specified epoch, batch index, and optimizer index." <EXPLAINS> """CODE.def optimizer_zero_grad(self, epoch, batch_idx, optimizer, optimizer_idx):
    optimizer.zero_grad()

def optimizer_zero_grad(self, epoch, batch_idx, optimizer, optimizer_idx):
    optimizer.zero_grad(set_to_none=True)""" .

"DESCRIPTION.The function parses and extracts metadata information (e.g., shape) from a given JSON description." <EXPLAINS> """CODE.json_description_metadata(description)  # doctest: +SKIP
json_description_metadata('shape=(256, 256, 3)')""" .

"DESCRIPTION.The function performs a training step by calculating the loss, then automatically scales and applies backward propagation with the loss and optimizer opt_a." <EXPLAINS> """CODE.def training_step(...):
    (opt_a, opt_b) = self.optimizers()
    loss = ...
    # automatically applies scaling, etc...
    self.manual_backward(loss, opt_a)""" .

"DESCRIPTION.The function performs recursive checkpointing for a list of functions. It recursively applies the checkpoint function from the JAX library to the provided functions in the list, splitting the list into two halves and then combining them using lambda functions until all functions are processed." <EXPLAINS> """CODE.@jax.checkpoint
def g(x):
  y = jnp.sin(x)
  z = jnp.sin(y)
  return z

recursive_checkpoint(funs):
  if len(funs) == 1:
    return funs[0]
  elif len(funs) == 2:
    f1, f2 = funs
    return lambda x: f1(f2(x))
  else:
    f1 = recursive_checkpoint(funs[:len(funs)//2])
    f2 = recursive_checkpoint(funs[len(funs)//2:])
    return lambda x: f1(jax.checkpoint(f2)(x))""" .

"DESCRIPTION.The function recursively removes leading and trailing zero values from nested lists and returns the updated list." <EXPLAINS> "CODE._recursive_unpad([[[0, 1, 0]], [2], [0, 0]], value=0)" .

"DESCRIPTION.The function removes reStructuredText roles from the input strings." <EXPLAINS> """CODE._strip_rst_role(':class:`ClassName`')
_strip_rst_role(':py:obj:`module.Object`')
_strip_rst_role('ClassName')""" .

"DESCRIPTION.The function returns a list of all the leaves (values) in the input tree dictionary, including leaves from nested sub-trees." <EXPLAINS> """CODE.tree = {'b': (2, [3, 4]), 'a': 1, 'c': None, 'd': 5}
tree_leaves(tree)
[1, 2, 3, 4, None, 5]
tree_leaves(1)
[1]
tree_leaves(None)
[None]""" .

"DESCRIPTION.The function returns the identity matrix." <EXPLAINS> """CODE.@linear_operator_algebra.RegisterCholesky(lin_op.LinearOperatorIdentity)
def _cholesky_identity(lin_op_a):
  # Return the identity matrix.""" .

"DESCRIPTION.The function reverse_bitorder takes in a sequence of bytes and reverses the bit order of each byte. It then takes in a numpy array of integers and reverses the bit order of each element in the array." <EXPLAINS> """CODE.reverse_bitorder(b'd')
data = numpy.array([1, 666], dtype='uint16')
reverse_bitorder(data)""" .

"DESCRIPTION.The function str_to_bool converts a given string representation of a boolean value ('YES' or 'FALSE') into a corresponding boolean value (True or False)." <EXPLAINS> """CODE.str_to_bool('YES')
str_to_bool('FALSE')""" .

"DESCRIPTION.The function takes a number as input and converts it into a human-readable format representing the size in bytes." <EXPLAINS> """CODE.bytes2human(10000)
bytes2human(100001221)""" .

"DESCRIPTION.The function takes a number representing bytes as input and converts it to a human-readable format with appropriate unit (KB, MB, GB, etc.)." <EXPLAINS> """CODE.bytes2human(10000)
bytes2human(100001221)""" .

"DESCRIPTION.The function takes an input tensor, converts it to a PaddlePaddle variable, calculates the mean of the tensor, and then subtracts 1 from the tensor if the mean is greater than 0, otherwise adds 1 to the tensor." <EXPLAINS> """CODE.import paddle.fluid as fluid
import numpy as np

def func(x):
    x = fluid.dygraph.to_variable(x)
    if fluid.layers.mean(x) > 0:
        x_v = x - 1
    else:
        x_v = x + 1
    return x_v

prog_trans = fluid.dygraph.ProgramTranslator()

static_func = prog_trans.get_func(func)
print(callable(static_func)) # True""" .

"DESCRIPTION.The function takes an input variable, converts it to a paddle fluid variable, and checks if the mean value of the variable is greater than 0. If it is, it subtracts 1 from the variable; if not, it adds 1 to the variable. Finally, it returns the modified variable." <EXPLAINS> """CODE.import paddle.fluid as fluid
import numpy as np

def func(x):
    x = fluid.dygraph.to_variable(x)
    if fluid.layers.mean(x) > 0:
        x_v = x - 1
    else:
        x_v = x + 1
    return x_v

prog_trans = fluid.dygraph.ProgramTranslator()

code = prog_trans.get_code(func)
print(type(code)) # <class 'str'>""" .

"DESCRIPTION.The function takes in a list of features and a list of feature columns, retrieves dense tensors from each feature column, concatenates them together, and returns the result." <EXPLAINS> """CODE.def input_layer(features, feature_columns, ...):
  outputs = [fc._get_dense_tensor(...) for fc in feature_columns]
  return tf.concat(outputs)
""" .

"DESCRIPTION.The function to_key_val_list converts a list of tuples or a dictionary to a list of key-value pairs." <EXPLAINS> """CODE.to_key_val_list([('key', 'val')])
to_key_val_list({'key': 'val'})""" .

"DESCRIPTION.The function tokenizes a given sentence into a list of individual words and punctuation marks." <EXPLAINS> """CODE.tokenize('Bob dropped the apple. Where is the apple?')
['Bob', 'dropped', 'the', 'apple', '.', 'Where', 'is', 'the', 'apple', '?']""" .

"DESCRIPTION.The function torch.argsort() returns the indices that would sort the input tensor along a specified dimension." <EXPLAINS> """CODE.torch.argsort(a, dim=1)
tensor([[2, 0, 3, 1],
        [3, 2, 1, 0],
        [2, 1, 0, 3],
        [3, 2, 1, 0]])""" .

"DESCRIPTION.The function warmstart_embedding_matrix is used to initialize or update the embedding matrix for a neural network model. It takes base and new vocabularies, base embeddings, and a new embeddings initializer as input, and returns a warmstarted embedding matrix. This function is commonly used in natural language processing tasks to transfer knowledge from a pretrained model to a new model." <EXPLAINS> """CODE.import keras
import numpy as np
import tensorflow as tf

def warmstart_embedding_matrix(base_vocabulary, new_vocabulary, base_embeddings, new_embeddings_initializer):
    # code for warmstarting embedding matrix
    pass

# Example usage of warmstart_embedding_matrix util
vocab_base = tf.convert_to_tensor(["unk", "a", "b", "c"])
vocab_new = tf.convert_to_tensor(["unk", "unk", "a", "b", "c", "d", "e"])
vectorized_vocab_base = np.random.rand(vocab_base.shape[0], 3)
vectorized_vocab_new = np.random.rand(vocab_new.shape[0], 3)
warmstarted_embedding_matrix = warmstart_embedding_matrix(
    base_vocabulary=vocab_base,
    new_vocabulary=vocab_new,
    base_embeddings=vectorized_vocab_base,
    new_embeddings_initializer=keras.initializers.Constant(vectorized_vocab_new))

# Example usage of getting vocabulary and embedding weights from layers
base_vocabulary = old_text_vectorization_layer.get_vocabulary()
new_vocabulary = new_text_vectorization_layer.get_vocabulary()
embedding_weights_base = model.get_layer('embedding').get_weights()[0]
warmstarted_embedding = keras.utils.warmstart_embedding_matrix(
    base_vocabulary,
    new_vocabulary,
    base_embeddings=embedding_weights_base,
    new_embeddings_initializer="uniform")
updated_embedding_variable = tf.Variable(warmstarted_embedding)

# Update embedding layer weights and continue with model training
model.layers[1].embeddings = updated_embedding_variable
model.fit(..)""" .

"DESCRIPTION.The function will encode bytes in the JSON tree using the utf-8 codec." <EXPLAINS> """CODE.BytesEncoder will walk the JSON tree and decode bytes with utf-8 codec.

Example:
json.dumps({b'a': b'c'}, cls=BytesEncoder)""" .

"DESCRIPTION.The function xsplit takes a string containing information about a file, extracts the paths and names of different components (e.g. folder, file, archive) from the string, and returns them separately." <EXPLAINS> "CODE.xsplit(\"zip://folder1/file.txt::https://host.com/archive.zip\")" .

"DESCRIPTION.The functionality of this code is to define a custom subclass of `tf.keras.Model` called `SubclassModel`, which has two dense layers `d1` and `d2`. The `call` method of `SubclassModel` applies these layers to the input and returns the output. It also creates a model using the functional API with two dense layers named 'd1' and 'd2'. Finally, it retrieves the weight paths of the layers in both the custom subclass model and the functional API model." <EXPLAINS> """CODE.class SubclassModel(tf.keras.Model):

  def __init__(self, name=None):
    super().__init__(name=name)
    self.d1 = tf.keras.layers.Dense(10)
    self.d2 = tf.keras.layers.Dense(20)

  def call(self, inputs):
    x = self.d1(inputs)
    return self.d2(x)

model = SubclassModel()
model(tf.zeros((10, 10)))
weight_paths = model.get_weight_paths()

inputs = tf.keras.Input((10,), batch_size=10)
x = tf.keras.layers.Dense(20, name='d1')(inputs)
output = tf.keras.layers.Dense(30, name='d2')(x)
model = tf.keras.Model(inputs, output)
d1 = model.layers[1]
d2 = model.layers[2]
weight_paths = model.get_weight_paths()
""" .

"DESCRIPTION.The functions generate random numbers from a beta distribution using TensorFlow's random gamma functions and return the ratio of two randomly generated gamma variables." <EXPLAINS> """CODE.def broken_beta(shape, alpha, beta, seed):
  x = tf.random_gamma(shape, alpha, seed=seed)
  y = tf.random_gamma(shape, beta, seed=seed)
  return x / (x + y)

def random_beta(shape, alpha, beta, seed):
  seed = SeedStream(seed, salt="random_beta")
  x = tf.random_gamma(shape, alpha, seed=seed())
  y = tf.random_gamma(shape, beta, seed=seed())
  return x / (x + y)
""" .

"DESCRIPTION.The functions test_stderr_pass and test_stderr_fail capture the standard error output, write \"foo\" to the standard error, get the value of the standard error, and then assert if the output is equal to \"foo\" or \"bar\" respectively." <EXPLAINS> """CODE.@capture_stderr
def test_stderr_pass():
    sys.stderr.write("foo")
    out = sys.stderr.getvalue()
    assert out == "foo"

@capture_stderr
def test_stderr_fail():
    sys.stderr.write("foo")
    out = sys.stderr.getvalue()
    assert out == "bar\"""" .

"DESCRIPTION.The given Python code calculates the gradient of the function y = a*b*c*a with respect to the inputs a, b, and c." <EXPLAINS> """CODE.def f(a, b, c):
    prod_1 = a * b
    with torch.autograd.graph.save_on_cpu():
        prod_2 = prod_1 * c
    y = prod_2 * a
    return y

y = f(a, b, c)

del a, b, c

y.sum().backward()
""" .

"DESCRIPTION.The given Python code calculates the receiver operating characteristic (ROC) curve by comparing the values in tensors x and y, and returns the false positive rates (fpr), true positive rates (tpr), and thresholds." <EXPLAINS> """CODE.x = torch.tensor([0, 1, 2, 3])
y = torch.tensor([0, 1, 2, 2])
fpr, tpr, thresholds = roc(x, y)
fpr
tpr
thresholds""" .

"""DESCRIPTION.The given Python code defines a class `Flow` that inherits from `LightningFlow`. Inside the class, there is a method `configure_layout` that is used to configure the layout of the flow.

In the first code snippet, the method returns an instance of `StaticWebFrontend` class with a specified folder path to serve static web content.

In the second code snippet, the method returns a `StreamlitFrontend` object with a custom rendering function `my_streamlit_ui`, which defines Streamlit UI elements.

In the third code snippet, the method returns a list of dictionaries, where each dictionary represents a tab in the layout. Each dictionary includes the name of the tab and its content, which can be another component or a direct URL.""" <EXPLAINS> """CODE.from lightning_app.frontend import StaticWebFrontend

class Flow(LightningFlow):
    ...

    def configure_layout(self):
        return StaticWebFrontend("path/to/folder/to/serve")

from lightning_app.frontend import StaticWebFrontend

class Flow(LightningFlow):
    ...

    def configure_layout(self):
        return StreamlitFrontend(render_fn=my_streamlit_ui)

def my_streamlit_ui(state):
    # add your streamlit code here!
    import streamlit as st

    st.button("Hello!")

class Flow(LightningFlow):
    ...

    def configure_layout(self):
        return [
            dict(name="First Tab", content=self.child0),
            dict(name="Second Tab", content=self.child1),
            # You can include direct URLs too
            dict(name="Lightning", content="https://lightning.ai"),
        ]
""" .

"DESCRIPTION.The given Python code performs quantization aware training on a neural network model using XNNPACKQuantizer for symmetric quantization. The code captures the pre-autograd graph of the model and then prepares the model for quantization using the defined quantizer. Finally, it runs the quantization aware training loop on the prepared model." <EXPLAINS> """CODE.import torch
from torch.ao.quantization.quantize_pt2e import prepare_qat_pt2e
from torch._export import capture_pre_autograd_graph
from torch.ao.quantization.quantizer import (
    XNNPACKQuantizer,
    get_symmetric_quantization_config,
)

class M(torch.nn.Module):
    def __init__(self):
        super().__init__()
        self.linear = torch.nn.Linear(5, 10)

   def forward(self, x):
       return self.linear(x)

# initialize a floating point model
float_model = M().eval()

# define the training loop for quantization aware training
def train_loop(model, train_data):
    model.train()
    for image, target in data_loader:
        ...

# Step 1. program capture
# NOTE: this API will be updated to torch.export API in the future, but the captured
# result shoud mostly stay the same
m = capture_pre_autograd_graph(m, *example_inputs)
# we get a model with aten ops

# Step 2. quantization
# backend developer will write their own Quantizer and expose methods to allow
# users to express how they
# want the model to be quantized
quantizer = XNNPACKQuantizer().set_global(get_symmetric_quantization_config())
m = prepare_qat_pt2e(m, quantizer)

# run quantization aware training
train_loop(prepared_model, train_loop)""" .

"DESCRIPTION.The given Python code trains a neural network model using TensorFlow by defining the model architecture, compiling it with evaluation metrics, creating a dataset, and using a SidecarEvaluator to evaluate the model's performance. The code then saves the model checkpoints during training and uses a ModelCheckpoint callback to save the model's weights after each epoch." <EXPLAINS> """CODE.model = tf.keras.models.Sequential(...)
model.compile(metrics=tf.keras.metrics.SparseCategoricalAccuracy(
    name="eval_metrics"))
data = tf.data.Dataset.from_tensor_slices(...)

SidecarEvaluator(
    model=model,
    data=data,
    checkpoint_dir='/tmp/checkpoint_dir',  # dir for training-saved checkpoint
    steps=None,  # Eval until dataset is exhausted
    max_evaluations=None,  # The evaluation needs to be stopped manually
    callbacks=[tf.keras.callbacks.TensorBoard(log_dir='/tmp/log_dir')]
).start()


checkpoint_dir = ...  # Same `checkpoint_dir` supplied to `SidecarEvaluator`.
checkpoint = tf.train.Checkpoint(model=model, optimizer=optimizer)
checkpoint_manager = tf.train.CheckpointManager(
    checkpoint, checkpoint_dir=..., max_to_keep=...)
checkpoint_manager.save()

checkpoint_dir = ...  # Same `checkpoint_dir` supplied to `SidecarEvaluator`.
model_checkpoint = tf.keras.callbacks.ModelCheckpoint(
    filepath=os.path.join(checkpoint_dir, 'ckpt-{epoch}'),
    save_weights_only=True)
model.fit(dataset, epochs, callbacks=[model_checkpoint])
""" .

"DESCRIPTION.The given Python code utilizes the PaddlePaddle framework to create and work with Beta distributions. The code demonstrates how to calculate the mean, variance, and entropy of the Beta distribution. It shows examples with both scalar and tensor inputs, showcasing how to handle broadcasting in tensor operations." <EXPLAINS> """CODE.import paddle

# scale input
beta = paddle.distribution.Beta(alpha=0.5, beta=0.5)
print(beta.mean)
# Tensor(shape=[1], dtype=float32, place=CUDAPlace(0), stop_gradient=True,
#        [0.50000000])
print(beta.variance)
# Tensor(shape=[1], dtype=float32, place=CUDAPlace(0), stop_gradient=True,
#        [0.12500000])
print(beta.entropy())
# Tensor(shape=[1], dtype=float32, place=CUDAPlace(0), stop_gradient=True,
#        [0.12500000])

# tensor input with broadcast
beta = paddle.distribution.Beta(alpha=paddle.to_tensor([0.2, 0.4]), beta=0.6)
print(beta.mean)
# Tensor(shape=[2], dtype=float32, place=CUDAPlace(0), stop_gradient=True,
#        [0.25000000, 0.40000001])
print(beta.variance)
# Tensor(shape=[2], dtype=float32, place=CUDAPlace(0), stop_gradient=True,
#        [0.10416666, 0.12000000])
print(beta.entropy())
# Tensor(shape=[2], dtype=float32, place=CUDAPlace(0), stop_gradient=True,
#        [-1.91923141, -0.38095069])""" .

"DESCRIPTION.The given code calculates a cosine decay learning rate based on the global step, decay steps, alpha, and initial learning rate." <EXPLAINS> """CODE.global_step = min(global_step, decay_steps)
cosine_decay = 0.5 * (1 + cos(pi * global_step / decay_steps))
decayed = (1 - alpha) * cosine_decay + alpha
decayed_learning_rate = learning_rate * decayed


decay_steps = 1000
lr_decayed = cosine_decay(learning_rate, global_step, decay_steps)
""" .

"DESCRIPTION.The given code calculates precision, recall, and thresholds for a multiclass classification model based on the predicted values and target labels. The output includes the number of classes, precision values, recall values, and thresholds for the model." <EXPLAINS> """CODE.pred = torch.tensor([[0.85, 0.05, 0.05, 0.05],
...                      [0.05, 0.85, 0.05, 0.05],
...                      [0.05, 0.05, 0.85, 0.05],
...                      [0.05, 0.05, 0.05, 0.85]])
target = torch.tensor([0, 1, 3, 2])
nb_classes, precision, recall, thresholds = multiclass_precision_recall_curve(pred, target)
nb_classes
(tensor([1., 1.]), tensor([1., 0.]), tensor([0.8500]))
precision
(tensor([1., 1.]), tensor([1., 0.]), tensor([0.8500]))
recall
(tensor([0.2500, 0.0000, 1.0000]), tensor([1., 0., 0.]), tensor([0.0500, 0.8500]))
thresholds   # doctest: +NORMALIZE_WHITESPACE
(tensor([0.2500, 0.0000, 1.0000]), tensor([1., 0., 0.]), tensor([0.0500, 0.8500]))""" .

"DESCRIPTION.The given code converts MIDI notes to corresponding Indian classical music svara symbols based on the provided Sa (tonic note). The output includes the svara symbols for the given MIDI notes in the specified format and representation." <EXPLAINS> """CODE.librosa.midi_svara_h([60, 61, 62], Sa=60)
['S', 'r', 'R']
librosa.midi_to_svara_h([60, 61, 62], Sa=67)
['mÌ£', 'MÌ£', 'PÌ£']
librosa.midi_to_svara_h([60, 61, 62], Sa=67, unicode=False)
['m,', 'M,', 'P,']
librosa.midi_to_svara_h([72, 73, 74], Sa=60, abbr=False)
['SÌa', 'rÌe', 'RÌe']""" .

"DESCRIPTION.The given code defines a function called coerce_odd that takes a value as input and checks if the value is odd. If the value is odd, the function returns the value, otherwise it returns None. It also creates a scalar type named 'Odd' in a GraphQL schema with the serialization function set to coerce_odd." <EXPLAINS> """CODE.def coerce_odd(value):
    if value % 2 == 1:
        return value
    return None

OddType = GraphQLScalarType(name='Odd', serialize=coerce_odd)""" .

"DESCRIPTION.The given code defines a function named func that takes a parameter x and returns the value of x." <EXPLAINS> """CODE.@Deprecated
def func(x):
    return x""" .

"DESCRIPTION.The given code segment implements a ReLU (Rectified Linear Unit) activation function with different configurations. The ReLU function is applied element-wise to the input tensor, where if the input is greater than or equal to the max_value threshold, the output is set to max_value. If the input is within the threshold range, the output remains the same. Otherwise, a negative_slope is applied to the input within the specified threshold range." <EXPLAINS> """CODE.
  f(x) = max_value if x >= max_value
  f(x) = x if threshold <= x < max_value
  f(x) = negative_slope * (x - threshold) otherwise



layer = tf.keras.layers.ReLU()
output = layer([-3.0, -1.0, 0.0, 2.0])
list(output.numpy())
[0.0, 0.0, 0.0, 2.0]



layer = tf.keras.layers.ReLU(max_value=1.0)
output = layer([-3.0, -1.0, 0.0, 2.0])
list(output.numpy())
[0.0, 0.0, 0.0, 1.0]



layer = tf.keras.layers.ReLU(negative_slope=1.0)
output = layer([-3.0, -1.0, 0.0, 2.0])
list(output.numpy())
[-3.0, -1.0, 0.0, 2.0]



layer = tf.keras.layers.ReLU(threshold=1.5)
output = layer([-3.0, -1.0, 1.0, 2.0])
list(output.numpy())
[0.0, 0.0, 0.0, 2.0]
""" .

"DESCRIPTION.The given code trains a model using the specified configuration for a certain number of epochs, calculates metrics during training and validation, reports the metrics using Ray.sgd, and finally returns the trained model. It also includes functionality for running an iterator that iterates over training results, performs some actions on each result, retrieves the latest checkpoint, checks if the iterator is finished, and gets the final model after training." <EXPLAINS> """CODE.def train_func(config):
    ...
    for _ in config["epochs"]:
        metrics = train()
        metrics = validate(...)
        ray.sgd.report(**metrics)
    return model

iterator = trainer.run_iterator(train_func, config=config)

for result in iterator:
    do_stuff(result)
    latest_ckpt = trainer.get_latest_checkpoint()

assert iterator.is_finished()
model = iterator.get_fin()[0]
""" .

"DESCRIPTION.The provided Python code creates a neural network model and an optimizer in default precision. It then iterates over data, performs forward pass of the model while enabling automatic mixed precision training with autocast, calculates loss, performs backward pass, and updates the model parameters using the optimizer." <EXPLAINS> """CODE.# Creates model and optimizer in default precision
model = Net().cuda()
optimizer = optim.SGD(model.parameters(), ...)

for input, target in data:
    optimizer.zero_grad()

    # Enables autocasting for the forward pass (model + loss)
    with autocast():
        output = model(input)
        loss = loss_fn(output, target)

    # Exits the context manager before backward()
    loss.backward()
    optimizer.step()


class AutocastModel(nn.Module):
    ...
    @autocast()
    def forward(self, input):
        ...
""" .

"DESCRIPTION.The provided Python code defines a Generative Adversarial Network (GAN) model using TensorFlow's `tfgan` module. It includes functions for generating data (`generator_fn`) and predicting labels (`discriminator_fn`). The GAN model is then trained and evaluated using the specified loss functions and optimizers. Finally, the model is used to generate samples from the generator." <EXPLAINS> """CODE.import tensorflow as tf
tfgan = tf.contrib.gan

# See TFGAN's `train.py` for a description of the generator and
# discriminator API.
def generator_fn(generator_inputs):
  ...
  return generated_data

def discriminator_fn(data, conditioning):
  ...
  return logits

# Create GAN estimator.
config = tpu_config.RunConfig(model_dir='/my/dir')
gan_estimator = tfgan.estimator.TPUGANEstimator(
    generator_fn=generator_fn,
    discriminator_fn=discriminator_fn,
    generator_loss_fn=tfgan.losses.wasserstein_generator_loss,
    discriminator_loss_fn=tfgan.losses.wasserstein_discriminator_loss,
    generator_optimizer=tf.compat.v1.train.AdamOptimizer(0.1, 0.5),
    discriminator_optimizer=tf.compat.v1.train.AdamOptimizer(0.1, 0.5),
    train_batch_size=4,
    config=config)

# Train estimator.
gan_estimator.train(train_input_fn, train_steps)

# Evaluate resulting estimator.
gan_estimator.evaluate(eval_input_fn, eval_steps)

# Generate samples from generator.
predictions = np.array([
    x['generated_data'] for x in gan_estimator.predict(predict_input_fn)])""" .

"DESCRIPTION.The provided Python code defines a Generative Adversarial Network (GAN) with a generator and a discriminator model. The generator model is responsible for generating new data samples based on random noise input, while the discriminator model is used to distinguish between real and generated data samples. The GAN is trained by optimizing the generator and discriminator models simultaneously in a adversarial manner to improve the quality of generated samples." <EXPLAINS> """CODE.GAN(img_shape=(1, 8, 8))
(generator): Generator(
(model): Sequential(...)
)
(discriminator): Discriminator(
(model): Sequential(...)
)""" .

"DESCRIPTION.The provided Python code defines a custom PyLayer class named cus_tanh that inherits from PyLayer. The forward method of this class takes an input tensor x, applies a specified function func1 to it, saves the result for backward calculation, and returns the output. The backward method of the PyLayer class calculates the gradient of the input with respect to the output and returns it. The code then generates a random tensor, applies the custom tanh function to it using the cus_tanh class, calculates the mean, and performs backward propagation to update the gradient of the input tensor. Finally, it prints the gradient of the input tensor after the backward pass." <EXPLAINS> """CODE.import paddle
from paddle.autograd import PyLayer

# Inherit from PyLayer
class cus_tanh(PyLayer):
    @staticmethod
    def forward(ctx, x, func1, func2=paddle.square):
        # ctx is a context object that store some objects for backward.
        ctx.func = func2
        y = func1(x)
        # Pass tensors to backward.
        ctx.save_for_backward(y)
        return y

    @staticmethod
    # forward has only one output, so there is only one gradient in the input of backward.
    def backward(ctx, dy):
        # Get the tensors passed by forward.
        y, = ctx.saved_tensor()
        grad = dy * (1 - ctx.func(y))
        # forward has only one input, so only one gradient tensor is returned.
        return grad

data = paddle.randn([2, 3], dtype="float64")
data.stop_gradient = False
z = cus_tanh.apply(data, func1=paddle.tanh)
z.mean().backward()

print(data.grad)""" .

"DESCRIPTION.The provided Python code defines a custom RNNModel class with an LSTMCell-based RNN implemented in the call method. It also demonstrates the creation of two instances of the RNNModel class (model_1 and model_2) and shows that without proper scoping, model_2 will raise an error due to the conflict of creating already existing variables. The subsequent code with the keras_style_scope ensures that each model creates its own variables, preventing conflicts, and asserts that the weights of model_1 and model_2 are not equal." <EXPLAINS> """CODE.class RNNModel(tf.keras.Model):

  def __init__(self, name):
    super(RNNModel, self).__init__(name=name)
    self.rnn = tf.compat.v1.nn.rnn_cell.MultiRNNCell(
      [tf.compat.v1.nn.rnn_cell.LSTMCell(64) for _ in range(2)])

  def call(self, input, state):
    return self.rnn(input, state)

model_1 = RNNModel("model_1")
model_2 = RNNModel("model_2")

# OK
output_1, next_state_1 = model_1(input, state)
# Raises an error about trying to create an already existing variable.
output_2, next_state_2 = model_2(input, state)

with keras_style_scope():
  model_1 = RNNModel("model_1")
  model_2 = RNNModel("model_2")

  # model_1 and model_2 are guaranteed to create their own variables.
  output_1, next_state_1 = model_1(input, state)
  output_2, next_state_2 = model_2(input, state)

  assert len(model_1.weights) > 0
  assert len(model_2.weights) > 0
  assert(model_1.weights != model_2.weights)
""" .

"DESCRIPTION.The provided Python code defines a multilayer perceptron (MLP) model for image classification with customizable layer sizes and activation functions. It also includes functions for training the MLP model using cross-entropy loss and the Adam optimizer. The training process involves iterating over a specified number of epochs and saving the model parameters after each epoch." <EXPLAINS> """CODE.def mlp(image, layer_sizes=[200, 100], activation="relu", num_classes=10):
    hidden = image
    for layer_size in layer_sizes:
        hidden = fluid.layers.fc(input=hidden, size=layer_size, act=activation)
    return fluid.layers.fc(input=hidden, size=num_classes, act="softmax")

def train_mnist_mlp():
    img = fluid.layers.data(name='image', shape=[784])
    label = fluid.layers.data(name='label', shape=[1], dtype='int64')
    prediction = mlp(img)
    return fluid.layers.mean(fluid.layers.cross_entropy(prediction, label))

def optimizer():
    return fluid.optimizer.Adam()

trainer = Trainer(train_func=train_mnist_mlp,
                  optimizer_func=optimizer,
                  place=fluid.CUDAPlace(0),
                  parallel=True)

def train_callback(event):
    if isinstance(event, fluid.EndStepEvent):
        print "Epoch ID", event.epoch, "Step ID", event.step, "AvgLoss", event.metrics[0]
    elif isinstance(event, fluid.EndEpochEvent):
        trainer.save_params("./model_{0}".format(event.epoch))

trainer.train(num_epochs=100, event_handler=train_callback)""" .

"DESCRIPTION.The provided Python code defines a series of training steps for a machine learning model. The code includes functions for optimizing model parameters using different optimizers, computing losses based on model predictions, applying dropout regularization, and updating model weights based on calculated gradients. Additionally, the code implements training strategies like MC dropout training and GANs training by alternating between generator and discriminator optimization steps, logging loss values, and specifying gradient accumulation settings." <EXPLAINS> """CODE.def training_step(...):
    (opt_a, opt_b) = self.optimizers()
    loss_a = ...
    # automatically applies scaling, etc...
    self.manual_backward(loss_a, opt_a)
    opt_a.step()

def training_step(self, batch, batch_idx):
    # using Boring Model
    opt = self.optimizers() # only 1 optimizer

    def compute_loss():
        x = batch[0]
        x = F.dropout(x, 0.1)
        predictions = self(x)
        predictions = F.dropout(predictions, 0.1)
        loss = self.loss(None, predictions)
        return loss

    def closure():
        # emulate MC dropout training
        num_backward = 1
        losses = []
        for backward_idx in range(num_backward + 1):
            loss = compute_loss()
            losses.append(loss)
            retain_graph = num_backward!= backward_idx
            self.manual_backward(loss, opt, retain_graph=retain_graph)
        loss_mean = torch.stack(losses).mean()
        loss_std = torch.stack(losses).std()
        self.log("train_loss_mean", loss_mean, on_step=True, prog_bar=True, on_epoch=True)
        self.log("train_loss_std", loss_std, on_step=True, prog_bar=True, on_epoch=True)

    opt.step(loss, closure=closure)

def training_step(self, batch, batch_idx, optimizer_idx):

    # emulate gans training
    opt_gen, opt_dis = self.optimizers()

    # Note: Be careful, don't log on the same key in self.log in both closure
    # as they will be aggregated together on epoch_end

    def gen_closure():
        ... forward and compute loss for generator
        loss_gen = ...
        self.log("loss_gen", loss_gen, on_step=True, on_epoch=True)
        self.manual_backward(loss_gen, opt_gen)

    def dis_closure():
        ... forward and compute loss for discriminator
        loss_dis = ...
        self.log("loss_dis", loss_dis, on_step=True, on_epoch=True)
        self.manual_backward(loss_dis, opt_dis)

    # this will accumulate gradients for 2 batches and then call opt_gen.step()
    opt_gen.step(closure=gen_closure, make_optimizer_step=batch_idx % 2 == 0)

    # update discriminator every 4 batches
    # therefore, no gradient accumulation for discriminator
    if batch_idx % 4 == 0 :
        # Note: Set make_optimizer_step to True or it will use by default
        # Trainer(accumulate_grad_batches=x)
        opt_dis.step(closure=optimizer_closure, make_optimizer_step=True)""" .

"""DESCRIPTION.The provided Python code defines three different classes that inherit from the LightningModule class. Each class initializes with specific arguments and saves hyperparameters based on the input arguments in different ways.

1. ManuallyArgsModel class manually assigns specific arguments (arg1, arg3) as hyperparameters using the save_hyperparameters method. The hparams attribute of the model then displays the assigned hyperparameters.

2. AutomaticArgsModel class automatically saves all input arguments as hyperparameters using the save_hyperparameters method. The hparams attribute of the model displays all arguments as hyperparameters.

3. SingleArgModel class manually assigns a single argument (params) as hyperparameters using the save_hyperparameters method. The hparams attribute of the model displays the assigned hyperparameters.""" <EXPLAINS> """CODE.from collections import OrderedDict
class ManuallyArgsModel(LightningModule):
...     def __init__(self, arg1, arg2, arg3):
...         super().__init__()
...         # manually assine arguments
...         self.save_hyperparameters('arg1', 'arg3')
...     def forward(self, *args, **kwargs):
...         ...
model = ManuallyArgsModel(1, 'abc', 3.14)
model.hparams
"arg1": 1
"arg3": 3.14

class AutomaticArgsModel(LightningModule):
...     def __init__(self, arg1, arg2, arg3):
...         super().__init__()
...         # equivalent automatic
...         self.save_hyperparameters()
...     def forward(self, *args, **kwargs):
...         ...
model = AutomaticArgsModel(1, 'abc', 3.14)
model.hparams
"arg1": 1
"arg2": abc
"arg3": 3.14

class SingleArgModel(LightningModule):
...     def __init__(self, params):
...         super().__init__()
...         # manually assign single argument
...         self.save_hyperparameters(params)
...     def forward(self, *args, **kwargs):
...         ...
model = SingleArgModel(Namespace(p1=1, p2='abc', p3=3.14))
model.hparams
"p1": 1
"p2": abc
"p3": 3.14""" .

"DESCRIPTION.The provided Python code downloads a file from a given URL and saves it to a specified location. It then extracts the downloaded archive file to a specified location." <EXPLAINS> """CODE.torchaudio.datasets.utils.download_from_url(url, from_path)
torchaudio.datasets.utils.extract_archive(from_path, to_path)""" .

"DESCRIPTION.The provided Python code uses the Flax library to manipulate nested dictionary structures. It demonstrates how to create a cursor object to interact with the dictionary, modify values at specific paths within the dictionary, and build a new modified dictionary object." <EXPLAINS> """CODE.from flax.cursor import cursor

dict_obj = {'a': 1, 'b': (2, 3), 'c': [4, 5]}
modified_dict_obj = cursor(dict_obj)['b'][0].set(10)
assert modified_dict_obj == {'a': 1, 'b': (10, 3), 'c': [4, 5]}


from flax.cursor import cursor

dict_obj = {'a': 1, 'b': (2, 3), 'c': [4, 5]}
c = cursor(dict_obj)
c['b'][0] = 10
c['a'] = (100, 200)
modified_dict_obj = c.build()
assert modified_dict_obj == {'a': (100, 200), 'b': (10, 3), 'c': [4, 5]}


from flax.cursor import cursor
from flax.training import train_state
import optax

def update_fn(path, value):
    '''Replace params with empty dictionary.'''
    if 'params' in path:
        return {}
    return value

state = train_state.TrainState.create(
    apply_fn=lambda x: x,
    params={'a': 1, 'b': 2},
    tx=optax.adam(1e-3),
)
c = cursor(state)
state2 = c.apply_update(update_fn).build()
assert state2.params == {}
assert state.params == {'a': 1, 'b': 2} # make sure original params are unchanged
""" .

"DESCRIPTION.The provided code initializes a CLVP (Conditional Latent Variable Prior) decoder model with a configuration based on the susnato/clvp_dev style. The code then accesses the model's configuration." <EXPLAINS> """CODE.from transformers import ClvpDecoderConfig, ClvpDecoder

# Initializing a ClvpDecoderConfig with susnato/clvp_dev style configuration
decoder_configuration = ClvpDecoderConfig()

# Initializing a ClvpDecoder (with random weights) from the susnato/clvp_dev style configuration
model = ClvpDecoder(decoder_configuration)

# Accessing the model configuration
configuration = model.config
""" .

"DESCRIPTION.The provided code segment is deserializing different activation functions using the TensorFlow Keras library. The code first deserializes the 'linear' activation function, then the 'sigmoid' activation function, and finally attempts to deserialize an activation function named 'abcd'." <EXPLAINS> """CODE.tf.keras.activations.deserialize('linear')
tf.keras.activations.deserialize('sigmoid')
tf.keras.activations.deserialize('abcd')""" .

"DESCRIPTION.This Python code calculates the categorical cross-entropy loss between the true labels (y_true) and the predicted labels (y_pred) in a classification task. The code demonstrates different ways to calculate the loss including using different reduction types such as 'auto', 'sum_over_batch_size', 'sum', and 'none'. The output of the code provides the calculated cross-entropy loss values based on these different configurations." <EXPLAINS> """CODE.y_true = [[0, 1, 0], [0, 0, 1]]
y_pred = [[0.05, 0.95, 0], [0.1, 0.8, 0.1]]
# Using 'auto'/'sum_over_batch_size' reduction type.
cce = tf.keras.losses.CategoricalCrossentropy()
cce(y_true, y_pred).numpy()
1.177

# Calling with 'sample_weight'.
cce(y_true, y_pred, sample_weight=tf.constant([0.3, 0.7])).numpy()
0.814

# Using 'sum' reduction type.
cce = tf.keras.losses.CategoricalCrossentropy(
...     reduction=tf.keras.losses.Reduction.SUM)
cce(y_true, y_pred).numpy()
2.354

# Using 'none' reduction type.
cce = tf.keras.losses.CategoricalCrossentropy(
...     reduction=tf.keras.losses.Reduction.NONE)
cce(y_true, y_pred).numpy()
array([0.0513, 2.303], dtype=float32)""" .

"DESCRIPTION.This Python code calculates the entropy of a Laplace distribution with a mean of 0 and a scale of 1." <EXPLAINS> """CODE.import paddle

m = paddle.distribution.Laplace(paddle.to_tensor(0.0), paddle.to_tensor(1.0))
m.entropy()""" .

"DESCRIPTION.This Python code defines a Flask route at \"/path\" which will trigger the function my_handler when accessed." <EXPLAINS> """CODE.@serve.route("/path")
def my_handler(flask_request):
    ...""" .

"DESCRIPTION.This Python code defines a GraphQL union type called PetType with two types: DogType and CatType. It includes a method resolve_type that determines the appropriate type based on the input value, returning either DogType or CatType." <EXPLAINS> """CODE.class PetType(GraphQLUnionType):
    name = 'Pet'
    types = [DogType, CatType]

    def resolve_type(self, value):
        if isinstance(value, Dog):
            return DogType()
        if isinstance(value, Cat):
            return CatType()""" .

"DESCRIPTION.This Python code defines a Ray actor class named MyActor which includes a method named my_method that takes an argument." <EXPLAINS> """CODE.@ray.remote
class MyActor(RayServeMixin):
    # This is optional, by default it is "__call__"
    serve_method = 'my_method'

    def my_method(self, arg):
        ...""" .

"DESCRIPTION.This Python code defines a TransformerBlock class that applies a dense layer followed by a GeGLU activation function to the input x and returns the transformed output." <EXPLAINS> """CODE.    class TransformerBlock(nn.Module):
    ...   @nn.compact
    ...   def __call__(self, x):
    ...     x = nn.Dense(2)(x)
    ...     x = nn.GeGLU()(x) # initialized
    ...     return x""" .

"DESCRIPTION.This Python code defines a class M which is a subclass of torch.nn.Module. It contains a method forward that takes two input tensors x and y with specific shapes and returns the element-wise sum of the two tensors using torch.add." <EXPLAINS> """CODE.class M(torch.nn.Module):
    def forward(self, x:TensorType((1,2,3, Dyn)), y:TensorType((1,2,3, Dyn))):
        return torch.add(x, y)""" .

"DESCRIPTION.This Python code defines a class called CSVDatasink that inherits from BlockBasedFileDatasink. It has an __init__ method that initializes the object with a given path and file_format set to \"csv\". Additionally, it includes a method called write_block_to_file that takes a BlockAccessor object and a pyarrow.NativeFile object, converts the block to an Arrow table, and writes it to the file in CSV format using the pyarrow csv module." <EXPLAINS> """CODE.class CSVDatasink(BlockBasedFileDatasink):
    def __init__(self, path: str):
        super().__init__(path, file_format="csv")

    def write_block_to_file(self, block: BlockAccessor, file: "pyarrow.NativeFile"):
        from pyarrow import csv
        csv.write_csv(block.to_arrow(), file)""" .

"DESCRIPTION.This Python code defines a class called Inplace that implements methods for forward and backward operations. The forward method takes an input tensor x, increments its values by 1 in place, marks the input tensor as dirty, and returns the modified input tensor. The backward method simply returns the gradient of the output." <EXPLAINS> """CODE.class Inplace(Function):
    @staticmethod
    def forward(ctx, x):
        x_npy = x.numpy() # x_npy shares storage with x
        x_npy += 1
        ctx.mark_dirty(x)
        return x

    @staticmethod
    @once_differentiable
    def backward(ctx, grad_output):
        return grad_output
""" .

"DESCRIPTION.This Python code defines a class called PositivePoint that represents a point with positive coordinates. The class only allows the creation of instances with positive x and y coordinates. If the coordinates are not positive, it raises an exception with the message 'Coordinates must be positive!'." <EXPLAINS> """CODE.class PositivePoint(pclass('x, y')):
    __slots__ = tuple()
    def __new__(cls, x, y):
        if x > 0 and y > 0:
            return super(PositivePoint, cls).__new__(cls, x, y)
        raise Exception('Coordinates must be positive!')""" .

"DESCRIPTION.This Python code defines a class called PositivePoint which represents a point in a two-dimensional space with positive coordinates. It restricts the creation of instances to only those with positive x and y coordinates, raising an exception if either x or y is not positive." <EXPLAINS> """CODE.class PositivePoint(immutable('x, y')):
    __slots__ = tuple()
    def __new__(cls, x, y):
        if x > 0 and y > 0:
            return super(PositivePoint, cls).__new__(cls, x, y)
        raise Exception('Coordinates must be positive!')""" .

"DESCRIPTION.This Python code defines a class called Positives that inherits from CheckedPVector. The class is specifically designed to store positive integers. An invariant function is defined to ensure that only non-negative integers are added to the list. A new instance of Positives is created with the values [1, 2, 3]." <EXPLAINS> """CODE.class Positives(CheckedPVector):
    __type__ = (long, int)
    __invariant__ = lambda n: (n >= 0, 'Negative')

Positives([1, 2, 3])""" .

"DESCRIPTION.This Python code defines a command line interface (CLI) using the Click library. It includes a group command with an option for input, which defaults to 23. The CLI returns the value 42. There is also a result callback function that adds the input value to the result and returns the sum." <EXPLAINS> """CODE.@click.group()
@click.option('-i', '--input', default=23)
def cli(input):
    return 42

@cli.result_callback()
def process_result(result, input):
    return result + input""",
        """CODE.@click.group()
@click.option('-i', '--input', default=23)
def cli(input):
    return 42

@cli.resultcallback()
def process_result(result, input):
    return result + input""" .

"DESCRIPTION.This Python code defines a custom 'clean' command for the setup.py file." <EXPLAINS> "CODE.python setup.py clean' custom command." .

"DESCRIPTION.This Python code defines a custom JVP (Jacobean-Vector Product) for a function f with two input variables x and y. The function f computes the sine of x multiplied by y. The custom JVP function f_jvp calculates the JVP of the function f with respect to its inputs x and y, returning the output of f and the JVP with respect to x and y." <EXPLAINS> """CODE.@jax.custom_jvp
def f(x, y):
    return np.sin(x) * y

@f.defjvp
def f_jvp(primals, tangents):
    x, y = primals
    x_dot, y_dot = tangents
    primal_out = f(x, y)
    tangent_out = np.cos(x) * x_dot * y - np.sin(x) * y_dot
    return primal_out, tangent_out""" .

"DESCRIPTION.This Python code defines a custom PyLayer class called cus_tanh that implements the forward and backward functions for calculating the hyperbolic tangent (tanh) activation function in a neural network. The forward function computes the tanh function on the input tensor x and saves the result for backpropagation. The backward function computes the gradient of the tanh function with respect to the input tensor." <EXPLAINS> """CODE.import paddle
from paddle.autograd import PyLayer

class cus_tanh(PyLayer):
    @staticmethod
    def forward(ctx, x):
        # ctx is a context object that store some objects for backward.
        y = paddle.tanh(x)
        # Pass tensors to backward.
        ctx.save_for_backward(y)
        return y

    @staticmethod
    def backward(ctx, dy):
        # Get the tensors passed by forward.
        y, = ctx.saved_tensor()
        grad = dy * (1 - paddle.square(y))
        return grad""" .

"DESCRIPTION.This Python code defines a custom PyLayer class that implements the Tanh activation function. The forward method calculates the tanh of the input tensor x and saves the result for backward computation. The backward method calculates the gradient of the loss function with respect to the input using the derivative of the tanh function." <EXPLAINS> """CODE.import paddle
from paddle.autograd import PyLayer

class cus_tanh(PyLayer):
    @staticmethod
    def forward(ctx, x):
        y = paddle.tanh(x)
        # Pass tensors to backward.
        ctx.save_for_backward(y)
        return y

    @staticmethod
    def backward(ctx, dy):
        # Get the tensors passed by forward.
        y, = ctx.saved_tensor()
        grad = dy * (1 - paddle.square(y))
        return grad""" .

"DESCRIPTION.This Python code defines a custom PyLayer class that implements the Tanh activation function. The forward method computes the Tanh function on the input tensor x and saves the result for the backward pass. The backward method takes the gradient dy and the saved tensor y from the forward pass, and computes the gradient of the Tanh function with respect to the input tensor x." <EXPLAINS> """CODE.import paddle
from paddle.autograd import PyLayer

class cus_tanh(PyLayer):
    @staticmethod
    def forward(ctx, x):
        # ctx is a context object that store some objects for backward.
        y = paddle.tanh(x)
        # Pass tensors to backward.
        ctx.save_for_backward(y)
        return y

    @staticmethod
    def backward(ctx, dy):
        # Get the tensors passed by forward.
        y, = ctx.saved_tensor()
        grad = dy * (1 - paddle.square(y))
        return grad""" .

"DESCRIPTION.This Python code defines a custom PyLayer class that implements the forward and backward methods for calculating the hyperbolic tangent (tanh) function. The forward method takes an input tensor x, applies the tanh function to it, saves the output tensor y for backward computation, and returns y. The backward method takes the gradient dy, retrieves the saved tensor y, computes the gradient of the tanh function with respect to y, and returns the gradient." <EXPLAINS> """CODE.import paddle
from paddle.autograd import PyLayer

class cus_tanh(PyLayer):
    @staticmethod
    def forward(ctx, x):
        # ctx is a context object that store some objects for backward.
        y = paddle.tanh(x)
        # Pass tensors to backward.
        ctx.save_for_backward(y)
        return y

    @staticmethod
    def backward(ctx, dy):
        # Get the tensors passed by forward.
        y, = ctx.saved_tensor()
        grad = dy * (1 - paddle.square(y))
        return grad""" .

"DESCRIPTION.This Python code defines a custom PyLayer class that implements the tanh activation function. The forward method computes the tanh activation of the input tensor x and saves the output for use in the backpropagation step. The backward method computes the gradient of the tanh activation function with respect to the input tensor based on the provided gradient dy." <EXPLAINS> """CODE.import paddle
from paddle.autograd import PyLayer

class cus_tanh(PyLayer):
    @staticmethod
    def forward(ctx, x):
        y = paddle.tanh(x)
        # Pass tensors to backward.
        ctx.save_for_backward(y)
        return y

    @staticmethod
    def backward(ctx, dy):
        # Get the tensors passed by forward.
        y, = ctx.saved_tensor()
        grad = dy * (1 - paddle.square(y))
        return grad""" .

"DESCRIPTION.This Python code defines a custom PyLayer named cus_tanh that implements the forward and backward methods for calculating the hyperbolic tangent function. The forward method calculates the hyperbolic tangent of the input tensor x and stores the result for backpropagation. The backward method computes the gradient of the output with respect to the input using the saved tensor from the forward pass." <EXPLAINS> """CODE.import paddle
from paddle.autograd import PyLayer

class cus_tanh(PyLayer):
    @staticmethod
    def forward(ctx, x):
        # ctx is a context object that store some objects for backward.
        y = paddle.tanh(x)
        # Pass tensors to backward.
        ctx.save_for_backward(y)
        return y

    @staticmethod
    def backward(ctx, dy):
        # Get the tensors passed by forward.
        y, = ctx.saved_tensor()
        grad = dy * (1 - paddle.square(y))
        return grad""" .

"DESCRIPTION.This Python code defines a custom PyLayer subclass called `cus_tanh` that implements the forward and backward computation for the hyperbolic tangent (tanh) activation function. In the forward method, it computes the tanh function of the input tensor x and saves the result for backpropagation. In the backward method, it computes the gradients with respect to the input tensor x using the saved tensor y from the forward pass." <EXPLAINS> """CODE.import paddle
from paddle.autograd import PyLayer

class cus_tanh(PyLayer):
    @staticmethod
    def forward(ctx, x):
        y = paddle.tanh(x)
        # Pass tensors to backward.
        ctx.save_for_backward(y)
        return y

    @staticmethod
    def backward(ctx, dy):
        # Get the tensors passed by forward.
        y, = ctx.saved_tensor()
        grad = dy * (1 - paddle.square(y))
        return grad""" .

"DESCRIPTION.This Python code defines a custom model class inheriting from tf.keras.Model and KerasModelHubMixin. The class initializes with optional configuration parameters, dummy inputs, and a layer. The call method is defined but its functionality is not specified in the provided code snippet." <EXPLAINS> """CODE.class MyModel(tf.keras.Model, KerasModelHubMixin):
...    def __init__(self, **kwargs):
...        super().__init__()
...        self.config = kwargs.pop("config", None)
...        self.dummy_inputs = ...
...        self.layer = ...
...    def call(self, ...)
...        return ...""" .

"DESCRIPTION.This Python code defines a custom tanh layer using Paddle's autograd functionality. The forward method computes the hyperbolic tangent of the input tensor and saves the output tensor for the backward pass. The backward method calculates the gradient of the output tensor with respect to the input tensor using the chain rule and the derivative of the tanh function." <EXPLAINS> """CODE.import paddle
from paddle.autograd import PyLayer

class cus_tanh(PyLayer):
    @staticmethod
    def forward(ctx, x):
        y = paddle.tanh(x)
        # Pass tensors to backward.
        ctx.save_for_backward(y)
        return y

    @staticmethod
    def backward(ctx, dy):
        # Get the tensors passed by forward.
        y, = ctx.saved_tensor()
        grad = dy * (1 - paddle.square(y))
        return grad""" .

"DESCRIPTION.This Python code defines a function named identityfunc that takes a single argument and returns the same argument without any modifications." <EXPLAINS> "CODE.identityfunc('arg')" .

"DESCRIPTION.This Python code defines a grammar for arithmetic expressions that can consist of signed integers or variables. It includes rules for the precedence and associativity of different operators like addition, subtraction, multiplication, and division." <EXPLAINS> """CODE.integer = pyparsing_common.signed_integer
varname = pyparsing_common.identifier

arith_expr = infixNotation(integer | varname,
    [
    ('-', 1, opAssoc.RIGHT),
    (oneOf('* /'), 2, opAssoc.LEFT),
    (oneOf('+ -'), 2, opAssoc.LEFT),
    ])
""" .

"DESCRIPTION.This Python code defines a method called configure_callbacks which creates an EarlyStopping object to monitor validation accuracy and a ModelCheckpoint object to monitor validation loss. The method returns a list containing these two objects." <EXPLAINS> """CODE.def configure_callbacks(self):
    early_stop = EarlyStopping(monitor="val_acc", mode="max")
    checkpoint = ModelCheckpoint(monitor="val_loss")
    return [early_stop, checkpoint]""" .

"""DESCRIPTION.This Python code defines a neural network module called MyLinearModule, which contains methods for applying a linear transformation on input data. The apply method takes input data x, desired output features, and a kernel initialization function as input, and computes the dot product of x and a learnable kernel parameter. The apply_transpose method takes input data x, retrieves the learned kernel parameter, and computes the dot product of x and the transpose of the kernel matrix.

The AutoEncoder class defines an autoencoder neural network module that utilizes the MyLinearModule for encoding and decoding input data. It instantiates a shared instance of the MyLinearModule with the specified number of output features. The apply method of the AutoEncoder class takes input data x, computes the encoding of x using the shared MyLinearModule instance, and then reconstructs the input data by applying the transpose of the learned encoding.""" <EXPLAINS> """CODE.class MyLinearModule(nn.Module):
    def apply(self, x, features, kernel_init):
      kernel = self.param('kernel', (x.shape[-1], features), kernel_init)
      return jnp.dot(x, kernel)

    @nn.module_method
    def apply_transpose(self, x, **kwargs):
      kernel = self.get_param('kernel')
      return jnp.dot(x, kernel.transpose((1, 0)))

class AutoEncoder(nn.module):
    def apply(self, x, features):
      linear_fn = MyLinearModule.shared(features=features)
      h = linear_fn(x)
      y = linear_fn.apply_transpose(h)
      return y
""" .

"DESCRIPTION.This Python code defines a random batch reader function that generates random images and labels. It then trains a static model using a fully connected neural network (FC) with softmax activation function and saves the inference model. Finally, it loads the inference model in dynamic graph mode, fine-tunes the model using stochastic gradient descent (SGD) optimizer, and calculates the average loss during training." <EXPLAINS> """CODE.import numpy as np
import paddle.fluid as fluid

BATCH_SIZE = 32
BATCH_NUM = 20
SAVE_DIRNAME = "fc.inference.model"

def random_batch_reader():
    def _get_random_images_and_labels(image_shape, label_shape):
        image = np.random.random(size=image_shape).astype('float32')
        label = np.random.random(size=label_shape).astype('int64')
        return image, label

    def __reader__():
        for _ in range(BATCH_NUM):
            batch_image, batch_label = _get_random_images_and_labels(
                [BATCH_SIZE, 784], [BATCH_SIZE, 1])
            yield batch_image, batch_label

    return __reader__

def train_and_save_static_model(place):
    img = fluid.data(name='img', shape=[None, 784], dtype='float32')
    label = fluid.data(name='label', shape=[None, 1], dtype='int64')

    pred = fluid.layers.fc(input=img, size=10, act='softmax')

    loss = fluid.layers.cross_entropy(input=pred, label=label)
    avg_loss = fluid.layers.mean(loss)

    optimizer = fluid.optimizer.SGD(learning_rate=0.001)
    optimizer.minimize(avg_loss)

    exe = fluid.Executor(place)
    exe.run(fluid.default_startup_program())

    loader = fluid.io.DataLoader.from_generator(
        feed_list=[img, label], capacity=5, iterable=True)
    loader.set_batch_generator(random_batch_reader(), places=place)

    for data in loader():
        exe.run(
            fluid.default_main_program(),
            feed=data,
            fetch_list=[avg_loss])

    # save model by fluid.io.save_inference_model
    fluid.io.save_inference_model(
        SAVE_DIRNAME, ["img"], [pred], exe)


# Step 1. train and save inference model in static graph mode
place = fluid.CPUPlace()
train_and_save_static_model(place)

# Step 2. load inference model in dygraph and fine-tune
with fluid.dygraph.guard(place):
    fc = fluid.dygraph.static_runner.StaticModelRunner(SAVE_DIRNAME)

    sgd = fluid.optimizer.SGD(learning_rate=0.001,
                            parameter_list=fc.parameters())

    train_loader = fluid.io.DataLoader.from_generator(capacity=5)
    train_loader.set_batch_generator(
        random_batch_reader(), places=place)

    for data in train_loader():
        img = data[0]
        label = data[1]
        label.stop_gradient = True

        cost = fc(inputs=img)

        loss = fluid.layers.cross_entropy(cost, label)
        avg_loss = fluid.layers.mean(loss)

        avg_loss.backward()
        sgd.minimize(avg_loss)""" .

"DESCRIPTION.This Python code defines a test dataloader for the MNIST dataset without shuffling the data." <EXPLAINS> """CODE.def test_dataloader(self):
    dataset = MNIST(root=PATH, train=False, transform=transforms.ToTensor(), download=False)
    loader = torch.utils.data.DataLoader(dataset=dataset, shuffle=False)
    return loader""" .

"DESCRIPTION.This Python code defines an AutoEncoder class using Flax, which consists of an encoder and a decoder. The encoder is a dense neural network layer with output size 3, and the decoder is a dense neural network layer with output size 5. The AutoEncoder class can be called with input data x to perform encoding and then decoding, resulting in x being reconstructed." <EXPLAINS> """CODE.import jax
import jax.numpy as jnp
import flax.linen as nn

class AutoEncoder(nn.Module):
  def setup(self):
    self.encoder = nn.Dense(3)
    self.decoder = nn.Dense(5)

  def __call__(self, x):
    return self.decoder(self.encoder(x))

x = jnp.ones((16, 9))
ae = AutoEncoder()
variables = ae.init(jax.random.PRNGKey(0), x)
model = ae.bind(variables)
z = model.encoder(x)
x_reconstructed = model.decoder(z)
""" .

"DESCRIPTION.This Python code defines an interface type named 'Entity' in GraphQL with a field called 'name' of type string." <EXPLAINS> """CODE.EntityType = GraphQLInterfaceType(
    name='Entity',
    fields={
        'name': GraphQLField(GraphQLString),
    })""" .

"DESCRIPTION.This Python code defines and processes columns for a linear model prediction." <EXPLAINS> """CODE.price = numeric_column('price')
price_buckets = bucketized_column(price, boundaries=[0., 10., 100., 1000.])
keywords = categorical_column_with_hash_bucket("keywords", 10K)
keywords_price = crossed_column('keywords', price_buckets, ...)
columns = [price_buckets, keywords, keywords_price ...]
features = tf.parse_example(..., features=make_parse_example_spec(columns))
prediction = linear_model(features, columns)
""" .

"DESCRIPTION.This Python code defines two functions that test printing output to stdout. The first function tests printing \"foo\" to stdout and asserts that the output is equal to \"foo\". The second function tests printing \"foo\" to stdout and asserts that the output is not equal to \"bar\"." <EXPLAINS> """CODE.@capture_stdout
def test_print_pass():
    print("foo")
    out = sys.stdout.getvalue()
    assert out == "foo"

@capture_stdout
def test_print_fail():
    print("foo")
    out = sys.stdout.getvalue()
    assert out == "bar\"""" .

"DESCRIPTION.This Python code defines two functions: 'is_undefined' checks if a variable is of type 'Undefined' and 'default' returns a default value if the input variable is undefined." <EXPLAINS> """CODE.def is_undefined(var):
    return isinstance(var, Undefined)

def default(var, default=''):
    if is_undefined(var):
        return default
    return var""" .

"DESCRIPTION.This Python code generates a unique layer name \"dense\" by appending a number if there are duplicate names." <EXPLAINS> """CODE._unique_layer_name('dense')
_unique_layer_name('dense')""" .

"DESCRIPTION.This Python code generates time series data and corresponding targets in batches using the TimeseriesGenerator from the Keras library. It creates batches of sequences with a specified length, sampling rate, and batch size." <EXPLAINS> """CODE.from keras.preprocessing.sequence import TimeseriesGenerator
import numpy as np

data = np.array([[i] for i in range(50)])
targets = np.array([[i] for i in range(50)])

data_gen = TimeseriesGenerator(data, targets,
                               length=10, sampling_rate=2,
                               batch_size=2)
assert len(data_gen) == 20

batch_0 = data_gen[0]
x, y = batch_0
assert np.array_equal(x,
                      np.array([[[0], [2], [4], [6], [8]],
                                [[1], [3], [5], [7], [9]]]))
assert np.array_equal(y,
                      np.array([[10], [11]]))
""" .

"DESCRIPTION.This Python code initializes automatic mixed precision training for a neural network model with the specified optimizers and amp level." <EXPLAINS> """CODE.def configure_apex(self, amp, model, optimizers, amp_level):
    model, optimizers = amp.initialize(
        model, optimizers, opt_level=amp_level,
    )

    return model, optimizers""" .

"DESCRIPTION.This Python code is used to access the epoch attribute of the Version class instances created with specific version strings as input." <EXPLAINS> """CODE.Version("2.0.0").epoch
Version("1!2.0.0").epoch""" .

"DESCRIPTION.This Python code is used to encode a given value into a coded value." <EXPLAINS> "CODE.real_value, coded_value = value_encode(VALUE)" .

"DESCRIPTION.This Python code registers a custom pickling mechanism for the set data structure." <EXPLAINS> """CODE.register_pytree_node(
    set,
    lambda s: (sorted(s), None, None),
    lambda children, _: set(children),
)""" .

"DESCRIPTION.This Python code reshapes a 5-dimensional array into a 6-dimensional array by adding an additional dimension of size 1 at the end." <EXPLAINS> """CODE.imagej_shape((2, 3, 4, 5, 3), False)
(2, 3, 4, 5, 3, 1)""" .

"DESCRIPTION.This Python code retrieves the micro version number from the Version object initialized with the specified input." <EXPLAINS> """CODE.Version("1.2.3").micro
3
Version("1").micro""" .

"DESCRIPTION.This Python code sets the configuration for a Streamlit web application, including the page title, icon, layout style, and initial sidebar state." <EXPLAINS> """CODE.st.beta_set_page_config(
    page_title="Ex-stream-ly Cool App",
    page_icon="ð§",
    layout="wide",
    initial_sidebar_state="expanded",
)""" .

"DESCRIPTION.This Python code showcases the functionality of creating and manipulating linear operators. The code demonstrates creating a 2x2 identity matrix, computing its log determinant, applying the operator to tensors, and solving linear equations. It also shows how the operator handles batching and broadcasting with tensors of different shapes." <EXPLAINS> """CODE.# Create a 2 x 2 identity matrix.
operator = LinearOperatorIdentity(num_rows=2, dtype=tf.float32)

operator.to_dense()
==> [[1., 0.]
     [0., 1.]]

operator.shape
==> [2, 2]

operator.log_determinant()
==> 0.

x = ... Shape [2, 4] Tensor
operator.apply(x)
==> Shape [2, 4] Tensor, same as x.

y = tf.random_normal(shape=[3, 2, 4])
# Note that y.shape is compatible with operator.shape because operator.shape
# is broadcast to [3, 2, 2].
# This broadcast does NOT require copying data, since we can infer that y
# will be passed through without changing shape.  We are always able to infer
# this if the operator has no batch_shape.
x = operator.solve(y)
==> Shape [3, 2, 4] Tensor, same as y.

# Create a 2-batch of 2x2 identity matrices
operator = LinearOperatorIdentity(num_rows=2, batch_shape=[2])
operator.to_dense()
==> [[[1., 0.]
      [0., 1.]],
     [[1., 0.]
      [0., 1.]]]

# Here, even though the operator has a batch shape, the input is the same as
# the output, so x can be passed through without a copy.  The operator is able
# to detect that no broadcast is necessary because both x and the operator
# have statically defined shape.
x = ... Shape [2, 2, 3]
operator.apply(x)
==> Shape [2, 2, 3] Tensor, same as x

# Here the operator and x have different batch_shape, and are broadcast.
# This requires a copy, since the output is different size than the input.
x = ... Shape [1, 2, 3]
operator.apply(x)
==> Shape [2, 2, 3] Tensor, equal to [x, x]
""" .

"DESCRIPTION.This Python code takes a string input representing metadata information for ImageJ software and returns a dictionary with the metadata key-value pairs." <EXPLAINS> """CODE.description = 'ImageJ=1.11a\\nimages=510\\nhyperstack=true\\n'
imagej_description_metadata(description)
{'ImageJ': '1.11a', 'images': 510, 'hyperstack': True}""" .

"DESCRIPTION.This class method constructs an instance of the class if the input string matches the class name, otherwise it raises a TypeError with a specific message." <EXPLAINS> """CODE.@classmethod
def construct_from_string(cls, string):
    if string == cls.name:
        return cls()
    else:
        raise TypeError("Cannot construct a '{}' from "
                        "'{}'".format(cls, string))""" .

"DESCRIPTION.This code acquires a lock with a timeout of 20 seconds." <EXPLAINS> """CODE... code-block:: python
    with lock.acquire_(timeout = 20):
        pass""" .

"DESCRIPTION.This code adds a comment to a specific discussion thread in a specified repository with formatted text content." <EXPLAINS> """CODE.comment = \"\"\"
Hello @otheruser!

# This is a title

**This is bold**, *this is italic* and ~this is strikethrough~
And [this](http://url) is a link
\"\"\"

HfApi().comment_discussion(
    repo_id="username/repo_name",
    discussion_num=34,
    comment=comment
)
""" .

"DESCRIPTION.This code adds a shared dense node named 'shared_dense' to the model, which takes the outputs of nodes 'node_a' and 'node_b' as inputs, and produces 'dense_output_a' and 'dense_output_b' as its outputs." <EXPLAINS> """CODE.model.add_shared_node(my_dense, name='shared_dense', inputs=['node_a', 'node_b'], ...)


model.add_shared_node(my_dense, name='shared_dense', inputs=['node_a', 'node_b'],
                      outputs=['dense_output_a', 'dense_outputs_b'])
""" .

"DESCRIPTION.This code adjusts the pitch of the audio signal \"y\" by a specific number of steps or tritones." <EXPLAINS> """CODE.y_third = librosa.effects.pitch_shift(y, sr, n_steps=4)
y_tritone = librosa.effects.pitch_shift(y, sr, n_steps=-6)
y_three_qt = librosa.effects.pitch_shift(y, sr, n_steps=3, bins_per_octave=24)
""" .

"DESCRIPTION.This code aggregates the data in grouped_ds by initializing an empty list for each key, accumulating values into the list, merging the lists for each key, and finalizing the aggregation by returning the list of values." <EXPLAINS> """CODE.grouped_ds.aggregate(AggregateFn(
...     init=lambda k: [],
...     accumulate=lambda a, r: a + [r],
...     merge=lambda a1, a2: a1 + a2,
...     finalize=lambda a: a
... ))""" .

"DESCRIPTION.This code allows the user to interact with an inference endpoint named \"my-text-to-image\". The code provides functionality to check the status of the endpoint, retrieve its URL, run inference using text-to-image model, pause the endpoint to save money, resume and wait for deployment, and continue running inference with the text-to-image model." <EXPLAINS> """CODE.from huggingface_hub import get_inference_endpoint
endpoint = get_inference_endpoint("my-text-to-image")
endpoint
InferenceEndpoint(name='my-text-to-image', ...)

# Get status
endpoint.status
'running'
endpoint.url
'https://my-text-to-image.region.vendor.endpoints.huggingface.cloud'

# Run inference
endpoint.client.text_to_image(...)

# Pause endpoint to save $$$
endpoint.pause()

# ...
# Resume and wait for deployment
endpoint.resume()
endpoint.wait()
endpoint.client.text_to_image(...)
""" .

"DESCRIPTION.This code allows the user to select a contact method (either Email, Home phone, or Mobile phone) using a select box and then displays the selected option." <EXPLAINS> """CODE.option = st.selectbox(
    'How would you like to be contacted?',
    ('Email', 'Home phone', 'Mobile phone'))

st.write('You selected:', option)""" .

"DESCRIPTION.This code allows the user to select how they would like to be contacted (via email, home phone, or mobile phone) using a selectbox input. The selected option is then displayed to the user." <EXPLAINS> """CODE.option = st.selectbox(
    'How would you like to be contacted?',
    ('Email', 'Home phone', 'Mobile phone'))
st.write('You selected:', option)""" .

"DESCRIPTION.This code allows the user to select their favorite colors from a given list and displays the selected colors." <EXPLAINS> """CODE.options = st.multiselect(
    'What are your favorite colors',
    ['Green', 'Yellow', 'Red', 'Blue'],
    ['Yellow', 'Red'])

st.write('You selected:', options)""" .

"DESCRIPTION.This code allows the user to upload a CSV file and then displays the contents of the uploaded file on the screen." <EXPLAINS> """CODE.uploaded_file = st.file_uploader("Choose a CSV file", type="csv")
if uploaded_file is not None:
...     data = pd.read_csv(uploaded_file)
...     st.write(data)""" .

"DESCRIPTION.This code allows the user to upload a CSV file. If a file is uploaded, the code reads the CSV data and displays it." <EXPLAINS> """CODE.uploaded_file = st.file_uploader("Choose a CSV file", type="csv")
if uploaded_file is not None:
...     data = pd.read_csv(uploaded_file)
...     st.write(data)""" .

"DESCRIPTION.This code appends the value 3 to the end of a deque ([1, 2])." <EXPLAINS> "CODE.pdeque([1, 2]).append(3)" .

"DESCRIPTION.This code applies gradient descent to update the parameters of a model using the samples collected by ev1." <EXPLAINS> """CODE.samples = ev1.sample()
grads, info = ev2.compute_gradients(samples)
ev1.apply_gradients(grads)""" .

"DESCRIPTION.This code applies random zoom transformation to the input image with specified zoom range and outputs the zoomed image." <EXPLAINS> """CODE.input_img = np.random.random((32, 224, 224, 3))
layer = tf.keras.layers.experimental.preprocessing.RandomZoom(.5, .2)
out_img = layer(input_img)
out_img.shape
""" .

"DESCRIPTION.This code asserts that the value of SpaceStage.BUILDING is equal to the string \"BUILDING\"." <EXPLAINS> """CODE.assert SpaceStage.BUILDING == "BUILDING"
""" .

"DESCRIPTION.This code asserts that the value of the attribute `SMALL` in the class `SpaceStorage` is equal to the string \"small\"." <EXPLAINS> """CODE.assert SpaceStorage.SMALL == "small"
""" .

"DESCRIPTION.This code assigns the return values of the lazy_loader.attach function to the variables __getattr__, __dir__, and __all. The lazy_loader.attach function is used to lazily load the modules \"sub1\" and \"sub2\" from the current module." <EXPLAINS> "CODE.__getattr__, __dir__, __all__ = lazy_loader.attach(__name__, [\"sub1\", \"sub2\"])" .

"DESCRIPTION.This code asynchronously renders a template with the variable 'knights' set to the string 'that say nih; asynchronously'." <EXPLAINS> "CODE.await template.render_async(knights='that say nih; asynchronously')" .

"DESCRIPTION.This code block demonstrates two ways to acquire and release a lock in Python: one using a context manager with the `with` statement, and the other using a try-finally construct." <EXPLAINS> """CODE.# You can use this method in the context manager (recommended)
with lock.acquire():
    pass

# Or use an equivalent try-finally construct:
lock.acquire()
try:
    pass
finally:
    lock.release()
""" .

"DESCRIPTION.This code block uses JAX's profiler to trace the execution of the operation jnp.dot(x, x.T) and then blocks until the operation is complete." <EXPLAINS> """CODE.with jax.profiler.TraceContext("acontext"):
    jnp.dot(x, x.T).block_until_ready()""" .

"DESCRIPTION.This code builds documentation using Sphinx with 4 jobs for parallel processing." <EXPLAINS> "CODE.DocBuilder(num_jobs=4)._sphinx_build('html')" .

"DESCRIPTION.This code calculates a confusion matrix by comparing predicted values with target values. The confusion matrix shows the number of correct predictions and misclassifications for each class." <EXPLAINS> """CODE.pred = torch.tensor([0, 1, 2, 2])
target = torch.tensor([0, 1, 2, 2])
metric = ConfusionMatrix()
metric(pred, target)
tensor([[1., 0., 0.],
        [0., 1., 0.],
        [0., 0., 2.]])""" .

"DESCRIPTION.This code calculates a learning rate using an exponential decay function based on the global step, and then uses gradient descent optimization to minimize a loss function with the calculated learning rate and updates the global step." <EXPLAINS> """CODE.global_step = tf.Variable(0, trainable=False)
starter_learning_rate = 0.1
learning_rate = tf.compat.v1.train.exponential_decay(starter_learning_rate, global_step, 100000, 0.96, staircase=True)
learning_step = (tf.compat.v1.train.GradientDescentOptimizer(learning_rate).minimize(...my loss..., global_step=global_step))
""" .

"DESCRIPTION.This code calculates a linear-cosine decay of a learning rate based on the global step, decay steps, alpha, and beta parameters." <EXPLAINS> """CODE.global_step = min(global_step, decay_steps)
linear_decay = (decay_steps - global_step) / decay_steps)
cosine_decay = 0.5 * (
    1 + cos(pi * 2 * num_periods * global_step / decay_steps))
decayed = (alpha + linear_decay) * cosine_decay + beta
decayed_learning_rate = learning_rate * decayed


decay_steps = 1000
lr_decayed = linear_cosine_decay(learning_rate, global_step, decay_steps)
""" .

"DESCRIPTION.This code calculates a moving average using a momentum update formula, then calculates a new value based on the moving average." <EXPLAINS> """CODE.x = tf.Variable(0.0)
momentum=0.9
x = x * momentum + value * (1 - momentum)
moving_average_update(x, value = 2.0, momentum=momentum).numpy()
x.numpy()
num_updates = 1.0
x_zdb = x/(1 - momentum**num_updates)
x_zdb.numpy()
""" .

"DESCRIPTION.This code calculates and displays the Receiver Operating Characteristic (ROC) curve for a binary classification model by comparing the true labels (y) with the predicted probabilities (pred). It then calculates the Area Under the Curve (AUC) for the ROC curve and displays the ROC curve with the AUC value using matplotlib." <EXPLAINS> """CODE.import matplotlib.pyplot as plt
import numpy as np
from sklearn import metrics
y = np.array([0, 0, 1, 1])
pred = np.array([0.1, 0.4, 0.35, 0.8])
fpr, tpr, thresholds = metrics.roc_curve(y, pred)
roc_auc = metrics.auc(fpr, tpr)
display = metrics.RocCurveDisplay(fpr=fpr, tpr=tpr, roc_auc=roc_auc, estimator_name='example estimator')
display.plot()
plt.show()""" .

"DESCRIPTION.This code calculates and returns the variance threshold of the input matrix X." <EXPLAINS> """CODE.X = [[0, 2, 0, 3], [0, 1, 4, 3], [0, 1, 1, 3]]
selector = VarianceThreshold()
selector.fit_transform(X)
""" .

"DESCRIPTION.This code calculates precision, recall, and thresholds based on predicted values and target values." <EXPLAINS> """CODE.pred = torch.tensor([0, 1, 2, 3])
target = torch.tensor([0, 1, 2, 2])
metric = PrecisionRecall()
prec, recall, thr = metric(pred, target)
prec
tensor([0.3333, 0.0000, 0.0000, 1.0000])
recall
tensor([1., 0., 0., 0.])
thr
tensor([1., 2., 3.])""" .

"DESCRIPTION.This code calculates the Area Under the Curve (AUC) score for a binary classification model using the true labels and predicted scores." <EXPLAINS> """CODE.import numpy as np
from sklearn.metrics import auc_score
y_true = np.array([0, 0, 1, 1])
y_scores = np.array([0.1, 0.4, 0.35, 0.8])
auc_score(y_true, y_scores)
0.75""" .

"DESCRIPTION.This code calculates the Area Under the Receiver Operating Characteristic Curve (AUROC) metric for the prediction values 'pred' compared to the target values 'target'. The AUROC metric value is then returned." <EXPLAINS> """CODE.pred = torch.tensor([0, 1, 2, 3])
target = torch.tensor([0, 1, 2, 2])
metric = AUROC()
metric(pred, target)
tensor(0.3333)""" .

"DESCRIPTION.This code calculates the Bessel function of the second kind of order 1 for the input values 0.5, 1, 2, and 4 using TensorFlow library." <EXPLAINS> "CODE.tf.math.special.bessel_y1([0.5, 1., 2., 4.]).numpy()" .

"DESCRIPTION.This code calculates the Dice coefficient metric between the predicted tensor \"pred\" and the target tensor \"target\" for a multi-class classification problem." <EXPLAINS> """CODE.pred = torch.tensor([[0.85, 0.05, 0.05, 0.05],
...                      [0.05, 0.85, 0.05, 0.05],
...                      [0.05, 0.05, 0.85, 0.05],
...                      [0.05, 0.05, 0.05, 0.85]])
target = torch.tensor([0, 1, 3, 2])
metric = DiceCoefficient()
metric(pred, target)
tensor(0.3333)""" .

"DESCRIPTION.This code calculates the Dice coefficient metric between the predicted tensor and the target tensor." <EXPLAINS> """CODE.pred = torch.tensor([[0.85, 0.05, 0.05, 0.05],
...                      [0.05, 0.85, 0.05, 0.05],
...                      [0.05, 0.05, 0.85, 0.05],
...                      [0.05, 0.05, 0.05, 0.85]])
target = torch.tensor([0, 1, 3, 2])
metric = DiceCoefficient()
metric(pred, target)
tensor(0.3333)""" .

"DESCRIPTION.This code calculates the Euclidean distances between the rows of a matrix X, including handling of NaN values." <EXPLAINS> """CODE.from sklearn.metrics.pairwise import nan_euclidean_distances
nan = float("NaN")
X = [[0, 1], [1, nan]]
nan_euclidean_distances(X, X) # distance between rows of X
nan_euclidean_distances(X, [[0, 0]]) # get distance to origin
""" .

"DESCRIPTION.This code calculates the Hamming loss between two sets of labels." <EXPLAINS> """CODE.from sklearn.metrics import hamming_loss
y_pred = [1, 2, 3, 4]
y_true = [2, 2, 3, 4]
hamming_loss(y_true, y_pred)

hamming_loss(np.array([[0.0, 1.0], [1.0, 1.0]]), np.zeros((2, 2)))

hamming_loss([(1, 2), (3, )], [(1, 2), tuple()])
""" .

"DESCRIPTION.This code calculates the Hinge loss between the true labels (y_true) and the predicted labels (y_pred) using TensorFlow (tf.keras) library, with different settings for reduction method. The Hinge loss value is returned as a numpy array or a single value depending on the reduction method specified." <EXPLAINS> """CODE.y_true = [[0., 1.], [0., 0.]]
y_pred = [[0.6, 0.4], [0.4, 0.6]]
h = tf.keras.losses.Hinge()
h(y_true, y_pred).numpy()
1.3

h(y_true, y_pred, sample_weight=[1, 0]).numpy()
0.55

h = tf.keras.losses.Hinge(
...     reduction=tf.keras.losses.Reduction.SUM)
h(y_true, y_pred).numpy()
2.6

h = tf.keras.losses.Hinge(
...     reduction=tf.keras.losses.Reduction.NONE)
h(y_true, y_pred).numpy()
array([1.1, 1.5], dtype=float32)
""" .

"DESCRIPTION.This code calculates the Huber loss function, which penalizes large errors more than Mean Squared Error loss function. It uses a piecewise function where the loss is calculated differently based on the size of the error (x) compared to a threshold value (d)." <EXPLAINS> """CODE.loss = 0.5 * x^2                  if |x| <= d
loss = 0.5 * d^2 + d * (|x| - d)  if |x| > d


model.compile(optimizer='sgd', loss=tf.keras.losses.Huber())
""" .

"DESCRIPTION.This code calculates the Mean Absolute Error (MAE) between the predicted values (pred) and the target values (target)." <EXPLAINS> """CODE.pred = torch.tensor([0., 1, 2, 3])
target = torch.tensor([0., 1, 2, 2])
metric = MAE()
metric(pred, target)
tensor(0.2500)""" .

"DESCRIPTION.This code calculates the Mean Absolute Error (MAE) between the true values (y_true) and the predicted values (y_pred) using different settings for reduction, such as SUM or NONE. Additionally, the code compiles a model using Stochastic Gradient Descent (SGD) optimizer and the MAE loss function." <EXPLAINS> """CODE.loss = abs(y_true - y_pred)
mae = tf.keras.losses.MeanAbsoluteError()
mae(y_true, y_pred).numpy()
mae(y_true, y_pred, sample_weight=[0.7, 0.3]).numpy()
mae = tf.keras.losses.MeanAbsoluteError(reduction=tf.keras.losses.Reduction.SUM)
mae(y_true, y_pred).numpy()
mae = tf.keras.losses.MeanAbsoluteError(reduction=tf.keras.losses.Reduction.NONE)
mae(y_true, y_pred).numpy()
model.compile(optimizer='sgd', loss=tf.keras.losses.MeanAbsoluteError())
""" .

"DESCRIPTION.This code calculates the Mean Absolute Error (MAE) between the true values (y_true) and the predicted values (y_pred) using the TensorFlow keras library. It also shows different ways to customize the calculation of MAE by setting parameters such as sample weights, reduction method, and the optimizer for model compilation." <EXPLAINS> """CODE.loss = abs(y_true - y_pred)
mae = tf.keras.losses.MeanAbsoluteError()
mae(y_true, y_pred).numpy()
mae(y_true, y_pred, sample_weight=[0.7, 0.3]).numpy()
mae = tf.keras.losses.MeanAbsoluteError(reduction=tf.keras.losses.Reduction.SUM)
mae(y_true, y_pred).numpy()
mae = tf.keras.losses.MeanAbsoluteError(reduction=tf.keras.losses.Reduction.NONE)
mae(y_true, y_pred).numpy()
model.compile(optimizer='sgd', loss=tf.keras.losses.MeanAbsoluteError())""" .

"DESCRIPTION.This code calculates the Mean Absolute Error (MAE) loss between two sets of values and compiles a model using stochastic gradient descent (sgd) optimizer with MAE loss." <EXPLAINS> """CODE.mae = keras.losses.MeanAbsoluteError()
loss = mae([0., 0., 1., 1.], [1., 1., 1., 0.])
model = keras.Model(inputs, outputs)
model.compile('sgd', loss=keras.losses.MeanAbsoluteError())
""" .

"DESCRIPTION.This code calculates the Mean Tweedie Deviance metric for the predicted values (y_pred) compared to the true values (y_true)." <EXPLAINS> """CODE.y_pred = torch.tensor([2, 0.5, 1, 4])
y_true = torch.tensor([0.5, 0.5, 2., 2.])
metric = MeanTweedieDeviance()
metric(y_pred, y_true)
tensor([1.8125])""" .

"DESCRIPTION.This code calculates the Mel-frequency cepstral coefficients (MFCCs) of a given audio signal." <EXPLAINS> """CODE.S           = librosa.melspectrogram(y, sr)
dct_filters = librosa.filters.dct(13, S.shape[0])
mfcc        = dct_filters.dot(librosa.logamplitude(S))
""" .

"DESCRIPTION.This code calculates the Peak Signal-to-Noise Ratio (PSNR) between two sets of tensors, 'pred' and 'target'." <EXPLAINS> """CODE.pred = torch.tensor([[0.0, 1.0], [2.0, 3.0]])
target = torch.tensor([[3.0, 2.0], [1.0, 0.0]])
metric = PSNR()
metric(pred, target)
tensor(2.5527)""" .

"DESCRIPTION.This code calculates the Peak Signal-to-Noise Ratio (PSNR) metric between two tensors, 'pred' and 'target'." <EXPLAINS> """CODE.pred = torch.tensor([[0.0, 1.0], [2.0, 3.0]])
target = torch.tensor([[3.0, 2.0], [1.0, 0.0]])
metric = PSNR()
metric(pred, target)
tensor(2.5527)""" .

"DESCRIPTION.This code calculates the Poisson loss between the predicted output (y_pred) and the true output (y_true) and compiles a keras model using stochastic gradient descent (sgd) optimizer with Poisson loss function." <EXPLAINS> """CODE.loss = y_pred - y_true * log(y_pred)
model = keras.Model(inputs, outputs)
model.compile('sgd', loss=keras.losses.Poisson())
""" .

"DESCRIPTION.This code calculates the Pythagorean intervals based on the number of bins per octave specified, with an option to sort the intervals and return the factors of the intervals." <EXPLAINS> """CODE.librosa.pythagorean_intervals(bins_per_octave=12)
librosa.pythagorean_intervals(bins_per_octave=7, sort=False)
librosa.pythagorean_intervals(bins_per_octave=7, sort=False, return_factors=True)""" .

"DESCRIPTION.This code calculates the Receiver Operating Characteristic (ROC) curve by comparing two sets of data (x and y) and returns the false positive rates (fpr), true positive rates (tpr), and thresholds." <EXPLAINS> """CODE.x = torch.tensor([0, 1, 2, 3])
y = torch.tensor([0, 1, 1, 1])
fpr, tpr, thresholds = __roc(x, y)
fpr
tensor([0., 0., 0., 0., 1.])
tpr
tensor([0.0000, 0.3333, 0.6667, 1.0000, 1.0000])
thresholds
tensor([4, 3, 2, 1, 0])
""" .

"DESCRIPTION.This code calculates the Receiver Operating Characteristic (ROC) curve using the input tensors x and y, and returns the false positive rate (fpr), true positive rate (tpr), and thresholds." <EXPLAINS> """CODE.x = torch.tensor([0, 1, 2, 3])
y = torch.tensor([0, 1, 1, 1])
fpr, tpr, thresholds = _roc(x, y)
fpr
tensor([0., 0., 0., 0., 1.])
tpr
tensor([0.0000, 0.3333, 0.6667, 1.0000, 1.0000])
thresholds
tensor([4, 3, 2, 1, 0])""" .

"DESCRIPTION.This code calculates the Root Mean Squared Logarithmic Error (RMSLE) between two sets of values." <EXPLAINS> """CODE.def rmsle(pred, target, reduction='elementwise_mean'):
    return torch.sqrt(torch.mean((torch.log1p(pred) - torch.log1p(target))**2))

x = torch.tensor([0., 1, 2, 3])
y = torch.tensor([0., 1, 2, 2])
rmsle(x, y)
""" .

"DESCRIPTION.This code calculates the accuracy of the model predictions by comparing the true labels with the predicted labels, and then compiles the model using stochastic gradient descent optimizer, mean squared error loss, and sparse categorical accuracy as a metric." <EXPLAINS> """CODE.acc = np.dot(sample_weight, np.equal(y_true, np.argmax(y_pred, axis=1))
model.compile(
    optimizer='sgd',
    loss='mse',
    metrics=[tf.keras.metrics.SparseCategoricalAccuracy()])
""" .

"DESCRIPTION.This code calculates the accuracy of the predicted values compared to the true values, and uses this accuracy metric as a performance metric for a Keras model during training." <EXPLAINS> """CODE.def accuracy(y_true, y_pred):
  return tf.cast(tf.math.equal(y_true, y_pred), tf.float32)

accuracy_metric = tf.keras.metrics.MeanMetricWrapper(fn=accuracy)

keras_model.compile(..., metrics=accuracy_metric)
""" .

"DESCRIPTION.This code calculates the accuracy of the predicted values compared to the true values." <EXPLAINS> """CODE.from sklearn.metrics import accuracy_score
y_pred = [0, 2, 1, 3]
y_true = [0, 1, 2, 3]
accuracy_score(y_true, y_pred)
0.5""" .

"DESCRIPTION.This code calculates the average precision metric for the predicted values compared to the target values." <EXPLAINS> """CODE.pred = torch.tensor([0, 1, 2, 3])
target = torch.tensor([0, 1, 2, 2])
metric = AveragePrecision()
metric(pred, target)
tensor(0.3333)""" .

"DESCRIPTION.This code calculates the average precision of the predicted values compared to the target values." <EXPLAINS> """CODE.pred = torch.tensor([0, 1, 2, 3])
target = torch.tensor([0, 1, 2, 2])
metric = AveragePrecision()
metric(pred, target)
tensor(0.3333)""" .

"DESCRIPTION.This code calculates the batch dot product between two batches of tensors `x_batch` and `y_batch` with specified axes, and returns the shape of the resulting tensor." <EXPLAINS> """CODE.    inner_products = []
    for xi, yi in zip(x, y):
        inner_products.append(xi.dot(yi))
    result = stack(inner_products)

    x_batch = K.ones(shape=(32, 20, 1))
    y_batch = K.ones(shape=(32, 30, 20))
    xy_batch_dot = K.batch_dot(x_batch, y_batch, axes=(1, 2))
    K.int_shape(xy_batch_dot)
    (32, 1, 30)
""" .

"DESCRIPTION.This code calculates the categorical accuracy between the true and predicted values of two arrays." <EXPLAINS> """CODE.y_true = [[0, 0, 1], [0, 1, 0]]
y_pred = [[0.1, 0.9, 0.8], [0.05, 0.95, 0]]
m = tf.keras.metrics.categorical_accuracy(y_true, y_pred)
assert m.shape == (2,)
m.numpy()
array([0., 1.], dtype=float32)""" .

"DESCRIPTION.This code calculates the categorical cross-entropy loss between the true labels and the predicted labels, and compiles a model using stochastic gradient descent optimizer with the categorical cross-entropy loss function." <EXPLAINS> """CODE.cce = keras.losses.CategoricalCrossentropy()
loss = cce(
    [[1., 0., 0.], [0., 1., 0.], [0., 0., 1.]],
    [[.9, .05, .05], [.5, .89, .6], [.05, .01, .94]])
model = keras.Model(inputs, outputs)
model.compile('sgd', loss=keras.losses.CategoricalCrossentropy())
""" .

"DESCRIPTION.This code calculates the categorical crossentropy loss between two arrays 'a' and 'b' using the TensorFlow library. The first calculation compares the two arrays 'a' and 'b' resulting in a loss array of [0.10536, 0.11653, 0.06188]. The second calculation compares array 'a' with itself, resulting in a loss array of [0, 0, 0]." <EXPLAINS> """CODE.a = tf.constant([1., 0., 0., 0., 1., 0., 0., 0., 1.], shape=[3,3])
print(a)
tf.Tensor(
  [[1. 0. 0.]
   [0. 1. 0.]
   [0. 0. 1.]], shape=(3, 3), dtype=float32)
b = tf.constant([.9, .05, .05, .05, .89, .06, .05, .01, .94], shape=[3,3])
print(b)
tf.Tensor(
  [[0.9  0.05 0.05]
   [0.05 0.89 0.06]
   [0.05 0.01 0.94]], shape=(3, 3), dtype=float32)
loss = tf.keras.backend.categorical_crossentropy(a, b)
print(np.around(loss, 5))
[0.10536 0.11653 0.06188]
loss = tf.keras.backend.categorical_crossentropy(a, a)
print(np.around(loss, 5))
[0. 0. 0.]""" .

"DESCRIPTION.This code calculates the categorical hinge loss between true and predicted values for a classification task with 3 classes. It then computes the positive and negative terms of the loss and verifies that the loss is computed correctly." <EXPLAINS> """CODE.y_true = np.random.randint(0, 3, size=(2,))
y_true = tf.keras.utils.to_categorical(y_true, num_classes=3)
y_pred = np.random.random(size=(2, 3))
loss = tf.keras.losses.categorical_hinge(y_true, y_pred)
assert loss.shape == (2,)
pos = np.sum(y_true * y_pred, axis=-1)
neg = np.amax((1. - y_true) * y_pred, axis=-1)
assert np.array_equal(loss.numpy(), np.maximum(0., neg - pos + 1.))""" .

"DESCRIPTION.This code calculates the chromagram representation of the input audio signal." <EXPLAINS> """CODE.CQT = librosa.cqt(y, sr)
chroma_map = librosa.filters.cq_to_chroma(CQT.shape[0])
chromagram = chroma_map.dot(CQT)
""" .

"DESCRIPTION.This code calculates the confusion matrix between the predicted values (pred) and the target values (target)." <EXPLAINS> """CODE.pred = torch.tensor([0, 1, 2, 2])
target = torch.tensor([0, 1, 2, 2])
metric = ConfusionMatrix()
metric(pred, target)
tensor([[1., 0., 0.],
        [0., 1., 0.],
        [0., 0., 2.])""",
        """CODE.pred = torch.tensor([0, 1, 2, 2])
target = torch.tensor([0, 1, 2, 2])
metric = ConfusionMatrix()
metric(pred, target)
tensor([[1., 0., 0.],
        [0., 1., 0.],
        [0., 0., 2.]])""" .

"DESCRIPTION.This code calculates the confusion matrix for the predicted values `pred` and the target values `target`. The confusion matrix is a 2D tensor where the rows represent the actual classes and the columns represent the predicted classes, showing the count of true positives in each cell." <EXPLAINS> """CODE.pred = torch.tensor([0, 1, 2, 2])
target = torch.tensor([0, 1, 2, 2])
metric = ConfusionMatrix()
metric(pred, target)
tensor([[1., 0., 0.],
        [0., 1., 0.],
        [0., 0., 2.]])""" .

"DESCRIPTION.This code calculates the distance between two input matrices x and y using the specified norm (0, 2, infinity, or negative infinity) and prints the resulting distance values." <EXPLAINS> """CODE.import paddle
import paddle.fluid as fluid
import numpy as np

with fluid.dygraph.guard():
    x = fluid.dygraph.to_variable(np.array([[3, 3],[3, 3]]).astype(np.float32))
    y = fluid.dygraph.to_variable(np.array([[3, 3],[3, 1]]).astype(np.float32))
    out = fluid.layers.dist(x, y, 0)
    print(out.numpy()) # out = [1.]

    out = fluid.layers.dist(x, y, 2)
    print(out.numpy()) # out = [2.]

    out = fluid.layers.dist(x, y, float("inf"))
    print(out.numpy()) # out = [2.]

    out = fluid.layers.dist(x, y, float("-inf"))
    print(out.numpy()) # out = [0.]""" .

"DESCRIPTION.This code calculates the dot product between two batches of tensors, where the first batch has a shape of (32, 20, 1) and the second batch has a shape of (32, 30, 20). The result is a tensor with a shape of (32, 1, 30)." <EXPLAINS> """CODE.    x_batch = K.ones(shape=(32, 20, 1))
    y_batch = K.ones(shape=(32, 30, 20))
    xy_batch_dot = K.batch_dot(x_batch, y_batch, axes=[1, 2])
    K.int_shape(xy_batch_dot)
    (32, 1, 30)
""" .

"DESCRIPTION.This code calculates the dot product of two randomly generated arrays of floats and prints the result." <EXPLAINS> """CODE.import paddle
import paddle.fluid as fluid
import numpy as np

with fluid.dygraph.guard():
  x = fluid.dygraph.to_variable(np.random.uniform(0.1, 1, [10]).astype(np.float32))
  y = fluid.dygraph.to_variable(np.random.uniform(1, 3, [10]).astype(np.float32))
  z = fluid.layers.dot(x, y)
  print(z.numpy())""" .

"DESCRIPTION.This code calculates the element-wise minimum of two matrices x1 and x2, both reshaped from numpy arrays, and returns the shape of the resulting matrix." <EXPLAINS> """CODE.x1 = tf.keras.layers.Dense(8)(np.arange(10).reshape(5, 2)
x2 = tf.keras.layers.Dense(8)(np.arange(10, 20).reshape(5, 2)
minned = tf.keras.layers.Minimum()([x1, x2)
minned.shape""" .

"DESCRIPTION.This code calculates the exponentially scaled Bessel function of the second kind k0 for the input values 0.5, 1., 2., and 4." <EXPLAINS> """CODE.tf.math.special.bessel_k0e([0.5, 1., 2., 4.]).numpy()
array([1.52410939, 1.14446308, 0.84156822, 0.60929767], dtype=float32)""" .

"DESCRIPTION.This code calculates the false positive rate (_fpr), true positive rate (_tpr), and thresholds using the input tensors _x and _y through the roc function." <EXPLAINS> """CODE._x = torch.tensor([0, 1, 2, 3])
_y = torch.tensor([0, 1, 1, 1])
_fpr, _tpr, _thresholds = _roc(_x, _y)""" .

"DESCRIPTION.This code calculates the frame numbers corresponding to every 256 samples in an audio signal with a sample rate of 22050Hz." <EXPLAINS> """CODE.import librosa
import numpy as np

# Get the frame numbers for every 256 samples
librosa.samples_to_frames(np.arange(0, 22050, 256))
""" .

"DESCRIPTION.This code calculates the frequencies corresponding to 24 constant-Q bins of a CQT (Constant-Q Transform) starting from the pitch C2 in MIDI scale." <EXPLAINS> "CODE.librosa.cqt_frequencies(24, fmin=librosa.midi_to_hz(librosa.note_to_midi('C2')))" .

"DESCRIPTION.This code calculates the frequencies of the Fast Fourier Transform (FFT) bins for a given sampling rate and number of FFT points." <EXPLAINS> """CODE.librosa.fft_frequencies(sr=22050, n_fft=16)
array([     0.   ,   1378.125,   2756.25 ,   4134.375,   5512.5  ,
         6890.625,   8268.75 ,   9646.875,  11025.   ])""" .

"DESCRIPTION.This code calculates the frequency of musical notes." <EXPLAINS> """CODE.# Get the frequency of a note
librosa.note_to_hz('C')
array([ 16.352])
# Or multiple notes
librosa.note_to_hz(['A3', 'A4', 'A5'])
array([ 220.,  440.,  880.])
# Or notes with tuning deviations
librosa.note_to_hz('C2-32', round_midi=False)
array([ 64.209])
""" .

"DESCRIPTION.This code calculates the hash value of a file located at '/path/to/file.zip' using the SHA-256 algorithm." <EXPLAINS> """CODE._hash_file('/path/to/file.zip')
'e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855'
""",
        "CODE.hash_file('/path/to/file.zip')" .

"DESCRIPTION.This code calculates the hash value of the file located at \"/path/to/file.zip\"." <EXPLAINS> """CODE.from keras.data_utils import _hash_file
_hash_file('/path/to/file.zip')
""" .

"DESCRIPTION.This code calculates the hash value of the specified file and returns it." <EXPLAINS> """CODE.from keras.data_utils import _hash_file
_hash_file('/path/to/file.zip')
""" .

"DESCRIPTION.This code calculates the hinge loss between the true labels and predicted labels for a binary classification task." <EXPLAINS> """CODE.y_true = np.random.choice([-1, 1], size=(2, 3))
y_pred = np.random.random(size=(2, 3))
loss = tf.keras.losses.hinge(y_true, y_pred)
assert loss.shape == (2,)
assert np.array_equal(
...     loss.numpy(),
...     np.mean(np.maximum(1. - y_true * y_pred, 0.), axis=-1))""" .

"DESCRIPTION.This code calculates the label ranking average precision score between the true labels (y_true) and the predicted labels (y_score)." <EXPLAINS> """CODE.import numpy as np
from sklearn.metrics import label_ranking_average_precision_score
y_true = np.array([[1, 0, 0], [0, 0, 1]])
y_score = np.array([[0.75, 0.5, 1], [1, 0.2, 0.1]])
label_ranking_average_precision_score(y_true, y_score)         # doctest: +ELLIPSIS""" .

"DESCRIPTION.This code calculates the layer normalization of the input data along axis 1, which involves normalizing the input data by subtracting the mean and dividing by the standard deviation, then scaling and shifting the normalized data using gamma and beta parameters." <EXPLAINS> """CODE.data = tf.constant(np.arange(10).reshape(5, 2) * 10, dtype=tf.float32)
print(data)


layer = tf.keras.layers.LayerNormalization(axis=1)
output = layer(data)
print(output)


mean_i = sum(x_i[j] for j in range(k)) / k
var_i = sum((x_i[j] - mean_i) ** 2 for j in range(k)) / k


x_i_normalized = (x_i - mean_i) / sqrt(var_i + epsilon)


output_i = x_i_normalized * gamma + beta
""" .

"DESCRIPTION.This code calculates the log sum of exponentials of the corresponding elements in the input and other tensors based on the provided masks." <EXPLAINS> """CODE.input = torch.tensor([-100.0, -200, -300])
input
tensor([-100., -200., -300.])
other = torch.tensor([-1.0, -2, -3])
other
tensor([-1., -2., -3.])
mask = torch.tensor([True, False, True])
mask
tensor([ True, False,  True])
torch.masked._ops.logaddexp(input, other, input_mask=mask, other_mask=mask)
tensor([-1., -inf, -3.])
""" .

"DESCRIPTION.This code calculates the loss function using the Poisson loss for a keras model with stochastic gradient descent (sgd) optimizer." <EXPLAINS> """CODE.loss = y_pred - y_true * log(y_pred)
model = keras.Model(inputs, outputs)
model.compile('sgd', loss=keras.losses.Poisson())
""" .

"DESCRIPTION.This code calculates the maximum value of each segment in the 'data' array based on the 'segment_ids'." <EXPLAINS> """CODE.data = jnp.arange(6)
segment_ids = jnp.array([0, 0, 1, 1, 2, 2])
segment_max(data, segment_ids)

from jax import jit
jit(segment_max, static_argnums=2)(data, segment_ids, 3)
""" .

"DESCRIPTION.This code calculates the mean absolute percentage error between two arrays and compiles a model using stochastic gradient descent as optimizer and mean absolute percentage error as loss function." <EXPLAINS> """CODE.mape = keras.losses.MeanAbsolutePercentageError()
loss = mape([0., 0., 1., 1.], [1., 1., 1., 0.])
model = keras.Model(inputs, outputs)
model.compile('sgd', loss=keras.losses.MeanAbsolutePercentageError())""" .

"DESCRIPTION.This code calculates the mean squared error between two arrays y_true and y_pred." <EXPLAINS> """CODE.y_true = np.random.randint(0, 2, size=(2, 3))
y_pred = np.random.random(size=(2, 3))
loss = tf.keras.losses.mean_squared_error(y_true, y_pred)
assert loss.shape == (2,)
assert np.array_equal(
    loss.numpy(), np.mean(np.square(y_true - y_pred), axis=-1))
""" .

"DESCRIPTION.This code calculates the mean squared error loss between true labels (y_true) and predicted labels (y_pred) for a machine learning model." <EXPLAINS> """CODE.class MeanSquaredError(Loss):

  def call(self, y_true, y_pred):
    y_pred = tf.convert_to_tensor_v2(y_pred)
    y_true = tf.cast(y_true, y_pred.dtype)
    return tf.reduce_mean(math_ops.square(y_pred - y_true), axis=-1)


with strategy.scope():
  loss_obj = tf.keras.losses.CategoricalCrossentropy(
      reduction=tf.keras.losses.Reduction.NONE)
  ....
  loss = (tf.reduce_sum(loss_obj(labels, predictions)) *
          (1. / global_batch_size))
""" .

"DESCRIPTION.This code calculates the mean squared logarithmic error loss between two sets of values and compiles a model using stochastic gradient descent optimizer with mean squared logarithmic error loss function." <EXPLAINS> """CODE.msle = keras.losses.MeanSquaredLogarithmicError()
loss = msle([0., 0., 1., 1.], [1., 1., 1., 0.])
model = keras.Model(inputs, outputs)
model.compile('sgd', loss=keras.losses.MeanSquaredLogarithmicError())
""" .

"DESCRIPTION.This code calculates the mean value of a list of numbers using the keras.metrics.Mean class." <EXPLAINS> """CODE.m = keras.metrics.Mean()
m.update_state([1, 3, 5, 7])
m.result()
""" .

"DESCRIPTION.This code calculates the minimum value for each segment defined by the segment_ids array from the data array." <EXPLAINS> """CODE.data = jnp.arange(6)
segment_ids = jnp.array([0, 0, 1, 1, 2, 2])
segment_min(data, segment_ids)

from jax import jit
jit(segment_min, static_argnums=2)(data, segment_ids, 3)
""" .

"DESCRIPTION.This code calculates the mode(s) of the provided array with a given tolerance level of 2." <EXPLAINS> """CODE.import pyarrow as pa
import pyarrow.compute as pc
arr = pa.array([1, 1, 2, 2, 3, 2, 2, 2])
modes = pc.mode(arr, 2)
modes[0]
modes[1]""" .

"DESCRIPTION.This code calculates the onset strength, tempogram, autocorrelation, and tempo of an audio file, and then plots the onset strength, tempogram, and autocorrelation features for visualization." <EXPLAINS> """CODE.y, sr = librosa.load(librosa.util.example_audio_file())
hop_length = 512
oenv = librosa.onset.onset_strength(y=y, sr=sr, hop_length=hop_length)
tempogram = librosa.feature.tempogram(onset_envelope=oenv, sr=sr,
...                                       hop_length=hop_length)
ac_global = librosa.autocorrelate(oenv, max_size=tempogram.shape[0])
ac_global = librosa.util.normalize(ac_global)
tempo = librosa.beat.estimate_tempo(oenv, sr=sr, hop_length=hop_length)
import matplotlib.pyplot as plt
plt.figure(figsize=(8, 6))
plt.subplot(3, 1, 1)
plt.plot(oenv, label='Onset strength')
plt.xticks([])
plt.legend(frameon=True)
plt.axis('tight')
plt.subplot(3, 1, 2)
librosa.display.specshow(tempogram[:100], sr=sr, hop_length=hop_length,
...                          x_axis='time', y_axis='tempo',
...                          tmin=tempo/4, tmax=2*tempo, n_yticks=4)
plt.subplot(3, 1, 3)
x = np.linspace(0, tempogram.shape[0] * float(hop_length) / sr, num=tempogram.shape[0])
plt.plot(x, np.mean(tempogram, axis=1), label='Mean local autocorrelation')
plt.plot(x, ac_global, '--', alpha=0.75, label='Global autocorrelation')
plt.xlabel('Lag (seconds)')
plt.axis('tight')
plt.legend(frameon=True)
plt.tight_layout()""" .

"DESCRIPTION.This code calculates the power spectrogram and spectral contrast of an audio signal, and displays them using matplotlib. The power spectrogram represents the magnitude of the audio signal's frequency content over time, while the spectral contrast highlights differences in spectral content within specific frequency bands." <EXPLAINS> """CODE.y, sr = librosa.load(librosa.util.example_audio_file())
S = np.abs(librosa.stft(y))
contrast = librosa.feature.spectral_contrast(S=S, sr=sr)
import matplotlib.pyplot as plt
plt.figure()
plt.subplot(2, 1, 1)
librosa.display.specshow(librosa.logamplitude(S ** 2, ref_power=np.max), y_axis='log')
plt.colorbar(format='%+2.0f dB')
plt.title('Power spectrogram')
plt.subplot(2, 1, 2)
librosa.display.specshow(contrast, x_axis='time')
plt.colorbar()
plt.ylabel('Frequency bands')
plt.title('Spectral contrast')
plt.tight_layout()""" .

"DESCRIPTION.This code calculates the power spectrogram of an audio signal using Short-Time Fourier Transform (STFT) and displays it with logarithmic scaling. It then visualizes the power spectrogram with color mapping representing different power levels. The code includes setting up the plot layout, displaying the spectrogram, adding color bar for reference, and labeling the plots accordingly." <EXPLAINS> """CODE.S = np.abs(librosa.stft(y))
librosa.power_to_db(S**2)
array([[-33.293, -27.32 , ..., -33.293, -33.293],
       [-33.293, -25.723, ..., -33.293, -33.293],
       ...,
       [-33.293, -33.293, ..., -33.293, -33.293],
       [-33.293, -33.293, ..., -33.293, -33.293]], dtype=float32)
librosa.power_to_db(S**2, ref=np.max)
array([[-80.   , -74.027, ..., -80.   , -80.   ],
       [-80.   , -72.431, ..., -80.   , -80.   ],
       ...,
       [-80.   , -80.   , ..., -80.   , -80.   ],
       [-80.   , -80.   , ..., -80.   , -80.   ]], dtype=float32)
librosa.power_to_db(S**2, ref=np.median)
array([[-0.189,  5.784, ..., -0.189, -0.189],
       [-0.189,  7.381, ..., -0.189, -0.189],
       ...,
       [-0.189, -0.189, ..., -0.189, -0.189],
       [-0.189, -0.189, ..., -0.189, -0.189]], dtype=float32)
import matplotlib.pyplot as plt
plt.figure()
plt.subplot(2, 1, 1)
librosa.display.specshow(S**2, sr=sr, y_axis='log')
plt.colorbar()
plt.title('Power spectrogram')
plt.subplot(2, 1, 2)
librosa.display.specshow(librosa.power_to_db(S**2, ref=np.max),
...                          sr=sr, y_axis='log', x_axis='time')
plt.colorbar(format='%+2.0f dB')
plt.title('Log-Power spectrogram')
plt.tight_layout()""" .

"DESCRIPTION.This code calculates the precision metric for a classification model with 4 classes based on the predicted values and the target values." <EXPLAINS> """CODE.pred = torch.tensor([0, 1, 2, 3])
target = torch.tensor([0, 1, 2, 2])
metric = Precision(num_classes=4)
metric(pred, target)
tensor(0.7500)""" .

"DESCRIPTION.This code calculates the probability density function (pdf) of multivariate Gaussian distributions. It initializes Gaussian distributions with specified mean and covariance matrix using the Cholesky parameterization. It can compute the pdf for single or multiple observations in R^3, returning scalars or vectors accordingly." <EXPLAINS> """CODE.ds = tf.contrib.distributions

# Initialize a single 3-variate Gaussian.
mu = [1., 2, 3]
cov = [[ 0.36,  0.12,  0.06],
       [ 0.12,  0.29, -0.13],
       [ 0.06, -0.13,  0.26]]
scale = tf.cholesky(cov)
# ==> [[ 0.6,  0. ,  0. ],
#      [ 0.2,  0.5,  0. ],
#      [ 0.1, -0.3,  0.4]])
mvn = ds.MultivariateNormalTriL(
    loc=mu,
    scale_tril=scale)

mvn.mean().eval()
# ==> [1., 2, 3]

# Covariance agrees with cholesky(cov) parameterization.
mvn.covariance().eval()
# ==> [[ 0.36,  0.12,  0.06],
#      [ 0.12,  0.29, -0.13],
#      [ 0.06, -0.13,  0.26]]

# Compute the pdf of an observation in `R^3` ; return a scalar.
mvn.prob([-1., 0, 1]).eval()  # shape: []

# Initialize a 2-batch of 3-variate Gaussians.
mu = [[1., 2, 3],
      [11, 22, 33]]              # shape: [2, 3]
tril = ...  # shape: [2, 3, 3], lower triangular, non-zero diagonal.
mvn = ds.MultivariateNormalTriL(
    loc=mu,
    scale_tril=tril)

# Compute the pdf of two `R^3` observations; return a length-2 vector.
x = [[-0.9, 0, 0.1],
     [-10, 0, 9]]     # shape: [2, 3]
mvn.prob(x).eval()    # shape: [2]
""" .

"DESCRIPTION.This code calculates the rank of each element in the provided list, assigning the same rank value to elements with the same value and handling ties appropriately." <EXPLAINS> "CODE.stats.rankdata([0, 2, 2, 3])" .

"DESCRIPTION.This code calculates the reciprocal square root of each element in the tensor x." <EXPLAINS> """CODE.x = tf.constant([2., 0., -2.])
tf.math.rsqrt(x)""" .

"DESCRIPTION.This code calculates the root mean squared log error between two sets of true and predicted values." <EXPLAINS> """CODE.from sklearn.metrics import root_mean_squared_log_error
y_true = [3, 5, 2.5, 7]
y_pred = [2.5, 5, 4, 8]
root_mean_squared_log_error(y_true, y_pred)
""" .

"DESCRIPTION.This code calculates the set difference between two lists x and y. It returns a list containing elements from x that are not present in y and the indices of these elements in list x." <EXPLAINS> """CODE.def setdiff1d(x, y):
    out = [val for val in x if val not in y]
    idx = [x.index(val) for val in out]
    return out, idx

x = [1, 2, 3, 4, 5, 6]
y = [1, 3, 5]
out, idx = setdiff1d(x, y)
print("ListDiff(out={}, idx={})".format(out, idx))
""" .

"DESCRIPTION.This code calculates the sine of each element in array x, multiplies the result by 5, and assigns the output to the variable 'out'." <EXPLAINS> """CODE.with loop('l', 4):
    out = xmap(
      lambda x: jnp.sin(x) * 5,
      in_axes=['i'], out_axes=['i'],
      axis_resources={'i': 'l'})(x)""" .

"DESCRIPTION.This code calculates the sine of each element in the input array 'x', multiplies the result by 5, and stores the output in the variable 'out'. The calculation is done in a loop with 4 iterations using the axis resource 'l'." <EXPLAINS> """CODE.with loop('l', 4):
    out = xmap(
      lambda x: jnp.sin(x) * 5,
      in_axes=['i'], out_axes=['i'],
      axis_resources={'i': 'l'})(x)""" .

"DESCRIPTION.This code calculates the square root of a given number using the iterative square root method." <EXPLAINS> """CODE.@tf.function
def sqrt(x):
    y = x / 2
    d = y
    for _ in range(10):
        d /= 2
        if y * y < x:
            y += d
        else:
            y -= d
        ys.append(y.numpy())
    return y

tf.config.experimental_run_functions_eagerly(True)""" .

"DESCRIPTION.This code calculates the structural similarity index between the predicted and target images, where the predicted image is generated using random values and the target image is created by multiplying the predicted image by 0.75. The SSIM metric is used to measure the similarity between the two images, and the resulting similarity index value is 0.9219." <EXPLAINS> """CODE.pred = torch.rand([16, 1, 16, 16])
target = pred * 0.75
metric = SSIM()
metric(pred, target)
tensor(0.9219)""" .

"DESCRIPTION.This code calculates the sum of the given list of numbers." <EXPLAINS> """CODE.m = keras.metrics.Sum()
m.update_state([1, 3, 5, 7])
m.result()
""" .

"DESCRIPTION.This code calculates the sum of the values [1, 3, 5, 7] using the keras Sum metric." <EXPLAINS> """CODE.m = keras.metrics.Sum()
m.update_state([1, 3, 5, 7])
m.result()
""" .

"DESCRIPTION.This code calculates the upper triangular part of a matrix 'x' with different diagonal offsets specified (0, 2, -1)." <EXPLAINS> """CODE.import paddle.fluid as fluid
import numpy as np

data = np.arange(1, 13, dtype="int64").reshape(3,-1)
x = fluid.data(shape=(-1, 4), dtype='int64', name='x')
exe = fluid.Executor(fluid.CPUPlace())

triu = fluid.layers.triu(x)
triu_out, = exe.run(fluid.default_main_program(), feed={"x": data},
    fetch_list=[triu], return_numpy=True)

triu = fluid.layers.triu(x, diagonal=2)
triu_out, = exe.run(fluid.default_main_program(), feed={"x": data},
    fetch_list=[triu], return_numpy=True)

triu = fluid.layers.triu(x, diagonal=-1)
triu_out, = exe.run(fluid.default_main_program(), feed={"x": data},
    fetch_list=[triu], return_numpy=True)""" .

"DESCRIPTION.This code calculates the values of a given function and its derivatives at a certain point using the Jet method. It computes the function value at the point, the first derivative multiplied by the step size, and the second derivative multiplied by the step size squared plus the first derivative multiplied by another step size." <EXPLAINS> """CODE.h0, h1, h2 = 0.5**3., 3.*0.5**2., 6.*0.5
f, df, ddf = np.sin, np.cos, lambda *args: -np.sin(*args)
f0, (f1, f2) =  jet(f, (h0,), ((h1, h2),))
print(f0,  f(h0))
print(f1, df(h0) * h1)
print(f2, ddf(h0) * h1 ** 2 + df(h0) * h2""" .

"DESCRIPTION.This code calculates the zeroth order Bessel function of the second kind for the input values 0.5, 1.0, 2.0, and 4.0." <EXPLAINS> "CODE.tf.math.special.bessel_y0([0.5, 1., 2., 4.]).numpy()" .

"DESCRIPTION.This code calls a function named \"snow\" on the object \"st\"." <EXPLAINS> "CODE.st.snow()" .

"DESCRIPTION.This code calls the function \"func_c\"." <EXPLAINS> "CODE.func_c()" .

"DESCRIPTION.This code captures and stores the output of the print statement \"foo!\" to the variable \"out\", and then prints the stored output." <EXPLAINS> """CODE.with capture('stdout') as out:
    print "foo!"
print out.getvalue()""" .

"DESCRIPTION.This code changes the current working directory to '/my/new/path' and then performs some actions in that new directory." <EXPLAINS> """CODE.with working_directory('/my/new/path'):
    # Do something in new directory""" .

"DESCRIPTION.This code checks if a given input contains only finite numbers or not by using the assert_all_finite function from the sklearn package." <EXPLAINS> """CODE.import sklearn
from sklearn.utils.validation import assert_all_finite
with sklearn.config_context(assume_finite=True):
    assert_all_finite([float('nan')])
with sklearn.config_context(assume_finite=True):
    with sklearn.config_context(assume_finite=False):
        assert_all_finite([float('nan')])
""" .

"DESCRIPTION.This code checks if a list with a maximum of 2 items is valid." <EXPLAINS> """CODE.schema = {"maxItems" : 2}
Draft3Validator(schema).is_valid([2, 3, 4])""" .

"DESCRIPTION.This code checks if a repository exists on the Hugging Face Hub by calling the `repo_exists` function with the repository name as a parameter. It returns `True` if the repository exists and `False` if it does not." <EXPLAINS> """CODE.from huggingface_hub import repo_exists
repo_exists("huggingface/transformers")
True
repo_exists("huggingface/not-a-repo")
False
""" .

"DESCRIPTION.This code checks if a specific file exists in a repository on the Hugging Face Hub." <EXPLAINS> """CODE.from huggingface_hub import file_exists
file_exists("bigcode/starcoder", "config.json")
True
file_exists("bigcode/starcoder", "not-a-file")
False
file_exists("bigcode/not-a-repo", "config.json")
False
""" .

"DESCRIPTION.This code checks if a test item has crashed, and if it should be rerun. If it should be rerun, the test item is marked as pending and the outcome of the report is set to \"rerun\"." <EXPLAINS> """CODE.def pytest_handlecrashitem(crashitem, report, sched):
    if should_rerun(crashitem):
        sched.mark_test_pending(crashitem)
        report.outcome = "rerun\"""" .

"DESCRIPTION.This code checks if an input is an iterable (such as a list or tuple) and not a string. It returns True if the input is an iterable and not a string, and False otherwise." <EXPLAINS> """CODE.iterable_not_string([1, 2, 3])
iterable_not_string("foo")
iterable_not_string(1)""" .

"DESCRIPTION.This code checks if the 'MyOp' is registered in the caffe2.python.core module." <EXPLAINS> """CODE.@skipIfNotRegistered('MyOp', 'MyOp is not linked!')
        This will check if 'MyOp' is in the caffe2.python.core""" .

"DESCRIPTION.This code checks if the function \"_matches_section_title\" correctly identifies if the input strings 'Yields' and 'yield' are matching titles of a section. If they are matching, the function returns True." <EXPLAINS> "CODE._matches_section_title('Yields', 'yield') == True" .

"DESCRIPTION.This code checks if the input data type is a floating-point number (float) or not." <EXPLAINS> """CODE.is_float_dtype(float)
True
is_float_dtype(pd.Index([1, 2.]))
True""" .

"DESCRIPTION.This code checks if the input data type is an unsigned integer data type." <EXPLAINS> """CODE.is_unsigned_integer_dtype(np.uint64)
True
is_unsigned_integer_dtype(np.array([1, 2], dtype=np.uint32))
True""" .

"DESCRIPTION.This code checks if the input is an iterable object that is not a string." <EXPLAINS> """CODE._iterable_not_string([1, 2, 3])
_iterable_not_string("foo")
_iterable_not_string(1)""",
        """CODE.iterable_not_string([1, 2, 3])
True
iterable_not_string("foo")
False
iterable_not_string(1)
False""" .

"DESCRIPTION.This code checks if the input string matches any of the allowed types ('32' or 'bf16') and returns True if it does, otherwise returns False." <EXPLAINS> """CODE._precision_allowed_type("32")
_precision_allowed_type("bf16")""" .

"DESCRIPTION.This code checks whether different types of variables are Keras tensors or not." <EXPLAINS> """CODE.    from keras import backend as K
    from keras.layers import Input, Dense
    np_var = numpy.array([1, 2])
    K.is_keras_tensor(np_var) # A numpy array is not a symbolic tensor.
    k_var = tf.placeholder('float32', shape=(1,1))
    K.is_keras_tensor(k_var)
    keras_var = K.variable(np_var)
    K.is_keras_tensor(keras_var)
    keras_placeholder = K.placeholder(shape=(2, 4, 5))
    K.is_keras_tensor(keras_placeholder)
    keras_input = Input([10])
    K.is_keras_tensor(keras_input) # An Input is a Keras tensor.
    keras_layer_output = Dense(10)(keras_input)
    K.is_keras_tensor(keras_layer_output)
""" .

"DESCRIPTION.This code checks whether the native byte order of the system is big-endian or little-endian." <EXPLAINS> """CODE.byteorder_isnative('=')
True""" .

"DESCRIPTION.This code checks whether the variable 'foo' is of type 'matplotlib.figure.Figure'." <EXPLAINS> "CODE.is_type(foo, 'matplotlib.figure.Figure')" .

"DESCRIPTION.This code clears the custom objects dictionary and then adds a key-value pair where the key is \"MyObject\" and the value is the class MyObject." <EXPLAINS> """CODE.    get_custom_objects().clear()
    get_custom_objects()["MyObject"] = MyObject
""" .

"DESCRIPTION.This code clears the custom objects dictionary and then adds a new custom object 'MyObject' to the dictionary." <EXPLAINS> """CODE.get_custom_objects().clear()
get_custom_objects()['MyObject'] = MyObject
""" .

"DESCRIPTION.This code clears the custom objects dictionary and then adds a new key-value pair to it, with the key being \"MyObject\" and the value being an instance of the class MyObject." <EXPLAINS> """CODE.get_custom_objects().clear()
get_custom_objects()["MyObject"] = MyObject
""" .

"DESCRIPTION.This code clears the gradients stored in the linear model." <EXPLAINS> "CODE.linear.clear_gradients()" .

"DESCRIPTION.This code combines the output of PCA and TruncatedSVD transformers into a single transformer called FeatureUnion. It does not utilize parallel processing (n_jobs=1) and specifies the parameters for the PCA and TruncatedSVD transformers." <EXPLAINS> """CODE.make_union(PCA(), TruncatedSVD())
FeatureUnion(n_jobs=1,
             transformer_list=[('pca', PCA(copy=True, n_components=None,
                                           whiten=False)),
                               ('truncatedsvd',
                                TruncatedSVD(algorithm='randomized',
                                             n_components=2, n_iter=5,
                                             random_state=None, tol=0.0))],
             transformer_weights=None)""" .

"DESCRIPTION.This code compares if the value of LoggerStages.TRAIN is equal to the string 'train'." <EXPLAINS> "CODE.LoggerStages.TRAIN == 'train'" .

"DESCRIPTION.This code compares two arrays element-wise and checks if the elements are close within a specified tolerance. The code also includes an option to consider NaN values as equal or not equal when comparing the arrays." <EXPLAINS> """CODE.import paddle.fluid as fluid
import numpy as np

use_cuda = fluid.core.is_compiled_with_cuda()

a = fluid.data(name="a", shape=[2], dtype='float32')
b = fluid.data(name="b", shape=[2], dtype='float32')

result = fluid.layers.allclose(a, b, rtol=1e-05, atol=1e-08,
                          equal_nan=False, name="ignore_nan")
result_nan = fluid.layers.allclose(a, b, rtol=1e-05, atol=1e-08,
                              equal_nan=True, name="equal_nan")

place = fluid.CUDAPlace(0) if use_cuda else fluid.CPUPlace()
exe = fluid.Executor(place)
exe.run(fluid.default_startup_program())

x = np.array([10000., 1e-07]).astype("float32")
y = np.array([10000.1, 1e-08]).astype("float32")
result_v, result_nan_v = exe.run(
    feed={'a': x, 'b': y},
    fetch_list=[result, result_nan])
print(result_v, result_nan_v)

x = np.array([10000., 1e-08]).astype("float32")
y = np.array([10000.1, 1e-09]).astype("float32")
result_v, result_nan_v = exe.run(
    feed={'a': x, 'b': y},
    fetch_list=[result, result_nan])
print(result_v, result_nan_v)

x = np.array([1.0, float('nan')]).astype("float32")
y = np.array([1.0, float('nan')]).astype("float32")
result_v, result_nan_v = exe.run(
    feed={'a': x, 'b': y},
    fetch_list=[result, result_nan])
print(result_v, result_nan_v)""" .

"DESCRIPTION.This code compares two arrays of integers element-wise and returns a boolean array indicating whether the elements are equal." <EXPLAINS> """CODE.import paddle.fluid as fluid
import numpy as np
label = fluid.layers.assign(np.array([3, 3], dtype="int32"))
limit = fluid.layers.assign(np.array([3, 2], dtype="int32"))
out1 = fluid.layers.elementwise_equal(x=label, y=limit) #out1=[True, False]""" .

"DESCRIPTION.This code compiles a model using stochastic gradient descent optimizer, mean squared error loss function, and SparseTopKCategoricalAccuracy as the evaluation metric." <EXPLAINS> """CODE.model.compile(
  optimizer='sgd',
  loss='mse',
  metrics=[tf.keras.metrics.SparseTopKCategoricalAccuracy()])
""" .

"DESCRIPTION.This code compiles a neural network model using the Adam optimizer with a learning rate of 1e-3, a BinaryCrossentropy loss function, and evaluates the model's performance with BinaryAccuracy and FalseNegatives metrics." <EXPLAINS> """CODE.model.compile(optimizer=tf.keras.optimizer.Adam(learning_rate=1e-3),
              loss=tf.keras.losses.BinaryCrossentropy(),
              metrics=[tf.keras.metrics.BinaryAccuracy(),
                       tf.keras.metrics.FalseNegatives()])
""" .

"DESCRIPTION.This code computes and applies a transformation on the input samples." <EXPLAINS> "CODE.ev.compute_apply(samples)" .

"DESCRIPTION.This code computes gradients of a given loss with respect to variables, aggregates the gradients across replicas, and then updates the variables using an optimizer." <EXPLAINS> """CODE.grads = tape.gradient(loss, vars)
grads = tf.distribute.get_replica_context().all_reduce('sum', grads)
# Processing aggregated gradients.
optimizer.apply_gradients(zip(grads, vars),
    experimental_aggregate_gradients=False)
""" .

"DESCRIPTION.This code computes the constant-Q transform (CQT) of an audio signal, then calculates perceptually weighted CQT." <EXPLAINS> """CODE.CQT             = librosa.cqt(y, sr, fmin=55, fmax=440)
freqs           = librosa.cqt_frequencies(CQT.shape[0], fmin=55)
percept_CQT     = librosa.feature.perceptual_weighting(CQT, freqs, ref_power=CQT.max())
""" .

"DESCRIPTION.This code computes the partial dependence of the first feature on the output of a Gradient Boosting classifier, using the specified samples and labels." <EXPLAINS> """CODE.from sklearn.ensemble import GradientBoostingClassifier
gb = GradientBoostingClassifier().fit(samples, labels)
kwargs = dict(X=samples, percentiles=(0, 1), grid_resolution=2)
partial_dependence(gb, [0], **kwargs)
(array([[-10.72892297,  10.72892297]]), [array([ 0.,  1.])])""" .

"DESCRIPTION.This code constructs a URL with the specified components and then retrieves the full URL as a string." <EXPLAINS> """CODE.Url('http', 'username:password', 'host.com', 80,
    '/path', 'query', 'fragment').url""" .

"DESCRIPTION.This code constructs a neural network model using Keras, compiles it with stochastic gradient descent optimizer, mean squared error loss function, and cosine similarity as the performance metric along the specified axis." <EXPLAINS> """CODE.model = keras.Model(inputs, outputs)
model.compile(
  'sgd',
  loss='mse',
  metrics=[keras.metrics.CosineSimilarity(axis=1)])
""" .

"DESCRIPTION.This code converts a byte string b' False ' to a boolean value False." <EXPLAINS> """CODE.asbool(b' False ')
False""" .

"DESCRIPTION.This code converts a file size represented as a string to an integer value in bytes. It checks if the input size is already an integer and returns it as is. If the size is a string, it looks for units such as B, KB, MB, GB, and TB at the end of the string, and converts the size to bytes according to the unit. If no valid unit is found, it raises a ValueError." <EXPLAINS> """CODE.def convert_file_size_to_int(size):
    if isinstance(size, int):
        return size
    units = {"B": 1, "KB": 1024, "MB": 1024 ** 2, "GB": 1024 ** 3, "TB": 1024 ** 4}
    size_str = str(size)
    for unit in units:
        if size_str.endswith(unit):
            return int(size_str[:-len(unit)]) * units[unit]
    raise ValueError("Invalid size format")
""" .

"DESCRIPTION.This code converts a list of labels into a binary matrix where each column represents a unique label and each row represents a sample, with a 1 indicating the presence of that label and a 0 indicating the absence of that label. The \"classes\" parameter specifies the unique labels that the input list can have, and the \"multilabel\" parameter specifies whether the input list contains multiple labels for each sample." <EXPLAINS> """CODE.from sklearn.preprocessing import label_binarize
label_binarize([1, 6], classes=[1, 2, 4, 6])
label_binarize([1, 6], classes=[1, 6, 4, 2])
label_binarize([(1, 2), (6,), ()], multilabel=True, classes=[1, 6, 4, 2])
""" .

"DESCRIPTION.This code converts a numeric value representing a datetime in Excel format into a datetime object in Python." <EXPLAINS> """CODE.excel_datetime(40237.029999999795)
datetime.datetime(2010, 2, 28, 0, 43, 11, 999982)""" .

"DESCRIPTION.This code converts a numpy array of floats to the default float type used by Keras backend." <EXPLAINS> """CODE.from keras import backend as K
K.floatx()
arr = numpy.array([1.0, 2.0], dtype='float64')
arr.dtype
new_arr = K.cast_to_floatx(arr)
new_arr
new_arr.dtype
""" .

"DESCRIPTION.This code converts a pandas DataFrame `ds` into a Python list." <EXPLAINS> """CODE.ds.to_list()
""" .

"DESCRIPTION.This code converts a sample text sentence into a list of individual words." <EXPLAINS> """CODE.sample_text = 'This is a sample sentence.'
tf.keras.preprocessing.text.text_to_word_sequence(sample_text)""" .

"DESCRIPTION.This code converts a signed integer to an unsigned hexadecimal representation with padding zeros, using the \"%08X\" format specifier." <EXPLAINS> "CODE.print \"%08X\" % signed_to_unsigned (-2147023174)" .

"DESCRIPTION.This code converts a string representation of a boolean value ('YES' and 'FALSE') to a corresponding boolean value (True and False)." <EXPLAINS> """CODE.strtobool('YES')
strtobool('FALSE')""" .

"DESCRIPTION.This code converts an XML string into a Python dictionary." <EXPLAINS> "CODE.xml2dict('<?xml version=\"1.0\" ?><root attr=\"name\"><key>1</key></root>')" .

"DESCRIPTION.This code converts the data type of a dataset to float64 ('f8') and then creates a new dataset 'double_precision' containing every other element from the original dataset up to index 100." <EXPLAINS> """CODE.with dataset.astype('f8'):
    double_precision = dataset[0:100:2]""" .

"DESCRIPTION.This code counts the number of occurrences of a specified element in a list." <EXPLAINS> """CODE.pbag([]).count('non-existent')
0
pbag([1, 1, 2]).count(1)""" .

"DESCRIPTION.This code counts the number of occurrences of the integer 1 in a deque initialized with the values [1, 2, 1]." <EXPLAINS> "CODE.pdeque([1, 2, 1]).count(1)" .

"DESCRIPTION.This code creates a 3D reflection padding layer with a padding of 1. It then applies the padding to a 3D input tensor of shape (1, 1, 2, 2, 2)." <EXPLAINS> """CODE.m = nn.ReflectionPad3d(1)
input = torch.arange(8, dtype=torch.float).reshape(1, 1, 2, 2, 2)
m(input)""" .

"DESCRIPTION.This code creates a DataFrame with one row and two columns ('x' and 'y'). It then extracts the first row from the DataFrame and prints the data type of the 'x' column in the extracted row and in the original DataFrame." <EXPLAINS> """CODE.df = DataFrame([[1, 1.0]], columns=['x', 'y'])
row = next(df.iterrows())[1]
print(row['x'].dtype)
print(df['x'].dtype)
""" .

"DESCRIPTION.This code creates a JSON object with keys \"foo\", \"baz\", and \"stuff\". The \"foo\" key has the value \"bar\", the \"baz\" key has the value \"boz\", and the \"stuff\" key contains a list of strings." <EXPLAINS> """CODE.st.json({
    'foo': 'bar',
    'baz': 'boz',
    'stuff': [
        'stuff 1',
        'stuff 2',
        'stuff 3',
        'stuff 5',
    ],
})""" .

"DESCRIPTION.This code creates a Keras variable with a specific numpy array value and then retrieves the size of the input data." <EXPLAINS> """CODE.from keras import backend as K
val = np.array([[1, 2], [3, 4]])
kvar = K.variable(value=val)
K.size(inputs)
""" .

"""DESCRIPTION.This code creates a LocalFS client and performs the following operations:
1. Creates a file named "test_rename_src" using the touch() method.
2. Checks if the file "test_rename_src" exists, which returns True.
3. Renames the file "test_rename_src" to "test_rename_dst".
4. Checks if the file "test_rename_src" exists, which returns False.
5. Checks if the file "test_rename_dst" exists, which returns True.
6. Deletes the file "test_rename_dst".""" <EXPLAINS> """CODE.        from paddle.distributed.fleet.utils import LocalFS
        client = LocalFS()
        client.touch("test_rename_src")
        print(client.is_exists("test_rename_src")) # True
        client.rename("test_rename_src", "test_rename_dst")
        print(client.is_exists("test_rename_src")) # False
        print(client.is_exists("test_rename_dst")) # True
        client.delete("test_rename_dst")""" .

"DESCRIPTION.This code creates a LocalFS client object, creates a file named \"test_is_file\" using the touch method, checks if the file exists using the is_file method and prints True, and finally deletes the file using the delete method." <EXPLAINS> """CODE.from paddle.distributed.fleet.utils import LocalFS
client = LocalFS()
client.touch("test_is_file")
print(client.is_file("test_is_file")) # True
client.delete("test_is_file")""" .

"DESCRIPTION.This code creates a ModelCardData object with specified language, license, library name, and tags, and then converts the object into a dictionary." <EXPLAINS> """CODE.from huggingface_hub import ModelCardData
card_data = ModelCardData(
    language="en",
    license="mit",
    library_name="timm",
    tags=['image-classification', 'resnet'],
)
card_data.to_dict()
{'language': 'en', 'license': 'mit', 'library_name': 'timm', 'tags': ['image-classification', 'resnet']}
""" .

"DESCRIPTION.This code creates a MultiDict object with key \"foo\" and values [1, 2, 3]. It then checks if the zipped keys and list values of the MultiDict object are equal to the lists of values." <EXPLAINS> """CODE.d = MultiDict({"foo": [1, 2, 3]})
zip(d.keys(), d.listvalues()) == d.lists()
True""" .

"DESCRIPTION.This code creates a MultiIndex object from the Cartesian product of the lists 'numbers' and 'colors', with the names 'number' and 'color' for the two levels of the index." <EXPLAINS> "CODE.MultiIndex.from_product([numbers, colors], names=['number', 'color'])" .

"DESCRIPTION.This code creates a MultiIndex series and then unstacks it along different levels to convert it into a DataFrame." <EXPLAINS> """CODE.index = pd.MultiIndex.from_tuples([('one', 'a'), ('one', 'b'),
                                   ('two', 'a'), ('two', 'b')])
s = pd.Series(np.arange(1.0, 5.0), index=index)
s.unstack(level=-1)
s.unstack(level=0)
df = s.unstack(level=0)
df.unstack()
""" .

"DESCRIPTION.This code creates a Pandas Index object containing a list of numbers, and then rounds the numbers to two decimal places." <EXPLAINS> """CODE.import pandas as pd
idx = pd.Index([10.1234, 20.5678, 30.9123, 40.4567, 50.7890])
idx.round(decimals=2)""" .

"DESCRIPTION.This code creates a PartialState object and then prints out a message indicating which process will print it." <EXPLAINS> """CODE.from accelerate.state import PartialState
state = PartialState()
with state.local_main_process_first():
    print(f"This will be printed by process {state.local_process_index}")
""" .

"DESCRIPTION.This code creates a PoolManager with 2 pools, sends GET requests to 'http://google.com/', 'http://google.com/mail', and 'http://yahoo.com/', and returns the number of pools in the manager." <EXPLAINS> """CODE.manager = PoolManager(num_pools=2)
r = manager.request('GET', 'http://google.com/')
r = manager.request('GET', 'http://google.com/mail')
r = manager.request('GET', 'http://yahoo.com/')
len(manager.pools)
""" .

"DESCRIPTION.This code creates a PyArrow array with values \"a\", \"b\", \"c\", None, \"e\", \"f\", then creates another PyArrow array with indices [0, None, 4, 3], and finally retrieves the values from the first array based on the indices provided in the second array." <EXPLAINS> """CODE.import pyarrow as pa
arr = pa.array(["a", "b", "c", None, "e", "f"])
indices = pa.array([0, None, 4, 3])
arr.take(indices)""" .

"DESCRIPTION.This code creates a SQLite database \"example.db\" and a table \"movie\" with columns title, year, and score. It then inserts two movie records into the table using the data provided in the code. Finally, it reads all records from the \"movie\" table and prints the result, ordered by the \"year\" column." <EXPLAINS> """CODE.import sqlite3
import ray

connection = sqlite3.connect("example.db")
connection.cursor().execute("CREATE TABLE movie(title, year, score)")
dataset = ray.data.from_items([
    {"title": "Monty Python and the Holy Grail", "year": 1975, "score": 8.2},
    {"title": "And Now for Something Completely Different", "year": 1971, "score": 7.5}
])

dataset.write_sql(
    "INSERT INTO movie VALUES(?, ?, ?)", lambda: sqlite3.connect("example.db")
)

result = connection.cursor().execute("SELECT * FROM movie ORDER BY year")
print(result.fetchall())""" .

"DESCRIPTION.This code creates a TensorFlow constant array with values [-20, -1.0, 0.0, 1.0, 20] of data type float32 and applies the softplus activation function from the Keras package to it, returning the result as a NumPy array." <EXPLAINS> """CODE.a = tf.constant([-20, -1.0, 0.0, 1.0, 20], dtype = tf.float32)
b = tf.keras.activations.softplus(a)
b.numpy()""" .

"DESCRIPTION.This code creates a TensorFlow constant tensor with values [[1.0, 2.0], [3.0, 4.0]] and prints the tensor." <EXPLAINS> """CODE.x = tf.constant([[1.0, 2.0], [3.0, 4.0]])
tf.keras.backend.print_tensor(x)""" .

"DESCRIPTION.This code creates a TensorFlow dataset from a range of numbers, squares each element, applies a distributed service to process the dataset in parallel across multiple epochs, increments each element by 1, and prints the resulting dataset elements." <EXPLAINS> """CODE.
dataset = tf.data.Dataset.range(5)
dataset = dataset.map(lambda x: x*x)
dataset = dataset.apply(
    tf.data.experimental.service.distribute("parallel_epochs",
                                            "grpc://dataservice:5000"))
dataset = dataset.map(lambda x: x+1)

for element in dataset:
  print(element)  # prints { 1, 2, 5, 10, 17 }

""" .

"DESCRIPTION.This code creates a category encoding layer with a maximum of 4 tokens and output mode set to \"count\". It then applies the layer to a set of input data using the specified count weights." <EXPLAINS> """CODE.layer = tf.keras.layers.experimental.preprocessing.CategoryEncoding(
    max_tokens=4, output_mode="count")
count_weights = np.array([[.1, .2], [.1, .1], [.2, .3], [.4, .2]])
layer([[0, 1], [0, 0], [1, 2], [3, 1]], count_weights=count_weights)""" .

"DESCRIPTION.This code creates a collection titled \"ICCV 2023\" with a description of \"Portfolio of models, papers and demos I presented at ICCV 2023\". It then generates a slug for the collection, which is \"username/iccv-2023-64f9a55bb3115b4f513ec026\"." <EXPLAINS> """CODE.from huggingface_hub import create_collection
collection = create_collection(
    title="ICCV 2023",
    description="Portfolio of models, papers and demos I presented at ICCV 2023",
)
collection.slug
"username/iccv-2023-64f9a55bb3115b4f513ec026"
""" .

"DESCRIPTION.This code creates a column transformer object that applies StandardScaler to numerical columns and OneHotEncoder to categorical columns." <EXPLAINS> """CODE.make_column_transformer(
    (['numerical_column'], StandardScaler()),
    (['categorical_column'], OneHotEncoder())
)""" .

"DESCRIPTION.This code creates a context manager that does nothing when entered and exited." <EXPLAINS> """CODE.with NullContext():
    pass""" .

"DESCRIPTION.This code creates a convolutional neural network model that takes inputs with shape (3, 32, 32), applies a 3x3 convolution with 64 filters, and then flattens the output to have a shape of (None, 65536)." <EXPLAINS> """CODE.model = Sequential()
model.add(Conv2D(64, (3, 3),
                 input_shape=(3, 32, 32), padding='same',))
# now: model.output_shape == (None, 64, 32, 32)

model.add(Flatten())
# now: model.output_shape == (None, 65536)
""" .

"DESCRIPTION.This code creates a custom SSL context to be used with urllib3 and disables the SSLv3 protocol." <EXPLAINS> """CODE.from urllib3.util import ssl_
context = ssl_.create_urllib3_context()
context.options &= ~ssl_.OP_NO_SSLv3
""" .

"DESCRIPTION.This code creates a data editor interface using the Streamlit library in Python to display a DataFrame containing a list of Streamlit widget commands." <EXPLAINS> """CODE.import pandas as pd
import streamlit as st

data_df = pd.DataFrame(
    {
        "widgets": ["st.selectbox", "st.number_input", "st.text_area", "st.button"],
    }
)

st.data_editor(
    data_df,
    column_config={
        "widgets": st.column_config.Column(
            "Streamlit Widgets",
            help="Streamlit **widget** commands ð",
            width="medium",
            required=True,
        )
    },
    hide_index=True,
    num_rows="dynamic",
)
""" .

"DESCRIPTION.This code creates a data loader object for the MNIST dataset using the specified root directory, transformation, and download settings. The data loader loads the dataset without shuffling the data." <EXPLAINS> """CODE.def test_dataloader(self):
    dataset = MNIST(root=PATH, train=False, transform=transforms.ToTensor(), download=False)
    loader = torch.utils.data.DataLoader(dataset=dataset, shuffle=False)
    return loader""" .

"DESCRIPTION.This code creates a data tensor of shape [-1, 23, 48] with name \"x\" and data type 'float32', and prints the gradient name of the data tensor, which is \"x@GRAD\"." <EXPLAINS> """CODE.import paddle.fluid as fluid
x = fluid.data(name="x", shape=[-1, 23, 48], dtype='float32')
print(x.grad_name) # output is "x@GRAD\"""" .

"DESCRIPTION.This code creates a dataset object using the PaddlePaddle framework and sets the batch size to 128." <EXPLAINS> """CODE.dataset = base.DatasetFactory().create_dataset()
dataset.set_batch_size(128)""",
        """CODE.import paddle
dataset = paddle.distributed.fleet.DatasetBase()
dataset._set_batch_size(128)""" .

"DESCRIPTION.This code creates a dataset object using the PaddlePaddle framework and then prints out the description of the dataset." <EXPLAINS> """CODE.import paddle
dataset = paddle.distributed.fleet.DatasetBase()
print(dataset._desc())""",
        """CODE.import paddle.base as base
dataset = base.DatasetFactory().create_dataset()
print(dataset.desc())""" .

"DESCRIPTION.This code creates a dataset object using the paddle.distributed.fleet.DatasetBase class and sets a download command for the dataset." <EXPLAINS> """CODE.import paddle
dataset = paddle.distributed.fleet.DatasetBase()
dataset._set_download_cmd("./read_from_afs")""",
        """CODE.import paddle.base as base
dataset = base.DatasetFactory().create_dataset()
dataset.set_download_cmd("./read_from_afs")""" .

"DESCRIPTION.This code creates a dataset object using the paddle.distributed.fleet.DatasetBase class and sets the number of threads to 12 for the dataset." <EXPLAINS> """CODE.import paddle
dataset = paddle.distributed.fleet.DatasetBase()
dataset._set_thread(12)""",
        """CODE.import paddle.base as base
dataset = base.DatasetFactory().create_dataset()
dataset.set_thread(12)""" .

"DESCRIPTION.This code creates a dataset object using the paddle.distributed.fleet.DatasetBase class and then prints the description of the dataset." <EXPLAINS> """CODE.import paddle
dataset = paddle.distributed.fleet.DatasetBase()
print(dataset._desc())""",
        """CODE.import paddle.base as base
dataset = base.DatasetFactory().create_dataset()
print(dataset.desc())""" .

"DESCRIPTION.This code creates a dataset using the PaddlePaddle framework and sets the variables 'data' and 'label' for the dataset." <EXPLAINS> """CODE.import paddle
dataset = paddle.distributed.fleet.DatasetBase()
dataset._set_use_var([data, label])""",
        """CODE.import paddle.fluid as fluid
dataset = fluid.DatasetFactory().create_dataset()
dataset.set_use_var([data, label])""" .

"DESCRIPTION.This code creates a dataset using the PaddlePaddle framework and sets the variables to be used as data and label." <EXPLAINS> """CODE.import paddle
dataset = paddle.distributed.fleet.DatasetBase()
dataset._set_use_var([data, label])""",
        """CODE.import paddle.fluid as fluid
dataset = fluid.DatasetFactory().create_dataset()
dataset.set_use_var([data, label])""" .

"DESCRIPTION.This code creates a dataset with a single element containing the integer value 42 using TensorFlow. It then creates an iterator for the dataset and retrieves the element specification of the iterator, which is a TensorFlow TensorSpec with shape (), data type int32." <EXPLAINS> """CODE.dataset = tf.data.Dataset.from_tensors(42)
iterator = iter(dataset)
iterator.element_spec
tf.TensorSpec(shape=(), dtype=tf.int32, name=None)""" .

"DESCRIPTION.This code creates a deployment named \"deployment1\" with version \"v1\" using the MyDeployment class, and then deploys it with the specified initialization arguments." <EXPLAINS> """CODE.@serve.deployment("deployment1", version="v1")
class MyDeployment:
    pass

MyDeployment.deploy(*init_args)""" .

"DESCRIPTION.This code creates a deque with elements [1, 2, 3], rotates the deque to the right by 1 position, and then rotates the deque to the left by 2 positions." <EXPLAINS> """CODE.x = pdeque([1, 2, 3])
x.rotate(1)
x.rotate(-2)""" .

"DESCRIPTION.This code creates a dictionary of different convolutional layers using PaddlePaddle's nn module. It then creates a LayerDict using the sublayers dictionary. The code then shows the length of the layer_dict which is 3. It then removes the 'conv2d' key from the layer_dict and shows the updated length, which is 2." <EXPLAINS> """CODE.import paddle
from collections import OrderedDict

sublayers = OrderedDict([
    ('conv1d', paddle.nn.Conv1D(3, 2, 3)),
    ('conv2d', paddle.nn.Conv2D(3, 2, 3)),
    ('conv3d', paddle.nn.Conv3D(4, 6, (3, 3, 3))),
])

layer_dict = paddle.nn.LayerDict(sublayers=sublayers)
len(layer_dict)
#3

layer_dict.pop('conv2d')
len(layer_dict)
#2""" .

"DESCRIPTION.This code creates a dictionary of different types of convolutional layers using PaddlePaddle framework and initializes a LayerDict with these layers. It then demonstrates clearing the LayerDict and checking its length." <EXPLAINS> """CODE.import paddle
from collections import OrderedDict

sublayers = OrderedDict([
    ('conv1d', paddle.nn.Conv1D(3, 2, 3)),
    ('conv2d', paddle.nn.Conv2D(3, 2, 3)),
    ('conv3d', paddle.nn.Conv3D(4, 6, (3, 3, 3))),
])

layer_dict = paddle.nn.LayerDict(sublayers=sublayers)
len(layer_dict)
#3

layer_dict.clear()
len(layer_dict)
#0""" .

"DESCRIPTION.This code creates a dictionary space with keys 'position' and 'velocity', each containing a Box space with specific low, high, shape, and dtype parameters. It also defines a function create_empty_array that takes in the space dictionary, a parameter n, and a function fn, and creates an empty array with the specified parameters." <EXPLAINS> """CODE.from gym.spaces import Box, Dict
space = Dict({
'position': Box(low=0, high=1, shape=(3,), dtype=np.float32),
'velocity': Box(low=0, high=1, shape=(2,), dtype=np.float32)})
create_empty_array(space, n=2, fn=np.zeros)""" .

"DESCRIPTION.This code creates a dictionary that contains information about an image processing software ImageJ, including its version, the number of images, and whether it supports hyperstack images." <EXPLAINS> """CODE.imagej_description_dict(description)  # doctest: +SKIP
{'ImageJ': '1.11a', 'images': 510, 'hyperstack': True}""" .

"DESCRIPTION.This code creates a global summary writer instance using the GlobalSummaryWriter class and adds text logs to it." <EXPLAINS> """CODE.from tensorboardX import GlobalSummaryWriter
writer = GlobalSummaryWriter.getSummaryWriter()  # This creates a new instance.
writer.add_text('my_log', 'greeting from global1')
writer = GlobalSummaryWriter.getSummaryWriter()  # Get the instance in global1.py.
writer.add_text('my_log', 'greeting from global2')""" .

"DESCRIPTION.This code creates a linear operator using a full matrix representation. The operator can perform operations such as converting to a dense matrix, obtaining its shape, calculating the log determinant, and applying the operator to a given tensor. Additionally, it demonstrates how to create a batch of linear operators using random normal matrices." <EXPLAINS> """CODE.# Create a 2 x 2 linear operator.
matrix = [[1., 2.], [3., 4.]]
operator = LinearOperatorFullMatrix(matrix)

operator.to_dense()
==> [[1., 2.]
     [3., 4.]]

operator.shape
==> [2, 2]

operator.log_determinant()
==> scalar Tensor

x = ... Shape [2, 4] Tensor
operator.apply(x)
==> Shape [2, 4] Tensor

# Create a [2, 3] batch of 4 x 4 linear operators.
matrix = tf.random_normal(shape=[2, 3, 4, 4])
operator = LinearOperatorFullMatrix(matrix)
""" .

"DESCRIPTION.This code creates a linear transformation network with two fully connected layers using the PaddlePaddle framework in Python." <EXPLAINS> """CODE.import paddle.fluid as fluid

with fluid.dygraph.guard():
    fc1 = fluid.Linear(10, 3)
    fc2 = fluid.Linear(3, 10, bias_attr=False)
    model = fluid.dygraph.Sequential(fc1, fc2)
    for prefix, layer in model.named_sublayers():
        print(prefix, layer)""" .

"DESCRIPTION.This code creates a list of linear layers with input and output sizes of 10, and inserts another linear layer with the same size into the list at index 3. It then prints whether the layer at index 3 in the list is the same as the newly inserted layer." <EXPLAINS> """CODE.import paddle.fluid as fluid

with fluid.dygraph.guard():
    linears = fluid.dygraph.LayerList([fluid.dygraph.Linear(10, 10) for i in range(10)])
    another = fluid.dygraph.Linear(10, 10)
    linears.insert(3, another)
    print(linears[3] is another)  # True""" .

"DESCRIPTION.This code creates a logger object that logs messages to both a file and standard output, logs messages to only a file, and demonstrates calling standard logger methods such as warning." <EXPLAINS> """CODE.logger = DatasetLogger(__name__)
logger.get_logger().info("This logs to file and stdout")
logger.get_logger(log_to_stdout=False).info("This logs to file only)
logger.get_logger().warning("Can call the usual Logger methods")
""" .

"DESCRIPTION.This code creates a mapping from strings to indices, where out-of-vocabulary words are assigned to a specific bucket. It then uses the mapping table to lookup the indices of the input strings and returns the corresponding indices." <EXPLAINS> """CODE.mapping_strings = t.constant(["emerson", "lake", "palmer")
table = tf.contrib.lookup.string_to_index_table_from_tensor(
    mapping=mapping_strings, num_oov_buckets=1, default_value=-1)
features = tf.constant(["emerson", "lake", "and", "palmer"])
ids = table.lookup(features)
...
tf.tables_initializer().run()

ids.eval()  ==> [0, 1, 4, 2]
""" .

"DESCRIPTION.This code creates a mask based on the input tensor x, where the values greater than 0 are set to 1 and the rest are set to 0. It then applies a custom softmax operation, XSoftmax, using the mask on the input tensor x along the last dimension." <EXPLAINS> """CODE.import torch
from transformers.modeling_deroberta import XSoftmax

# Make a tensor
x = torch.randn([4,20,100])

# Create a mask
mask = (x>0).int()

y = XSoftmax.apply(x, mask, dim=-1)""" .

"DESCRIPTION.This code creates a meta package using the source directory located at the project root." <EXPLAINS> "CODE.create_meta_package(os.path.join(_PROJECT_ROOT, \"src\"))" .

"DESCRIPTION.This code creates a mixture model of two distributions: one using scalar Gaussians and another using bivariate Gaussians. The code calculates the mean and variance for the scalar Gaussian mixture, as well as the mean and covariance for the bivariate Gaussian mixture. It then plots the probability density function (PDF) for both mixtures." <EXPLAINS> """CODE.import matplotlib.pyplot as plt
ds = tf.contrib.distributions

### Create a mixture of two scalar Gaussians:

gm = ds.MixtureSameFamily(
    mixture_distribution=ds.Categorical(
        probs=[0.3, 0.7]),
    components_distribution=ds.Normal(
      loc=[-1., 1],       # One for each component.
      scale=[0.1, 0.5]))  # And same here.

gm.mean()
# ==> 0.4

gm.variance()
# ==> 1.018

# Plot PDF.
x = np.linspace(-2., 3., int(1e4), dtype=np.float32)
plt.plot(x, gm.prob(x).eval());

### Create a mixture of two Bivariate Gaussians:

gm = ds.MixtureSameFamily(
    mixture_distribution=ds.Categorical(
        probs=[0.3, 0.7]),
    components_distribution=ds.MultivariateNormalDiag(
        loc=[[-1., 1],  # component 1
             [1, -1]],  # component 2
        scale_identity_multiplier=[.3, .6]))

gm.mean()
# ==> array([ 0.4, -0.4], dtype=float32)

gm.covariance()
# ==> array([[ 1.119, -0.84],
#            [-0.84,  1.119]], dtype=float32)

# Plot PDF contours.
def meshgrid(x, y=x):
  [gx, gy] = np.meshgrid(x, y, indexing='ij')
  gx, gy = np.float32(gx), np.float32(gy)
  grid = np.concatenate([gx.ravel()[None, :], gy.ravel()[None, :]], axis=0)
  return grid.T.reshape(x.size, y.size, 2)
grid = meshgrid(np.linspace(-2, 2, 100, dtype=np.float32)
plt.contour(grid[..., 0], grid[..., 1], gm.prob(grid).eval());""" .

"DESCRIPTION.This code creates a model using Keras with specified inputs and outputs, compiles the model using stochastic gradient descent optimizer, mean squared error loss function, and cosine similarity metric along axis 1." <EXPLAINS> """CODE.model = keras.Model(inputs, outputs)
model.compile(
  'sgd',
  loss='mse',
  metrics=[keras.metrics.CosineSimilarity(axis=1)])
""" .

"DESCRIPTION.This code creates a namespace object with specified parameters, saves the parameters to a YAML file, loads the parameters from the YAML file into a new namespace object, compares the parameters of the original and loaded namespace objects, and then removes the YAML file." <EXPLAINS> """CODE.hparams = Namespace(batch_size=32, learning_rate=0.001, data_root='./any/path/here')
path_yaml = './testing-hparams.yaml'
save_hparams_to_yaml(path_yaml, hparams)
hparams_new = load_hparams_from_yaml(path_yaml)
vars(hparams) == hparams_new
os.remove(path_yaml)""" .

"DESCRIPTION.This code creates a neural network model using Keras that takes two inputs, applies a subtract operation between two hidden layers, and then outputs the result after passing through a final dense layer." <EXPLAINS> """CODE.    import keras

    input1 = keras.layers.Input(shape=(16,))
    x1 = keras.layers.Dense(8, activation='relu')(input1)
    input2 = keras.layers.Input(shape=(32,))
    x2 = keras.layers.Dense(8, activation='relu')(input2)
    subtracted = keras.layers.subtract([x1, x2])

    out = keras.layers.Dense(4)(subtracted)
    model = keras.models.Model(inputs=[input1, input2], outputs=out)
""" .

"DESCRIPTION.This code creates a neural network model using Keras, specifies the optimizer as stochastic gradient descent (sgd), and sets the loss function as Kullback-Leibler Divergence." <EXPLAINS> """CODE.model = keras.Model(inputs, outputs)
model.compile('sgd', loss=keras.losses.KLDivergence())
""" .

"DESCRIPTION.This code creates a neural network model using PaddlePaddle framework that takes an input array of shape [2, 3], fills it with value 2.0 using the full_like function, runs the model on a CPU device, feeds an input array of [[1, 2, 3], [4, 5, 6]] to the model, and retrieves the output array which is filled with 2.0 values. The output is printed as [array([[2., 2., 2.], [2., 2., 2.]], dtype=float32)]." <EXPLAINS> """CODE.import paddle
import paddle.fluid as fluid
import numpy as np
input = fluid.data(name='input', dtype='float32', shape=[2, 3])
output = fluid.layers.full_like(input, 2.0)
exe = fluid.Executor(fluid.CPUPlace())
exe.run(fluid.default_startup_program())
img=np.array([[1, 2, 3], [4, 5, 6]]).astype(np.float32)
res = exe.run(fluid.default_main_program(), feed={'input':img}, fetch_list=[output])
print(res) # [array([[2., 2., 2.], [2., 2., 2.]], dtype=float32)]""" .

"DESCRIPTION.This code creates a neural network model using PaddlePaddle to process tree-structured data. It defines the structure of the tree with given information, initializes parameters based on the tree structure, and runs the model to predict child nodes and leaf mask for input data." <EXPLAINS> """CODE.import paddle.fluid as fluid
import numpy as np
x = fluid.data(name="x", shape=[None, 1], dtype="int32", lod_level=1)
tree_info = [[0,0,0,1,2],
             [0,1,0,3,4],[0,1,0,5,6],
             [0,2,1,0,0],[1,2,1,0,0],[2,2,2,0,0],[3,2,2,0,0]]
tree_info_np = np.array(tree_info)
tree_info_np = np.reshape(tree_info_np, (7,5))
node_nums = 7
child_nums = 2
child, leaf_mask  = fluid.contrib.layers.tdm_child(x, node_nums, child_nums,
                        param_attr=fluid.ParamAttr(
                            initializer=fluid.initializer.NumpyArrayInitializer(
                                                                    tree_info_np)))
place = fluid.CPUPlace()
exe = fluid.Executor(place)
exe.run(fluid.default_startup_program())
xx = np.array([[2],[3]]).reshape((2,1)).astype("int32")
child_res, leaf_mask_res = exe.run(feed={"x":xx}, fetch_list=[child, leaf_mask])""" .

"DESCRIPTION.This code creates a neural network model using keras library, compiles the model with stochastic gradient descent optimizer, mean squared error loss, and specificity at sensitivity metric." <EXPLAINS> """CODE.model = keras.Model(inputs, outputs)
model.compile(
    'sgd',
    loss='mse',
    metrics=[keras.metrics.SpecificityAtSensitivity()])
""" .

"DESCRIPTION.This code creates a neural network model using the Keras framework, compiles it with stochastic gradient descent as the optimizer, mean squared error as the loss function, and SpecificityAtSensitivity as the metric." <EXPLAINS> """CODE.model = keras.Model(inputs, outputs)
model.compile(
    'sgd',
    loss='mse',
    metrics=[keras.metrics.SpecificityAtSensitivity()])
""" .

"DESCRIPTION.This code creates a neural network model with an embedding layer that takes an integer matrix of size (batch, input_length) as input. The embedding layer converts the integers into dense vectors of fixed size. The largest integer in the input should not exceed 999. After compiling the model with 'rmsprop' optimizer and 'mse' loss function, it predicts the output based on a randomly generated input array of shape (32, 10). The predicted output array should have a shape of (32, 10, 64)." <EXPLAINS> """CODE.model = Sequential()
model.add(Embedding(1000, 64, input_length=10))
# the model will take as input an integer matrix of size (batch,
input_length).
# the largest integer (i.e. word index) in the input should be no larger
than 999 (vocabulary size).
# now model.output_shape == (None, 10, 64), where None is the batch
dimension.

input_array = np.random.randint(1000, size=(32, 10))

model.compile('rmsprop', 'mse')
output_array = model.predict(input_array)
assert output_array.shape == (32, 10, 64)
""" .

"DESCRIPTION.This code creates a neural network model with an input data shape of [64, 784], a weight matrix of shape [784, 200], and a bias vector of shape [200]. It also retrieves a list of persistent variables in the default main program." <EXPLAINS> """CODE.import paddle.fluid as fluid
data = fluid.data(name="img", shape=[64, 784])
w = fluid.layers.create_parameter(shape=[784, 200], dtype='float32', name='fc_w')
b = fluid.layers.create_parameter(shape=[200], dtype='float32', name='fc_b')
list_para  = fluid.io.get_program_persistable_vars(  fluid.default_main_program() )""" .

"DESCRIPTION.This code creates a neural network model with an input layer of 32 units and a dense layer with 16 units using the softmax activation function." <EXPLAINS> """CODE.x = Input(shape=(32,))
y = Dense(16, activation='softmax')(x)
model = Model(x, y)
""" .

"DESCRIPTION.This code creates a neural network model with two fully connected layers. The first layer has 32 nodes and input shape of 500 dimensions. The second layer has 10 nodes with a softmax activation function. The model is compiled with the optimizer 'rmsprop', the loss function 'categorical_crossentropy', and the metric 'accuracy'." <EXPLAINS> """CODE.model = Sequential()
model.add(Dense(32, input_shape=(500,)))
model.add(Dense(10, activation='softmax'))
model.compile(optimizer='rmsprop',
              loss='categorical_crossentropy',
              metrics=['accuracy'])
""" .

"DESCRIPTION.This code creates a neural network model with two input layers, each followed by a dense layer with 8 neurons and ReLU activation function. The output of the two dense layers is added together using the Add layer, and then passed through another dense layer with 4 neurons. The final model takes two inputs and produces a single output." <EXPLAINS> """CODE.import keras
input1 = keras.layers.Input(shape=(16,))
x1 = keras.layers.Dense(8, activation='relu')(input1)
input2 = keras.layers.Input(shape=(32,))
x2 = keras.layers.Dense(8, activation='relu')(input2)
added = keras.layers.Add()([x1, x2])
out = keras.layers.Dense(4)(added)
model = keras.models.Model(inputs=[input1, input2], outputs=out)""" .

"DESCRIPTION.This code creates a new list by adding the element 3 at the beginning of the list [1, 2]." <EXPLAINS> "CODE.plist([1, 2]).cons(3)" .

"DESCRIPTION.This code creates a pandas DataFrame with 10 rows and 5 columns filled with random numbers, and then converts the DataFrame into an Apache Arrow table." <EXPLAINS> """CODE.df = pd.DataFrame(
...    np.random.randn(10, 5),
...    columns=("col %d" % i for i in range(5)))
...
st._arrow_table(df)""" .

"DESCRIPTION.This code creates a pandas DataFrame with columns of different data types including floats and booleans, and then selects and returns only the columns with float64 data type." <EXPLAINS> """CODE.df = pd.DataFrame({'a': np.random.randn(6).astype('f4'),
                    'b': [True, False] * 3,
                    'c': [1.0, 2.0] * 3})
df.select_dtypes(include=['float64'])
df.select_dtypes(exclude=['floating'])""" .

"DESCRIPTION.This code creates a pandas Series containing lists of integers, and then calculates the length of each list in the Series." <EXPLAINS> """CODE.import pyarrow as pa
import pandas as pd

s = pd.Series(
    [
        [1, 2, 3],
        [3],
    ],
    dtype=pd.ArrowDtype(pa.list_(
        pa.int64()
    ))
)

s.list.len()""" .

"DESCRIPTION.This code creates a pandas Series with integer values and corresponding index. It then groups the series by the first level of the index and checks if the grouped data is in monotonic increasing order." <EXPLAINS> """CODE.s = pd.Series([2, 1, 3, 4], index=['Falcon', 'Falcon', 'Parrot', 'Parrot'])
s.groupby(level=0).is_monotonic_increasing""" .

"DESCRIPTION.This code creates a pandas Series with values and corresponding datetime index, then resamples the series by month and calculates the product of each month." <EXPLAINS> """CODE.ser = pd.Series([1, 2, 3, 4], index=pd.DatetimeIndex(
                ['2023-01-01', '2023-01-15', '2023-02-01', '2023-02-15']))
ser.resample('MS').prod()""" .

"DESCRIPTION.This code creates a pandas Series with values and timestamps, then resamples the data to monthly frequency and calculates the sum for each month." <EXPLAINS> """CODE.ser = pd.Series([1, 2, 3, 4], index=pd.DatetimeIndex(
                ['2023-01-01', '2023-01-15', '2023-02-01', '2023-02-15']))
ser.resample('MS').sum()""" .

"DESCRIPTION.This code creates a placeholder tensor with shape (2, 4, 5) using Keras backend, and then creates a variable tensor initialized with the value `[[1, 2], [3, 4]]`. Finally, the code checks the number of dimensions of the input tensor and the variable tensor." <EXPLAINS> """CODE.from keras import backend as K
inputs = K.placeholder(shape=(2, 4, 5))
val = np.array([[1, 2], [3, 4]])
kvar = K.variable(value=val)
K.ndim(inputs)
K.ndim(kvar)
""" .

"DESCRIPTION.This code creates a placeholder tensor with shape (2, 4, 5), and then determines the shape of the tensor. Next, it creates a variable tensor with value [[1, 2], [3, 4]] and determines the shape of the variable tensor, which is (2, 2)." <EXPLAINS> """CODE.input = tf.keras.backend.placeholder(shape=(2, 4, 5))
tf.keras.backend.int_shape(input)
(2, 4, 5)
val = np.array([[1, 2], [3, 4]])
kvar = tf.keras.backend.variable(value=val)
tf.keras.backend.int_shape(kvar)
(2, 2)""" .

"DESCRIPTION.This code creates a progress bar that updates every 0.1 seconds until it reaches 100%." <EXPLAINS> """CODE.my_bar = st.progress(0)

for percent_complete in range(100):
...     time.sleep(0.1)
...     my_bar.progress(percent_complete + 1)""" .

"DESCRIPTION.This code creates a progress bar that updates incrementally from 0% to 100% with a delay of 0.1 seconds between each update." <EXPLAINS> """CODE.my_bar = st.progress(0)

for percent_complete in range(100):
...     time.sleep(0.1)
...     my_bar.progress(percent_complete + 1)""" .

"DESCRIPTION.This code creates a progress bar with 100 steps and updates it with a percentage value from 0 to 100. It pauses for 0.07 seconds between each update to simulate a loading process. Finally, it updates the progress bar to 100%." <EXPLAINS> """CODE.import time

pbar = NotebookProgressBar(100)
for val in range(100):
    pbar.update(val)
    time.sleep(0.07)
pbar.update(100)""" .

"DESCRIPTION.This code creates a reference to the variable x and then uses the reference to check if x is the same object as the dereferenced x_ref." <EXPLAINS> """CODE.x_ref = Reference(x)
print(x is x_ref.deref())
""" .

"DESCRIPTION.This code creates a repeat timer object with warmup time of 1 second, number of threads as 1, and number of runs as 6. It then iterates through the timer object to perform an operation. Finally, it prints the average time in milliseconds taken for the operation, along with the standard deviation of the time taken." <EXPLAINS> """CODE.timer = RepeatTimer(warmup=1, nt=1, runs=6)
for _ in timer:
    # perform operation
print(f"time={timer.get_ms():.1f} Â± {timer.get_ms_std():.1f} ms")""" .

"DESCRIPTION.This code creates a sequence of numbers ranging from 0 to 6 with a step size of 2 using the PaddlePaddle library in Python." <EXPLAINS> """CODE.import paddle.fluid as fluid
data = fluid.layers.arange(0, 10, 2, 'int32')

import paddle.fluid as fluid
with fluid.dygraph.guard():
    x = fluid.layers.arange(0, 6, 2)
    # x: [0, 2, 4]
    # x dtype: float32""" .

"DESCRIPTION.This code creates a simple RNN model in TensorFlow for sequence data, where the temperature data is used as input features. The RNN cell used is BasicRNNCell with a specified hidden size. The code processes the input sequence data through the RNN cell and outputs the final state of the RNN and the output sequence." <EXPLAINS> """CODE.temperature = sequence_numeric_column('temperature')
columns = [temperature]

features = tf.parse_example(..., features=make_parse_example_spec(columns))
input_layer, sequence_length = sequence_input_layer(features, columns)

rnn_cell = tf.nn.rnn_cell.BasicRNNCell(hidden_size)
outputs, state = tf.nn.dynamic_rnn(
    rnn_cell, inputs=input_layer, sequence_length=sequence_length)""" .

"DESCRIPTION.This code creates a sparse tensor placeholder with shape (2, 2) using the Keras backend, checks if it is a sparse tensor, converts it to a dense tensor, and then checks if the converted tensor is sparse." <EXPLAINS> """CODE.    from keras import backend as K
    b = K.placeholder((2, 2), sparse=True)
    print(K.is_sparse(b))
    c = K.to_dense(b)
    print(K.is_sparse(c))
""" .

"DESCRIPTION.This code creates a sparse tensor with shape (2, 2) using the Keras backend. It then checks if the tensor 'b' is sparse, converts it to a dense tensor and checks if the converted tensor 'c' is sparse." <EXPLAINS> """CODE.from keras import backend as K
b = K.placeholder((2, 2), sparse=True)
print(K.is_sparse(b))
c = K.to_dense(b)
print(K.is_sparse(c))
""" .

"DESCRIPTION.This code creates a synthetic Kitti dataset of images with specific dimensions and then loads the Kitti dataset for the training set." <EXPLAINS> """CODE.from examples import DATASETS_PATH
dataset_path = os.path.join(DATASETS_PATH, "Kitti")
_create_synth_kitti_dataset(dataset_path, image_dims=(1024, 512))
KITTI(dataset_path, 'train')
""" .

"DESCRIPTION.This code creates a tar.gz archive file named \"file.tar.gz\" and adds a nested directory \"logfiles\" to it. Within the \"logfiles\" directory, a subdirectory \"nested\" is created with the file \"file.txt\" added to it." <EXPLAINS> """CODE.with Archive("file.tar.gz") as archive:
    with archive.subdir("logfiles", root="/tmp/logs") as sd:
        # Will be added as `logfiles/nested/file.txt`
        sd.add("/tmp/logs/nested/file.txt")
""" .

"DESCRIPTION.This code creates a tensor `x` filled with ones of shape (2,3), initializes a Sigmoid transform `t`, computes the forward transformation of `x` using the Sigmoid transform, computes the inverse transformation of the forward transformation result, and computes the forward log determinant of the Jacobian for `x` using the Sigmoid transform." <EXPLAINS> """CODE.import paddle

x = paddle.ones((2,3))
t = paddle.distribution.SigmoidTransform()
print(t.forward(x))
print(t.inverse(t.forward(x)))
print(t.forward_log_det_jacobian(x))""" .

"DESCRIPTION.This code creates a tensor of shape (3,4) filled with ones using TensorFlow's Keras backend, and then evaluates and returns the value of the tensor." <EXPLAINS> """CODE.kvar = tf.keras.backend.ones((3,4))
tf.keras.backend.eval(kvar)""" .

"DESCRIPTION.This code creates a trace annotation with the label \"my_label\" and then calculates the dot product of x and its transpose using JAX, blocking until the result is ready." <EXPLAINS> """CODE.with jax.profiler.TraceAnnotation("my_label"):
    jnp.dot(x, x.T).block_until_ready()""" .

"DESCRIPTION.This code creates a tuple named 's1' containing the values 1, 2, 3, and 2." <EXPLAINS> """CODE.s1 = s(1, 2, 3, 2)
s1""" .

"DESCRIPTION.This code creates a variable \"kvar\" with random values of shape (2,3) using Keras backend, and then creates a new variable \"kvar_ones\" with the same shape as \"kvar\" filled with ones. Finally, it evaluates and returns the value of \"kvar_ones\"." <EXPLAINS> """CODE.from keras import backend as K
kvar = K.variable(np.random.random((2,3)))
kvar_ones = K.ones_like(kvar)
K.eval(kvar_ones)
""" .

"DESCRIPTION.This code creates a variable (kvar) with a 2x2 array of float32 data type using Keras backend, and then evaluates the variable using Keras." <EXPLAINS> """CODE.from keras import backend as K
kvar = K.variable(np.array([[1, 2], [3, 4]]), dtype='float32')
K.eval(kvar)
""" .

"DESCRIPTION.This code creates a worker server object with a specified port number and dispatcher address, and then joins the worker server to the dispatcher network." <EXPLAINS> """CODE.
worker_server = tf.data.experimental.service.WorkerServer(
    port=5051, dispatcher_address="grpc://localhost:5050")
worker_server.join()
""" .

"DESCRIPTION.This code creates an array with shape (4, 2) by distributing input data across devices and then combining them back into a single array." <EXPLAINS> """CODE.arrays = [
    jax.device_put(inp_data[index], d)
    for d, index in sharding.addressable_devices_indices_map(shape).items()]
arr = jax.make_array_from_single_device_arrays(shape, sharding, arrays)
arr.addressable_data(0).shape
(4, 2)
""" .

"DESCRIPTION.This code creates an empty optional tensor with a shape of () and data type of int32, and then checks if the optional tensor has a value." <EXPLAINS> """CODE.optional = tf.experimental.Optional.empty(
  tf.TensorSpec(shape=(), dtype=tf.int32, name=None))
print(optional.has_value())
""" .

"DESCRIPTION.This code creates an empty placeholder in Streamlit, then displays the text \"Hello world!\" and an image using the provided image bytes." <EXPLAINS> """CODE.my_placeholder = st.empty()
my_placeholder.text("Hello world!")
my_placeholder.image(my_image_bytes)""" .

"DESCRIPTION.This code creates an in-memory dataset using PaddlePaddle framework and sets the number of queues to 12." <EXPLAINS> """CODE.import paddle
paddle.enable_static()
dataset = paddle.distributed.InMemoryDataset()
dataset._set_queue_num(12)""",
        """CODE.import paddle.fluid as fluid
dataset = fluid.DatasetFactory().create_dataset("InMemoryDataset")
dataset.set_queue_num(12)""" .

"DESCRIPTION.This code creates an in-memory dataset using PaddlePaddle, loads data from \"a.txt\" and \"b.txt\" files into memory, preprocesses the instances in the dataset, and then trains a model using the dataset." <EXPLAINS> """CODE.import paddle.fluid as fluid
dataset = fluid.DatasetFactory().create_dataset("InMemoryDataset")
filelist = ["a.txt", "b.txt"]
dataset.set_filelist(filelist)
dataset.load_into_memory()
dataset.preprocess_instance()
exe.train_from_dataset(dataset)
dataset.postprocess_instance()""" .

"DESCRIPTION.This code creates an index table from a list of strings and then uses the table to lookup the indices of the given input strings." <EXPLAINS> """CODE.mapping_strings = t.constant(["emerson", "lake", "palmer")
table = tf.contrib.lookup.index_table_from_tensor(
    mapping=mapping_strings, num_oov_buckets=1, default_value=-1)
features = tf.constant(["emerson", "lake", "and", "palmer"])
ids = table.lookup(features)
...
tf.tables_initializer().run()

ids.eval()  ==> [0, 1, 4, 2]
""" .

"DESCRIPTION.This code creates an inference pipeline using a dataset from a directory. The pipeline is defined with different stages and parallelism levels, allowing for efficient inference computation. Lastly, the pipeline output is iteratively read and printed." <EXPLAINS> """CODE.# Create an inference pipeline.
ds = ray.data.read_binary_files(dir)
pipe = ds.pipeline(parallelism=10).map(infer)
DatasetPipeline(num_stages=2, length=40)

# The higher the stage parallelism, the shorter the pipeline.
pipe = ds.pipeline(parallelism=20).map(infer)
DatasetPipeline(num_stages=2, length=20)

# Outputs can be incrementally read from the pipeline.
for item in pipe.iter_rows():
    print(item)
""" .

"DESCRIPTION.This code creates an instance of LocalFS from the paddle.distributed.fleet.utils module and uses it to list directories in the current directory." <EXPLAINS> """CODE.from paddle.distributed.fleet.utils import LocalFS
client = LocalFS()
subdirs = client.list_dirs("./")""" .

"DESCRIPTION.This code creates an instance of a replay buffer with a capacity of 5. The replay buffer is used for storing and sampling past experiences for reinforcement learning algorithms." <EXPLAINS> """CODE.ReplayBuffer(5)  # doctest: +ELLIPSIS
<...reinforce_learn_Qnet.ReplayBuffer object at ...>""" .

"DESCRIPTION.This code creates an instance of the AffineScalar class with optional shift and scale parameters." <EXPLAINS> """CODE.b = AffineScalar()
b = AffineScalar(shift=[1., 2, 3])
b = AffineScalar(
  shift=[1., 2, 3],
  scale=2.)""" .

"DESCRIPTION.This code creates an object 'v1' and sets its values at indices 1, 3, and -1 to 4." <EXPLAINS> """CODE.v1 = v(1, 2, 3)
v1.set(1, 4)
v1.set(3, 4)
v1.set(-1, 4)""" .

"DESCRIPTION.This code creates an object v1 containing the values 1, 2, 3, 2, 1. Then it removes the value 1 from v1 and assigns the modified object to v2. Finally, it attempts to remove the value 1 from v2, but since value 1 has already been removed, this operation will raise an error." <EXPLAINS> """CODE.v1 = v(1, 2, 3, 2, 1)
v2 = v1.remove(1)
v2
v2.remove(1)""" .

"DESCRIPTION.This code creates and updates a dictionary of layers in a neural network model using the PaddlePaddle framework. The code includes layers such as Conv1D, Conv2D, Conv3D, and ReLU, and prints out the key-value pairs in the updated dictionary." <EXPLAINS> """CODE.import paddle
from collections import OrderedDict

sublayers = OrderedDict([
    ('conv1d', paddle.nn.Conv1D(3, 2, 3)),
    ('conv2d', paddle.nn.Conv2D(3, 2, 3)),
    ('conv3d', paddle.nn.Conv3D(4, 6, (3, 3, 3))),
])

new_sublayers = OrderedDict([
    ('relu', paddle.nn.ReLU()),
    ('conv2d', paddle.nn.Conv2D(4, 2, 4)),
])
layer_dict = paddle.nn.LayerDict(sublayers=sublayers)

layer_dict.update(new_sublayers)

for k, v in layer_dict.items():
    print(k, ":", v)
#conv1d : Conv1D(3, 2, kernel_size=[3], data_format=NCL)
#conv2d : Conv2D(4, 2, kernel_size=[4, 4], data_format=NCHW)
#conv3d : Conv3D(4, 6, kernel_size=[3, 3, 3], data_format=NCDHW)
#relu : ReLU()""" .

"DESCRIPTION.This code creates configurations for CLAP model using both text and audio input." <EXPLAINS> """CODE.from transformers import ClapConfig, ClapModel
configuration = ClapConfig()
model = ClapModel(configuration)
configuration = model.config
from transformers import ClapTextConfig, ClapAudioConfig
config_text = ClapTextConfig()
config_audio = ClapAudioConfig()
config = ClapConfig.from_text_audio_configs(config_text, config_audio)
""" .

"DESCRIPTION.This code creates input data (`x` and `y`) for a machine learning model using numpy arrays, and then sets up a data input function for TensorFlow session with specific parameters (batch size, shuffle, num_epochs)." <EXPLAINS> """CODE.age = np.arange(4) * 1.0
height = np.arange(32, 36)
x = {'age': age, 'height': height}
y = np.arange(-32, -28)

with tf.Session() as session:
  input_fn = numpy_io.numpy_input_fn(
      x, y, batch_size=2, shuffle=False, num_epochs=1)
""" .

"DESCRIPTION.This code creates instances of the Logistic distribution using TensorFlow and calculates the cumulative distribution function, probability density function, and generates random samples from the distribution for different parameters." <EXPLAINS> """CODE.dist = tf.contrib.distributions.Logistic(loc=0., scale=3.)
dist.cdf(1.)
dist = tf.contrib.distributions.Logistic(loc=[1, 2.], scale=[11, 22.])
dist.prob([0, 1.5])
dist.sample([3])
dist = tf.contrib.distributions.Logistic(loc=1., scale=[11, 22.])
dist.prob(3.0)
""" .

"DESCRIPTION.This code creates linear operators for lower-triangular matrices and performs operations such as converting to a dense matrix, calculating the shape, calculating the log determinant, and applying the operator to a tensor. Additionally, it demonstrates creating a batch of linear operators with specific shapes." <EXPLAINS> """CODE.# Create a 2 x 2 lower-triangular linear operator.
tril = [[1., 2.], [3., 4.]]
operator = LinearOperatorTriL(tril)

# The upper triangle is ignored.
operator.to_dense()
==> [[1., 0.]
     [3., 4.]]

operator.shape
==> [2, 2]

operator.log_determinant()
==> scalar Tensor

x = ... Shape [2, 4] Tensor
operator.apply(x)
==> Shape [2, 4] Tensor

# Create a [2, 3] batch of 4 x 4 linear operators.
tril = tf.random_normal(shape=[2, 3, 4, 4])
operator = LinearOperatorTriL(tril)
""" .

"DESCRIPTION.This code creates sparse columns using hash bucketing, crosses the two sparse features, initializes an SDCARegressor estimator with specified parameters, defines input functions for training, evaluation, and testing, fits the estimator on training data, evaluates it on evaluation data, and predicts scores on test data." <EXPLAINS> """CODE.sparse_column_a = sparse_column_with_hash_bucket(...)
sparse_column_b = sparse_column_with_hash_bucket(...)

sparse_feature_a_x_sparse_feature_b = crossed_column(...)

estimator = SDCARegressor(
    example_id_column='example_id',
    feature_columns=[sparse_column_a, sparse_feature_a_x_sparse_feature_b]),
    weight_column_name=...,
    l2_regularization=...,
    num_loss_partitions=...,
)

def input_fn_{train, eval}:
  ...

def input_fn_test:
  ...

estimator.fit(input_fn=input_fn_train)
estimator.evaluate(input_fn=input_fn_eval)
estimator.predict_scores(input_fn=input_fn_test) # returns predicted scores.""" .

"DESCRIPTION.This code creates subgroups for intra-machine communication, performs an allreduce operation within the machine using the created subgroups, and cleans up by destroying the process groups." <EXPLAINS> """CODE.# Create intra-machine subgroups.
cur_subgroup, subgroups = dist.new_subgroups()
# Allreduce within the machine.
rank = dist.get_rank()
tensor = torch.ones(1, device=rank) * rank
dist.all_reduce(tensor, group=cur_subgroup)
tensor
tensor([8])     # Assume 8 is the number of CUDA devices per machine.
# Cleanup.
for subgroup in subgroups:
    dist.destroy_process_group(subgroup)
""" .

"DESCRIPTION.This code creates tensors A, B, C, D, and E using torch, and then it uses the torch.block_diag function to concatenate them diagonally into a single tensor." <EXPLAINS> """CODE.import torch
A = torch.tensor([[0, 1], [1, 0]])
B = torch.tensor([[3, 4, 5], [6, 7, 8]])
C = torch.tensor(7)
D = torch.tensor([1, 2, 3])
E = torch.tensor([[4], [5], [6]])
torch.block_diag(A, B, C, D, E)""" .

"DESCRIPTION.This code creates three parameters with different shapes and then creates a tuple of these parameters. It then gets and flattens the shapes of the tuple and the first parameter." <EXPLAINS> """CODE.c = xc.XlaBuilder("example")
p0 = parameter(c, 1, xc.shape_from_pyval(jnp.ones([1])))
p1 = parameter(c, 2, xc.shape_from_pyval(jnp.ones([2])))
p2 = parameter(c, 3, xc.shape_from_pyval(jnp.ones([3]))
o = xops.Tuple(c, [p0, p1, p2])
flatten_shape(c.GetShape(o))
flatten_shape(c.GetShape(p0))""" .

"DESCRIPTION.This code creates two 2x2 placeholders in Keras and prints whether each of them is sparse or not." <EXPLAINS> """CODE.from keras import backend as K
a = K.placeholder((2, 2), sparse=False)
print(K.is_sparse(a))
b = K.placeholder((2, 2), sparse=True)
print(K.is_sparse(b))
""" .

"DESCRIPTION.This code creates two Counter objects 'c' and 'd' which count the occurrences of each character in the strings 'which' and 'watch', updates the counter objects with the characters from the string 'witch' and the Counter object 'd', and finally returns the count of the character 'h' in Counter object 'c', which is 4." <EXPLAINS> """CODE.c = Counter('which')
c.update('witch')
d = Counter('watch')
c.update(d)
c['h']
4""" .

"DESCRIPTION.This code creates two Namespaced Key-Value Stores, `store_ns1` and `store_ns2`, and allows them to store and retrieve key-value pairs within their respective namespaces. It demonstrates putting and getting values for the same key in different namespaces." <EXPLAINS> """CODE.store_ns1 = NamespacedKVStore(namespace="ns1")
store_ns2 = NamespacedKVStore(namespace="ns2")
store_ns1.put("same-key", 1)
store_ns1.get("same-key")
store_ns2.put("same-key", 2)
store_ns2.get("same-key", 2)""" .

"DESCRIPTION.This code creates two Version objects, compares them using comparison operators, and returns the results of the comparisons." <EXPLAINS> """CODE.v1 = Version("1.0a5")
v2 = Version("1.0")
v1
<Version('1.0a5')>
v2
<Version('1.0')>
v1 < v2
True
v1 == v2
False
v1 > v2
False
v1 >= v2
False
v1 <= v2
True""" .

"DESCRIPTION.This code creates two separate neural networks with one input layer and one hidden layer each, and then merges these two models by concatenating their output layers along axis 1." <EXPLAINS> """CODE.model1 = Sequential()
model1.add(Dense(32, input_dim=32))
model2 = Sequential()
model2.add(Dense(32, input_dim=32))
merged_model = Sequential()
merged_model.add(Merge([model1, model2], mode='concat', concat_axis=1))
""" .

"DESCRIPTION.This code creates two variables 'v' and 'w' within the variable scope \"foo\" and then retrieves the variable 'v' within the same scope, with reuse enabled." <EXPLAINS> """CODE.with tf.variable_scope("foo"):
    v = tf.get_variable("v", [1])  # v.name == "foo/v:0"
    w = tf.get_variable("w", [1])  # w.name == "foo/w:0"
with tf.variable_scope("foo", reuse=True):
    v1 = tf.get_variable("v")  # The same as v above.
""" .

"DESCRIPTION.This code crops the input tensor `x` along the spatial dimensions by discarding the specified number of rows and columns from the top, bottom, left, and right side of the tensor. The output tensor `y` has a shape of (2, 24, 20, 3)." <EXPLAINS> """CODE.input_shape = (2, 28, 28, 3)
x = np.arange(np.prod(input_shape)).reshape(input_shape)
y = tf.keras.layers.Cropping2D(cropping=((2, 2), (4, 4)))(x)
print(y.shape)
(2, 24, 20, 3)""" .

"DESCRIPTION.This code decomposes a spectrogram matrix S into two matrices W and H using the librosa library. It then sorts the values of W along different axes and extracts specific rows based on certain criteria. Finally, it sorts H based on the indices extracted from W sorting." <EXPLAINS> """CODE.W, H = librosa.decompose.decompose(S)
W_sort = librosa.util.axis_sort(W)
W_sort = librosa.util.axis_sort(W, value=np.argmin)
W_sort_rows = librosa.util.axis_sort(W, axis=0)
W_sort, idx = librosa.util.axis_sort(W, index=True)
H_sort = H[index, :]
""" .

"DESCRIPTION.This code defines a 3D max pooling layer with a pool size of 3x3x3 applied to the input tensor, resulting in an output tensor with a shape of (batch_size, 10, 10, 10, 3)." <EXPLAINS> """CODE.inputs = tf.keras.Input(shape=(depth, height, width, input_channels))
layer = tf.keras.layers.MaxPooling3D(pool_size=3)
outputs = layer(inputs)  # Shape: (batch_size, 10, 10, 10, 3)
""" .

"DESCRIPTION.This code defines a CLI command group using Click library in Python. It creates an option for the command to accept a name parameter and passes the context object. The function then sets the context object with a database connection resource based on the provided name." <EXPLAINS> """CODE.@click.group()
@click.option("--name")
@click.pass_context
def cli(ctx):
    ctx.obj = ctx.with_resource(connect_db(name))""" .

"DESCRIPTION.This code defines a Counter object with the attributes \"name\" and \"description\", and initializes it with the default tags. It then prints out the information of the Counter object, which includes the name, description, tag keys, and default tags." <EXPLAINS> """CODE.counter = Counter("name", description="desc")
print(counter.info)
{
    "name": "name",
    "description": "desc"
    "tag_keys": ("ray.key")
    "default_tags": {"ray.key": "abc"}
}""" .

"DESCRIPTION.This code defines a DataFrame 'df' with 10 rows and 2 columns filled with random numbers. It then evaluates the sum of columns 'a' and 'b' in the DataFrame. Finally, it creates a new column 'c' in the DataFrame which stores the sum of columns 'a' and 'b'." <EXPLAINS> """CODE.df = DataFrame(randn(10, 2), columns=list('ab'))
df.eval('a + b')
df.eval('c=a + b')""" .

"DESCRIPTION.This code defines a Discriminator model for image data with a shape of 1 channel and 28x28 pixels. The model is implemented using a Sequential neural network structure." <EXPLAINS> """CODE.Discriminator(img_shape=(1, 28, 28))
Discriminator(
  (model): Sequential(...)
)""" .

"DESCRIPTION.This code defines a Discriminator neural network with the specified input image shape of (1, 28, 28) and model architecture defined as a Sequential neural network." <EXPLAINS> """CODE.Discriminator(img_shape=(1, 28, 28))
Discriminator(
  (model): Sequential(...)
)""" .

"DESCRIPTION.This code defines a Features object with a single feature named 'x', which is a 2D array with a shape of (1, 3) and data type of 'int32'." <EXPLAINS> """CODE.from datasets import Features
features = Features({'x': Array2D(shape=(1, 3), dtype='int32')})
""" .

"DESCRIPTION.This code defines a Generative Adversarial Network (GAN) model with a generator and discriminator. The generator creates fake images, while the discriminator evaluates the authenticity of the images." <EXPLAINS> """CODE.GAN(img_shape=(1, 8, 8))
GAN(
  (generator): Generator(
    (model): Sequential(...)
  )
  (discriminator): Discriminator(
    (model): Sequential(...)
  )
)""" .

"DESCRIPTION.This code defines a Generative Adversarial Network (GAN) with a generator and discriminator models for generating and discriminating images respectively. The GAN is initialized with an image shape of 1 channel and 8x8 dimensions." <EXPLAINS> """CODE.GAN(img_shape=(1, 8, 8))
GAN(
  (generator): Generator(
    (model): Sequential(...)
  )
  (discriminator): Discriminator(
    (model): Sequential(...)
  )
)""" .

"DESCRIPTION.This code defines a Generative Adversarial Network (GAN) with a generator and discriminator models. The GAN is initialized with an image shape of 1x8x8." <EXPLAINS> """CODE.GAN(img_shape=(1, 8, 8))
GAN(
  (generator): Generator(
    (model): Sequential(...)
  )
  (discriminator): Discriminator(
    (model): Sequential(...)
  )
)""" .

"DESCRIPTION.This code defines a Generative Adversarial Network (GAN) with a specified image shape and includes a generator and a discriminator model within the GAN. The generator model generates fake images, while the discriminator model distinguishes between real and fake images." <EXPLAINS> """CODE.GAN(img_shape=(1, 8, 8))
GAN(
  (generator): Generator(
    (model): Sequential(...)
  )
  (discriminator): Discriminator(
    (model): Sequential(...)
  )
)""" .

"DESCRIPTION.This code defines a GraphQL input object type called GeoPoint with fields for latitude, longitude, and altitude. The latitude and longitude fields are required (non-null), while the altitude field has a default value of 0." <EXPLAINS> """CODE.class GeoPoint(GraphQLInputObjectType):
    name = 'GeoPoint'
    fields = {
        'lat': GraphQLInputObjectField(NonNullFloat),
        'lon': GraphQLInputObjectField(NonNullFloat),
        'alt': GraphQLInputObjectField(GraphQLFloat(),
            default_value=0)""" .

"DESCRIPTION.This code defines a GraphQL interface type named \"Entity\" with a field called \"name\" of type String." <EXPLAINS> """CODE.EntityType = GraphQLInterfaceType(
    name='Entity',
    fields={
        'name': GraphQLField(GraphQLString),
    })""" .

"DESCRIPTION.This code defines a GraphQL object type called 'Person' with two fields: 'parents' and 'children', both of type list of 'Person' objects." <EXPLAINS> """CODE.class PersonType(GraphQLObjectType):
    name = 'Person'

    def get_fields(self):
        return {
            'parents': GraphQLField(GraphQLList(PersonType())),
            'children': GraphQLField(GraphQLList(PersonType())),
        }""" .

"DESCRIPTION.This code defines a GraphQL schema with two object types: Address and Person. The Address object type has fields for street (string), number (integer), and formatted (string), which concatenates the number and street fields. The Person object type has a field for name (string) and a field for bestFriend that references another Person object." <EXPLAINS> """CODE.AddressType = GraphQLObjectType('Address', {
    'street': GraphQLField(GraphQLString),
    'number': GraphQLField(GraphQLInt),
    'formatted': GraphQLField(GraphQLString,
        resolver=lambda obj, args, context, info: obj.number + ' ' + obj.street),
})

PersonType = GraphQLObjectType('Person', lambda: {
    'name': GraphQLField(GraphQLString),
    'bestFriend': GraphQLField(PersonType)
})""" .

"DESCRIPTION.This code defines a LightningModule called LitModel and a Trainer object. It uses the TrainsLogger to log training progress and metrics during the training process. The Trainer object is initialized with the TrainsLogger for logging purposes." <EXPLAINS> """CODE.from pytorch_lightning import Trainer
from pytorch_lightning.loggers import TrainsLogger
trains_logger = TrainsLogger(
    project_name='pytorch lightning',
    task_name='default',
    output_uri='.',
)
trainer = Trainer(logger=trains_logger)

from pytorch_lightning import LightningModule
class LitModel(LightningModule):
    def training_step(self, batch, batch_idx):
        # example
        self.logger.experiment.whatever_trains_supports(...)

    def any_lightning_module_function_or_hook(self):
        self.logger.experiment.whatever_trains_supports(...)""" .

"DESCRIPTION.This code defines a LinearOperatorMatrix class that allows the user to create linear operators from matrices. The class provides methods to convert the operator to a dense matrix, retrieve the shape of the operator, calculate the log determinant of the operator, and apply the operator to a given input tensor. The code also shows an example of creating a batch of linear operators from randomly generated matrices." <EXPLAINS> """CODE.# Create a 2 x 2 linear operator.
matrix = [[1., 2.], [3., 4.]]
operator = LinearOperatorMatrix(matrix)

operator.to_dense()
==> [[1., 2.]
     [3., 4.]]

operator.shape
==> [2, 2]

operator.log_determinant()
==> scalar Tensor

x = ... Shape [2, 4] Tensor
operator.apply(x)
==> Shape [2, 4] Tensor

# Create a [2, 3] batch of 4 x 4 linear operators.
matrix = tf.random_normal(shape=[2, 3, 4, 4])
operator = LinearOperatorMatrix(matrix)
""" .

"DESCRIPTION.This code defines a Mish activation function using the PaddlePaddle framework. The input data 'x_data' is created as a numpy array and reshaped. Then, the Mish activation function is applied to the input data 'x'. The code sets up the execution environment, runs the computation using a CPU or a CUDA place, and finally prints the output after running the computation." <EXPLAINS> """CODE.import paddle.fluid as fluid
import numpy as np

DATATYPE='float32'

x_data = np.array([i for i in range(1,5)]).reshape([1,1,4]).astype(DATATYPE)

x = fluid.data(name="x", shape=[None,1,4], dtype=DATATYPE)
y = fluid.layers.mish(x)

place = fluid.CPUPlace()
# place = fluid.CUDAPlace(0)
exe = fluid.Executor(place)
out, = exe.run(feed={'x':x_data}, fetch_list=[y.name])
print(out)  # [[0.66666667, 1.66666667, 3., 4.]]
""" .

"DESCRIPTION.This code defines a Poisson-lognormal quadrature compound distribution with specified parameters." <EXPLAINS> """CODE.ds = tf.contrib.distributions
pln = ds.PoissonLogNormalQuadratureCompound(
    loc=[0., -0.5],
    scale=1.,
    quadrature_polynomial_degree=10,
    validate_args=True)
""" .

"DESCRIPTION.This code defines a RootFlow class that contains two CounterWork instances in a dictionary. The CounterWork class increments a counter value by 1 each time its run method is called. After creating an instance of RootFlow and calling its run method, it asserts that the counter value of the first CounterWork instance in the dictionary is equal to 1." <EXPLAINS> """CODE.from lightning_app import LightningFlow, LightningWork
from lightning_app.core import Dict

class CounterWork(LightningWork):
    def __init__(self):
        super().__init__()
        self.counter = 0
    def run(self):
        self.counter += 1

class RootFlow(LightningFlow):
    def __init__(self):
        super().__init__()
        self.dict = Dict(**{"work_0": CounterWork(), "work_1": CounterWork()})
    def run(self):
        for work_name, work in self.dict.items():
            work.run()

flow = RootFlow()
flow.run()
assert flow.dict["work_0"].counter == 1
""" .

"DESCRIPTION.This code defines a TensorFlow session and creates a variable with a specific value. It also creates a placeholder tensor with a certain shape. The code then prints the shape of the variable and the placeholder tensor, and evaluates these shapes using the TensorFlow session." <EXPLAINS> """CODE.from keras import backend as K
import numpy as np

tf_session = K.get_session()
val = np.array([[1, 2], [3, 4]])
kvar = K.variable(value=val)
inputs = keras.backend.placeholder(shape=(2, 4, 5))
print(K.shape(kvar))
print(K.shape(inputs))
print(K.shape(kvar).eval(session=tf_session))
print(K.shape(inputs).eval(session=tf_session)
""" .

"DESCRIPTION.This code defines a TensorFlow session, creates a TensorFlow variable with a specified value, creates a placeholder for input data, and then retrieves the shapes of the variable and input data using TensorFlow backend operations." <EXPLAINS> """CODE.    from keras import backend as K
    tf_session = K.get_session()
    val = np.array([[1, 2], [3, 4]])
    kvar = K.variable(value=val)
    inputs = keras.backend.placeholder(shape=(2, 4, 5))
    K.shape(kvar)
    K.shape(inputs)
    K.shape(kvar).eval(session=tf_session)
    K.shape(inputs).eval(session=tf_session)
""" .

"DESCRIPTION.This code defines a Trainer class with a constructor that takes a config dictionary as input. It also has a method train_epoch that performs some training operation and returns 1. The code then creates an instance of Trainer, distributes the training to multiple workers using the Ray framework, and asserts that the training returns the expected output." <EXPLAINS> """CODE.class Trainer:
    def __init__(self, config):
        self.config = config

    def train_epoch(self):
        ...
        return 1

config = {"lr": 0.1}
trainer = Trainer(num_workers=2, backend="torch")
workers = trainer.to_worker_group(train_cls=Trainer, config=config)
futures = [w.train_epoch.remote() for w in workers]
assert ray.get(futures) == [1, 1]
assert ray.get(workers[0].train_epoch.remote()) == 1
workers.shutdown()
""" .

"DESCRIPTION.This code defines a Version class that takes a version number as input and allows comparisons between different versions based on their values." <EXPLAINS> """CODE.v1 = Version("1.0a5")
v2 = Version("1.0")
v1
<Version('1.0a5')>
v2
<Version('1.0')>
v1 < v2
True
v1 == v2
False
v1 > v2
False
v1 >= v2
False
v1 <= v2
True""" .

"DESCRIPTION.This code defines a `BaseSparsifier` class that initializes with a model, configuration settings, and default parameters. It includes a method `update_mask` that computes a new mask for all keys in the `module_groups`. An instance of `BaseSparsifier` is then created with the provided configuration and default parameters." <EXPLAINS> """CODE.class BaseSparsifier:
    def __init__(self, model, config, defaults):
        self.model = model
        self.config = config
        self.defaults = defaults

    def update_mask(self):
        # Function to compute a new mask for all keys in the `module_groups`
        pass

# Example code
config = [model.layer1, {'module': model.linear2, 'sparsity_level': 0.5}]
defaults = {'sparsity_level': 0.7}
sparsifier = BaseSparsifier(config, defaults)

class BaseSparsifier:
    def __init__(self, model, config, defaults):
        self.model = model
        self.config = config
        self.defaults = defaults

    def update_mask(self):
        # Function to compute a new mask for all keys in the `module_groups`
        pass

# Example code
config = [model.layer1, {'module': model.linear2, 'sparsity_level': 0.5}]
defaults = {'sparsity_level': 0.7}
sparsifier = BaseSparsifier(config, defaults)
""" .

"DESCRIPTION.This code defines a `Markup` class that marks a string as safe for inclusion in HTML/XML output without needing to be escaped. The class implements the `__html__` interface and ensures that any arguments passed are escaped. The `escape` function prevents double escaping by returning markup objects. The `Markup` class constructor can be used with Unicode objects, objects with an HTML representation, or objects that are converted into Unicode strings and assumed to be safe. Operations on a markup string are markup aware, ensuring that all arguments are passed through the `escape` function." <EXPLAINS> """CODE.Marks a string as being safe for inclusion in HTML/XML output without
needing to be escaped.  This implements the `__html__` interface a couple
of frameworks and web applications use.  :class:`Markup` is a direct
subclass of `unicode` and provides all the methods of `unicode` just that
it escapes arguments passed and always returns `Markup`.

The `escape` function returns markup objects so that double escaping can't
happen.

The constructor of the :class:`Markup` class can be used for three
different things:  When passed an unicode object it's assumed to be safe,
when passed an object with an HTML representation (has an `__html__`
method) that representation is used, otherwise the object passed is
converted into a unicode string and then assumed to be safe:

Markup("Hello <em>World</em>!")
Markup(u'Hello <em>World</em>!')
class Foo(object):
...  def __html__(self):
...   return '<a href="#">foo</a>'
...
Markup(Foo())
Markup(u'<a href="#">foo</a>')

If you want object passed being always treated as unsafe you can use the
:meth:`escape` classmethod to create a :class:`Markup` object:

Markup.escape("Hello <em>World</em>!")
Markup(u'Hello &lt;em&gt;World&lt;/em&gt;')

Operations on a markup string are markup aware which means that all
arguments are passed through the :func:`escape` function:

em = Markup("<em>%s</em>")
em % "foo & bar"
Markup(u'<em>foo &amp; bar</em>')
strong = Markup("<strong>%(text)s</strong>")
strong % {'text': '<blink>hacker here</blink>'}
Markup(u'<strong>&lt;blink&gt;hacker here&lt;/blink&gt;</strong>')
Markup("<em>Hello</em> ") + "<foo>"
Markup(u'<em>Hello</em> &lt;foo&gt;')""" .

"DESCRIPTION.This code defines a backbone model consisting of two linear layers." <EXPLAINS> """CODE.Backbone()
Backbone(
  (l1): Linear(...)
  (l2): Linear(...)
)""" .

"DESCRIPTION.This code defines a basic autoencoder model in Python." <EXPLAINS> """CODE.LitAutoEncoder()  # doctest: +ELLIPSIS +NORMALIZE_WHITESPACE
LitAutoEncoder(
  (encoder): ...
  (decoder): ...
)""" .

"DESCRIPTION.This code defines a batch processing function that converts a list of strings to lowercase. It uses the serve.batch decorator to specify a maximum batch size of 50 and a batch wait timeout of 0.5 seconds." <EXPLAINS> """CODE.@serve.batch(max_batch_size=50, batch_wait_timeout_s=0.5)
async def handle_batch(self, batch: List[str]):
    return [s.lower() for s in batch]""" .

"DESCRIPTION.This code defines a bijector to calculate the absolute value of a given input. It provides methods to forward the input, inverse the input, and calculate the logarithm of the absolute value of the determinant of the Jacobian for the input." <EXPLAINS> """CODE.abs = ds.bijectors.AbsoluteValue()

abs.forward([-1., 0., 1.])
==> [1., 0.,  1.]

abs.inverse(1.)
==> [-1., 1.]

# The |dX/dY| is constant, == 1.  So Log|dX/dY| == 0.
abs.inverse_log_det_jacobian(1.)
==> [0., 0.]

# Special case handling of 0.
abs.inverse(0.)
==> [0., 0.]

abs.inverse_log_det_jacobian(0.)
==> [0., 0.]
""" .

"DESCRIPTION.This code defines a boolean state 'enable_foo' with a default value of False. It then sets 'enable_foo' to True and executes the code block inside the 'with' statement." <EXPLAINS> """CODE.enable_foo = config.define_bool_state(
    name='jax_enable_foo',
    default=False,
    help='Enable foo.')

with enable_foo(True):
    ...
""" .

"DESCRIPTION.This code defines a boolean state called \"jax_enable_foo\" with a default value of False, and it provides the option to enable the \"foo\" function when setting enable_foo to True." <EXPLAINS> """CODE.enable_foo = config.define_bool_state(
    name='jax_enable_foo',
    default=False,
    help='Enable foo.')

with enable_foo(True):
    ...
""" .

"DESCRIPTION.This code defines a boolean state named 'jax_enable_foo' with a default value of False and provides a help message indicating that it enables 'foo'." <EXPLAINS> """CODE.config.define_bool_state(
      name='jax_enable_foo',
      default=False,
      help='Enable foo.'
)""" .

"DESCRIPTION.This code defines a cache system that allows getting the value associated with a key or setting a key-value pair in the cache." <EXPLAINS> """CODE.cache.get(key, default)
cache.set(key, value)""" .

"DESCRIPTION.This code defines a class A with a method x that returns the value 10, with memoization implemented using underscore_memoization decorator." <EXPLAINS> """CODE.class A(object):
    @underscore_memoization
    def x(self):
        return 10""" .

"DESCRIPTION.This code defines a class AClass with a single field x. It creates an instance of AClass with x set to 1, creates two more instances using the set method to update the value of x to 2 and 3 respectively, and prints out the values of x for each instance of AClass." <EXPLAINS> """CODE.from pyrsistent import PClass, field
class AClass(PClass):
...     x = field()
...
a = AClass(x=1)
a2 = a.set(x=2)
a3 = a.set('x', 3)
a
AClass(x=1)
a2
AClass(x=2)
a3
AClass(x=3)""" .

"DESCRIPTION.This code defines a class Example that has a method named apply. Inside the apply method, an exponentially moving average (ema) is calculated using the inputs passed to the method and a decay factor. The ema value is updated by applying the decay factor to the current ema value and adding the difference between the inputs and the current ema value. Finally, the method returns the original inputs." <EXPLAINS> """CODE.class Example(nn.Module):
    def apply(self, inputs, decay=0.9):
      ema = self.state('ema', inputs.shape, initializers.zeros)
      ema.value = decay * ema.value + (1 - decay) * inputs
      return inputs
""" .

"DESCRIPTION.This code defines a class Foo with class methods `with_callable_args` and `with_args`, which are used to create instances of Foo with specific arguments. Two instances `foo_instance1` and `foo_instance2` with the same creation time are created using the `foo_builder` object." <EXPLAINS> """CODE.Foo.with_callable_args = classmethod(_with_callable_args)
Foo.with_args = classmethod(_with_args)
foo_builder = Foo.with_callable_args(cur_time=get_time_func).with_args(name="dan")
foo_instance1 = foo_builder()
wait 50
foo_instance2 = foo_builder()
id(foo_instance1.creation_time) == id(foo_instance2.creation_time)
""" .

"DESCRIPTION.This code defines a class MeanSquaredError that inherits from the Loss class. It has a method call that calculates the mean squared error between the predicted values y_pred and the actual values y_true." <EXPLAINS> """CODE.class MeanSquaredError(Loss):
    def call(self, y_true, y_pred):
        y_pred = ops.convert_to_tensor(y_pred)
        y_true = math_ops.cast(y_true, y_pred.dtype)
        return K.mean(math_ops.square(y_pred - y_true), axis=-1)
""" .

"DESCRIPTION.This code defines a class Reference with a constructor that takes an object as input. The class also defines an equality method that checks if the object being referred to is the same. It creates instances of the Reference class referencing lists x and y, and then compares these instances using the equality method." <EXPLAINS> """CODE.x = [1]
y = [1]

class Reference:
    def __init__(self, obj):
        self.obj = obj

    def __eq__(self, other):
        return self.obj is other.obj

x_ref1 = Reference(x)
x_ref2 = Reference(x)
y_ref2 = Reference(y)

print(x_ref1 == x_ref2)
print(x_ref1 == y)
""" .

"DESCRIPTION.This code defines a class `Experiment` with an enumerated constant `EXP_A`. It also defines a class `MyBuilder` with a constant `VERSION` which includes experiment `EXP_A` with the value `True`." <EXPLAINS> """CODE.class Experiment(enum.Enum):
  EXP_A = enum.auto()  # Short description of experiment.

class MyBuilder(...):
  VERSION = tfds.core.Version('1.2.3', experiments={
      tfds.core.Experiment.EXP_A: True,
      })""" .

"DESCRIPTION.This code defines a class `SimpleDataFetcher` that inherits from `AbstractDataFetcher`. It contains a method `fetching_function` that fetches data from a `dataloader_iter` object using a `while True` loop. The function returns the next data element from `dataloader_iter` along with a boolean value indicating whether there are more elements to fetch. If `StopIteration` is raised, it returns `None` along with a boolean value indicating that there are no more elements to fetch." <EXPLAINS> """CODE.class SimpleDataFetcher(AbstractDataFetcher):
    def fetching_function(self):
        while True:
            try:
                return next(self.dataloader_iter), False
            except StopIteration:
                return None, True""" .

"""DESCRIPTION.This code defines a class called CollatorIterDataPipe, which is an Iterable DataPipe used to collate samples from another Iterable DataPipe into Tensors by default or using a custom collate function. The collate function can collect and combine data or a batch of data. The class takes in the original datapipe, the collate function, positional arguments for the collate function, and keyword arguments for the collate function.

The example provided demonstrates converting integer data to float Tensors by creating a custom Iterable DataPipe class called MyIterDataPipe and then using CollateIterDataPipe with a custom collate function to convert the data.""" <EXPLAINS> """CODE.:class:`CollatorIterDataPipe`.

Iterable DataPipe to collate samples from datapipe to Tensor(s) by `util_.collate.default_collate`,
or customized Data Structure by collate_fn.

Args:
    datapipe: Iterable DataPipe being collated
    collate_fn: Customized collate function to collect and combine data or a batch of data.
        Default function collates to Tensor(s) based on data type.
    fn_args: Positional arguments for `collate_fn`
    fn_kwargs: Keyword arguments for `collate_fn`

Example: Convert integer data to float Tensor
    class MyIterDataPipe(torch.utils.data.IterDataPipe):
    ...     def __init__(self, start, end):
    ...         super(MyIterDataPipe).__init__()
    ...         assert end > start, "this example code only works with end >= start"
    ...         self.start = start
    ...         self.end = end
    ...
    ...     def __iter__(self):
    ...         return iter(range(self.start, self.end))
    ...
    ...     def __len__(self):
    ...         return self.end - self.start
    ...
    ds = MyIterDataPipe(start=3, end=7)
    print(list(ds))
    [3, 4, 5, 6]

    def collate_fn(batch):
    ...     return torch.tensor(batch, dtype=torch.float)
    ...
    collated_ds = CollateIterDataPipe(ds, collate_fn=collate_fn)
    print(list(collated_ds))
    [tensor(3.), tensor(4.), tensor(5.), tensor(6.)]""" .

"DESCRIPTION.This code defines a class called DiscreteActions that is a subclass of gym.ActionWrapper. It takes an environment and a list of discrete actions mapped to continuous actions as input. It initializes the action space with the length of the discrete actions list. The action method maps a discrete action to a continuous action based on the input mapping. In the main block, it creates an environment instance, wraps it with the DiscreteActions class, and prints the action space which is Discrete(4)." <EXPLAINS> """CODE.class DiscreteActions(gym.ActionWrapper):
    def __init__(self, env, disc_to_cont):
        super().__init__(env)
        self.disc_to_cont = disc_to_cont
        self.action_space = Discrete(len(disc_to_cont))

    def action(self, act):
        return self.disc_to_cont[act]

if __name__ == "__main__":
    env = gym.make("LunarLanderContinuous-v2")
    wrapped_env = DiscreteActions(env, [np.array([1,0]), np.array([-1,0]),
                                        np.array([0,1]), np.array([0,-1])])
    print(wrapped_env.action_space)         #Discrete(4)
""" .

"DESCRIPTION.This code defines a class called Flow that inherits from LightningFlow. It initializes an empty list called names. It has a method called configure_commands that returns a dictionary mapping a command name 'my_command_name' to a method my_remote_method. The my_remote_method takes a name as input and appends it to the names list." <EXPLAINS> """CODE.class Flow(LightningFlow):
    def __init__(self):
        super().__init__()
        self.names = []

    def configure_commands(self):
        return {"my_command_name": self.my_remote_method}

    def my_remote_method(self, name):
        self.names.append(name)
""" .

"DESCRIPTION.This code defines a class called MapOperator that inherits from PhysicalOperator. It has methods to add input tasks, check if there are any active tasks ready, and get the next ready task." <EXPLAINS> """CODE.class MapOperator(PhysicalOperator):
    def __init__(self):
        self.active_tasks = []

    def add_input(self, refs, _):
        self.active_tasks.append(map_task.remote(refs))

    def has_next(self):
        ready, _ = ray.wait(self.active_tasks, timeout=0)
        return len(ready) > 0

    def get_next(self):
        ready, remaining = ray.wait(self.active_tasks, num_returns=1)
        self.active_tasks = remaining
        return ready[0]""" .

"DESCRIPTION.This code defines a class called MetaEst with a constructor that initializes a sub_est attribute. It also has a method predict that calls the predict method of the sub_est attribute." <EXPLAINS> """CODE.class MetaEst(object):
    def __init__(self, sub_est):
        self.sub_est = sub_est

    @if_delegate_has_method(delegate='sub_est')
    def predict(self, X):
        return self.sub_est.predict(X)

class HasPredict(object):
    def predict(self, X):
        return X.sum(axis=1)

class HasNoPredict(object):
    pass
""" .

"DESCRIPTION.This code defines a class called PositivePoint that represents a point with positive x and y coordinates. It utilizes immutable to create an immutable class with x and y attributes. The class also contains a __new__ method that checks if the x and y coordinates are positive and raises an exception if they are not." <EXPLAINS> """CODE.class PositivePoint(immutable('x, y')):
    __slots__ = tuple()
    def __new__(cls, x, y):
        if x > 0 and y > 0:
            return super(PositivePoint, cls).__new__(cls, x, y)
        raise Exception('Coordinates must be positive!')""" .

"DESCRIPTION.This code defines a class called PositivePoint which represents a point with positive coordinates. It specifies that the x and y coordinates of the point must be greater than 0, otherwise it raises an Exception stating that the coordinates must be positive." <EXPLAINS> """CODE.class PositivePoint(pclass('x, y')):
    __slots__ = tuple()
    def __new__(cls, x, y):
        if x > 0 and y > 0:
            return super(PositivePoint, cls).__new__(cls, x, y)
        raise Exception('Coordinates must be positive!')""" .

"DESCRIPTION.This code defines a class called Positives that inherits from CheckedPSet. It specifies that the elements of the class must be of type long or int and ensures that all elements are non-negative. An instance of the Positives class is then created with the elements [1, 2, 3]." <EXPLAINS> """CODE.class Positives(CheckedPSet):
    __type__ = (long, int)
    __invariant__ = lambda n: (n >= 0, 'Negative')

Positives([1, 2, 3])""" .

"DESCRIPTION.This code defines a class called ReplayBuffer with a capacity of 5." <EXPLAINS> "CODE.ReplayBuffer(5)" .

"DESCRIPTION.This code defines a class called RootFlow that inherits from LightningFlow. It initializes a counter attribute to 0 and has a method called run that increments the counter by 1. An instance of RootFlow is created and its run method is called. Two assertions are made to check if the counter attribute is incremented correctly." <EXPLAINS> """CODE.from lightning import LightningFlow
class RootFlow(LightningFlow):
    def __init__(self):
        super().__init__()
        self.counter = 0
    def run(self):
        self.counter += 1

flow = RootFlow()
flow.run()
assert flow.counter == 1
assert flow.state["vars"]["counter"] == 1
""" .

"DESCRIPTION.This code defines a class called Special with attributes x and y. It provides methods for flattening and unflattening the class instance into a tree structure." <EXPLAINS> """CODE.@register_pytree_with_keys_class
class Special:
    def __init__(self, x, y):
        self.x = x
        self.y = y
    def tree_flatten_with_keys(self):
        return (((GetAttrKey('x'), self.x), (GetAttrKey('y'), self.y)), None)
    @classmethod
    def tree_unflatten(cls, aux_data, children):
        return cls(*children)""" .

"DESCRIPTION.This code defines a class called Special with two attributes x and y. It also includes methods tree_flatten and tree_unflatten for serialization and deserialization purposes." <EXPLAINS> """CODE.@register_pytree_node_class
class Special:
    def __init__(self, x, y):
        self.x = x
        self.y = y
    def tree_flatten(self):
        return ((self.x, self.y), None)
    @classmethod
    def tree_unflatten(cls, aux_data, children):
        return cls(*children)""" .

"DESCRIPTION.This code defines a class method that constructs an instance of the calling class from a string input that matches a specific pattern. If the string matches the pattern, it creates an instance of the class with named arguments extracted from the string, otherwise it raises a TypeError." <EXPLAINS> """CODE.@classmethod
def construct_from_string(cls, string):
    pattern = re.compile(r"^my_type\\[(?P<arg_name>.+)\\]$")
    match = pattern.match(string)
    if match:
        return cls(**match.groupdict())
    else:
        raise TypeError(
            f"Cannot construct a '{cls.__name__}' from '{string}'"
        )""" .

"DESCRIPTION.This code defines a class method that implements a double dense layer neural network using TensorFlow. The method initializes the weights and biases of two dense layers with random normal initializers and applies L2 regularization to the kernel weights. The method performs matrix multiplication and bias addition operations for each dense layer to transform the input data." <EXPLAINS> """CODE.@tf.compat.v1.keras.utils.track_tf1_style_variables
def call(self, inputs):
    with tf.compat.v1.variable_scope("double_dense_layer"):
        out = tf.compat.v1.layers.dense(
            inputs, self.units, name="dense_one",
            kernel_initializer=tf.compat.v1.random_normal_initializer,
            kernel_regularizer="l2")
        out = tf.compat.v1.layers.dense(
            out, self.units, name="dense_two",
            kernel_initializer=tf.compat.v1.random_normal_initializer(),
            kernel_regularizer="l2")
    return out


@tf.compat.v1.keras.utils.track_tf1_style_variables
def call(self, inputs):
    out = inputs
    with tf.compat.v1.variable_scope("double_dense_layer"):
        with tf.compat.v1.variable_scope("dense_one"):
            kernel = tf.compat.v1.get_variable(
                shape=[out.shape[-1], self.units],
                regularizer=regularizers.L2(),
                initializer=init_ops.ones_initializer(),
                name="kernel")
            bias = tf.compat.v1.get_variable(
                shape=[self.units,],
                initializer=init_ops.zeros_initializer(),
                name="bias")
            out = tf.compat.v1.math.matmul(out, kernel)
            out = tf.compat.v1.nn.bias_add(out, bias)
        with tf.compat.v1.variable_scope("dense_two"):
            kernel = tf.compat.v1.get_variable(
                shape=[out.shape[-1], self.units],
                regularizer=regularizers.L2(),
                initializer=init_ops.ones_initializer(),
                name="kernel")
            bias = tf.compat.v1.get_variable(
                shape=[self.units,],
                initializer=init_ops.zeros_initializer(),
                name="bias")
            out = tf.compat.v1.math.matmul(out, kernel)
            out = tf.compat.v1.nn.bias_add(out, bias)
    return out

@tf.compat.v1.keras.utils.track_tf1_style_variables
def call(self, inputs):
    with tf.compat.v1.variable_scope("double_dense_layer"):
        out = tf.compat.v1.layers.dense(
            inputs, self.units, name="dense_one",
            kernel_initializer=tf.compat.v1.random_normal_initializer,
            kernel_regularizer="l2")
        out = tf.compat.v1.layers.dense(
            out, self.units, name="dense_two",
            kernel_initializer=tf.compat.v1.random_normal_initializer(),
            kernel_regularizer="l2")
    return out

@tf.compat.v1.keras.utils.track_tf1_style_variables
def call(self, inputs):
    out = inputs
    with tf.compat.v1.variable_scope("double_dense_layer"):
        with tf.compat.v1.variable_scope("dense_one"):
            kernel = tf.compat.v1.get_variable(
                shape=[out.shape[-1], self.units],
                regularizer=regularizers.L2(),
                initializer=init_ops.ones_initializer(),
                name="kernel")
            bias = tf.compat.v1.get_variable(
                shape=[self.units,],
                initializer=init_ops.zeros_initializer(),
                name="bias")
            out = tf.compat.v1.math.matmul(out, kernel)
            out = tf.compat.v1.nn.bias_add(out, bias)
        with tf.compat.v1.variable_scope("dense_two"):
            kernel = tf.compat.v1.get_variable(
                shape=[out.shape[-1], self.units],
                regularizer=regularizers.L2(),
                initializer=init_ops.ones_initializer(),
                name="kernel")
            bias = tf.compat.v1.get_variable(
                shape=[self.units,],
                initializer=init_ops.zeros_initializer(),
                name="bias")
            out = tf.compat.v1.math.matmul(out, kernel)
            out = tf.compat.v1.nn.bias_add(out, bias)
    return out

@tf.compat.v1.keras.utils.track_tf1_style_variables
def call(self, inputs):
    with tf.compat.v1.variable_scope("double_dense_layer"):
        out = tf.compat.v1.layers.dense(
            inputs, self.units, name="dense_one",
            kernel_initializer=tf.compat.v1.random_normal_initializer,
            kernel_regularizer="l2")
        out = tf.compat.v1.layers.dense(
            out, self.units, name="dense_two",
            kernel_initializer=tf.compat.v1.random_normal_initializer(),
            kernel_regularizer="l2")
    return out

@tf.compat.v1.keras.utils.track_tf1_style_variables
def call(self, inputs):
    out = inputs
    with tf.compat.v1.variable_scope("double_dense_layer"):
        with tf.compat.v1.variable_scope("dense_one"):
            kernel = tf.compat.v1.get_variable(
                shape=[out.shape[-1], self.units],
                regularizer=regularizers.L2(),
                initializer=init_ops.ones_initializer(),
                name="kernel")
            bias = tf.compat.v1.get_variable(
                shape=[self.units,],
                initializer=init_ops.zeros_initializer(),
                name="bias")
            out = tf.compat.v1.math.matmul(out, kernel)
            out = tf.compat.v1.nn.bias_add(out, bias)
        with tf.compat.v1.variable_scope("dense_two"):
            kernel = tf.compat.v1.get_variable(
                shape=[out.shape[-1], self.units],
                regularizer=regularizers.L2(),
                initializer=init_ops.ones_initializer(),
                name="kernel")
            bias = tf.compat.v1.get_variable(
                shape=[self.units,],
                initializer=init_ops.zeros_initializer(),
                name="bias")
            out = tf.compat.v1.math.matmul(out, kernel)
            out = tf.compat.v1.nn.bias_add(out, bias)
    return out

@tf.compat.v1.keras.utils.track_tf1_style_variables
def call(self, inputs):
    with tf.compat.v1.variable_scope("double_dense_layer"):
        out = tf.compat.v1.layers.dense(
            inputs, self.units, name="dense_one",
            kernel_initializer=tf.compat.v1.random_normal_initializer,
            kernel_regularizer="l2")
        out = tf.compat.v1.layers.dense(
            out, self.units, name="dense_two",
            kernel_initializer=tf.compat.v1.random_normal_initializer(),
            kernel_regularizer="l2")
    return out

@tf.compat.v1.keras.utils.track_tf1_style_variables
def call(self, inputs):
    out = inputs
    with tf.compat.v1.variable_scope("double_dense_layer"):
        with tf.compat.v1.variable_scope("dense_one"):
            kernel = tf.compat.v1.get_variable(
                shape=[out.shape[-1], self.units],
                regularizer=regularizers.L2(),
                initializer=init_ops.ones_initializer(),
                name="kernel")
            bias = tf.compat.v1.get_variable(
                shape=[self.units,],
                initializer=init_ops.zeros_initializer(),
                name="bias")
            out = tf.compat.v1.math.matmul(out, kernel)
            out = tf.compat.v1.nn.bias_add(out, bias)
        with tf.compat.v1.variable_scope("dense_two"):
            kernel = tf.compat.v1.get_variable(
                shape=[out.shape[-1], self.units],
                regularizer=regularizers.L2(),
                initializer=init_ops.ones_initializer(),
                name="kernel")
            bias = tf.compat.v1.get_variable(
                shape=[self.units,],
                initializer=init_ops.zeros_initializer(),
                name="bias")
            out = tf.compat.v1.math.matmul(out, kernel)
            out = tf.compat.v1.nn.bias_add(out, bias)
    return out

@tf.compat.v1.keras.utils.track_tf1_style_variables
def call(self, inputs):
    with tf.compat.v1.variable_scope("double_dense_layer"):
        out = tf.compat.v1.layers.dense(
            inputs, self.units, name="dense_one",
            kernel_initializer=tf.compat.v1.random_normal_initializer,
            kernel_regularizer="l2")
        out = tf.compat.v1.layers.dense(
            out, self.units, name="dense_two",
            kernel_initializer=tf.compat.v1.random_normal_initializer(),
            kernel_regularizer="l2")
    return out

@tf.compat.v1.keras.utils.track_tf1_style_variables
def call(self, inputs):
    out = inputs
    with tf.compat.v1.variable_scope("double_dense_layer"):
        with tf.compat.v1.variable_scope("dense_one"):
            kernel = tf.compat.v1.get_variable(
                shape=[out.shape[-1], self.units],
                regularizer=regularizers.L2(),
                initializer=init_ops.ones_initializer(),
                name="kernel")
            bias = tf.compat.v1.get_variable(
                shape=[self.units,],
                initializer=init_ops.zeros_initializer(),
                name="bias")
            out = tf.compat.v1.math.matmul(out, kernel)
            out = tf.compat.v1.nn.bias_add(out, bias)
        with tf.compat.v1.variable_scope("dense_two"):
            kernel = tf.compat.v1.get_variable(
                shape=[out.shape[-1], self.units],
                regularizer=regularizers.L2(),
                initializer=init_ops.ones_initializer(),
                name="kernel")
            bias = tf.compat.v1.get_variable(
                shape=[self.units,],
                initializer=init_ops.zeros_initializer(),
                name="bias")
            out = tf.compat.v1.math.matmul(out, kernel)
            out = tf.compat.v1.nn.bias_add(out, bias)
    return out

@tf.compat.v1.keras.utils.track_tf1_style_variables
def call(self, inputs):
    with tf.compat.v1.variable_scope("double_dense_layer"):
        out = tf.compat.v1.layers.dense(
            inputs, self.units, name="dense_one",
            kernel_initializer=tf.compat.v1.random_normal_initializer,
            kernel_regularizer="l2")
        out = tf.compat.v1.layers.dense(
            out, self.units, name="dense_two",
            kernel_initializer=tf.compat.v1.random""" .

"DESCRIPTION.This code defines a class named Example that extends the nn.Module class. Inside this class, there is a method called apply which calculates the exponential moving average (EMA) of the input data using the decay parameter. The EMA is updated based on the current inputs and the previous EMA value. The method returns the original inputs." <EXPLAINS> """CODE.class Example(nn.Module):
    def apply(self, inputs, decay=0.9):
      ema = self.state('ema', inputs.shape, initializers.zeros)
      ema.value = decay * ema.value + (1 - decay) * inputs
      return inputs
""" .

"DESCRIPTION.This code defines a class named Example that has a method called apply. The apply method calculates an exponential moving average (EMA) of the inputs using the decay parameter and updates the EMA value accordingly. The method then returns the original inputs." <EXPLAINS> """CODE.class Example(nn.Module):
    def apply(self, inputs, decay=0.9):
      ema = self.state('ema', inputs.shape, initializers.zeros)
      ema.value = decay * ema.value + (1 - decay) * inputs
      return inputs
""" .

"DESCRIPTION.This code defines a class named LitModel with an initialization method that sets an attribute self.l1 to None. It includes a method prepare_data that downloads data and tokenizes it. There is a comment indicating not to assign something to self.something. Lastly, there is a setup method that loads data using Load_data and initializes a neural network linear layer with input size of 28 and output size based on the number of classes in the data." <EXPLAINS> """CODE.class LitModel(...):
    def __init__(self):
        self.l1 = None

    def prepare_data(self):
        download_data()
        tokenize()

        # don't do this
        self.something = else

    def setup(stage):
        data = Load_data(...)
        self.l1 = nn.Linear(28, data.num_classes)""" .

"DESCRIPTION.This code defines a class named MeanSquaredError, which extends from the Loss class. It has a method call that takes two parameters y_true and y_pred, converts them to tensors, calculates the mean squared error between the predicted values (y_pred) and the true values (y_true), and returns the result." <EXPLAINS> """CODE.class MeanSquaredError(Loss):
    def call(self, y_true, y_pred):
        y_pred = ops.convert_to_tensor(y_pred)
        y_true = math_ops.cast(y_true, y_pred.dtype)
        return K.mean(math_ops.square(y_pred - y_true), axis=-1)
""" .

"DESCRIPTION.This code defines a class named RayServeActor that inherits from RayServeMixin and MyClass." <EXPLAINS> """CODE.@ray.remote
class RayServeActor(RayServeMixin, MyClass):
    pass""" .

"DESCRIPTION.This code defines a class named SerializationExampleFeatureColumn that extends FeatureColumn and implements methods for serializing and deserializing the configuration of the object. The _get_config method creates a dictionary from the attributes of the object, serializes the parent FeatureColumn, converts the dtype to its name, and serializes the normalizer function using Keras serialization. The _from_config method does the inverse operation by deserializing the parent FeatureColumn, converting the dtype back to tf.DType, and deserializing the normalizer function using Keras deserialization." <EXPLAINS> """CODE.class SerializationExampleFeatureColumn(
    FeatureColumn, collections.namedtuple(
        'SerializationExampleFeatureColumn',
        ('dimension', 'parent', 'dtype', 'normalizer_fn'))):

  def _get_config(self):
    # Create a dict from the namedtuple.
    # Python attribute literals can be directly copied from / to the config.
    # For example 'dimension', assuming it is an integer literal.
    config = dict(zip(self._fields, self))

    # (De)serialization of parent FeatureColumns should use the provided
    # (de)serialize_feature_column() methods that take care of de-duping.
    config['parent'] = serialize_feature_column(self.parent)

    # Many objects provide custom (de)serialization e.g: for tf.DType
    # tf.DType.name, tf.as_dtype() can be used.
    config['dtype'] = self.dtype.name

    # Non-trivial dependencies should be Keras-(de)serializable.
    config['normalizer_fn'] = generic_utils.serialize_keras_object(
        self.normalizer_fn)

    return config

  @classmethod
  def _from_config(cls, config, custom_objects=None, columns_by_name=None):
    # This should do the inverse transform from `_get_config` and construct
    # the namedtuple.
    kwargs = config.copy()
    kwargs['parent'] = deserialize_feature_column(
        config['parent'], custom_objects, columns_by_name)
    kwargs['dtype'] = dtypes.as_dtype(config['dtype'])
    kwargs['normalizer_fn'] = generic_utils.deserialize_keras_object(
      config['normalizer_fn'], custom_objects=custom_objects)
    return cls(**kwargs)
""",
        """CODE.class SerializationExampleFeatureColumn(
    FeatureColumn, collections.namedtuple(
        'SerializationExampleFeatureColumn',
        ('dimension', 'parent', 'dtype', 'normalizer_fn'))):

  def get_config(self):
    # Create a dict from the namedtuple.
    # Python attribute literals can be directly copied from / to the config.
    # For example 'dimension', assuming it is an integer literal.
    config = dict(zip(self._fields, self))

    # (De)serialization of parent FeatureColumns should use the provided
    # (de)serialize_feature_column() methods that take care of de-duping.
    config['parent'] = serialize_feature_column(self.parent)

    # Many objects provide custom (de)serialization e.g: for tf.DType
    # tf.DType.name, tf.as_dtype() can be used.
    config['dtype'] = self.dtype.name

    # Non-trivial dependencies should be Keras-(de)serializable.
    config['normalizer_fn'] = generic_utils.serialize_keras_object(
        self.normalizer_fn)

    return config

  @classmethod
  def from_config(cls, config, custom_objects=None, columns_by_name=None):
    # This should do the inverse transform from `get_config` and construct
    # the namedtuple.
    kwargs = config.copy()
    kwargs['parent'] = deserialize_feature_column(
        config['parent'], custom_objects, columns_by_name)
    kwargs['dtype'] = dtypes.as_dtype(config['dtype'])
    kwargs['normalizer_fn'] = generic_utils.deserialize_keras_object(
      config['normalizer_fn'], custom_objects=custom_objects)
    return cls(**kwargs)
""" .

"DESCRIPTION.This code defines a class that wraps around a gym environment's reward system. The class clips the rewards between a minimum and maximum value specified during initialization. The reward method clips the input reward value within the specified range." <EXPLAINS> """CODE.    class ClipReward(gym.RewardWrapper):
        def __init__(self, env, min_reward, max_reward):
            super().__init__(env)
            self.min_reward = min_reward
            self.max_reward = max_reward
            self.reward_range = (min_reward, max_reward)

        def reward(self, reward):
            return np.clip(reward, self.min_reward, self.max_reward)""" .

"DESCRIPTION.This code defines a constant tensor 'a' with values [-3.0, -1.0, 0.0, 1.0, 3.0] and data type tf.float32. It then applies the linear activation function to tensor 'a' using tf.keras.activations.linear(), resulting in tensor 'b'. Finally, it retrieves and returns the numpy array representation of tensor 'b'." <EXPLAINS> """CODE.a = tf.constant([-3.0,-1.0, 0.0,1.0,3.0], dtype = tf.float32)
b = tf.keras.activations.linear(a)
b.numpy()
array([-3., -1.,  0.,  1.,  3.], dtype=float32)""" .

"DESCRIPTION.This code defines a context manager that streams data to a report identified by report_id. It asynchronously iterates over a series of delta_list objects and consumes each delta_list data by passing it to the consume method within the context manager." <EXPLAINS> """CODE.with menagerie.stream_to(report_id) as consume:
    async for delta_list in delta_list_iter:
        consume(delta_list)""" .

"DESCRIPTION.This code defines a convolutional neural network layer with 64 filters of size 3x3, applies it to a 5-dimensional input tensor of shape (None, 10, 128, 128, 3) using a TimeDistributed layer, and outputs a tensor of shape (None, 10, 126, 126, 64)." <EXPLAINS> """CODE.inputs = tf.keras.Input(shape=(10, 128, 128, 3)
conv_2d_layer = tf.keras.layers.Conv2D(64, (3, 3))
outputs = tf.keras.layers.TimeDistributed(conv_2d_layer)(inputs)
outputs.shape
TensorShape([None, 10, 126, 126, 64])""" .

"DESCRIPTION.This code defines a convolutional neural network model using PaddlePaddle's fluid library in Python. It generates random input data, applies the convolution operation with the defined model, calculates the loss function based on the output, scales the loss using automatic mixed precision, computes gradients, and minimizes the loss using stochastic gradient descent optimization." <EXPLAINS> """CODE.import numpy as np
import paddle.fluid as fluid

data = np.random.uniform(-1, 1, [10, 3, 32, 32]).astype('float32')
with fluid.dygraph.guard():
    model = fluid.dygraph.Conv2D(3, 2, 3)
    optimizer = fluid.optimizer.SGDOptimizer(
            learning_rate=0.01, parameter_list=model.parameters())
    scaler = fluid.dygraph.AmpScaler(init_loss_scaling=1024)
    data = fluid.dygraph.to_variable(data)
    with fluid.dygraph.amp_guard():
        conv = model(data)
        loss = fluid.layers.reduce_mean(conv)
        scaled = scaler.scale(loss)
        scaled.backward()
        scaler.minimize(optimizer, scaled)""" .

"DESCRIPTION.This code defines a convolutional neural network model using TensorFlow's Keras API. The model consists of multiple convolutional and max pooling layers with the 'elu' activation function." <EXPLAINS> """CODE.import tensorflow as tf
model = tf.keras.Sequential()
model.add(tf.keras.layers.Conv2D(32, (3, 3), activation='elu', input_shape=(28, 28, 1))
model.add(tf.keras.layers.MaxPooling2D((2, 2))
model.add(tf.keras.layers.Conv2D(64, (3, 3), activation='elu'))
model.add(tf.keras.layers.MaxPooling2D((2, 2))
model.add(tf.keras.layers.Conv2D(64, (3, 3), activation='elu'))
""" .

"DESCRIPTION.This code defines a convolutional neural network model using paddle's fluid framework in Python. It initializes random input data with specific shape and type, creates a Conv2D model with specific parameters, defines a SGD optimizer with a learning rate, applies automatic mixed precision training using AmpScaler, computes the loss, scales the loss, calculates gradients, and updates the model parameters using the optimizer." <EXPLAINS> """CODE.import numpy as np
import paddle.fluid as fluid

data = np.random.uniform(-1, 1, [10, 3, 32, 32]).astype('float32')
with fluid.dygraph.guard():
    model = fluid.dygraph.Conv2D(3, 2, 3)
    optimizer = fluid.optimizer.SGDOptimizer(
            learning_rate=0.01, parameter_list=model.parameters())
    scaler = fluid.dygraph.AmpScaler(init_loss_scaling=1024)
    data = fluid.dygraph.to_variable(data)
    with fluid.dygraph.amp_guard():
        conv = model(data)
        loss = fluid.layers.reduce_mean(conv)
        scaled = scaler.scale(loss)
        scaled.backward()
        scaler.minimize(optimizer, scaled)""" .

"DESCRIPTION.This code defines a convolutional neural network with a 2D convolutional layer applied to a 5D input tensor. The output shape of the layer is [None, 10, 126, 126, 64]." <EXPLAINS> """CODE.inputs = tf.keras.Input(shape=(10, 128, 128, 3)
conv_2d_layer = tf.keras.layers.Conv2D(64, (3, 3))
outputs = tf.keras.layers.TimeDistributed(conv_2d_layer)(inputs)
outputs.shape
TensorShape([None, 10, 126, 126, 64])""" .

"DESCRIPTION.This code defines a custom Callback class named InterruptingCallback that interrupts the training process at epoch 4 by raising a RuntimeError exception. It then creates a backup and restore Callback instance and trains a Sequential model using Stochastic Gradient Descent optimizer and mean squared error loss. The training is done for 10 epochs, with the InterruptingCallback and backup and restore callbacks being utilized. If the training is interrupted, it catches the RuntimeError exception and continues training from where it left off. The code then runs the second training session and calculates the loss history for the last 6 epochs." <EXPLAINS> """CODE.class InterruptingCallback(tf.keras.callbacks.Callback):
  def on_epoch_begin(self, epoch, logs=None):
    if epoch == 4:
      raise RuntimeError('Interrupting!')

callback = tf.keras.callbacks.experimental.BackupAndRestore(
backup_dir="/tmp")

model = tf.keras.models.Sequential([tf.keras.layers.Dense(10)])
model.compile(tf.keras.optimizers.SGD(), loss='mse')

try:
  model.fit(np.arange(100).reshape(5, 20), np.zeros(5), epochs=10,
            batch_size=1, callbacks=[callback, InterruptingCallback()],
            verbose=0)
except:
  pass

history = model.fit(np.arange(100).reshape(5, 20), np.zeros(5), epochs=10,
            batch_size=1, callbacks=[callback], verbose=0)
# Only 6 more epochs are run, since first trainning got interrupted at
# zero-indexed epoch 4, second training will continue from 4 to 9.
len(history.history['loss'])
""" .

"DESCRIPTION.This code defines a custom DDP (Distributed Data Parallel) class that inherits from the DDPPlugin class. It contains a method called configure_ddp, which takes a model and a list of device IDs as input, creates a MyDDPWrapper object with the model and device IDs, and returns the resulting model." <EXPLAINS> """CODE.class MyDDP(DDPPlugin):

    def configure_ddp(self, model, device_ids):
        model = MyDDPWrapper(model, device_ids)
        return model
""" .

"DESCRIPTION.This code defines a custom JaccardScore metric class for evaluating model performance in TensorFlow. The class includes methods for updating the metric value based on true and predicted values, resetting the metric value, and calculating the final metric result." <EXPLAINS> """CODE.from sklearn.metrics import jaccard_score
import tensorflow as tf

class JaccardScore(tf.keras.metrics.experimental.PyMetric):

  def __init__(self, name='jaccard_score', **kwargs):
    super().__init__(name=name, **kwargs)

  def update_state(self, y_true, y_pred, sample_weight=None):
    self.jaccard_sum += jaccard_score(y_pred, y_true, average="macro")
    self.count += 1

  def reset_state(self):
    self.jaccard_sum = 0.
    self.count = 0.

  def result(self):
    return self.jaccard_sum / self.count
""" .

"DESCRIPTION.This code defines a custom Keras model class `MyModel`, which extends `tf.keras.Model` and `KerasModelHubMixin`. The `MyModel` class has methods to initialize the model with configuration, execute the model using dummy inputs, save the model locally, push the model to a public model repository, and download model weights from the Hugging Face model hub for initialization." <EXPLAINS> """CODE.from huggingface_hub import KerasModelHubMixin

class MyModel(tf.keras.Model, KerasModelHubMixin):
    def __init__(self, **kwargs):
        super().__init__()
        self.config = kwargs.pop("config", None)
        self.dummy_inputs = ...
        self.layer = ...

    def call(self, *args):
        return ...

# Init and compile the model as you normally would
model = MyModel()
model.compile(...)
# Build the graph by training it or passing dummy inputs
_ = model(model.dummy_inputs)
# You can save your model like this
model.save_pretrained("local_model_dir/", push_to_hub=False)
# Or, you can push to a new public model repo like this
model.push_to_hub(
    "super-cool-model",
    git_user="your-hf-username",
    git_email="you@somesite.com",
)

# Downloading weights from hf-hub & model will be initialized from those weights
model = MyModel.from_pretrained("username/mymodel@main")
""" .

"DESCRIPTION.This code defines a custom PyLayer class called cus_tanh that implements a custom tanh activation function. The forward method applies the custom tanh function to the input tensor x using the specified function func1 and saves the output tensor for backward propagation. The backward method calculates the gradient of the input tensor dy based on the saved output tensor y and the specified function func2. Finally, it creates a random tensor data, sets it to be trainable, and applies the custom tanh Layer to the data tensor using the tanh function as func1." <EXPLAINS> """CODE.import paddle
from paddle.autograd import PyLayer

class cus_tanh(PyLayer):
    @staticmethod
    def forward(ctx, x, func1, func2=paddle.square):
        ctx.func = func2
        y = func1(x)
        # Pass tensors to backward.
        ctx.save_for_backward(y)
        return y

    @staticmethod
    def backward(ctx, dy):
        # Get the tensors passed by forward.
        y, = ctx.saved_tensor()
        grad = dy * (1 - ctx.func(y))
        return grad


data = paddle.randn([2, 3], dtype="float64")
data.stop_gradient = False
# run custom Layer.
z = cus_tanh.apply(data, func1=paddle.tanh)""" .

"DESCRIPTION.This code defines a custom PyLayer class called cus_tanh that performs forward and backward calculations for the hyperbolic tangent function using the PaddlePaddle library. The forward method calculates the tanh function of the input tensor x and saves the result for the backward pass. The backward method computes the gradient of the input tensor x based on the saved tanh result y and the gradient dy." <EXPLAINS> """CODE.import paddle
from paddle.autograd import PyLayer

class cus_tanh(PyLayer):
    @staticmethod
    def forward(ctx, x):
        y = paddle.tanh(x)
        # Pass tensors to backward.
        ctx.save_for_backward(y)
        return y

    @staticmethod
    def backward(ctx, dy):
        # Get the tensors passed by forward.
        y, = ctx.saved_tensor()
        grad = dy * (1 - paddle.square(y))
        return grad""" .

"DESCRIPTION.This code defines a custom PyLayer class named cus_tanh that implements the forward and backward functions for the hyperbolic tangent (tanh) activation function. The forward function computes the tanh function of the input tensor x and saves the result for backpropagation. The backward function computes the gradient of the input tensor with respect to the output tensor y using the chain rule and the derivative of the tanh function." <EXPLAINS> """CODE.import paddle
from paddle.autograd import PyLayer

class cus_tanh(PyLayer):
    @staticmethod
    def forward(ctx, x):
        # ctx is a context object that store some objects for backward.
        y = paddle.tanh(x)
        # Pass tensors to backward.
        ctx.save_for_backward(y)
        return y

    @staticmethod
    def backward(ctx, dy):
        # Get the tensors passed by forward.
        y, = ctx.saved_tensor()
        grad = dy * (1 - paddle.square(y))
        return grad""" .

"DESCRIPTION.This code defines a custom PyLayer class named cus_tanh that implements the forward and backward pass of the hyperbolic tangent (tanh) activation function. The forward method computes the tanh activation of the input tensor x, saves the intermediate result for backpropagation, and returns the result. The backward method computes the gradient of the loss with respect to the input given the gradient dy and the saved tensor y from the forward pass. The gradient is calculated as dy multiplied by (1 - tanh^2(y))." <EXPLAINS> """CODE.import paddle
from paddle.autograd import PyLayer

class cus_tanh(PyLayer):
    @staticmethod
    def forward(ctx, x):
        # ctx is a context object that store some objects for backward.
        y = paddle.tanh(x)
        # Pass tensors to backward.
        ctx.save_for_backward(y)
        return y

    @staticmethod
    def backward(ctx, dy):
        # Get the tensors passed by forward.
        y, = ctx.saved_tensor()
        grad = dy * (1 - paddle.square(y))
        return grad""" .

"DESCRIPTION.This code defines a custom PyLayer in PaddlePaddle that applies the hyperbolic tangent (tanh) function to the input data during forward pass, and calculates the gradient using the derivative of tanh function during the backward pass." <EXPLAINS> """CODE.import paddle
from paddle.autograd import PyLayer

class cus_tanh(PyLayer):
    @staticmethod
    def forward(ctx, x):
        y = paddle.tanh(x)
        # Pass tensors to backward.
        ctx.save_for_backward(y)
        return y

    @staticmethod
    def backward(ctx, dy):
        # Get the tensors passed by forward.
        y, = ctx.saved_tensor()
        grad = dy * (1 - paddle.square(y))
        return grad""" .

"DESCRIPTION.This code defines a custom Sequence class called CIFAR10Sequence for generating batches of data for a machine learning model. The class takes input data (x_set and y_set) and batch size as parameters. The __len__ method calculates the number of batches needed to iterate over the input data. The __getitem__ method retrieves a batch of input data and their corresponding labels, resizes the images to a specific dimension, and returns the processed data in numpy arrays." <EXPLAINS> """CODE.from skimage.io import imread
from skimage.transform import resize
import numpy as np
import math

class CIFAR10Sequence(Sequence):

    def __init__(self, x_set, y_set, batch_size):
        self.x, self.y = x_set, y_set
        self.batch_size = batch_size

    def __len__(self):
        return math.ceil(len(self.x) / self.batch_size)

    def __getitem__(self, idx):
        batch_x = self.x[idx * self.batch_size:(idx + 1) * self.batch_size]
        batch_y = self.y[idx * self.batch_size:(idx + 1) * self.batch_size]

        return np.array([
            resize(imread(file_name), (200, 200))
               for file_name in batch_x]), np.array(batch_y)
""" .

"DESCRIPTION.This code defines a custom Sequence class for handling CIFAR-10 dataset. It initializes with x_set, y_set, and batch_size. It calculates the length of the dataset based on batch size. It retrieves batches of images and labels from the dataset and resizes the images to (200, 200) pixels before returning them." <EXPLAINS> """CODE.    class CIFAR10Sequence(Sequence):

        def __init__(self, x_set, y_set, batch_size):
            self.x, self.y = x_set, y_set
            self.batch_size = batch_size

        def __len__(self):
            return int(np.ceil(len(self.x) / float(self.batch_size)))

        def __getitem__(self, idx):
            batch_x = self.x[idx * self.batch_size:(idx + 1) * self.batch_size]
            batch_y = self.y[idx * self.batch_size:(idx + 1) * self.batch_size]

            return np.array([
                resize(imread(file_name), (200, 200))
                   for file_name in batch_x]), np.array(batch_y)
""" .

"DESCRIPTION.This code defines a custom Torch library function \"mylib::sin\" for calculating the sine of a Tensor input, implements it for the CPU, and tests its correctness by comparing the output with the input after applying the function." <EXPLAINS> """CODE.torch.library.define("mylib::sin", "(Tensor x) -> Tensor")
@torch.library.impl("mylibrary::sin", "cpu")
def f(x):
    return torch.from_numpy(np.sin(x.numpy()))
x = torch.randn(3)
y = torch.ops.mylib.sin(x)
assert torch.allclose(y, x)""" .

"DESCRIPTION.This code defines a custom cache class that extends BytecodeCache. The class has methods to load bytecode from a file and dump bytecode into a file based on the given directory and bucket key." <EXPLAINS> """CODE.    from os import path

    class MyCache(BytecodeCache):

        def __init__(self, directory):
            self.directory = directory

        def load_bytecode(self, bucket):
            filename = path.join(self.directory, bucket.key)
            if path.exists(filename):
                with file(filename, 'rb') as f:
                    bucket.load_bytecode(f)

        def dump_bytecode(self, bucket):
            filename = path.join(self.directory, bucket.key)
            with file(filename, 'wb') as f:
                bucket.write_bytecode(f)""" .

"DESCRIPTION.This code defines a custom callback class named MyCallback that extends the Callback class from Ray Tune. The callback keeps track of trial IDs for trials that have started using the on_trial_start method. It provides methods to get and set the state, capturing and updating the trial IDs in the callback." <EXPLAINS> """CODE.from typing import Dict, List, Optional

from ray.tune import Callback
from ray.tune.experiment import Trial

class MyCallback(Callback):
    def __init__(self):
        self._trial_ids = set()

    def on_trial_start(
        self, iteration: int, trials: List["Trial"], trial: "Trial", **info
    ):
        self._trial_ids.add(trial.trial_id)

    def get_state(self) -> Optional[Dict]:
        return {"trial_ids": self._trial_ids.copy()}

    def set_state(self, state: Dict) -> Optional[Dict]:
        self._trial_ids = state["trial_ids"]
""" .

"DESCRIPTION.This code defines a custom constraint class in TensorFlow for enforcing non-negativity on weight values in a neural network model during training. The __call__ method of the class applies the constraint by setting any negative weight values to zero." <EXPLAINS> """CODE.class NonNegative(tf.keras.constraints.Constraint):
    def __call__(self, w):
        return w * tf.cast(tf.math.greater_equal(w, 0.), w.dtype)""" .

"DESCRIPTION.This code defines a custom data module class that extends the LightningDataModule class. It includes methods to prepare the data, set up data assignments for training, validation, and testing, and return corresponding data loaders for training, validation, and testing datasets." <EXPLAINS> """CODE.class MyDataModule(LightningDataModule):
    def __init__(self):
        super().__init__()
    def prepare_data(self):
        # download, split, etc...
        # only called on 1 GPU/TPU in distributed
    def setup(self):
        # make assignments here (val/train/test split)
        # called on every process in DDP
    def train_dataloader(self):
        train_split = Dataset(...)
        return DataLoader(train_split)
    def val_dataloader(self):
        val_split = Dataset(...)
        return DataLoader(val_split)
    def test_dataloader(self):
        test_split = Dataset(...)
        return DataLoader(test_split)
""" .

"DESCRIPTION.This code defines a custom data sequence class named CIFAR10Sequence that is used for loading batches of data (images) and corresponding labels in the CIFAR-10 dataset. The class takes in a set of image file paths (x_set), corresponding labels (y_set), and batch size as input. The __len__ method returns the number of batches that can be generated from the input data. The __getitem__ method generates a batch of resized images and their corresponding labels by loading the images, resizing them to (200, 200) using skimage, and returning them as numpy arrays." <EXPLAINS> """CODE.from skimage.io import imread
from skimage.transform import resize
import numpy as np

class CIFAR10Sequence(Sequence):
    def __init__(self, x_set, y_set, batch_size):
        self.X,self.y = x_set,y_set
        self.batch_size = batch_size

    def __len__(self):
        return len(self.X) // self.batch_size

    def __getitem__(self,idx):
        batch_x = self.X[idx*self.batch_size:(idx+1)*self.batch_size]
        batch_y = self.y[idx*self.batch_size:(idx+1)*self.batch_size]

        return np.array([
            resize(imread(file_name), (200,200))
               for file_name in batch_x]), np.array(batch_y)
""" .

"DESCRIPTION.This code defines a custom decorator `@batch_function(1, 2, 3)` for the `layer` function, which calculates the matrix multiplication of input `a` with itself using TensorFlow." <EXPLAINS> """CODE.@batch_function(1, 2, 3)
def layer(a):
  return tf.matmul(a, a)""" .

"DESCRIPTION.This code defines a custom dictionary class called AttributeDictionary, assigns the value \"a\" to the key \"test\" in the dictionary, and prints the value associated with the key \"test\" which is \"a\"." <EXPLAINS> """CODE.d = AttributeDictionary()
d["test"] = "a"
print(d.test) # prints "a"
""" .

"DESCRIPTION.This code defines a custom function in PyTorch for sorting an input tensor and marking some elements as non-differentiable during the forward pass. During the backward pass, it computes the gradient of the sorted tensor with respect to the input tensor." <EXPLAINS> """CODE.    class Func(Function):
        @staticmethod
        def forward(ctx, x):
            sorted, idx = x.sort()
            ctx.mark_non_differentiable(idx)
            ctx.save_for_backward(x, idx)
            return sorted, idx

        @staticmethod
        @once_differentiable
        def backward(ctx, g1, g2):  # still need to accept g2
            x, idx = ctx.saved_tensors
            grad_input = torch.zeros_like(x)
            grad_input.index_add_(0, idx, g1)
            return grad_input""" .

"DESCRIPTION.This code defines a custom initializer class called ExampleRandomNormal that generates random values from a normal distribution with specified mean and standard deviation when initializing weights in a neural network model." <EXPLAINS> """CODE.import tensorflow as tf

class ExampleRandomNormal(tf.keras.initializers.Initializer):

  def __init__(self, mean, stddev):
    self.mean = mean
    self.stddev = stddev

  def __call__(self, shape, dtype=None, **kwargs):
    return tf.random.normal(
        shape, mean=self.mean, stddev=self.stddev, dtype=dtype)

  def get_config(self):  # To support serialization
    return {"mean": self.mean, "stddev": self.stddev}
""" .

"DESCRIPTION.This code defines a custom layer called ExampleLayer that contains a linear transformation operation (self._fc). It then creates an instance of ExampleLayer and generates random input data. The TracedLayer is used to trace the execution of the layer and create a static graph model. The static graph model is then used to make predictions using the input data. Finally, the static graph model is saved for future inference." <EXPLAINS> """CODE.import paddle.fluid as fluid
from paddle.fluid.dygraph import Linear, to_variable, TracedLayer
import numpy as np

class ExampleLayer(fluid.dygraph.Layer):
    def __init__(self):
        super(ExampleLayer, self).__init__()
        self._fc = Linear(3, 10)

    def forward(self, input):
        return self._fc(input)

with fluid.dygraph.guard():
    layer = ExampleLayer()
    in_np = np.random.random([2, 3]).astype('float32')
    in_var = to_variable(in_np)
    out_dygraph, static_layer = TracedLayer.trace(layer, inputs=[in_var])

    # run the static graph model using Executor inside
    out_static_graph = static_layer([in_var])

    print(len(out_static_graph)) # 1
    print(out_static_graph[0].shape) # (2, 10)

    # save the static graph model for inference
    static_layer.save_inference_model(dirname='./saved_infer_model')""" .

"DESCRIPTION.This code defines a custom layer in Python that specifies the input shape it can accept and allows squeezing the last axis if needed." <EXPLAINS> """CODE.class MyLayer(Layer):
    def __init__(self):
        super(MyLayer, self).__init__()
        # The layer will accept inputs with shape (?, 28, 28) & (?, 28, 28, 1)
        # and raise an appropriate error message otherwise.
        self.input_spec = InputSpec(
            shape=(None, 28, 28, 1),
            allow_last_axis_squeeze=True)
""" .

"DESCRIPTION.This code defines a custom layer in Python using PaddlePaddle's fluid library. The layer consists of a list of linear transformation modules, each with input and output dimensions of 10. In the forward function of the layer, it iterates over the list of linear modules, applies them to the input tensor, and returns the final output tensor after the transformations." <EXPLAINS> """CODE.import paddle.fluid as fluid
import numpy as np

class MyLayer(fluid.Layer):
    def __init__(self):
        super(MyLayer, self).__init__()
        self.linears = fluid.dygraph.LayerList(
            [fluid.dygraph.Linear(10, 10) for i in range(10)])

    def forward(self, x):
        # LayerList can act as an iterable, or be indexed using ints
        for i, l in enumerate(self.linears):
            x = self.linears[i // 2](x) + l(x)
        return x""" .

"DESCRIPTION.This code defines a custom layer in TensorFlow that adds a variable named \"name1\" without tracking it manually, and then tracks another variable named \"name2\" within the layer." <EXPLAINS> """CODE.class TestLayer(tf.keras.Layer):
  def build():
    with no_manual_dependency_tracking_scope(self):
      var = self.add_variable("name1")  # Creates a var and doesn't track it
    self._track_trackable("name2", var)  # We track variable with name `name2`
""" .

"DESCRIPTION.This code defines a custom layer in TensorFlow that implements a double dense layer with L2 regularization. The `WrappedDoubleDenseLayer` class initializes with the number of units for the layer. The `forward_pass` method calculates the output by applying two dense layers with L2 regularization to the input. After creating an instance of the layer, it can be called on inputs, and the layer's weights and trainable variables can be accessed. Regularization losses are captured in `layer.losses` after a call, similar to other Keras layers." <EXPLAINS> """CODE.class WrappedDoubleDenseLayer(variable_scope_shim.VariableScopeLayer):

  def __init__(self, units, *args, **kwargs):
    super().__init__(*args, **kwargs)
    self.units = units

  def forward_pass(self, inputs):
    with variable_scope.variable_scope("double_dense_layer"):
      out = tf.compat.v1.layers.dense(
          inputs, self.units, name="dense_one",
          kernel_initializer=tf.compat.v1.random_normal_initializer,
          kernel_regularizer="l2")
      out = tf.compat.v1.layers.dense(
          out, self.units, name="dense_two",
          kernel_initializer=tf.compat.v1.random_normal_initializer(),
          kernel_regularizer="l2")
    return out

# Create a layer that can be used as a standard keras layer
layer = WrappedDoubleDenseLayer(10)

# call the layer on inputs
layer(...)

# Variables created/used within the scope will be tracked by the layer
layer.weights
layer.trainable_variables

# Regularization losses will be captured in layer.losses after a call,
# just like any other Keras layer
reg_losses = layer.losses


class WrappedDoubleDenseLayer(variable_scope_shim.VariableScopeLayer):

  def __init__(self, units, *args, **kwargs):
    super().__init__(*args, **kwargs)
    self.units = units

  def forward_pass(self, inputs):
    out = inputs
    with tf.compat.v1.variable_scope("double_dense_layer"):
      with tf.compat.v1.variable_scope("dense_one"):
        # The weights are created with a `regularizer`,
        # so the layer should track their regularization losses
        kernel = tf.compat.v1.get_variable(
            shape=[out.shape[-1], self.units],
            regularizer=regularizers.L2(),
            initializer=init_ops.ones_initializer(),
            name="kernel")
        bias = tf.compat.v1.get_variable(
            shape=[self.units,],
            initializer=init_ops.zeros_initializer(),
            name="bias")
        out = tf.compat.v1.math.matmul(out, kernel)
        out = tf.compat.v1.nn.bias_add(out, bias)
      with tf.compat.v1.variable_scope("dense_two"):
        kernel = tf.compat.v1.get_variable(
            shape=[out.shape[-1], self.units],
            regularizer=regularizers.L2(),
            initializer=init_ops.ones_initializer(),
            name="kernel")
        bias = tf.compat.v1.get_variable(
            shape=[self.units,],
            initializer=init_ops.zeros_initializer(),
            name="bias")
        out = tf.compat.v1.math.matmul(out, kernel)
        out = tf.compat.v1.nn.bias_add(out, bias)
    return out

# Create a layer that can be used as a standard keras layer
layer = WrappedDoubleDenseLayer(10)

# call the layer on inputs
layer(...)

# Variables created/used within the scope will be tracked by the layer
layer.weights
layer.trainable_variables

# Regularization losses will be captured in layer.losses after a call,
# just like any other Keras layer
reg_losses = layer.losses
""" .

"DESCRIPTION.This code defines a custom layer using TensorFlow that performs a simple dense computation. The layer initializes weights in the `build` method, and applies the dense computation in the `call` method by multiplying the input with the weights and adding a bias. The code also demonstrates the instantiation of the layer, calculating output with some input data, and asserting the properties of the layer's weights, trainable weights, and non-trainable weights." <EXPLAINS> """CODE.class SimpleDense(Layer):

  def __init__(self, units=32):
      super(SimpleDense, self).__init__()
      self.units = units

  def build(self, input_shape):  # Create the state of the layer (weights)
    w_init = tf.random_normal_initializer()
    self.w = tf.Variable(
        initial_value=w_init(shape=(input_shape[-1], self.units),
                             dtype='float32'),
        trainable=True)
    b_init = tf.zeros_initializer()
    self.b = tf.Variable(
        initial_value=b_init(shape=(self.units,), dtype='float32'),
        trainable=True)

  def call(self, inputs):  # Defines the computation from inputs to outputs
      return tf.matmul(inputs, self.w) + self.b

# Instantiates the layer.
linear_layer = SimpleDense(4)

# This will also call `build(input_shape)` and create the weights.
y = linear_layer(tf.ones((2, 2)))
assert len(linear_layer.weights) == 2

# These weights are trainable, so they're listed in `trainable_weights`:
assert len(linear_layer.trainable_weights) == 2


class SimpleDense(Layer):

  def __init__(self, units=32):
      super(SimpleDense, self).__init__()
      self.units = units

  def build(self, input_shape):
      self.w = self.add_weight(shape=(input_shape[-1], self.units),
                               initializer='random_normal',
                               trainable=True)
      self.b = self.add_weight(shape=(self.units,),
                               initializer='random_normal',
                               trainable=True)

  def call(self, inputs):
      return tf.matmul(inputs, self.w) + self.b


class ComputeSum(Layer):

  def __init__(self, input_dim):
      super(ComputeSum, self).__init__()
      # Create a non-trainable weight.
      self.total = tf.Variable(initial_value=tf.zeros((input_dim,)),
                               trainable=False)

  def call(self, inputs):
      self.total.assign_add(tf.reduce_sum(inputs, axis=0))
      return self.total

my_sum = ComputeSum(2)
x = tf.ones((2, 2))

y = my_sum(x)
print(y.numpy())  # [2. 2.]

y = my_sum(x)
print(y.numpy())  # [4. 4.]

assert my_sum.weights == [my_sum.total]
assert my_sum.non_trainable_weights == [my_sum.total]
assert my_sum.trainable_weights == []
""" .

"DESCRIPTION.This code defines a custom metric class named BinaryTruePositives in TensorFlow/Keras, which calculates the number of true positive predictions. The class includes methods to update the state by comparing true labels and predicted labels, considering sample weights if provided, and to return the total number of true positive predictions." <EXPLAINS> """CODE.class BinaryTruePositives(tf.keras.metrics.Metric):

  def __init__(self, name='binary_true_positives', **kwargs):
    super(BinaryTruePositives, self).__init__(name=name, **kwargs)
    self.true_positives = self.add_weight(name='tp', initializer='zeros')

  def update_state(self, y_true, y_pred, sample_weight=None):
    y_true = tf.cast(y_true, tf.bool)
    y_pred = tf.cast(y_pred, tf.bool)

    values = tf.logical_and(tf.equal(y_true, True), tf.equal(y_pred, True))
    values = tf.cast(values, self.dtype)
    if sample_weight is not None:
      sample_weight = tf.cast(sample_weight, self.dtype)
      sample_weight = tf.broadcast_to(sample_weight, values.shape)
      values = tf.multiply(values, sample_weight)
    self.true_positives.assign_add(tf.reduce_sum(values))

  def result(self):
    return self.true_positives
""" .

"DESCRIPTION.This code defines a custom neural network layer \"WrappedDoubleDenseLayer\" that consists of two dense layers. The first dense layer applies a linear transformation to the input data and adds regularization for the weights. The second dense layer is nested within the first layer and also applies a linear transformation with regularization. The output of the final layer is returned." <EXPLAINS> """CODE.class WrappedDoubleDenseLayer(variable_scope_shim.VariableScopeWrapperLayer):

  def __init__(self, units, *args, **kwargs):
    super().__init__(*args, **kwargs)
    self.units = units

  def forward_pass(self, inputs, training=None):
    out = tf.compat.v1.layers.dense(
        inputs, self.units, name="dense_one",
        kernel_initializer=init_ops.ones_initializer(),
        kernel_regularizer="l2")
    with variable_scope.variable_scope("nested_scope"):
      out = tf.compat.v1.layers.dense(
          out, self.units, name="dense_two",
          kernel_initializer=init_ops.ones_initializer(),
          kernel_regularizer="l2")
    return out


class WrappedDoubleDenseLayer(variable_scope_shim.VariableScopeWrapperLayer):

  def __init__(self, units, *args, **kwargs):
    super().__init__(*args, **kwargs)
    self.units = units

  def forward_pass(self, inputs, training=None):
    out = inputs
    with tf.compat.v1.variable_scope("dense_one"):
      # The weights are created with a `regularizer`,
      # so the layer should track their regularization losses
      kernel = tf.compat.v1.get_variable(
          shape=[out.shape[-1], self.units],
          regularizer=regularizers.L2(),
          initializer=init_ops.ones_initializer(),
          name="kernel")
      bias = tf.compat.v1.get_variable(
          shape=[self.units,],
          initializer=init_ops.zeros_initializer(),
          name="bias")
      out = tf.compat.v1.math.matmul(out, kernel)
      out = tf.compat.v1.nn.bias_add(out, bias)
    with tf.compat.v1.variable_scope("nested_scope"):
      with tf.compat.v1.variable_scope("dense_two"):
        kernel = tf.compat.v1.get_variable(
            shape=[out.shape[-1], self.units],
            regularizer=regularizers.L2(),
            initializer=init_ops.ones_initializer(),
            name="kernel")
        bias = tf.compat.v1.get_variable(
            shape=[self.units,],
            initializer=init_ops.zeros_initializer(),
            name="bias")
        out = tf.compat.v1.math.matmul(out, kernel)
        out = tf.compat.v1.nn.bias_add(out, bias)
    return out
""" .

"DESCRIPTION.This code defines a custom neural network layer called WrappedDoubleDenseLayer. The layer consists of two dense layers with L2 kernel regularization. The first dense layer (dense_one) takes the input, applies ones_initializer to the kernel, and then passes it through a L2 regularizer. The output is then fed into a second dense layer (dense_two) within a nested scope called nested_scope. The second dense layer also applies ones_initializer to the kernel, L2 regularization, and produces the final output." <EXPLAINS> """CODE.class WrappedDoubleDenseLayer(variable_scope_shim.VariableScopeWrapperLayer):

  def __init__(self, units, *args, **kwargs):
    super().__init__(*args, **kwargs)
    self.units = units

  def forward_pass(self, inputs, training=None):
    out = tf.compat.v1.layers.dense(
        inputs, self.units, name="dense_one",
        kernel_initializer=init_ops.ones_initializer(),
        kernel_regularizer="l2")
    with variable_scope.variable_scope("nested_scope"):
      out = tf.compat.v1.layers.dense(
          out, self.units, name="dense_two",
          kernel_initializer=init_ops.ones_initializer(),
          kernel_regularizer="l2")
    return out


class WrappedDoubleDenseLayer(variable_scope_shim.VariableScopeWrapperLayer):

  def __init__(self, units, *args, **kwargs):
    super().__init__(*args, **kwargs)
    self.units = units

  def forward_pass(self, inputs, training=None):
    out = inputs
    with tf.compat.v1.variable_scope("dense_one"):
      # The weights are created with a `regularizer`,
      # so the layer should track their regularization losses
      kernel = tf.compat.v1.get_variable(
          shape=[out.shape[-1], self.units],
          regularizer=regularizers.L2(),
          initializer=init_ops.ones_initializer(),
          name="kernel")
      bias = tf.compat.v1.get_variable(
          shape=[self.units,],
          initializer=init_ops.zeros_initializer(),
          name="bias")
      out = tf.compat.v1.math.matmul(out, kernel)
      out = tf.compat.v1.nn.bias_add(out, bias)
    with tf.compat.v1.variable_scope("nested_scope"):
      with tf.compat.v1.variable_scope("dense_two"):
        kernel = tf.compat.v1.get_variable(
            shape=[out.shape[-1], self.units],
            regularizer=regularizers.L2(),
            initializer=init_ops.ones_initializer(),
            name="kernel")
        bias = tf.compat.v1.get_variable(
            shape=[self.units,],
            initializer=init_ops.zeros_initializer(),
            name="bias")
        out = tf.compat.v1.math.matmul(out, kernel)
        out = tf.compat.v1.nn.bias_add(out, bias)
    return out
""" .

"DESCRIPTION.This code defines a custom object scope for the \"MyObject\" class and uses it to set a weight regularization for a Dense layer to \"MyObject\". This allows for saving, loading, and other operations to recognize the custom object by name." <EXPLAINS> """CODE.    with custom_object_scope({"MyObject":MyObject}):
        layer = Dense(..., W_regularizer="MyObject")
        # save, load, etc. will recognize custom object by name
""" .

"DESCRIPTION.This code defines a custom object scope which includes a custom object named \"MyObject\". It then creates a layer object using the custom object \"MyObject\" as a regularizer. This allows the layer to be saved, loaded, etc. with the custom object recognized by name." <EXPLAINS> """CODE.    with custom_object_scope({"MyObject":MyObject}):
        layer = Dense(..., W_regularizer="MyObject")
        # save, load, etc. will recognize custom object by name
""" .

"DESCRIPTION.This code defines a custom observation wrapper class that calculates the relative position between the \"target\" and \"agent\" in the observation data." <EXPLAINS> """CODE.class RelativePosition(gym.ObservationWrapper):
    def __init__(self, env):
        super().__init__(env)
        self.observation_space = Box(shape=(2,), low=-np.inf, high=np.inf)

    def observation(self, obs):
        return obs["target"] - obs["agent"]
""" .

"DESCRIPTION.This code defines a custom operator called \"custom_nonzero\" that calculates the indices of non-zero elements in a given input tensor. The output shape of the result is data-dependent, and the implementation includes handling for both abstract and concrete computations on CPU and CUDA devices." <EXPLAINS> """CODE.# An operator with data-dependent output shape
    lib = torch.library.Library("mymodule", "FRAGMENT")
    lib.define("mymodule::custom_nonzero(Tensor x) -> Tensor")

    @torch.library.impl_abstract("mymodule::custom_nonzero")
    def custom_nonzero_abstract(x):
        # Number of nonzero-elements is data-dependent.
        # Since we cannot peek at the data in an abstract impl,
        # we use the ctx object to construct a new symint that
        # represents the data-dependent size.
        ctx = torch.library.get_ctx()
        nnz = ctx.new_dynamic_size()
        shape = [nnz, x.dim()]
        result = x.new_empty(shape, dtype=torch.int64)
        return result

    @torch.library.impl(lib, "custom_nonzero", "CPU")
    def custom_nonzero_cpu(x):
        x_np = x.numpy()
        res = np.stack(np.nonzero(x_np), axis=1)
        return torch.tensor(res, device=x.device)""",
        """CODE.# an operator with data-dependent output shape
    @custom_op("mylibrary::custom_nonzero")
    def custom_nonzero(x: Tensor) -> Tensor:
        ...

    @custom_nonzero.impl_abstract():
    def custom_nonzero_abstract(x):
        # Number of nonzero-elements is data-dependent
        ctx = torch._custom_op.get_ctx()
        nnz = ctx.create_unbacked_symint()
        shape = [x.dim(), nnz]
        result = x.new_empty(shape, dtype=torch.long)
        return result

    @numpy_nonzero.impl(['cpu', 'cuda'])
    def custom_nonzero_impl(x):
        x_np = to_numpy(x)
        res = np.stack(np.nonzero(x_np), axis=1)
        # the size associated with ctx.create_unbacked_symint()
        # must be constrained in the same way, so we add an assertion here.
        if res.shape[0] < 2 or res.shape[0] > x.numel():
            raise RuntimeError("not supported")
        return torch.tensor(res, device=x.device)""" .

"DESCRIPTION.This code defines a custom parser for a delimited list of values where the delimiter is ', '. It then parses the input string \"dkls,lsdkjf,s12 34,@!#,213\" using the defined parser." <EXPLAINS> """CODE.csv_value = CharsNotIn(',')
print(delimitedList(csv_value).parseString("dkls,lsdkjf,s12 34,@!#,213"))""" .

"DESCRIPTION.This code defines a custom policy function that checks if a given function is in the no_recompute_list. It also defines a function gn that computes a value based on input tensors x and y. The function fn uses PyTorch's checkpointing mechanism to optimize memory usage during gradient computations for the function gn. Finally, the code creates random input tensors x and y with gradient tracking enabled and compiles the function fn for optimized execution." <EXPLAINS> """CODE.def get_custom_policy():
    no_recompute_list = [
        torch.ops.aten.mm.default,
    ]
    def custom_policy(mode, func, *args, **kwargs):
        return func in no_recompute_list
    return custom_policy

def selective_checkpointing_context_fn():
    return _pt2_selective_checkpoint_context_fn_gen(get_custom_policy())

def gn(x, y):
    return torch.sigmoid(torch.matmul(torch.matmul(x, y), y)) * y

def fn(x, y):
    return torch.utils.checkpoint.checkpoint(
        gn, x, y,
        use_reentrant=False,
        context_fn=selective_checkpointing_context_fn,
    )

x = torch.randn(4, 4, requires_grad=True)
y = torch.randn(4, 4, requires_grad=True)

compiled_fn = torch.compile(fn)""" .

"DESCRIPTION.This code defines a custom prediction writer class that extends the BasePredictionWriter class in the PyTorch Lightning library. The CustomWriter class has methods to write predictions to specified output directories at the end of training batches and epochs. The write_on_batch_end method saves individual batch predictions to separate files, while the write_on_epoch_end method saves all epoch predictions to a single file." <EXPLAINS> """CODE.import torch
from pytorch_lightning.callbacks import BasePredictionWriter

class CustomWriter(BasePredictionWriter):

    def __init__(self, output_dir: str, write_interval: str):
        super().__init__(write_interval)
        self.output_dir

    def write_on_batch_end(
        self, trainer, pl_module: 'LightningModule', prediction: Any, batch_indices: List[int], batch: Any,
        batch_idx: int, dataloader_idx: int
    ):
        torch.save(prediction, os.path.join(self.output_dir, dataloader_idx, f"{batch_idx}.pt"))

    def write_on_epoch_end(
        self, trainer, pl_module: 'LightningModule', predictions: List[Any], batch_indices: List[Any]
    ):
        torch.save(predictions, os.path.join(self.output_dir, "predictions.pt"))""" .

"DESCRIPTION.This code defines a custom vector-Jacobian product (VJP) for a function f that takes two inputs x and y, and returns the product of the sine of x with y. The VJP allows for calculating the gradient of f with respect to its inputs." <EXPLAINS> """CODE.@jax.custom_vjp
def f(x, y):
    return np.sin(x) * y

def f_fwd(x, y):
    return f(x, y), (np.cos(x), np.sin(x), y)

def f_bwd(res, g):
    cos_x, sin_x, y = res
    return (cos_x * g * y, -sin_x * g)

f.defvjp(f_fwd, f_bwd)""" .

"DESCRIPTION.This code defines a custom vector-Jacobian product (vjp) for a function f, which calculates the product of the sine of x and y. The custom vjp function calculates the forward and backward pass for the function f." <EXPLAINS> """CODE.@jax.custom_vjp
def f(x, y):
    return np.sin(x) * y

def f_fwd(x, y):
    return f(x, y), (np.cos(x), np.sin(x), y)

def f_bwd(res, g):
    cos_x, sin_x, y = res
    return (cos_x * g * y, -sin_x * g)

f.defvjp(f_fwd, f_bwd)""" .

"DESCRIPTION.This code defines a data editor function that takes in a data dataframe and a column configuration dictionary. The column configuration specifies the properties of the \"price\" column, including the column name, help text, min and max values, step size, and format. The function also hides the index column in the displayed data." <EXPLAINS> """CODE.st.data_editor(
    data_df,
    column_config={
        "price": st.column_config.NumberColumn(
            "Price (in USD)",
            help="The price of the product in USD",
            min_value=0,
            max_value=1000,
            step=1,
            format="$%d",
        )
    },
    hide_index=True,
)""" .

"DESCRIPTION.This code defines a dataset containing sequences of frames (images) and corresponding actions (labels) for a certain number of frames (NB_FRAME). Each sequence in the dataset consists of frames with shape 64x64x3 and actions selected from a list of possible labels ('up', 'down', 'left', 'right'). The code also includes a generator that yields sequences of frames and actions in the specified format using numpy arrays and lists. Finally, the dataset is returned as tensors with the specified shapes and data types (uint8 for frames and int64 for actions)." <EXPLAINS> """CODE.tfds.SequenceDict({
    'frame': tfds.features.Image(shape=(64, 64, 3))
    'action': tfds.features.ClassLabel(['up', 'down', 'left', 'right'])
}, length=NB_FRAME)

yield {
    'frame': np.ones(shape=(NB_FRAME, 64, 64, 3)),
    'action': ['left', 'left', 'up', ...],
}

{
    'frame': tf.Tensor(shape=(NB_FRAME, 64, 64, 3), dtype=tf.uint8),
    'action': tf.Tensor(shape=(NB_FRAME,), dtype=tf.int64),
}""" .

"DESCRIPTION.This code defines a deep learning model using TensorFlow's Keras API with an input layer of shape (100,) and data type 'int32', followed by an Embedding layer, LSTM layer, and three Dense layers with RELU activation function. The model outputs a single unit with a sigmoid activation function. Finally, the code generates a visualization of the model's architecture and saves it as an image file." <EXPLAINS> """CODE.input = tf.keras.Input(shape=(100,), dtype='int32', name='input')
x = tf.keras.layers.Embedding(
    output_dim=512, input_dim=10000, input_length=100)(input)
x = tf.keras.layers.LSTM(32)(x)
x = tf.keras.layers.Dense(64, activation='relu')(x)
x = tf.keras.layers.Dense(64, activation='relu')(x)
x = tf.keras.layers.Dense(64, activation='relu')(x)
output = tf.keras.layers.Dense(1, activation='sigmoid', name='output')(x)
model = tf.keras.Model(inputs=[input], outputs=[output])
dot_img_file = '/tmp/model_1.png'
tf.keras.utils.plot_model(model, to_file=dot_img_file, show_shapes=True)
""" .

"DESCRIPTION.This code defines a dense layer neural network module that takes an input x and the number of features as input, applies a fully connected layer operation, followed by a rectified linear unit (ReLU) activation function, and returns the output." <EXPLAINS> """CODE.@nn.module
def DenseLayer(x, features):
    x = flax.nn.Dense(x, features)
    x = flax.nn.relu(x)
    return x

class DenseLayer(nn.Module):
    def apply(self, x, features):
        x = flax.nn.Dense(x, features)
        x = flax.nn.relu(x)
        return x""" .

"DESCRIPTION.This code defines a dictionary `pred_dict` containing two torch tensors as values, with keys 'pred1' and 'pred2'. It then calls a method `write_prediction_dict` with `pred_dict` as an argument." <EXPLAINS> """CODE.pred_dict = {'pred1': torch.tensor(...), 'pred2': torch.tensor(...)}
self.write_prediction_dict(pred_dict)""" .

"DESCRIPTION.This code defines a dictionary of convolutional layers using PaddlePaddle framework. The keys in the dictionary represent the type of convolutional layer (1D, 2D, or 3D) and the corresponding values are instances of the respective convolutional layer classes with specified parameters such as input channels, output channels, and kernel size. Finally, it prints out the key-value pairs of the layer names and their corresponding convolutional layer objects." <EXPLAINS> """CODE.import paddle
from collections import OrderedDict

sublayers = OrderedDict([
    ('conv1d', paddle.nn.Conv1D(3, 2, 3)),
    ('conv2d', paddle.nn.Conv2D(3, 2, 3)),
    ('conv3d', paddle.nn.Conv3D(4, 6, (3, 3, 3))),
])

layer_dict = paddle.nn.LayerDict(sublayers=sublayers)
for k, v in layer_dict.items():
    print(k, ":", v)

#conv1d : Conv1D(3, 2, kernel_size=[3], data_format=NCL)
#conv2d : Conv2D(3, 2, kernel_size=[3, 3], data_format=NCHW)
#conv3d : Conv3D(4, 6, kernel_size=[3, 3, 3], data_format=NCDHW)""" .

"DESCRIPTION.This code defines a dictionary of convolutional layers with different dimensions and kernel sizes using the PaddlePaddle library. It then creates a LayerDict object using the dictionary of layers. Finally, it prints out the details of each convolutional layer including the type (Conv1D, Conv2D, Conv3D), number of input and output channels, kernel size, and data format." <EXPLAINS> """CODE.import paddle
from collections import OrderedDict

sublayers = OrderedDict([
    ('conv1d', paddle.nn.Conv1D(3, 2, 3)),
    ('conv2d', paddle.nn.Conv2D(3, 2, 3)),
    ('conv3d', paddle.nn.Conv3D(4, 6, (3, 3, 3))),
])

layer_dict = paddle.nn.LayerDict(sublayers=sublayers)
for v in layer_dict.values():
    print(v)

#Conv1D(3, 2, kernel_size=[3], data_format=NCL)
#Conv2D(3, 2, kernel_size=[3, 3], data_format=NCHW)
#Conv3D(4, 6, kernel_size=[3, 3, 3], data_format=NCDHW)""" .

"DESCRIPTION.This code defines a dictionary of different Convolutional layers using the PaddlePaddle framework and performs operations like accessing, deleting, and clearing entries in the dictionary. The code also demonstrates the use of LayerDict to manage these layers efficiently." <EXPLAINS> """CODE.import paddle
import numpy as np
from collections import OrderedDict

sublayers = OrderedDict([
    ('conv1d', paddle.nn.Conv1D(3, 2, 3)),
    ('conv2d', paddle.nn.Conv2D(3, 2, 3)),
    ('conv3d', paddle.nn.Conv3D(4, 6, (3, 3, 3))),
])

layers_dict = paddle.nn.LayerDict(sublayers=sublayers)

l = layers_dict['conv1d']

for k in layers_dict:
    l = layers_dict[k]

len(layers_dict)
#3

del layers_dict['conv2d']
len(layers_dict)
#2

conv1d = layers_dict.pop('conv1d')
len(layers_dict)
#1

layers_dict.clear()
len(layers_dict)
#0""" .

"DESCRIPTION.This code defines a discriminator model for image classification with an input shape of 1 channel and image size 28x28. The discriminator model is implemented using a Sequential model in PyTorch." <EXPLAINS> """CODE.Discriminator(img_shape=(1, 28, 28))
Discriminator(
  (model): Sequential(...)
)""" .

"DESCRIPTION.This code defines a function `bar` that creates a resolution callback with a parameter of 1 and then prints the result of calling the callback with the argument \"foo\". The function `baz` sets a variable `foo` to 2 and then calls the function `bar`. Finally, the code executes the function `baz`." <EXPLAINS> """CODE.def bar():
    cb = createResolutionCallback(1)
    print(cb("foo"))

def baz():
    foo = 2
    bar()

baz()""",
        """CODE.def bar():
    cb = createResolutionCallbackFromFrame(1)
    print(cb("foo"))

def baz():
    foo = 2
    bar()

baz()""" .

"DESCRIPTION.This code defines a function `catboost_train_classifier` which trains a CatBoost classification model using the input training data and returns the accuracy, precision, and recall of the trained model. The function takes in the path to the training data file, the path to save the trained model, and the number of trees as input parameters." <EXPLAINS> """CODE.def add(a: float, b: float) -> float:
    \"\"\"Returns sum of two arguments\"\"\"
    return a + b

add_op = create_component_from_func(
    func=add,
    base_image='python:3.7', # Optional
    output_component_file='add.component.yaml', # Optional
    packages_to_install=['pandas==0.24'], # Optional
)

add_op.component_spec.save('add.component.yaml')

add_task = add_op(1, 3)

sum_output_ref = add_task.outputs['Output']

task2 = add_op(sum_output_ref, 5)

from typing import NamedTuple

def add_multiply_two_numbers(a: float, b: float) -> NamedTuple('Outputs', [('sum', float), ('product', float)]):
    \"\"\"Returns sum and product of two arguments\"\"\"
    return (a + b, a * b)

add_multiply_op = create_component_from_func(add_multiply_two_numbers)

add_multiply_task = add_multiply_op(1, 3)

sum_output_ref = add_multiply_task.outputs['sum']

task2 = add_multiply_op(sum_output_ref, 5)

def catboost_train_classifier(
    training_data_path: InputPath('CSV'),            # Path to input data file of type "CSV"
    trained_model_path: OutputPath('CatBoostModel'), # Path to output data file of type "CatBoostModel"
    number_of_trees: int = 100,                      # Small output of type "Integer"
) -> NamedTuple('Outputs', [
    ('Accuracy', float),  # Small output of type "Float"
    ('Precision', float), # Small output of type "Float"
    ('JobUri', 'URI'),    # Small output of type "URI"
]):
    \"\"\"Trains CatBoost classification model\"\"\"
    ...

    return (accuracy, precision, recall)
""" .

"DESCRIPTION.This code defines a function `decayed_learning_rate(step)` that calculates a learning rate based on an exponential decay formula. The learning rate is calculated as the initial learning rate multiplied by the decay rate raised to the power of the step divided by the number of decay steps. The code then uses this decayed learning rate to create a learning rate schedule using TensorFlow's ExponentialDecay function, and it compiles a model using stochastic gradient descent optimizer with this learning rate schedule. Finally, the model is trained on the provided data and labels for 5 epochs." <EXPLAINS> """CODE.def decayed_learning_rate(step):
  return initial_learning_rate * decay_rate ^ (step / decay_steps)


initial_learning_rate = 0.1
lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(
    initial_learning_rate,
    decay_steps=100000,
    decay_rate=0.96,
    staircase=True)

model.compile(optimizer=tf.keras.optimizers.SGD(learning_rate=lr_schedule),
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])

model.fit(data, labels, epochs=5)
""" .

"DESCRIPTION.This code defines a function `f` that computes the element-wise sine of the dot product between two matrices `A` and `x`. It then evaluates the polymorphic shape of the output of this function using different shape configurations (\"a, b\" and \"b, c\"), and prints the shape of the output tensor and the polymorphic shape. Finally, it defines a lambda function that transposes the input and evaluates the polymorphic shape with the previously determined output shape configuration." <EXPLAINS> """CODE.import jax
from jax.experimental import jax2tf
from jax import numpy as jnp

f = lambda A, x: jnp.sin(jnp.dot(A, x))
A = jax.ShapeDtypeStruct((2000, 3000), jnp.float32)
x = jax.ShapeDtypeStruct((3000, 1000), jnp.float32)
out_spec, out_poly_shape = jax2tf.eval_polymorphic_shape(f, polymorphic_shapes=["a, b", "b, c"])(A, x)
print(out_spec.shape)
print(out_poly_shape)
res_spec, res_poly_shape = jax2tf.eval_polymorphic_shape(lambda x: x.T, polymorphic_shapes=[out_poly_shape])(out_spec)
print(res_poly_shape)
""" .

"DESCRIPTION.This code defines a function `my_function` that prints \"Hello world!\" to the console when called." <EXPLAINS> """CODE.@experimental
def my_function():
    print("Hello world!")
""" .

"DESCRIPTION.This code defines a function `print_log_msg` that prints out a message. It then creates a logs socket using a provided `client` object with specific parameters, and runs the socket indefinitely to listen for log messages. Finally, it starts a separate thread to run the socket and then closes the socket when finished." <EXPLAINS> """CODE.def print_log_msg(ws_app, msg):
    print(msg)

logs_socket = client.create_cluster_logs_socket("cluster_id", 1661100000, 1661101000, print_log_msg)
logs_socket.run_forever()


def print_log_msg(ws_app, msg):
    print(msg)

logs_socket = client.create_cluster_logs_socket("cluster_id", 1661100000, 1661101000, print_log_msg)

logs_thread = Thread(target=cluster_logs_socket.run_forever)

logs_thread.start()
# .......

logs_socket.close()
""" .

"DESCRIPTION.This code defines a function `unpack_rgb()` that takes in a binary data, a data type, bit depths per sample, and a boolean parameter for rescaling. The function parses the binary data based on the specified data type and converts the samples if rescale is set to True. The function then returns an array of the parsed and optionally rescaled samples. The code also generates a data sample and calls the `unpack_rgb()` function with different parameters to demonstrate its functionality." <EXPLAINS> """CODE.import struct
import numpy as np

def unpack_rgb(data, dtype, bitspersample, rescale=True):
    result = []
    for i in range(0, len(data), len(bitspersample)):
        sample = struct.unpack(dtype, data[i:i+len(bitspersample)])[0]
        if rescale:
            sample = sample * (2**len(bitspersample) - 1) // 255
        result.append(sample)
    return np.array(result)

data = struct.pack('BBBB', 0x21, 0x08, 0xff, 0xff)
print(unpack_rgb(data, '<B', (5, 6, 5), False))
print(unpack_rgb(data, '<B', (5, 6, 5)))
print(unpack_rgb(data, '<B', (5, 5, 5)))
""" .

"DESCRIPTION.This code defines a function called \"cb\" that takes an index as input and returns the element at that index in the global_input_data array. It then initializes a GlobalDeviceArray object \"gda\" using the GlobalDeviceArray.from_callback method with specified parameters. Finally, it accesses the shape of the addressable data at index 0, which is (4, 2)." <EXPLAINS> """CODE.def cb(index):
    return global_input_data[index]

gda = GlobalDeviceArray.from_callback(global_input_shape, global_mesh, mesh_axes, cb)
gda.addressable_data(0).shape
(4, 2)""" .

"DESCRIPTION.This code defines a function called `call` that takes another function as input. Inside the `call` function, it uses TensorFlow profiler to trace the execution of the input function. It then compiles the input function using just-in-time compilation and executes the compiled binary code." <EXPLAINS> """CODE.  def call(function):
    with tf.profiler.experimental.Trace("call",
         function_name=function.name) as tm:
      binary, in_cache = jit_compile(function)
      tm.set_metadata(in_cache=in_cache)
      execute(binary)
""" .

"DESCRIPTION.This code defines a function called `range` in a class called `Dataset`, which generates a list of numbers based on the input parameters. The function can generate a list of numbers starting from a specified start value (defaulting to 0 if not provided) up to but not including a specified end value, with an optional step value provided to determine the spacing between numbers in the list. If the step value is negative, the function generates an empty list." <EXPLAINS> """CODE.Dataset.range(5) == [0, 1, 2, 3, 4]
Dataset.range(2, 5) == [2, 3, 4]
Dataset.range(1, 5, 2) == [1, 3]
Dataset.range(1, 5, -2) == []
Dataset.range(5, 1) == []
Dataset.range(5, 1, -2) == [5, 3]
""" .

"DESCRIPTION.This code defines a function called find_pad_index that takes an array as input and iterates through the array to find the index of the first occurrence of the number 0. It then returns this index." <EXPLAINS> """CODE.def find_pad_index(array):
    for idx, num in enumerate(array):
        if num == 0:
            return idx
""" .

"DESCRIPTION.This code defines a function called inc that takes an integer as input and returns the input value incremented by 1. It also demonstrates how to retrieve the implementation of the inc function for integers and calls it with an integer input value to get the result. Lastly, it shows that there is no implementation of the inc function for float input values." <EXPLAINS> """CODE.from multipledispatch import dispatch
@dispatch(int)
... def inc(x):
...     return x + 1
implementation = inc.dispatch(int)
implementation(3)
4
print(inc.dispatch(float))
None""" .

"DESCRIPTION.This code defines a function called linear that takes an input x. If torch.jit.is_scripting() returns False, it calls torch.linear(x), otherwise it calls unsupported_linear_op(x)." <EXPLAINS> """CODE.def linear(x):
   if not torch.jit.is_scripting():
      return torch.linear(x)
   else:
      return unsupported_linear_op(x)""" .

"DESCRIPTION.This code defines a function called myParamValidator that performs advanced validation logic on a parameter and returns True. It then calls a method DoSomething on a mock data access object (dao) with Func(myParamValidator) as a parameter." <EXPLAINS> """CODE.def myParamValidator(param):
  # Advanced logic here
  return True

mock_dao.DoSomething(Func(myParamValidator), true)""" .

"DESCRIPTION.This code defines a function called nullfunc that takes one positional argument (arg) and one keyword argument (kwarg) without performing any operations." <EXPLAINS> "CODE.nullfunc('arg', kwarg='kwarg')" .

"DESCRIPTION.This code defines a function called print_something, which prints a message indicating the process index when called." <EXPLAINS> """CODE.@accelerator.on_local_process(local_process_index=2)
def print_something():
    print(f"Printed on process {accelerator.local_process_index}")


print_something()
""" .

"DESCRIPTION.This code defines a function called product that takes an iterable as input, multiplies all the elements in the iterable together, and returns the result. It then calculates and prints the product of the elements in two lists, [256, 1073741824] and an empty list respectively." <EXPLAINS> """CODE.def product(iterable):
    result = 1
    for num in iterable:
        result *= num
    return result

print(product([2**8, 2**30]))
print(product([]))""" .

"DESCRIPTION.This code defines a function called some_function that takes two arguments but does not perform any specific operation." <EXPLAINS> """CODE.@_clean_up_sig
def some_function(self, unused_element_argument, stuff):
    pass""" .

"DESCRIPTION.This code defines a function named \"rmse\" which calculates the root mean squared error (RMSE) between the predictions \"pred\" and the target values \"target\". The function allows for different methods of reduction such as element-wise mean or sum. It also has an option to return the RMSE score and the error values. The code then calculates RMSE between two tensor arrays \"x\" and \"y\" and prints the result." <EXPLAINS> """CODE.def rmse(pred, target, reduction='elementwise_mean', return_state=False):
    error = (pred - target) ** 2
    if reduction == 'elementwise_mean':
        score = error.mean().sqrt()
    elif reduction == 'sum':
        score = error.sum().sqrt()
    else:
        score = error.sqrt()
    if return_state:
        return score, error
    return score

x = torch.tensor([0., 1, 2, 3])
y = torch.tensor([0., 1, 2, 2])
print(rmse(x, y))
""" .

"DESCRIPTION.This code defines a function named `unpackrgb` that takes in `data`, `dtype`, `bitspersample`, and `rescale` as input parameters. The function unpacks the RGB data in `data` using the specified data type and bit depth per sample. It then rescales the sample values if `rescale` is set to True. The function returns an array of the unpacked and possibly rescaled RGB sample values." <EXPLAINS> """CODE.import struct
import numpy as np

def unpackrgb(data, dtype, bitspersample, rescale=True):
    result = []
    for i in range(0, len(data), len(bitspersample)):
        sample = struct.unpack(dtype, data[i:i+len(bitspersample)])[0]
        if rescale:
            sample = sample * (2**len(bitspersample) - 1) // 255
        result.append(sample)
    return np.array(result)

data = struct.pack('BBBB', 0x21, 0x08, 0xff, 0xff)
print(unpackrgb(data, '<B', (5, 6, 5), False))
print(unpackrgb(data, '<B', (5, 6, 5)))
print(unpackrgb(data, '<B', (5, 5, 5)))
""" .

"DESCRIPTION.This code defines a function named func_nested_input that takes a dictionary x_dict with keys \"a\" or \"b\" mapping to torch tensors, and a tuple y_tuple containing a torch tensor followed by a nested tuple of two torch tensors. The function returns the sum of the tensors x, y1, y2, and y3. The code then uses torch.onnx.dynamo_export to export func_nested_input as an ONNX program, and prints the original x_dict and y_tuple as well as the adapted inputs for the ONNX program." <EXPLAINS> """CODE.import torch
import torch.onnx
from typing import Dict, Tuple
def func_nested_input(
...     x_dict: Dict[str, torch.Tensor],
...     y_tuple: Tuple[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]
... ):
...     if "a" in x_dict:
...         x = x_dict["a"]
...     elif "b" in x_dict:
...         x = x_dict["b"]
...     else:
...         x = torch.randn(3)
...
...     y1, (y2, y3) = y_tuple
...
...     return x + y1 + y2 + y3
x_dict = {"a": torch.tensor(1.)}
y_tuple = (torch.tensor(2.), (torch.tensor(3.), torch.tensor(4.)))
onnx_program = torch.onnx.dynamo_export(func_nested_input, x_dict, y_tuple)
print(x_dict, y_tuple)
{'a': tensor(1.)} (tensor(2.), (tensor(3.), tensor(4.)))
print(onnx_program.adapt_torch_inputs_to_onnx(x_dict, y_tuple, model_with_state_dict=func_nested_input))
(tensor(1.), tensor(2.), tensor(3.), tensor(4.))
""" .

"DESCRIPTION.This code defines a function named print_something that will print a specific message only on process 0 of each server." <EXPLAINS> """CODE.@state.on_local_main_process
def print_something():
    print("This will be printed by process 0 only on each server.")

print_something()
""" .

"DESCRIPTION.This code defines a function named subsegment that takes in data, frames, and n_segments as input parameters. It then initializes an empty list called boundaries. The function does some computation (not provided in the code snippet) and returns an array of boundaries. The code snippet also provides an example usage of the subsegment function by loading an audio file using librosa, extracting beats and tempo, computing the constant-Q transform (cqt), and then calling the subsegment function with cqt, beats, and n_segments=2 arguments. Finally, it prints the result of the subsegment function." <EXPLAINS> """CODE.import numpy as np
import librosa

def subsegment(data, frames, n_segments):
    boundaries = []
    # Your code here
    return np.array(boundaries)

# Example usage
y, sr = librosa.load(librosa.util.example_audio_file(), duration=15)
tempo, beats = librosa.beat.beat_track(y=y, sr=sr, hop_length=512)
cqt = librosa.cqt(y, sr=sr, hop_length=512)
subseg = subsegment(cqt, beats, n_segments=2)
print(subseg)""" .

"DESCRIPTION.This code defines a function named test that prints the values of three parameters a, b, and c with default values when called. The function is wrapped with an environment variable wrapper that uses \"FOO_\" as a prefix for the environment variables." <EXPLAINS> """CODE.from tqdm.utils import envwrap
@envwrap("FOO_")
def test(a=1, b=2, c=3):
    print(f"received: a={a}, b={b}, c={c}")

""" .

"DESCRIPTION.This code defines a function named test_foo that takes a record_testsuite_property function as its argument. Inside the function, it calls the record_testsuite_property function twice with different arguments. The first call records the property \"ARCH\" with the value \"PPC\", and the second call records the property \"STORAGE_TYPE\" with the value \"CEPH\"." <EXPLAINS> """CODE... code-block:: python

    def test_foo(record_testsuite_property):
        record_testsuite_property("ARCH", "PPC")
        record_testsuite_property("STORAGE_TYPE", "CEPH")""" .

"DESCRIPTION.This code defines a function named test_foo that takes two arguments, self and config, and does nothing inside the function block. The function is decorated with @test_all_configs, indicating that it is a test function designed to be run with multiple configurations." <EXPLAINS> """CODE.@test_all_configs
def test_foo(self, config):
    pass""" .

"DESCRIPTION.This code defines a function named test_tifffile that accepts an optional boolean parameter verbose." <EXPLAINS> "CODE.test_tifffile(verbose=False)" .

"DESCRIPTION.This code defines a function setting_is_true that takes a cli argument and returns a boolean value. It then creates a ConditionalRegistry instance with the registry and the setting_is_true function as arguments." <EXPLAINS> """CODE.@Condition
def setting_is_true(cli):
    return True  # or False

registy = ConditionalRegistry(registry, setting_is_true)""" .

"DESCRIPTION.This code defines a function that builds a PCollection in Apache Beam. The PCollection is created by reading the list of files in the given directory and processing each file using the `_process_file` function." <EXPLAINS> """CODE.
def _build_pcollection(pipeline, extracted_dir):
  return (
      pipeline
      | beam.Create(gfile.io.listdir(extracted_dir))
      | beam.Map(_process_file)
  )
""" .

"DESCRIPTION.This code defines a function that cleans a given text by removing specific tags and extracts entities with bounding boxes. The function then returns the cleaned text and a list of entities, where each entity is represented by a tuple containing the entity name, position in the text, and corresponding bounding box coordinates." <EXPLAINS> """CODE.def clean_text_and_extract_entities_with_bboxes(text):
    clean_text = text.replace('<grounding>', '').replace('<phrase>', '').replace('</phrase>', '').replace('<object>', '').replace('</object>', '').replace('<patch_index_0044>', '').replace('<patch_index_0863>', '').replace('<patch_index_0005>', '').replace('<patch_index_0911>', '')
    entities = []
    entities.append(('a snowman', (12, 21), [(0.390625, 0.046875, 0.984375, 0.828125)]))
    entities.append(('a fire', (41, 47), [(0.171875, 0.015625, 0.484375, 0.890625)])
    return clean_text, entities
""" .

"DESCRIPTION.This code defines a function that creates a data loader for validation data. The function sets up data transformation, loads the MNIST dataset, and creates a DataLoader object with specified batch size and shuffle settings. It returns the DataLoader object for the validation data. Additionally, it can also return multiple data loaders if needed." <EXPLAINS> """CODE.def val_dataloader(self):
    transform = transforms.Compose([transforms.ToTensor(),
                                    transforms.Normalize((0.5,), (1.0,))])
    dataset = MNIST(root='/path/to/mnist/', train=False,
                    transform=transform, download=True)
    loader = torch.utils.data.DataLoader(
        dataset=dataset,
        batch_size=self.batch_size,
        shuffle=False
    )

    return loader

# can also return multiple dataloaders
def val_dataloader(self):
    return [loader_a, loader_b, ..., loader_n]
""" .

"DESCRIPTION.This code defines a function that creates a neural network model using a single fully connected layer to make predictions based on input data with 13 features. It then sets up an inferencer object with the defined inference program and specified parameter path for making predictions on new data using the trained model." <EXPLAINS> """CODE.def inference_program():
    x = fluid.layers.data(name='x', shape=[13], dtype='float32')
    y_predict = fluid.layers.fc(input=x, size=1, act=None)
    return y_predict

place = fluid.CPUPlace()
inferencer = fluid.Inferencer(
    infer_func=inference_program, param_path="/tmp/model", place=place)""" .

"DESCRIPTION.This code defines a function that learns a scaling parameter and applies it to an input x using JAX library functions." <EXPLAINS> """CODE.def learn_scale(scope, x):
    p = scope.param('scale', nn.initializers.zeros, ())
    return p * x

def f(scope, x):
    vars_t = jax.tree_map(jnp.ones_like, scope.variables().get('params', {}))
    x, out_t = lift.jvp(
        learn_scale, scope, (x,), (jnp.zeros_like(x),),
        variable_tangents={'params': vars_t})
    return out_t""" .

"DESCRIPTION.This code defines a function that learns the scale of the input and returns the scaled product of the input variables x and y. It also includes a function that computes the value and gradients of the scaled product function with respect to x and y." <EXPLAINS> """CODE.def learn_scale(scope, x, y):
    p = scope.param('scale', nn.initializers.zeros_init(), ())
    return p * x * y
def f(scope, x, y):
    z, x_grad, y_grad = lift.value_and_grad(learn_scale, scope, x, y)
    return z, x_grad, y_grad
""" .

"DESCRIPTION.This code defines a function that reads data from a file line by line, processes each line to create numpy arrays of input data and labels, and yields batches of data as dictionaries. The generated data is then used to train a model using model.fit_generator()." <EXPLAINS> """CODE.def generate_arrays_from_file(path):
    while True:
        with open(path) as f:
            for line in f:
                # create numpy arrays of input data
                # and labels, from each line in the file
                x1, x2, y = process_line(line)
                yield ({'input_1': x1, 'input_2': x2}, {'output': y})

model.fit_generator(generate_arrays_from_file('/my_file.txt'),
                    steps_per_epoch=10000, epochs=10)
""" .

"DESCRIPTION.This code defines a function that returns a sorted list of exported variable names from the provided context." <EXPLAINS> """CODE.@contextfunction
def get_exported_names(context):
    return sorted(context.exported_vars)""" .

"DESCRIPTION.This code defines a function that scales an input tensor based on a parameter called 'scale'. The function first retrieves the 'scale' parameter from the scope, then multiplies it with the input tensor 'x' to produce the output. Another function 'f' is defined that computes the Jacobian-vector product of the 'learn_scale' function with respect to the input tensor 'x' using JAX library. The function computes the output and returns it." <EXPLAINS> """CODE.def learn_scale(scope, x):
    p = scope.param('scale', nn.initializers.zeros, ())
    return p * x

def f(scope, x):
    vars_t = jax.tree_map(jnp.ones_like, scope.variables().get('params', {}))
    x, out_t = lift.jvp(
        learn_scale, scope, (x,), (jnp.zeros_like(x),),
        variable_tangents={'params': vars_t})
    return out_t""" .

"DESCRIPTION.This code defines a function that takes a single input parameter, applies the sine function to the input, adds the input to the result and returns the sum. Then it utilizes a library to perform error checking on the function with respect to float type checks, specifically checking for errors when the input is infinity." <EXPLAINS> """CODE.@jax.jit
def f(x):
  y = jnp.sin(x)
  return x+y
err, out = checkify.checkify(f, errors=checkify.float_checks)(jnp.inf)
err.throw()  # doctest: +IGNORE_EXCEPTION_DETAIL
""" .

"DESCRIPTION.This code defines a function that takes a tree data structure as input and returns a list of all the leaf nodes in the tree. Leaf nodes are nodes that do not have any child nodes." <EXPLAINS> """CODE.tree = {'b': (2, [3, 4]), 'a': 1, 'c': None, 'd': 5}
tree_leaves(tree)
[1, 2, 3, 4, None, 5]
tree_leaves(1)
[1]
tree_leaves(None)
[None]""" .

"DESCRIPTION.This code defines a function that takes an input array, converts it to a PaddlePaddle variable, checks if the mean of the array is greater than 0, and then subtracts 1 if the mean is positive, or adds 1 if the mean is not positive. Finally, it uses a program translator to obtain the output of the function when the input is an array of ones, and prints the resulting array." <EXPLAINS> """CODE.import paddle.fluid as fluid
import numpy as np

def func(x):
    x = fluid.dygraph.to_variable(x)
    if fluid.layers.mean(x) > 0:
        x_v = x - 1
    else:
        x_v = x + 1
    return x_v

prog_trans = fluid.dygraph.ProgramTranslator()

x = np.ones([1, 2])
x_v = prog_trans.get_output(func, x)
print(x_v.numpy()) # [[0. 0.]]
""" .

"DESCRIPTION.This code defines a function that takes in a scope, a variable x, and a predicate. It creates state variables true_count and false_count which are incremented based on the outcome of the predicate. It then defines two functions, true_fn and false_fn, which update the state variables and return the result of a dense neural network layer. Finally, it uses a conditional statement based on the predicate to choose between true_fn and false_fn and return the result." <EXPLAINS> """CODE.def cond_example(scope, x, pred):
    scope.variable('state', 'true_count', lambda: 0)
    scope.variable('state', 'false_count', lambda: 0)
    def true_fn(scope, x):
        scope.variable('state', 'true_count').value += 1
        return scope.child(nn.dense)(x, 2)
    def false_fn(scope, x):
        scope.variable('state', 'false_count').value += 1
        return -scope.child(nn.dense)(x, 2)
    return lift.cond(pred, true_fn, false_fn, scope, x)""" .

"DESCRIPTION.This code defines a function train that takes a configuration and a dataset as input, iterates through the dataset, computes loss for each sample, and reports the loss using Ray Tune. It then creates a HugeDataset object with download set to True and runs the tune experiment using the train function with the specified dataset." <EXPLAINS> """CODE... code-block:: python

    from ray import tune

    def train(config, data=None):
        for sample in data:
            # ...
            tune.report(loss=loss)

    data = HugeDataset(download=True)

    tune.run(
        tune.with_parameters(train, data=data),
        #...
    )""" .

"DESCRIPTION.This code defines a generator function that reads data from a file line by line, processes each line to create numpy arrays of input data and labels, and yields batches of data in the format suitable for training a neural network model. The generator function is then used to train a model using the fit_generator method with specified number of samples per epoch and number of epochs." <EXPLAINS> """CODE.    def generate_arrays_from_file(path):
        while 1:
            f = open(path)
            for line in f:
                # create numpy arrays of input data
                # and labels, from each line in the file
                x1, x2, y = process_line(line)
                yield ({'input_1': x1, 'input_2': x2}, {'output': y})
            f.close()

    model.fit_generator(generate_arrays_from_file('/my_file.txt'),
                        samples_per_epoch=10000, nb_epoch=10)
""" .

"DESCRIPTION.This code defines a generator model for a deep learning model. The generator model takes an input image shape of 1x8x8 and uses a sequential model for generating outputs." <EXPLAINS> """CODE.Generator(img_shape=(1, 8, 8))
Generator(
  (model): Sequential(...)
)""" .

"DESCRIPTION.This code defines a generator model for generating images with a specified shape of 1 channel and 8x8 pixels." <EXPLAINS> """CODE.Generator(img_shape=(1, 8, 8))
Generator(
  (model): Sequential(...)
)""" .

"DESCRIPTION.This code defines a generator model in Python for generating images of a specified shape (1 channel, 8x8 pixels)." <EXPLAINS> """CODE.Generator(img_shape=(1, 8, 8))
Generator(
  (model): Sequential(...)
)""" .

"DESCRIPTION.This code defines a generator model in Python for generating images with a specific shape of 1 channel and 8x8 pixels." <EXPLAINS> """CODE.Generator(img_shape=(1, 8, 8))
Generator(
  (model): Sequential(...)
)""" .

"DESCRIPTION.This code defines a global input shape of 8x8, creates a mesh with x and y axes, generates global input data as a 1D array of numbers from 0 to 63, defines a callback function that retrieves data from the global input data based on index, and initializes a GlobalDeviceArray object with specified parameters. Finally, it retrieves the shape of the addressable data at index 0 from the GlobalDeviceArray object." <EXPLAINS> """CODE.global_input_shape = (8, 8)
mesh_axes = P('x', 'y')
global_mesh = Mesh(np.array(jax.devices()).reshape(2, 4), ('x', 'y'))
global_input_data = np.arange(math.prod(global_input_shape)).reshape(global_input_shape)

def cb(index):
    return global_input_data[index]

gda = GlobalDeviceArray.from_callback(global_input_shape, global_mesh, mesh_axes, cb)
gda.addressable_data(0).shape
""" .

"DESCRIPTION.This code defines a language key and provides functions to set and get the language value in the current context object." <EXPLAINS> """CODE.LANG_KEY = __name__ + '.lang'

def set_language(value):
    ctx = get_current_context()
    ctx.meta[LANG_KEY] = value

def get_language():
    return get_current_context().meta.get(LANG_KEY, 'en_US')
""" .

"DESCRIPTION.This code defines a language key variable, sets the language value in the current context metadata, and retrieves the language value from the current context metadata with a default value of 'en_US'." <EXPLAINS> """CODE.LANG_KEY = __name__ + '.lang'

def set_language(value):
    ctx = get_current_context()
    ctx.meta[LANG_KEY] = value

def get_language():
    return get_current_context().meta.get(LANG_KEY, 'en_US')
""" .

"DESCRIPTION.This code defines a lightweight autoencoder model in Python, consisting of an encoder and a decoder." <EXPLAINS> """CODE.LitAutoEncoder()  # doctest: +ELLIPSIS +NORMALIZE_WHITESPACE
LitAutoEncoder(
  (encoder): ...
  (decoder): ...
)""" .

"DESCRIPTION.This code defines a linear function that takes an input variable and applies a linear transformation using a Linear layer with 32 output units. It then calculates two outputs 'y' and 'z' by passing the input through the Linear layer. It uses the Adam optimizer to minimize the loss value of 'z' during training for 10 iterations. Finally, it saves the inference model with 'y' as the output." <EXPLAINS> """CODE.import paddle.fluid as fluid
import numpy as np

from paddle.fluid.dygraph.nn import Linear

@fluid.dygraph.declarative
def linear_func(x):
    x = fluid.dygraph.to_variable(x)
    linear = Linear(32, 1)
    y = linear(x)
    z = linear(x)
    return y, z

prog_trans = fluid.dygraph.ProgramTranslator()

adam = fluid.optimizer.AdamOptimizer(learning_rate=0.001)
prog_trans.set_optimizer(adam,index_of_loss=1) # minimize on 'z'

for i in range(10):
    y, z_loss = linear_func(np.ones(32).astype('float32'))
    print(z_loss.numpy())

# Save inference model.
# Note that fetch=[0] means we set 'y' as the inference output.
prog_trans.save_inference_model("./dy2stat_infer_model", fetch=[0])""" .

"DESCRIPTION.This code defines a linear model with 13 input features and 5 output features using the PaddlePaddle framework. It initializes the model parameters, performs a forward pass, calculates gradients using backpropagation, optimizes the model using the Adam optimizer with a learning rate of 0.01, and clears the gradients for the next iteration." <EXPLAINS> """CODE.import paddle.fluid as fluid
import numpy as np

with fluid.dygraph.guard():
    value = np.arange(26).reshape(2, 13).astype("float32")
    a = fluid.dygraph.to_variable(value)
    linear = fluid.Linear(13, 5, dtype="float32")
    # This can be any optimizer supported by dygraph.
    adam = fluid.optimizer.Adam(learning_rate = 0.01,
                                parameter_list = linear.parameters())
    out = linear(a)
    out.backward()
    adam.minimize(out)
    adam.clear_gradients()""" .

"DESCRIPTION.This code defines a linear model, trains it using stochastic gradient descent optimizer and mean squared error loss function for a specified number of epochs." <EXPLAINS> """CODE.model = LinearModel()
model.compile(optimizer='sgd', loss='mse')
model.fit(x, y, epochs=epochs)


model = LinearModel()
opt = tf.keras.optimizers.Adam()
loss_fn = tf.keras.losses.MeanSquaredError()
with tf.GradientTape() as tape:
  output = model(sparse_input)
  loss = tf.reduce_mean(loss_fn(target, output))
grads = tape.gradient(loss, model.weights)
opt.apply_gradients(zip(grads, model.weights))
""" .

"DESCRIPTION.This code defines a list s2 with elements [1, 1, 2], removes the first occurrence of value 1 from s2, then removes the value 2 from the resulting list, and finally returns a list with elements [1, 1]." <EXPLAINS> """CODE.s2 = pbag([1, 1, 2])
s3 = s2.remove(1)
s4 = s3.remove(2)
s4
pbag([1, 1])""" .

"DESCRIPTION.This code defines a mapping class called IntToFloatMap that ensures the keys are integers and the values are floats. It also includes an invariant check to validate the mapping between keys and values." <EXPLAINS> """CODE.class IntToFloatMap(CheckedPMap):
    __key_type__ = int
    __value_type__ = float
    __invariant__ = lambda k, v: (int(v) == k, 'Invalid mapping')

IntToFloatMap({1: 1.5, 2: 2.25})""" .

"DESCRIPTION.This code defines a method to create a data loader for training data using the MNIST dataset with specified transformations and parameters." <EXPLAINS> """CODE.def train_dataloader(self):
    dataset = MNIST(root=PATH, train=True, transform=transforms.ToTensor(), download=False)
    loader = torch.utils.data.DataLoader(dataset=dataset)
    return loader""" .

"DESCRIPTION.This code defines a method to create a data loader for validation data. It normalizes and converts the data to tensors using predefined transformations on the MNIST dataset. It then creates a DataLoader object with specified batch size and shuffle settings. It can also return multiple data loaders for different datasets." <EXPLAINS> """CODE.def val_dataloader(self):
    transform = transforms.Compose([transforms.ToTensor(),
                                    transforms.Normalize((0.5,), (1.0,))])
    dataset = MNIST(root='/path/to/mnist/', train=False,
                    transform=transform, download=True)
    loader = torch.utils.data.DataLoader(
        dataset=dataset,
        batch_size=self.batch_size,
        shuffle=False
    )

    return loader

# can also return multiple dataloaders
def val_dataloader(self):
    return [loader_a, loader_b, ..., loader_n]
""" .

"DESCRIPTION.This code defines a method to create a dense layer and then applies this dense layer to the input data x." <EXPLAINS> """CODE.@nowrap
def _make_dense(self, num_features):
  return nn.Dense(num_features)

@compact
def __call__(self, x):
  # now safe to use constructor helper even if using named_call
  dense = self._dense(self.num_features)
  return dense(x)""" .

"DESCRIPTION.This code defines a minimal implementation of a recurrent neural network (RNN) cell in Python. The RNN cell has methods for initializing the cell with a specific number of units, building the cell by adding weights, and performing the computation for a single time step by taking inputs and previous states as input and returning the output and new states." <EXPLAINS> """CODE.  class MinimalRNNCell(AbstractRNNCell):

    def __init__(self, units, **kwargs):
      self.units = units
      super(MinimalRNNCell, self).__init__(**kwargs)

    @property
    def state_size(self):
      return self.units

    def build(self, input_shape):
      self.kernel = self.add_weight(shape=(input_shape[-1], self.units),
                                    initializer='uniform',
                                    name='kernel')
      self.recurrent_kernel = self.add_weight(
          shape=(self.units, self.units),
          initializer='uniform',
          name='recurrent_kernel')
      self.built = True

    def call(self, inputs, states):
      prev_output = states[0]
      h = backend.dot(inputs, self.kernel)
      output = h + backend.dot(prev_output, self.recurrent_kernel)
      return output, output
""" .

"DESCRIPTION.This code defines a neural network architecture that takes in variable-length sequences of integers as inputs. It first embeds the input sequences using an embedding layer, then applies a 1D convolutional layer to encode the sequences. It then performs query-value attention on the encoded sequences and reduces them over the sequence axis to produce encodings. Finally, it concatenates the query and document encodings to be used as input for a deep neural network." <EXPLAINS> """CODE.# Variable-length int sequences.
query_input = tf.keras.Input(shape=(None,), dtype='int32')
value_input = tf.keras.Input(shape=(None,), dtype='int32')

# Embedding lookup.
token_embedding = tf.keras.layers.Embedding(input_dim=1000, output_dim=64)
# Query embeddings of shape [batch_size, Tq, dimension].
query_embeddings = token_embedding(query_input)
# Value embeddings of shape [batch_size, Tv, dimension].
value_embeddings = token_embedding(value_input)

# CNN layer.
cnn_layer = tf.keras.layers.Conv1D(
    filters=100,
    kernel_size=4,
    # Use 'same' padding so outputs have the same shape as inputs.
    padding='same')
# Query encoding of shape [batch_size, Tq, filters].
query_seq_encoding = cnn_layer(query_embeddings)
# Value encoding of shape [batch_size, Tv, filters].
value_seq_encoding = cnn_layer(value_embeddings)

# Query-value attention of shape [batch_size, Tq, filters].
query_value_attention_seq = tf.keras.layers.Attention()(
    [query_seq_encoding, value_seq_encoding])

# Reduce over the sequence axis to produce encodings of shape
# [batch_size, filters].
query_encoding = tf.keras.layers.GlobalAveragePooling1D()(
    query_seq_encoding)
query_value_attention = tf.keras.layers.GlobalAveragePooling1D()(
    query_value_attention_seq)

# Concatenate query and document encodings to produce a DNN input layer.
input_layer = tf.keras.layers.Concatenate()(
    [query_encoding, query_value_attention])

# Add DNN layers, and create Model.
# ...
""" .

"DESCRIPTION.This code defines a neural network architecture with two linear layers." <EXPLAINS> """CODE.Backbone()
(l1): Linear(...)
(l2): Linear(...)  """ .

"DESCRIPTION.This code defines a neural network backbone with two linear layers." <EXPLAINS> """CODE.Backbone()
Backbone(
  (l1): Linear(...)
  (l2): Linear(...)
)""" .

"DESCRIPTION.This code defines a neural network layer that calculates the dot product of weights and input data, applies a rectified linear activation function, and returns the resulting output." <EXPLAINS> """CODE.import jax

@jax.jit
def layer(w, x):
  with jax.named_scope("dot_product"):
    logits = w.dot(x)
  with jax.named_scope("activation"):
    return jax.nn.relu(logits)
""" .

"DESCRIPTION.This code defines a neural network layer with 3 input nodes and 2 output nodes. It adds two metrics to the layer to calculate the maximum and minimum values of the output. The resulting metrics are named 'max' and 'min'." <EXPLAINS> """CODE.input = tf.keras.layers.Input(shape=(3,))
d = tf.keras.layers.Dense(2)
output = d(input)
d.add_metric(tf.reduce_max(output), name='max')
d.add_metric(tf.reduce_min(output), name='min')
[m.name for m in d.metrics]
['max', 'min']""" .

"DESCRIPTION.This code defines a neural network model that takes in input features, such as 'price' and 'keywords', and processes them through a series of dense layers with ReLU activations to make a prediction output. The model is constructed using TensorFlow's feature columns, fixed size partitioner, and Keras layers." <EXPLAINS> """CODE.price = tf.feature_column.numeric_column('price')
keywords_embedded = tf.feature_column.embedding_column(
    tf.feature_column.categorical_column_with_hash_bucket("keywords", 10K),
    dimension=16)
columns = [price, keywords_embedded, ...]
partitioner = tf.compat.v1.fixed_size_partitioner(num_shards=4)
feature_layer = tf.compat.v1.keras.layers.DenseFeatures(
    feature_columns=columns, partitioner=partitioner)

features = tf.io.parse_example(
    ..., features=tf.feature_column.make_parse_example_spec(columns))
dense_tensor = feature_layer(features)
for units in [128, 64, 32]:
  dense_tensor = tf.compat.v1.keras.layers.Dense(
                     units, activation='relu')(dense_tensor)
prediction = tf.compat.v1.keras.layers.Dense(1)(dense_tensor)
""" .

"DESCRIPTION.This code defines a neural network model using Flax framework in Python. The model consists of three dense layers with a relu activation function after each layer. It then initializes the model parameters using random keys and empty input data. The code also includes a function that finds all dense layer parameters and increments the bias of each dense layer by 1. It finally asserts that the updated bias values are correct and that the original parameters remain unchanged." <EXPLAINS> """CODE.import flax.linen as nn
from flax.cursor import cursor
import jax
import jax.numpy as jnp

class Model(nn.Module):
  @nn.compact
  def __call__(self, x):
    x = nn.Dense(3)(x)
    x = nn.relu(x)
    x = nn.Dense(3)(x)
    x = nn.relu(x)
    x = nn.Dense(3)(x)
    x = nn.relu(x)
    return x

params = Model().init(jax.random.PRNGKey(0), jnp.empty((1, 2)))['params']

def cond_fn(path, value):
  '''Find all dense layer params.'''
  return 'Dense' in path

c = cursor(params)
for dense_params in c.find_all(cond_fn):
  dense_params['bias'] += 1
new_params = c.build()

for layer in ('Dense_0', 'Dense_1', 'Dense_2'):
  assert (new_params[layer]['bias'] == params[layer]['bias'] + 1).all()

assert jax.tree_util.tree_all(
      jax.tree_util.tree_map(
          lambda x, y: (x == y).all(),
          params,
          Model().init(jax.random.PRNGKey(0), jnp.empty((1, 2)))[
              'params'
          ],
      )
  ) # make sure original params are unchanged""" .

"DESCRIPTION.This code defines a neural network model using Keras library to take two inputs, apply two dense layers with relu activation function to each input separately, subtract the output of the two dense layers element-wise, and then pass the result through another dense layer to produce the final output." <EXPLAINS> """CODE.    import keras

    input1 = keras.layers.Input(shape=(16,))
    x1 = keras.layers.Dense(8, activation='relu')(input1)
    input2 = keras.layers.Input(shape=(32,))
    x2 = keras.layers.Dense(8, activation='relu')(input2)
    subtracted = keras.layers.subtract([x1, x2])

    out = keras.layers.Dense(4)(subtracted)
    model = keras.models.Model(inputs=[input1, input2], outputs=out)
""" .

"DESCRIPTION.This code defines a neural network model using Keras, compiles it using stochastic gradient descent as the optimizer, mean squared error as the loss function, and sensitivity at a specific specificity as the evaluation metric." <EXPLAINS> """CODE.model = keras.Model(inputs, outputs)
model.compile(
    'sgd',
    loss='mse',
    metrics=[keras.metrics.SensitivityAtSpecificity()])
""" .

"DESCRIPTION.This code defines a neural network model using the Flax library in Python. The model consists of two dense layers with 4 and 2 units respectively. The code then creates an input tensor `x` with shape (16, 9) and passes it through the neural network model to output the result." <EXPLAINS> """CODE.import jax
import jax.numpy as jnp
import flax.linen as nn

class Foo(nn.Module):
    @nn.compact
    def __call__(self, x):
        h = nn.Dense(4)(x)
        return nn.Dense(2)(h)

x = jnp.ones((16, 9))

print(Foo().tabulate(jax.random.PRNGKey(0), x))
""" .

"DESCRIPTION.This code defines a neural network model using the Keras framework, compiles the model using stochastic gradient descent optimizer with mean squared error loss function and true positives metric." <EXPLAINS> """CODE.model = keras.Model(inputs, outputs)
model.compile('sgd', loss='mse', metrics=[keras.metrics.TruePositives()])
""" .

"DESCRIPTION.This code defines a neural network model using the Keras framework. The model takes two inputs, each with different shapes. The inputs go through separate Dense layers with ReLU activation function before being subtracted from each other. The output of the subtraction is then passed through another Dense layer to generate the final output." <EXPLAINS> """CODE.import keras

input1 = keras.layers.Input(shape=(16,))
x1 = keras.layers.Dense(8, activation='relu')(input1)
input2 = keras.layers.Input(shape=(32,))
x2 = keras.layers.Dense(8, activation='relu')(input2)
subtracted = keras.layers.Subtract()([x1, x2])

out = keras.layers.Dense(4)(subtracted)
model = keras.models.Model(inputs=[input1, input2], outputs=out)
""" .

"DESCRIPTION.This code defines a neural network model using the PyTorch library. The model consists of a single fully connected layer (nn.Linear) with input size 10 and output size 10. The forward function of the model applies the linear transformation and then applies a ReLU activation function to the output. The model is initialized and saved using log_model function with the name \"my-simple-model\" and aliases \"best\"." <EXPLAINS> """CODE.import torch.nn as nn
import torch.nn.functional as F

class Net(nn.Module):
    def __init__(self):
        super(Net, self).__init__()
        self.fc1 = nn.Linear(10, 10)

    def forward(self, x):
        x = self.fc1(x)
        x = F.relu(x)
        return x

model = Net()
sm = log_model(model, "my-simple-model", aliases=["best"])""" .

"DESCRIPTION.This code defines a neural network model using the keras framework, compiles the model using stochastic gradient descent optimizer, mean squared error loss function, and Mean Intersection over Union as the evaluation metric with 2 classes." <EXPLAINS> """CODE.model = keras.Model(inputs, outputs)
model.compile(
    'sgd',
    loss='mse',
    metrics=[keras.metrics.MeanIoU(num_classes=2)])
""" .

"DESCRIPTION.This code defines a neural network model using the keras library, compiles the model using stochastic gradient descent optimizer with mean squared error loss function and Mean Intersection over Union metric for binary classification with 2 classes." <EXPLAINS> """CODE.model = keras.Model(inputs, outputs)
model.compile(
    'sgd',
    loss='mse',
    metrics=[keras.metrics.MeanIoU(num_classes=2)])
""" .

"DESCRIPTION.This code defines a neural network model using the keras library, compiles the model using the stochastic gradient descent optimizer, and specifies the loss function as Kullback-Leibler Divergence." <EXPLAINS> """CODE.model = keras.Model(inputs, outputs)
model.compile('sgd', loss=keras.losses.KLDivergence())
""" .

"DESCRIPTION.This code defines a neural network model using the specified inputs and outputs, compiles the model using the stochastic gradient descent optimizer ('sgd') and evaluates the model's performance using top-k categorical accuracy metric." <EXPLAINS> """CODE.model = keras.Model(inputs, outputs)
model.compile('sgd', metrics=[keras.metrics.TopKCategoricalAccuracy()])
""" .

"DESCRIPTION.This code defines a neural network model with 2 dense layers, the first layer has 32 units and the input shape is (500,), the second layer has 10 units with softmax activation function. The model is compiled with optimizer as 'rmsprop', loss function as 'categorical_crossentropy', and accuracy as the evaluation metric." <EXPLAINS> """CODE.model = Sequential()
model.add(Dense(32, input_shape=(500,)))
model.add(Dense(10, activation='softmax'))
model.compile(optimizer='rmsprop',
              loss='categorical_crossentropy',
              metrics=['accuracy'])
""" .

"DESCRIPTION.This code defines a neural network model with a linear layer that takes an input tensor 'x' and returns two output tensors 'y' and 'z'. It then sets up an Adam optimizer with a learning rate of 0.001 and applies it to minimize the loss function defined by the output tensor 'z'. Finally, it iterates 10 times, running the model with an input tensor of all ones and printing the loss value calculated by 'z' after each iteration." <EXPLAINS> """CODE.import paddle.fluid as fluid
import numpy as np

from paddle.fluid.dygraph.nn import Linear

@fluid.dygraph.declarative
def linear_func(x):
    x = fluid.dygraph.to_variable(x)
    linear = Linear(32, 1)
    y = linear(x)
    z = linear(x)
    return y, z

prog_trans = fluid.dygraph.ProgramTranslator()

adam = fluid.optimizer.AdamOptimizer(learning_rate=0.001)
prog_trans.set_optimizer(adam,index_of_loss=1) # minimize on 'z'

for i in range(10):
    y, z_loss = linear_func(np.ones(32).astype('float32'))
    print(z_loss.numpy())""" .

"DESCRIPTION.This code defines a neural network model with a single layer that permutes the dimensions of the input tensor." <EXPLAINS> """CODE.model = Sequential()
model.add(Permute((2, 1), input_shape=(10, 64)))
# now: model.output_shape == (None, 64, 10)
# note: `None` is the batch dimension
""" .

"DESCRIPTION.This code defines a neural network model with an input layer of shape (32,) and a dense layer with 16 units and softmax activation function." <EXPLAINS> """CODE.a = Input(shape=(32,))
b = Dense(16, activation='softmax')(a)
model = Model(input=a, output=b)
""" .

"DESCRIPTION.This code defines a neural network model with an input layer of size 32 and a fully connected layer with 16 units using a softmax activation function." <EXPLAINS> """CODE.x = Input(shape=(32,))
y = Dense(16, activation='softmax')(x)
model = Model(x, y)
""" .

"DESCRIPTION.This code defines a neural network model with two Bidirectional LSTM layers followed by a Dense layer with 5 units and a softmax activation function. The model is compiled using categorical crossentropy loss and the rmsprop optimizer." <EXPLAINS> """CODE.model = Sequential()
model.add(Bidirectional(LSTM(10, return_sequences=True), input_shape=(5, 10)))
model.add(Bidirectional(LSTM(10)))
model.add(Dense(5))
model.add(Activation('softmax'))
model.compile(loss='categorical_crossentropy', optimizer='rmsprop')
""" .

"DESCRIPTION.This code defines a neural network model with two input layers and one output layer. The model takes two sets of input data, applies a dense layer with ReLU activation function to each input, subtracts the results of the two dense layers, and then passes the result through another dense layer with linear activation function to produce the final output." <EXPLAINS> """CODE.    import keras

    input1 = keras.layers.Input(shape=(16,))
    x1 = keras.layers.Dense(8, activation='relu')(input1)
    input2 = keras.layers.Input(shape=(32,))
    x2 = keras.layers.Dense(8, activation='relu')(input2)
    subtracted = keras.layers.subtract([x1, x2])

    out = keras.layers.Dense(4)(subtracted)
    model = keras.models.Model(inputs=[input1, input2], outputs=out)
""" .

"DESCRIPTION.This code defines a neural network model with two input layers of shapes (16,) and (32,), respectively. Each input is passed through a dense layer with 8 units and ReLU activation function. The outputs of the dense layers are then added together using the Add layer. Finally, the sum is passed through another dense layer with 4 units to produce the output of the model." <EXPLAINS> """CODE.input_shape = (2, 3, 4)
x1 = tf.random.normal(input_shape)
x2 = tf.random.normal(input_shape)
y = tf.keras.layers.Add()([x1, x2])
print(y.shape)

input1 = tf.keras.layers.Input(shape=(16,))
x1 = tf.keras.layers.Dense(8, activation='relu')(input1)
input2 = tf.keras.layers.Input(shape=(32,))
x2 = tf.keras.layers.Dense(8, activation='relu')(input2)
added = tf.keras.layers.Add()([x1, x2])
out = tf.keras.layers.Dense(4)(added)
model = tf.keras.models.Model(inputs=[input1, input2], outputs=out)""" .

"DESCRIPTION.This code defines a neural network model with two input layers of shapes 16 and 32 respectively. Each input layer is connected to a hidden layer with 8 units and ReLU activation function. The outputs of the two hidden layers are then averaged using the Average layer, and the result is passed through another dense layer with 4 units. Finally, a model is created using the defined inputs and outputs." <EXPLAINS> """CODE.x1 = np.ones((2, 2))
x2 = np.zeros((2, 2))
y = tf.keras.layers.Average()([x1, x2])
y.numpy().tolist()
[[0.5, 0.5], [0.5, 0.5]]
input1 = tf.keras.layers.Input(shape=(16,))
x1 = tf.keras.layers.Dense(8, activation='relu')(input1)
input2 = tf.keras.layers.Input(shape=(32,))
x2 = tf.keras.layers.Dense(8, activation='relu')(input2)
avg = tf.keras.layers.Average()([x1, x2])
out = tf.keras.layers.Dense(4)(avg)
model = tf.keras.models.Model(inputs=[input1, input2], outputs=out)""" .

"DESCRIPTION.This code defines a neural network model with two input layers, each followed by a dense layer with relu activation. The outputs of these two dense layers are added element-wise. The result is then passed through another dense layer with 4 units to produce the final output of the model." <EXPLAINS> """CODE.    import keras

    input1 = keras.layers.Input(shape=(16,))
    x1 = keras.layers.Dense(8, activation='relu')(input1)
    input2 = keras.layers.Input(shape=(32,))
    x2 = keras.layers.Dense(8, activation='relu')(input2)
    added = keras.layers.add([x1, x2])

    out = keras.layers.Dense(4)(added)
    model = keras.models.Model(inputs=[input1, input2], outputs=out)
""" .

"DESCRIPTION.This code defines a neural network model with two locally connected 2D layers. The first layer has 64 filters with a kernel size of 3x3 and takes input with a shape of 3 channels, 32 height, and 32 width. The second layer has 32 filters with a kernel size of 3x3." <EXPLAINS> """CODE.    model = Sequential()
    model.add(LocallyConnected2D(64, 3, 3, input_shape=(3, 32, 32)))
    model.add(LocallyConnected2D(32, 3, 3))
""" .

"DESCRIPTION.This code defines a neural network module called Foo, which consists of two dense layers. It creates an instance of Foo, generates random module paths using JAX, and then prints a dictionary showing the module paths and their corresponding types." <EXPLAINS> """CODE.class Foo(nn.Module):
  @nn.compact
  def __call__(self, x):
    h = nn.Dense(4)(x)
    return nn.Dense(2)(h)


x = jnp.ones((16, 9))
modules = Foo().module_paths(jax.random.key(0), x)
print({
    p: type(m).__name__ for p, m in modules.items()
})
""" .

"""DESCRIPTION.This code defines a neural network module using Flax and JAX in Python. The "Foo" class is a neural network module that consists of dense layers. The "__call__" method applies the dense layers and a perturbation function to the input "x" and returns the output after passing through another dense layer.

The "loss" function calculates the mean squared error loss between the predictions from the model and the target values.

Finally, the code initializes the model, calculates intermediate gradients using the loss function, and prints out the intermediate gradients for the 'dense3' layer.""" <EXPLAINS> """CODE.import jax
import jax.numpy as jnp
import flax.linen as nn

class Foo(nn.Module):
    @nn.compact
    def __call__(self, x):
        x = nn.Dense(3)(x)
        x = self.perturb('dense3', x)
        return nn.Dense(2)(x)

def loss(params, perturbations, inputs, targets):
    variables = {'params': params, 'perturbations': perturbations}
    preds = model.apply(variables, inputs)
    return jnp.square(preds - targets).mean()

x = jnp.ones((2, 9))
y = jnp.ones((2, 2))
model = Foo()
variables = model.init(jax.random.PRNGKey(0), x)
intm_grads = jax.grad(loss, argnums=1)(variables['params'], variables['perturbations'], x, y)
print(intm_grads['dense3']) # ==> [[-1.456924   -0.44332537  0.02422847]
                            #      [-1.456924   -0.44332537  0.02422847]]""" .

"DESCRIPTION.This code defines a neural network with a single fully connected layer, computes the mean of the hidden layer outputs, and then minimizes the loss using the SGD optimizer with a learning rate of 0.01. Finally, it prints out all the parameters of the neural network." <EXPLAINS> """CODE.import paddle.fluid as fluid

program = fluid.default_main_program()
data = fluid.data(name='x', shape=[None, 13], dtype='float32')
hidden = fluid.layers.fc(input=data, size=10)
loss = fluid.layers.mean(hidden)
fluid.optimizer.SGD(learning_rate=0.01).minimize(loss)

for param in program.all_parameters():
    print(param)""" .

"DESCRIPTION.This code defines a neural network with dense reparameterization layers using TensorFlow Probability. It then calculates the negative log likelihood and Kullback-Leibler divergence, and combines them to form the loss function. Finally, it uses the Adam optimizer to minimize the loss function during training." <EXPLAINS> """CODE.tfp = tf.contrib.bayesflow

net = tfp.layers.dense_reparameterization(
    features, 512, activation=tf.nn.relu)
logits = tfp.layers.dense_reparameterization(net, 10)
neg_log_likelihood = tf.nn.softmax_cross_entropy_with_logits(
    labels=labels, logits=logits)
kl = sum(tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES))
loss = neg_log_likelihood + kl
train_op = tf.train.AdamOptimizer().minimize(loss)
""" .

"DESCRIPTION.This code defines a neural network with two dense layers using the Flipout estimator for Bayesian neural networks. It calculates the negative log likelihood using softmax cross entropy with logits, adds the Kullback-Leibler divergence regularization term, and defines a training operation to minimize the loss using the Adam optimizer." <EXPLAINS> """CODE.tfp = tf.contrib.bayesflow

net = tfp.layers.dense_flipout(
    features, 512, activation=tf.nn.relu)
logits = tfp.layers.dense_flipout(net, 10)
neg_log_likelihood = tf.nn.softmax_cross_entropy_with_logits(
    labels=labels, logits=logits)
kl = sum(tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES))
loss = neg_log_likelihood + kl
train_op = tf.train.AdamOptimizer().minimize(loss)
""" .

"DESCRIPTION.This code defines a pandas DataFrame containing a column of Streamlit widget commands. It then displays the DataFrame in a Streamlit data editor, with custom column configurations for the \"widgets\" column. The \"widgets\" column is intended for Streamlit widget commands and has a help message, default value, maximum character limit, and validation pattern specified. The index column is hidden in the displayed data editor." <EXPLAINS> """CODE.import pandas as pd
import streamlit as st

data_df = pd.DataFrame(
    {
        "widgets": ["st.selectbox", "st.number_input", "st.text_area", "st.button"],
    }
)

st.data_editor(
    data_df,
    column_config={
        "widgets": st.column_config.TextColumn(
            "Widgets",
            help="Streamlit **widget** commands ð",
            default="st.",
            max_chars=50,
            validate="^st\\.[a-z_]+$",
        )
    },
    hide_index=True,
)""" .

"DESCRIPTION.This code defines a parameterized test function named test_eval, which evaluates mathematical expressions provided as input and compares the result with the expected output. It uses pytest marks for specifying which test cases are expected to fail." <EXPLAINS> """CODE.@pytest.mark.parametrize("test_input,expected", [
    ("3+5", 8),
    pytest.param("6*9", 42, marks=pytest.mark.xfail),
])
def test_eval(test_input, expected):
    assert eval(test_input) == expected""" .

"DESCRIPTION.This code defines a parsing expression for extracting text data grouped under a specified label, where the data words are joined together into a single string and printed out in a formatted manner." <EXPLAINS> """CODE.    attr_expr = Group(label + Suppress(':') + OneOrMore(data_word).setParseAction(' '.join))
    attr_expr = Group(label + Suppress(':') + OneOrMore(data_word, stopOn=label).setParseAction(' '.join))
    (attr_expr * (1,)).parseString(text).pprint()""" .

"DESCRIPTION.This code defines a power transformation distribution with a power of 2, applies the forward transformation to the input tensor x, calculates the inverse transformation of the forward transformation, and calculates the forward log determinant of the Jacobian." <EXPLAINS> """CODE.import paddle

x = paddle.to_tensor([1., 2.])
power = paddle.distribution.PowerTransform(paddle.to_tensor(2.))

print(power.forward(x))
print(power.inverse(power.forward(x)))
print(power.forward_log_det_jacobian(x))""" .

"DESCRIPTION.This code defines a progress bar class `LitProgressBar` that can be enabled or disabled. It also includes a method to update the progress bar during training batches. An instance of the `LitProgressBar` class, `bar`, is created and passed as a callback to a `Trainer` object." <EXPLAINS> """CODE.class LitProgressBar(ProgressBar):
    def __init__(self):
        super().__init__()  # don't forget this :)
        self.enable = True

    def disable(self):
        self.enable = False

    def on_train_batch_end(self, trainer, pl_module, outputs, batch_idx):
        super().on_train_batch_end(trainer, pl_module, outputs, batch_idx)  # don't forget this :)
        percent = (self.train_batch_idx / self.total_train_batches) * 100
        sys.stdout.flush()
        sys.stdout.write(f'{percent:.01f} percent complete \\r')

bar = LitProgressBar()
trainer = Trainer(callbacks=[bar])""" .

"DESCRIPTION.This code defines a random dataset consisting of images and their corresponding labels. The dataset contains multiple samples, with each sample containing a randomly generated image represented as a 32-element float32 array and a randomly generated label as a single int64 value between 0 and 9. The code then combines two instances of the RandomDataset class to create a larger dataset using ComposeDataset. Finally, it iterates through the combined dataset, retrieving and printing the images and labels from each sample." <EXPLAINS> """CODE.import numpy as np
import paddle
from paddle.io import Dataset, ComposeDataset

# define a random dataset
class RandomDataset(Dataset):
    def __init__(self, num_samples):
        self.num_samples = num_samples

    def __getitem__(self, idx):
        image = np.random.random([32]).astype('float32')
        label = np.random.randint(0, 9, (1, )).astype('int64')
        return image, label

    def __len__(self):
        return self.num_samples

dataset = ComposeDataset([RandomDataset(10), RandomDataset(10)])
for i in range(len(dataset)):
    image1, label1, image2, label2 = dataset[i]
    print(image1)
    print(label1)
    print(image2)
    print(label2)""" .

"DESCRIPTION.This code defines a recurrent neural network (RNN) model using LSTM (Long Short-Term Memory) cells. It takes input data, including the input layer, initial states for the LSTM cell, and sequence length. The LSTM cell processes the input data and produces output through a dense layer. The RNN model is created with the input and output layers, as well as the LSTM cell. Finally, the model's variables are registered and a summary of the model is displayed." <EXPLAINS> """CODE.def __init__(self, *args, **kwargs):
    super(MyModelClass, self).__init__(*args, **kwargs)
    cell_size = 256

    # Define input layers
    input_layer = tf.keras.layers.Input(
        shape=(None, obs_space.shape[0]))
    state_in_h = tf.keras.layers.Input(shape=(256, ))
    state_in_c = tf.keras.layers.Input(shape=(256, ))
    seq_in = tf.keras.layers.Input(shape=())

    # Send to LSTM cell
    lstm_out, state_h, state_c = tf.keras.layers.LSTM(
        cell_size, return_sequences=True, return_state=True,
        name="lstm")(
            inputs=input_layer,
            mask=tf.sequence_mask(seq_in),
            initial_state=[state_in_h, state_in_c])
    output_layer = tf.keras.layers.Dense(...)(lstm_out)

    # Create the RNN model
    self.rnn_model = tf.keras.Model(
        inputs=[input_layer, seq_in, state_in_h, state_in_c],
        outputs=[output_layer, state_h, state_c])
    self.register_variables(self.rnn_model.variables)
    self.rnn_model.summary()""" .

"DESCRIPTION.This code defines a regression model using a linear regressor with SDCALinearRegressor. It includes real and sparse columns for features, with options for example_id, weight_column_name, l2_regularization, and num_loss_partitions. It also provides input functions for training, evaluation, and testing the model, followed by fitting, evaluating, and predicting scores with the regressor." <EXPLAINS> """CODE.real_column_a = real_valued_column(...)
sparse_column_b = sparse_column_with_hash_bucket(...)

regressor = SDCALinearRegressor(
    example_id_column='example_id',
    feature_columns=[real_column_a, sparse_column_b]),
    weight_column_name=...,
    l2_regularization=...,
    num_loss_partitions=...,
)

# Input builders
# returns x, y (where y is the label Tensor (with 0/1 values)
def input_fn_{train, eval}:

# returns x (features dict)
def input_fn_test:
  ...
regressor.fit(input_fn=input_fn_train)
regressor.evaluate(input_fn=input_fn_eval)
regressor.predict_scores(input_fn=input_fn_test) # returns predicted scores.
""" .

"DESCRIPTION.This code defines a scheduling strategy for nodes based on the presence of a GPU of any type in the US region." <EXPLAINS> """CODE.scheduling_strategy=NodeLabelSchedulingStrategy({
      "region": In("us"),
      "gpu_type": Exists()
})""" .

"DESCRIPTION.This code defines a sequence model using TensorFlow to process input sequences of ratings and watches data. The code includes defining feature columns for the rating and watches data, creating sparse tensors for the input features, setting up a sequence input layer using the defined feature columns, and applying a SimpleRNNCell to process the input sequences." <EXPLAINS> """CODE.import tensorflow as tf

training = True
rating = tf.feature_column.sequence_numeric_column('rating')
watches = tf.feature_column.sequence_categorical_column_with_identity(
    'watches', num_buckets=1000)
watches_embedding = tf.feature_column.embedding_column(watches,
                                            dimension=10)
columns = [rating, watches_embedding]

features = {
 'rating': tf.sparse.from_dense([[1.0,1.1, 0, 0, 0],
                                             [2.0,2.1,2.2, 2.3, 2.5]]),
 'watches': tf.sparse.from_dense([[2, 85, 0, 0, 0],[33,78, 2, 73, 1]])
}

sequence_input_layer = tf.keras.experimental.SequenceFeatures(columns)
sequence_input, sequence_length = sequence_input_layer(
   features, training=training)
sequence_length_mask = tf.sequence_mask(sequence_length)
hidden_size = 32
rnn_cell = tf.keras.layers.SimpleRNNCell(hidden_size)
rnn_layer = tf.keras.layers.RNN(rnn_cell)
outputs, state = rnn_layer(sequence_input, mask=sequence_length_mask)
""" .

"DESCRIPTION.This code defines a sequential model in Python using the Keras library. The model adds layers using the TimeDistributed wrapper for a Dense layer with 8 units and another Dense layer with 32 units." <EXPLAINS> """CODE.model = Sequential()
model.add(TimeDistributed(Dense(8), input_shape=(10, 16)))
model.add(TimeDistributed(Dense(32)))


model = Sequential()
model.add(TimeDistributed(Conv2D(64, (3, 3)), input_shape=(10, 299, 299, 3))
""" .

"DESCRIPTION.This code defines a set of parameters for optimization as a list of dictionaries, where each dictionary contains the name, type, and bounds of a parameter. It then initializes an AxSearch algorithm with the specified parameters for optimization, setting the objective function to \"hartmann6\" and allowing a maximum of 4 concurrent evaluations." <EXPLAINS> """CODE.parameters = [
    {"name": "x1", "type": "range", "bounds": [0.0, 1.0]},
    {"name": "x2", "type": "range", "bounds": [0.0, 1.0]},
]
algo = AxSearch(parameters=parameters,
    objective_name="hartmann6", max_concurrent=4)""" .

"DESCRIPTION.This code defines a shape using the RaggedShape class and calculates the number of slices in each dimension of the shape. The number of slices in each dimension is returned using positive and negative indices." <EXPLAINS> """CODE.shape = RaggedShape._from_inner_shape([2, 3, 4])
shape._num_slices_in_dimension(0) = 2
shape._num_slices_in_dimension(1) = 6
shape._num_slices_in_dimension(2) = 24
shape._num_slices_in_dimension(-1) = 24
shape._num_slices_in_dimension(-2) = 6
shape._num_slices_in_dimension(-2) = 2""" .

"DESCRIPTION.This code defines a simple filter function called lowercase that takes as input a lexer, stream, and options. It iterates over the stream tuples, converts the values to lowercase, and yields the lowercase values along with their types." <EXPLAINS> """CODE.@simplefilter
def lowercase(lexer, stream, options):
    for ttype, value in stream:
        yield ttype, value.lower()""" .

"DESCRIPTION.This code defines a simple neural network model in PyTorch to perform regression on a dataset. The model consists of a single parameter 'w' that is updated using the LBFGS optimizer. The training process involves calculating the mean squared error loss between the model predictions and the target values. The optimizer is used to minimize this loss by updating the model parameter 'w' iteratively." <EXPLAINS> """CODE.import paddle
import numpy as np
from paddle.incubate.optimizer import LBFGS

paddle.disable_static()
np.random.seed(0)
np_w = np.random.rand(1).astype(np.float32)
np_x = np.random.rand(1).astype(np.float32)

inputs = [np.random.rand(1).astype(np.float32) for i in range(10)]
# y = 2x
targets = [2 * x for x in inputs]

class Net(paddle.nn.Layer):
    def __init__(self):
        super().__init__()
        w = paddle.to_tensor(np_w)
        self.w = paddle.create_parameter(shape=w.shape, dtype=w.dtype, default_initializer=paddle.nn.initializer.Assign(w))

    def forward(self, x):
        return self.w * x

net = Net()
opt = LBFGS(learning_rate=1, max_iter=1, max_eval=None, tolerance_grad=1e-07, tolerance_change=1e-09, history_size=100, line_search_fn='strong_wolfe', parameters=net.parameters())
def train_step(inputs, targets):
    def closure():
        outputs = net(inputs)
        loss = paddle.nn.functional.mse_loss(outputs, targets)
        print('loss: ', loss.item())
        opt.clear_grad()
        loss.backward()
        return loss
    opt.step(closure)


for input, target in zip(inputs, targets):
    input = paddle.to_tensor(input)
    target = paddle.to_tensor(target)
    train_step(input, target)""" .

"DESCRIPTION.This code defines a simple neural network model using PaddlePaddle's Dynamic Graph mode. It initializes a linear layer with input and output size of 10, generates random initial values for the layer's parameters, computes the output of the model, calculates the mean squared error loss, performs backpropagation to compute gradients, and updates the model parameters using the SGD optimizer with gradient clipping by norm to avoid exploding gradients." <EXPLAINS> """CODE.import numpy as np
import paddle
import paddle.fluid as fluid

from paddle.fluid.dygraph.base import to_variable
from paddle.fluid.dygraph.nn import Linear

from paddle.fluid.clip import GradClipByValue, GradClipByNorm, GradClipByGlobalNorm

from paddle.fluid.optimizer import SGDOptimizer

with fluid.dygraph.guard():
    norm_clip = GradClipByNorm( 5.0 )
    sgd = SGDOptimizer(learning_rate=1.0)

    init_value = np.random.uniform( -1, 1, (10, 10)).astype('float32')

    linear = Linear( 10, 10)

    out = linear( to_variable(init_value) )

    loss = fluid.layers.reduce_mean( out )

    loss.backward()
    sgd.minimize(loss, grad_clip = norm_clip)""" .

"DESCRIPTION.This code defines a static data variable 'x' with a shape of [3, 2, 1] and computes the size of 'x' and assigns it to the variable 'y'." <EXPLAINS> """CODE.import paddle
paddle.enable_static()
x = paddle.static.data(name='x', shape=[3, 2, 1])
y = x.size()""" .

"DESCRIPTION.This code defines a task that involves training a transformer model using the Hugging Face library. It initializes a trainer with specified parameters and callbacks, and then trains the model using the provided trainer configuration. The training process may resume from a checkpoint if one is available." <EXPLAINS> """CODE.from flytekit import current_context, task


@task
def train_hf_transformer():
    cp = current_context().checkpoint
    trainer = Trainer(..., callbacks=[FlyteCallback()])
    output = trainer.train(resume_from_checkpoint=cp.restore())
""" .

"DESCRIPTION.This code defines a template with a macro named 'foo' that returns the value '42'. When the template is rendered, the result '23' is returned. Additionally, calling the macro 'foo' directly returns '42'." <EXPLAINS> """CODE.t = Template('{% macro foo() %}42{% endmacro %}23')
unicode(t.module)
u'23'
t.module.foo()
u'42'""" .

"DESCRIPTION.This code defines a tensor placeholder with shape (2, 4, 5) using Keras backend, then creates a variable initialized with a numpy array values, and finally retrieves the shape of the variable." <EXPLAINS> """CODE.from keras import backend as K
inputs = K.placeholder(shape=(2, 4, 5))
K.int_shape(inputs)
val = np.array([[1, 2], [3, 4]])
kvar = K.variable(value=val)
K.int_shape(kvar)
""" .

"DESCRIPTION.This code defines a test class MyTests that inherits from the KerasTestCase class. The test_foo method within the class creates a small MLP model, specifies an optimizer, loss function, and metrics, compiles the model, creates input and target data, creates a tensorflow dataset from the data, and fits the model to the dataset for one epoch with 2 steps per epoch. Finally, the code runs the tests using TensorFlow's test framework." <EXPLAINS> """CODE.class MyTests(testing_utils.KerasTestCase):

  @testing_utils.run_all_keras_modes
  def test_foo(self):
    model = testing_utils.get_small_functional_mlp(1, 4, input_dim=3)
    optimizer = RMSPropOptimizer(learning_rate=0.001)
    loss = 'mse'
    metrics = ['mae']
    model.compile(
        optimizer, loss, metrics=metrics,
        run_eagerly=testing_utils.should_run_eagerly())

    inputs = np.zeros((10, 3))
    targets = np.zeros((10, 4))
    dataset = dataset_ops.Dataset.from_tensor_slices((inputs, targets))
    dataset = dataset.repeat(100)
    dataset = dataset.batch(10)

    model.fit(dataset, epochs=1, steps_per_epoch=2, verbose=1)

if __name__ == "__main__":
  tf.test.main()
""" .

"DESCRIPTION.This code defines a test class `HfApiCommonTest` which contains two test methods: `test_create_tag_on_model` and `test_create_tag_on_dataset`. These test methods utilize the `use_tmp_repo()` decorator to set up temporary repositories for testing the creation of tags on a model and a dataset respectively." <EXPLAINS> """CODE.from .testing_utils import use_tmp_repo

class HfApiCommonTest(unittest.TestCase):
    _api = HfApi(endpoint=ENDPOINT_STAGING, token=TOKEN)
    _user = USER
    _repo_id: str

    @use_tmp_repo()
    def test_create_tag_on_model(self) -> None:
        self._repo_id  # populated
        (...)

    @use_tmp_repo("dataset")
    def test_create_tag_on_dataset(self) -> None:
        self._repo_id  # populated
        (...)
""" .

"DESCRIPTION.This code defines a test class called BaseTest that inherits from absltest.TestCase. It also includes a test method called test_simple_functionality, which asserts that the method called system_under_test returns the value 1." <EXPLAINS> """CODE.@absltest.skipThisClass("Shared functionality")
class BaseTest(absltest.TestCase):
    def test_simple_functionality(self):
        self.assertEqual(self.system_under_test.method(), 1)""" .

"DESCRIPTION.This code defines a test class for testing Space API, specifically testing the functionality of retrieving repository information in a space." <EXPLAINS> """CODE.@pytest.mark.usefixtures("fx_production_space")
class TestSpaceAPI(unittest.TestCase):
    repo_id: str
    api: HfApi

    def test_space(self) -> None:
        api.repo_info(repo_id, repo_type="space")""" .

"DESCRIPTION.This code defines a test class named TestHelloWorld that contains three test methods: test_hello_foo, test_hello_bar, and test_hello_both. These test methods are decorated with @patch and @handle_injection decorators for mocking purposes. Each test method takes mock objects as arguments to test different functionalities of the code." <EXPLAINS> """CODE.@patch("something.foo")
@patch("something_else.foo.bar") # order doesn't matter
@handle_injection # after @patch calls
def TestHelloWorld(unittest.TestCase):

    def test_hello_foo(self, mock_foo: Mock) -> None:
        (...)

    def test_hello_bar(self, mock_bar: Mock) -> None
        (...)

    def test_hello_both(self, mock_foo: Mock, mock_bar: Mock) -> None:
        (...)
""" .

"DESCRIPTION.This code defines a test function called test_foo that saves a Sequential model with two Dense layers to a specified directory using a specified save format. It then loads the saved model from the directory." <EXPLAINS> """CODE.@testing_utils.run_with_all_saved_model_formats
def test_foo(self):
    save_format = testing_utils.get_save_format()
    saved_model_dir = '/tmp/saved_model/'
    model = keras.models.Sequential()
    model.add(keras.layers.Dense(2, input_shape=(3,)))
    model.add(keras.layers.Dense(3))
    model.compile(loss='mse', optimizer='sgd', metrics=['acc'])

    keras.models.save_model(model, saved_model_dir, save_format=save_format)
    model = keras.models.load_model(saved_model_dir)""" .

"DESCRIPTION.This code defines a test function called test_foo which is expected to fail if the Python version is 2." <EXPLAINS> """CODE.@expectedFailureIf(sys.version.major == 2, "Not yet working in py2")
def test_foo(self):
    ...""" .

"DESCRIPTION.This code defines a test function named \"test_wrapper\" that takes a parameter \"arg1\" and asserts that \"arg1\" is greater than 0.0. It also includes a decorator \"RunIf\" that specifies a minimum value for the parameter \"min_torch\" and a parameterized test case using \"arg1\" with values 1 and 2.0." <EXPLAINS> """CODE.@RunIf(min_torch="0.0")
@pytest.mark.parametrize("arg1", [1, 2.0])
def test_wrapper(arg1):
    assert arg1 > 0.0""" .

"DESCRIPTION.This code defines a test function named test_foo that is tagged as \"knownfailure\"." <EXPLAINS> """CODE.@testlib.tag("knownfailure")
def test_foo(self):
    #...""" .

"DESCRIPTION.This code defines a test function that tests whether the input argument 'arg1' is greater than 0.0. It uses the pytest.mark.parametrize decorator to parameterize the test with different values for 'arg1'." <EXPLAINS> """CODE.@RunIf(...)
@pytest.mark.parametrize("arg1", [1, 2.0])
def test_wrapper(arg1):
    assert arg1 > 0.0""" .

"DESCRIPTION.This code defines a two-layer LSTM neural network model with 128 units in each layer. It takes input data `x` with shape `(batch_size, sentence_max_length, n_features)` and processes it through the LSTM layers to generate an output `result`." <EXPLAINS> """CODE.batch_size = 3
sentence_max_length = 5
n_features = 2
new_shape = (batch_size, sentence_max_length, n_features)
x = tf.constant(np.reshape(np.arange(30), new_shape), dtype = tf.float32)

rnn_cells = [tf.keras.layers.LSTMCell(128) for _ in range(2)]
stacked_lstm = tf.keras.layers.StackedRNNCells(rnn_cells)
lstm_layer = tf.keras.layers.RNN(stacked_lstm)

result = lstm_layer(x)
""" .

"DESCRIPTION.This code defines a unit test class called TestWithCache that tests if the cache directory specified by the fixture fx_cache_dir exists." <EXPLAINS> """CODE.@pytest.mark.usefixtures("fx_cache_dir")
class TestWithCache(unittest.TestCase):
    cache_dir: Path

    def test_cache_dir(self) -> None:
        self.assertTrue(self.cache_dir.is_dir())
""" .

"DESCRIPTION.This code defines a unit test class named TestHelloWorld with a test method test_hello_world. Inside the test method, it captures the output produced by printing \"hello world\" and asserts that the output value is equal to the string \"hello world\"." <EXPLAINS> """CODE.class TestHelloWorld(unittest.TestCase):
    def test_hello_world(self):
        with capture_output() as output:
            print("hello world")
        self.assertEqual(output.getvalue(), "hello world
")
""" .

"DESCRIPTION.This code defines an RNN (Recurrent Neural Network) estimator for regression tasks. It creates token embeddings using categorical columns and hash buckets, and then sets up the RNN model with specified layers and cell types (in this case, LSTM cells). It also includes the option to use a custom RNN cell function. The code further prepares input functions for training, evaluation, and prediction. Finally, it trains the model, evaluates its performance, and generates predictions." <EXPLAINS> """CODE.token_sequence = sequence_categorical_column_with_hash_bucket(...)
token_emb = embedding_column(categorical_column=token_sequence, ...)

estimator = RNNEstimator(
    head=tf.contrib.estimator.regression_head(),
    sequence_feature_columns=[token_emb],
    num_units=[32, 16], cell_type='lstm')

# Or with custom RNN cell:
def rnn_cell_fn(mode):
  cells = [ tf.contrib.rnn.LSTMCell(size) for size in [32, 16] ]
  if mode == tf.estimator.ModeKeys.TRAIN:
    cells = [ tf.contrib.rnn.DropoutWrapper(cell, input_keep_prob=0.5)
                  for cell in cells ]
  return tf.contrib.rnn.MultiRNNCell(cells)

estimator = RNNEstimator(
    head=tf.contrib.estimator.regression_head(),
    sequence_feature_columns=[token_emb],
    rnn_cell_fn=rnn_cell_fn)

# Input builders
def input_fn_train: # returns x, y
  pass
estimator.train(input_fn=input_fn_train, steps=100)

def input_fn_eval: # returns x, y
  pass
metrics = estimator.evaluate(input_fn=input_fn_eval, steps=10)
def input_fn_predict: # returns x, None
  pass
predictions = estimator.predict(input_fn=input_fn_predict)
""" .

"DESCRIPTION.This code defines an autoencoder model using Keras for data compression and decompression. The model is trained using stochastic gradient descent with mean squared error loss for a given number of epochs. The model can also be used to predict compressed representations of input data and further trained with these representations as targets. Finally, the model can be switched back to training against the original input data." <EXPLAINS> """CODE.    from keras.layers import containers, AutoEncoder, Dense
    from keras import models

    # input shape: (nb_samples, 32)
    encoder = containers.Sequential([Dense(16, input_dim=32), Dense(8)])
    decoder = containers.Sequential([Dense(16, input_dim=8), Dense(32)])

    autoencoder = AutoEncoder(encoder=encoder, decoder=decoder, output_reconstruction=True)
    model = models.Sequential()
    model.add(autoencoder)

    # training the autoencoder:
    model.compile(optimizer='sgd', loss='mse')
    model.fit(X_train, X_train, nb_epoch=10)

    # predicting compressed representations of inputs:
    autoencoder.output_reconstruction = False  # the model has to be recompiled after modifying this property
    model.compile(optimizer='sgd', loss='mse')
    representations = model.predict(X_test)

    # the model is still trainable, although it now expects compressed representations as targets:
    model.fit(X_test, representations, nb_epoch=1)  # in this case the loss will be 0, so it's useless

    # to keep training against the original inputs, just switch back output_reconstruction to True:
    autoencoder.output_reconstruction = True
    model.compile(optimizer='sgd', loss='mse')
    model.fit(X_train, X_train, nb_epoch=10)
""" .

"DESCRIPTION.This code defines an autoencoder neural network model with an encoder and a decoder. The encoder takes input data and compresses it into a lower-dimensional representation, while the decoder takes the compressed representation and reconstructs the original input data." <EXPLAINS> """CODE.class AutoEncoder(nn.Module):
    def setup(self):
      self.encoder = nn.Dense(3)
      self.decoder = nn.Dense(5)

ae = AutoEncoder()
model = ae.bind(variables)
z = model.encode(x)
x_reconstructed = model.decode(z)
""" .

"DESCRIPTION.This code defines an autoencoder neural network model, with an encoder and a decoder. The model can encode input data and reconstruct the data from the encoded representation." <EXPLAINS> """CODE.class AutoEncoder(nn.Module):
    def setup(self):
      self.encoder = nn.Dense(3)
      self.decoder = nn.Dense(5)

ae = AutoEncoder()
model = ae.bind(variables)
z = model.encode(x)
x_reconstructed = model.decode(z)
""" .

"DESCRIPTION.This code defines an embedding layer with a specific size for the input variable var, which is used for fleet learning." <EXPLAINS> """CODE.with fleet_embedding(click_name=label.name):
    emb = fluid.layers.embedding(
        input=var,
        size=[-1, 11],
        is_sparse=True,
        is_distributed=True,
        param_attr=fluid.ParamAttr(name="embedding"))""" .

"DESCRIPTION.This code defines an enumeration type in GraphQL called RGB with three values: RED, GREEN, and BLUE, each corresponding to a numeric value." <EXPLAINS> """CODE.RGBType = GraphQLEnumType(
    name='RGB',
    values=OrderedDict([
        ('RED', GraphQLEnumValue(0)),
        ('GREEN', GraphQLEnumValue(1)),
        ('BLUE', GraphQLEnumValue(2))
    ])
)""" .

"DESCRIPTION.This code defines an environment specification tree object and sets a specific environment specification for the \"My/Env-v0\" environment. It then asserts that the newly set environment specification is equal to the one previously set and also asserts that accessing the environment specification through the tree structure returns the same object." <EXPLAINS> """CODE.specs = EnvSpecTree()

specs["My/Env-v0"] = EnvSpec(...)
assert specs["My/Env-v0"] == EnvSpec(...)

assert specs.tree["My"]["Env"]["0"] == specs["My/Env-v0"]
""" .

"DESCRIPTION.This code defines an expression that consists of an integer for ID, a string for name, and another integer for age." <EXPLAINS> """CODE.integer = Word(nums)
name_expr = OneOrMore(Word(alphas))
expr = And([integer("id"),name_expr("name"),integer("age")])
expr = integer("id") + name_expr("name") + integer("age")""" .

"DESCRIPTION.This code defines an initializer function for a neural network layer with specified units, kernel initializer (defaulted to 'zeros'), and bias initializer (defaulted to 'zeros')." <EXPLAINS> """CODE.@allow_initializer_layout
def __init__(self, units,
             kernel_initializer='zeros',
             bias_initializer='zeros',
             **kwargs):
   super().__init__(**kwargs)""" .

"DESCRIPTION.This code defines an input function that creates a dataset with a batch size based on input context, shards the dataset based on input context parameters, and then uses a distributed strategy to run a replica function on the dataset iterator." <EXPLAINS> """CODE.
def input_fn(input_context):
  batch_size = input_context.get_per_replica_batch_size(global_batch_size)
  d = tf.data.Dataset.from_tensors([[1.]]).repeat().batch(batch_size)
  return d.shard(input_context.num_input_pipelines,
                 input_context.input_pipeline_id)
with strategy.scope():
  iterator = strategy.make_input_fn_iterator(input_fn)
  replica_results = strategy.experimental_run(replica_fn, iterator)
""" .

"DESCRIPTION.This code defines an interface type with a method `run` that takes a torch.Tensor as input and returns a torch.Tensor. It also defines two implementations of this interface: Impl1 applies a rectified linear unit (ReLU) to the input tensor, while Impl2 adds a randomly generated value to the input tensor. The user_fn function takes a list of implementations, an index, and a tensor value as input, and calls the `run` method of the implementation at the specified index with the given tensor value. Finally, the user_fn function and implementations are compiled using TorchScript and then called with the list of implementations and a tensor value using the user_fn_jit function." <EXPLAINS> """CODE.@torch.jit.interface
class InterfaceType:
    def run(self, x: torch.Tensor) -> torch.Tensor:
        pass

@torch.jit.script
class Impl1:
    def run(self, x: torch.Tensor) -> torch.Tensor:
        return x.relu()

class Impl2(torch.nn.Module):
    def __init__(self):
        super().__init__()
        self.val = torch.rand(())

    @torch.jit.export
    def run(self, x: torch.Tensor) -> torch.Tensor:
        return x + self.val

def user_fn(impls: List[InterfaceType], idx: int, val: torch.Tensor) -> torch.Tensor:
    return impls[idx].run(val)

user_fn_jit = torch.jit.script(user_fn)

impls = [Impl1(), torch.jit.script(Impl2())]
val = torch.rand(4, 4)
user_fn_jit(impls, 0, val)
user_fn_jit(impls, 1, val)""" .

"DESCRIPTION.This code defines an upsampling module that consists of a convolutional transpose layer to upsample the input followed by a double convolutional neural network block." <EXPLAINS> """CODE.Up(
  (upsample): ConvTranspose2d(8, 4, kernel_size=(2, 2), stride=(2, 2))
  (conv): DoubleConv(
    (net): Sequential(...)
  )
)""" .

"DESCRIPTION.This code defines an upsampling operation followed by a convolution operation using a DoubleConv network." <EXPLAINS> """CODE.Up(
  (upsample): ConvTranspose2d(8, 4, kernel_size=(2, 2), stride=(2, 2))
  (conv): DoubleConv(
    (net): Sequential(...)
  )
)""" .

"DESCRIPTION.This code defines and creates options with default values and descriptions in different sections." <EXPLAINS> """CODE._create_option('section.optionName',
    description = 'Put the description here.',
    default_val = 12345)

@_create_option('section.optionName')
def _section_option_name():
    \"\"\"Put the description here.\"\"\"
    return 12345

@_create_option('section.memoizedOptionName')
@util.memoize
def _section_memoized_option_name():
    \"\"\"Put the description here.\"\"\"
    return 12345""" .

"DESCRIPTION.This code defines and demonstrates the functionality of creating linear operators and applying them to tensors. The code creates linear operators of different dimensions and compositions, and then applies these operators to tensors, returning tensors of the appropriate shapes." <EXPLAINS> """CODE.# Create a 2 x 2 linear operator composed of two 2 x 2 operators.
operator_1 = LinearOperatorMatrix([[1., 2.], [3., 4.]])
operator_2 = LinearOperatorMatrix([[1., 0.], [0., 1.]])
operator = LinearOperatorComposition([operator_1, operator_2])

operator.to_dense()
==> [[1., 2.]
     [3., 4.]]

operator.shape
==> [2, 2]

operator.log_determinant()
==> scalar Tensor

x = ... Shape [2, 4] Tensor
operator.apply(x)
==> Shape [2, 4] Tensor

# Create a [2, 3] batch of 4 x 5 linear operators.
matrix_45 = tf.random_normal(shape=[2, 3, 4, 5])
operator_45 = LinearOperatorMatrix(matrix)

# Create a [2, 3] batch of 5 x 6 linear operators.
matrix_56 = tf.random_normal(shape=[2, 3, 5, 6])
operator_56 = LinearOperatorMatrix(matrix_56)

# Compose to create a [2, 3] batch of 4 x 6 operators.
opeartor_46 = LinearOperatorComposition([operator_45, operator_56])

# Create a shape [2, 3, 6, 2] vector.
x = tf.random_normal(shape=[2, 3, 6, 2])
operator.apply(x)
==> Shape [2, 3, 4, 2] Tensor
""" .

"DESCRIPTION.This code defines and initializes multivariate Gaussian distributions using TensorFlow, calculates the mean and covariance of the distributions, and computes the probability density function (pdf) of given observations in R^3. It also demonstrates how to initialize multiple 3-variate Gaussian distributions and compute the pdf for multiple observations, returning the results as vectors." <EXPLAINS> """CODE.ds = tf.contrib.distributions

# Initialize a single 3-variate Gaussian.
mu = [1., 2, 3]
cov = [[ 0.36,  0.12,  0.06],
       [ 0.12,  0.29, -0.13],
       [ 0.06, -0.13,  0.26]]
mvn = ds.MultivariateNormalFullCovariance(
    loc=mu,
    covariance_matrix=cov)

mvn.mean().eval()
# ==> [1., 2, 3]

# Covariance agrees with covariance_matrix.
mvn.covariance().eval()
# ==> [[ 0.36,  0.12,  0.06],
#      [ 0.12,  0.29, -0.13],
#      [ 0.06, -0.13,  0.26]]

# Compute the pdf of an observation in `R^3` ; return a scalar.
mvn.prob([-1., 0, 1]).eval()  # shape: []

# Initialize a 2-batch of 3-variate Gaussians.
mu = [[1., 2, 3],
      [11, 22, 33]]              # shape: [2, 3]
covariance_matrix = ...  # shape: [2, 3, 3], symmetric, positive definite.
mvn = ds.MultivariateNormalFullCovariance(
    loc=mu,
    covariance=covariance_matrix)

# Compute the pdf of two `R^3` observations; return a length-2 vector.
x = [[-0.9, 0, 0.1],
     [-10, 0, 9]]     # shape: [2, 3]
mvn.prob(x).eval()    # shape: [2]
""" .

"DESCRIPTION.This code defines and manipulates linear operators in TensorFlow. It creates a 4x4 linear operator by combining two 2x2 operators. It then converts the operator to a dense matrix, retrieves its shape, calculates the logarithm of the absolute determinant, and performs matrix multiplication with tensors. Finally, it creates batches of linear operators, combines them into a larger batch, and performs matrix multiplication with a tensor batch." <EXPLAINS> """CODE.# Create a 4 x 4 linear operator combined of two 2 x 2 operators.
operator_1 = LinearOperatorFullMatrix([[1., 2.], [3., 4.]])
operator_2 = LinearOperatorFullMatrix([[1., 0.], [0., 1.]])
operator = LinearOperatorBlockDiag([operator_1, operator_2])

operator.to_dense()
==> [[1., 2., 0., 0.],
     [3., 4., 0., 0.],
     [0., 0., 1., 0.],
     [0., 0., 0., 1.]]

operator.shape
==> [4, 4]

operator.log_abs_determinant()
==> scalar Tensor

x1 = ... # Shape [2, 2] Tensor
x2 = ... # Shape [2, 2] Tensor
x = tf.concat([x1, x2], 0)  # Shape [2, 4] Tensor
operator.matmul(x)
==> tf.concat([operator_1.matmul(x1), operator_2.matmul(x2)])

# Create a [2, 3] batch of 4 x 4 linear operators.
matrix_44 = tf.random_normal(shape=[2, 3, 4, 4])
operator_44 = LinearOperatorFullMatrix(matrix)

# Create a [1, 3] batch of 5 x 5 linear operators.
matrix_55 = tf.random_normal(shape=[1, 3, 5, 5])
operator_55 = LinearOperatorFullMatrix(matrix_55)

# Combine to create a [2, 3] batch of 9 x 9 operators.
operator_99 = LinearOperatorBlockDiag([operator_44, operator_55])

# Create a shape [2, 3, 9] vector.
x = tf.random_normal(shape=[2, 3, 9])
operator_99.matmul(x)
==> Shape [2, 3, 9] Tensor
""" .

"DESCRIPTION.This code defines and prints the parameters of a sequential model with two linear layers, where the first layer has 10 input features and 3 output features, and the second layer has 3 input features and 10 output features, without bias." <EXPLAINS> """CODE.import paddle.fluid as fluid

with fluid.dygraph.guard():
    fc1 = fluid.Linear(10, 3)
    fc2 = fluid.Linear(3, 10, bias_attr=False)
    model = fluid.dygraph.Sequential(fc1, fc2)
    for name, param in model.named_parameters():
        print(name, param)""" .

"DESCRIPTION.This code defines asynchronous functions for adding numbers using Torch library. The `async_add_chained` function asynchronously adds three numbers x, y, and z, using torch add operation in a chained manner. The `async_add` function asynchronously adds two numbers x and y by calling the script function `script_add`. The `AsyncExecutionClass` class contains static and class methods for adding numbers asynchronously using Torch library." <EXPLAINS> """CODE.@rpc.functions.async_execution
def async_add_chained(to, x, y, z):
    return rpc.rpc_async(to, torch.add, args=(x, y)).then(
        lambda fut: fut.wait() + z
    )

@torch.jit.script
def script_add(x, y):
    return x + y

@rpc.functions.async_execution
@torch.jit.script
def async_add(to, x, y):
    return rpc.rpc_async(to, script_add, (x, y))

class AsyncExecutionClass:

    @staticmethod
    @rpc.functions.async_execution
    def static_async_add(to, x, y, z):
        return rpc.rpc_async(to, torch.add, args=(x, y)).then(
            lambda fut: fut.wait() + z
        )

    @classmethod
    @rpc.functions.async_execution
    def class_async_add(cls, to, x, y, z):
        ret_fut = torch.futures.Future()
        rpc.rpc_async(to, torch.add, args=(x, y)).then(
            lambda fut: ret_fut.set_result(fut.wait() + z)
        )
        return ret_fut""" .

"DESCRIPTION.This code defines classes for logging optimizer iterations during model training. The `TimedLogIterations` class logs optimizer iterations every specified interval of time, while the `LogThreadCallback` class logs optimizer iterations at the beginning of each epoch during model training." <EXPLAINS> """CODE.class TimedLogIterations(keras.utils.TimedThread):
    def __init__(self, model, interval):
        self.model = model
        super().__init__(interval)

    def on_interval(self):
        # Logs Optimizer iterations every x seconds
        try:
            opt_iterations = self.model.optimizer.iterations.numpy()
            print(f"Epoch: {epoch}, Optimizer Iterations: {opt_iterations}")
        except Exception as e:
            print(str(e))  # To prevent thread from getting killed

timed_logs = TimedLogIterations(model=model, interval=5)
timed_logs.start()
try:
    model.fit(...)
finally:
    timed_logs.stop()

with TimedLogIterations(model=model, interval=5):
    model.fit(...)

class LogThreadCallback(
    keras.utils.TimedThread, keras.callbacks.Callback
):
    def __init__(self, interval):
        self._epoch = 0
        keras.utils.TimedThread.__init__(self, interval)
        keras.callbacks.Callback.__init__(self)

    def on_interval(self):
        if self.epoch:
            opt_iter = self.model.optimizer.iterations.numpy()
            logging.info(f"Epoch: {self._epoch}, Opt Iteration: {opt_iter}")

    def on_epoch_begin(self, epoch, logs=None):
        self._epoch = epoch

with LogThreadCallback(interval=5) as thread_callback:
    model.fit(..., callbacks=[thread_callback])
""" .

"DESCRIPTION.This code defines custom styling attributes for different tokens in a console output, such as color, bold, underline, blink, and reverse." <EXPLAINS> """CODE.style_from_dict({
    Token: '#ff0000 bold underline',
    Token.Title: 'blink',
    Token.SomethingElse: 'reverse',
})""" .

"DESCRIPTION.This code defines feature columns for a machine learning model, creates a dense feature layer using these columns, parses input features using the defined columns, passes the features through a neural network with multiple dense layers, and produces a prediction output." <EXPLAINS> """CODE.price = tf.feature_column.numeric_column('price')
keywords_embedded = tf.feature_column.embedding_column(
    tf.feature_column.categorical_column_with_hash_bucket("keywords", 10K),
    dimension=16)
columns = [price, keywords_embedded, ...]
partitioner = tf.compat.v1.fixed_size_partitioner(num_shards=4)
feature_layer = tf.compat.v1.keras.layers.DenseFeatures(
    feature_columns=columns, partitioner=partitioner)

features = tf.io.parse_example(
    ..., features=tf.feature_column.make_parse_example_spec(columns))
dense_tensor = feature_layer(features)
for units in [128, 64, 32]:
  dense_tensor = tf.compat.v1.keras.layers.Dense(
                     units, activation='relu')(dense_tensor)
prediction = tf.compat.v1.keras.layers.Dense(1)(dense_tensor)
""" .

"DESCRIPTION.This code defines functions for booking a flight and hotel, and finalizing a trip by combining bookings. The code then creates two flight bookings, one hotel booking, and finalizes a trip with these bookings to return a Trip object." <EXPLAINS> """CODE.@workflow.step
def book_flight(origin: str, dest: str) -> Flight:
    return Flight(...)

@workflow.step
def book_hotel(location: str) -> Reservation:
    return Reservation(...)

@workflow.step
def finalize_trip(bookings: List[Any]) -> Trip:
    return Trip(...)

flight1 = book_flight.step("OAK", "SAN")
flight2 = book_flight.step("SAN", "OAK")
hotel = book_hotel.step("SAN")
trip = finalize_trip.step([flight1, flight2, hotel])
result = ray.get(trip.run_async())""" .

"DESCRIPTION.This code defines functions that implement a switch mechanism based on the index value provided. The switch function selects and runs one of the defined functions (a_fn, b_fn, c_fn) based on the index value. The multihead_switch_example function follows a similar pattern but with different functions (a_fn, b_fn, c_fn) and additional layers of dense operations within each function. The code initializes and runs all branches when the 'params' collection in the scope is mutable." <EXPLAINS> """CODE.def switch_example(scope, x, index):
    scope.variable('state', 'a_count', lambda: 0)
    scope.variable('state', 'b_count', lambda: 0)
    scope.variable('state', 'c_count', lambda: 0)
    def a_fn(scope, x):
      scope.variable('state', 'a_count').value += 1
      return scope.child(nn.dense)(x, 2)
    def b_fn(scope, x):
      scope.variable('state', 'b_count').value += 1
      return -scope.child(nn.dense)(x, 2)
    def c_fn(scope, x):
      scope.variable('state', 'c_count').value += 1
      return scope.child(nn.dense)(x, 2)
    return lift.switch(index, [a_fn, b_fn, c_fn], scope, x)

def multihead_switch_example(scope, x, index):
    def a_fn(scope, x):
      x = scope.child(nn.dense)(x, 10)
      x = scope.child(nn.dense)(x, 7)
      return scope.child(nn.dense)(x, 5)
    def b_fn(scope, x):
      x = scope.child(nn.dense)(x, 11)
      return scope.child(nn.dense)(x, 5)
    def c_fn(scope, x):
      return scope.child(nn.dense)(x, 5)

    branches = [a_fn, b_fn, c_fn]

    # run all branches on init
    if scope.is_mutable_collection('params'):
      for branch in branches:
        _ = branch(scope, x)

    return lift.switch(index, branches, scope, x)""" .

"DESCRIPTION.This code defines functions that perform different operations on input data based on the value of the index parameter. The switch_example function creates functions a_fn, b_fn, and c_fn that increment counters and apply neural network operations based on the input data. The multihead_switch_example function defines similar functions a_fn, b_fn, and c_fn that apply multiple neural network operations on the input data. Depending on the index value, either a_fn, b_fn, or c_fn is applied to the input data using the lift.switch function." <EXPLAINS> """CODE.def switch_example(scope, x, index):
    scope.variable('state', 'a_count', lambda: 0)
    scope.variable('state', 'b_count', lambda: 0)
    scope.variable('state', 'c_count', lambda: 0)
    def a_fn(scope, x):
      scope.variable('state', 'a_count').value += 1
      return scope.child(nn.dense)(x, 2)
    def b_fn(scope, x):
      scope.variable('state', 'b_count').value += 1
      return -scope.child(nn.dense)(x, 2)
    def c_fn(scope, x):
      scope.variable('state', 'c_count').value += 1
      return scope.child(nn.dense)(x, 2)
    return lift.switch(index, [a_fn, b_fn, c_fn], scope, x)

def multihead_switch_example(scope, x, index):
    def a_fn(scope, x):
      x = scope.child(nn.dense)(x, 10)
      x = scope.child(nn.dense)(x, 7)
      return scope.child(nn.dense)(x, 5)
    def b_fn(scope, x):
      x = scope.child(nn.dense)(x, 11)
      return scope.child(nn.dense)(x, 5)
    def c_fn(scope, x):
      return scope.child(nn.dense)(x, 5)

    branches = [a_fn, b_fn, c_fn]

    # run all branches on init
    if scope.is_mutable_collection('params'):
      for branch in branches:
        _ = branch(scope, x)

    return lift.switch(index, branches, scope, x)""" .

"DESCRIPTION.This code defines input data 'img' with shape [64, 784], creates weight 'fc_w' with shape [784, 200] and bias 'fc_b' with shape [200], and retrieves parameters from the default main program." <EXPLAINS> """CODE.import paddle.fluid as fluid
data = fluid.data(name="img", shape=[64, 784])
w = fluid.layers.create_parameter(shape=[784, 200], dtype='float32', name='fc_w')
b = fluid.layers.create_parameter(shape=[200], dtype='float32', name='fc_b')
list_para  = fluid.io.get_program_parameter(  fluid.default_main_program() )""" .

"DESCRIPTION.This code defines multiple aggregation functions with relabeling for the 'a' column, including 'max' and 'min'." <EXPLAINS> """CODE._is_multi_agg_with_relabel(a_max=('a', 'max'),
                            a_min=('a', 'min'))""",
        """CODE.is_multi_agg_with_relabel(a="max")
is_multi_agg_with_relabel(a_max=("a", "max"), a_min=("a", "min"))
is_multi_agg_with_relabel()""" .

"DESCRIPTION.This code defines regular expressions for matching real numbers, dates in the format YYYY-MM-DD, and Roman numerals." <EXPLAINS> """CODE.realnum = Regex(r"[+-]?\\d+\\.\\d*")
date = Regex(r'(?P<year>\\d{4})-(?P<month>\\d\\d?)-(?P<day>\\d\\d?)')
roman = Regex(r"M{0,4}(CM|CD|D?C{0,3})(XC|XL|L?X{0,3})(IX|IV|V?I{0,3})")""" .

"DESCRIPTION.This code defines two GraphQL object types: AddressType with fields street, number, and formatted, and PersonType with fields name and bestFriend." <EXPLAINS> """CODE.AddressType = GraphQLObjectType('Address', {
    'street': GraphQLField(GraphQLString),
    'number': GraphQLField(GraphQLInt),
    'formatted': GraphQLField(GraphQLString,
        resolver=lambda obj, args, context, info: obj.number + ' ' + obj.street),
})

PersonType = GraphQLObjectType('Person', lambda: {
    'name': GraphQLField(GraphQLString),
    'bestFriend': GraphQLField(PersonType)
})""" .

"""DESCRIPTION.This code defines two classes, SimpleFunc and Func, which are used to define custom forward and backward functions for automatic differentiation in PyTorch. SimpleFunc's forward function simply returns a tuple of the input tensor x cloned twice. Its backward function adds the two input gradients g1 and g2.

Func is then modified to handle non-materialized gradient outputs. Its forward function sets materialize_grads to False, saves the input tensor x for backward computation, and returns a tuple of x cloned twice. The backward function accesses the saved tensor x, creates a gradient input tensor with zeros like x, and adds g1 and g2 to the gradient input if they are not None.""" <EXPLAINS> """CODE.class SimpleFunc(Function):
    @staticmethod
    def forward(ctx, x):
        return x.clone(), x.clone()

    @staticmethod
    @once_differentiable
    def backward(ctx, g1, g2):
        return g1 + g2  # No check for None necessary

# We modify SimpleFunc to handle non-materialized grad outputs
class Func(Function):
    @staticmethod
    def forward(ctx, x):
        ctx.set_materialize_grads(False)
        ctx.save_for_backward(x)
        return x.clone(), x.clone()

    @staticmethod
    @once_differentiable
    def backward(ctx, g1, g2):
        x, = ctx.saved_tensors
        grad_input = torch.zeros_like(x)
        if g1 is not None:  # We must check for None now
            grad_input += g1
        if g2 is not None:
            grad_input += g2
        return grad_input

a = torch.tensor(1., requires_grad=True)
b, _ = Func.apply(a)  # induces g2 to be undefined""" .

"DESCRIPTION.This code defines two functions. The first function `learn_scale` takes in a scope, two input variables x and y, and returns the product of a parameter `p`, x, and y. The second function `f` takes in a scope, x, and y, and computes the value of `learn_scale` and its gradients with respect to x and y." <EXPLAINS> """CODE.def learn_scale(scope, x, y):
    p = scope.param('scale', nn.initializers.zeros_init(), ())
    return p * x * y

def f(scope, x, y):
    z, x_grad, y_grad = lift.value_and_grad(learn_scale, scope, x, y)
    return z, x_grad, y_grad
""" .

"DESCRIPTION.This code defines two linear operators A1 and A2 with diagonal matrices. It then adds A1 and A2 using an \"Adder\" class, resulting in a new linear operator B with a diagonal matrix that is the sum of A1 and A2. The new linear operator B has a name reflecting the addition operation performed." <EXPLAINS> """CODE.A1 = LinearOperatorDiag(diag=[1., 1.], name="A1")
A2 = LinearOperatorDiag(diag=[2., 2.], name="A2")

# Use two tiers, the first contains an Adder that returns Diag.  Since both
# A1 and A2 are Diag, they can use this Adder.  The second tier will not be
# used.
addition_tiers = [
    [_AddAndReturnDiag()],
    [_AddAndReturnMatrix()]]
B_list = add_operators([A1, A2], addition_tiers=addition_tiers)

len(B_list)
==> 1

B_list[0].__class__.__name__
==> 'LinearOperatorDiag'

B_list[0].to_dense()
==> [[3., 0.],
     [0., 3.]]

B_list[0].name
==> 'Add/A1__A2/'
""" .

"DESCRIPTION.This code defines two name scopes ('scope1' and 'scope2') using TensorFlow and prints the current name scope using tf.contrib.framework.get_name_scope()." <EXPLAINS> """CODE.with tf.name_scope('scope1'):
    with tf.name_scope('scope2'):
      print(tf.contrib.framework.get_name_scope())
""" .

"DESCRIPTION.This code defines variables with specific shapes and values using the PaddlePaddle library. It creates arrays filled with specific values and data types, such as integers, floats, and booleans, and also demonstrates how to use both constant values and variable tensors to fill the arrays." <EXPLAINS> """CODE.import paddle.fluid as fluid

data1 = fluid.layers.full(shape=[2,1], fill_value=0, dtype='int64') # data1=[[0],[0]]
data2 = fluid.layers.full(shape=[2,1], fill_value=5, dtype='int64', device='gpu') # data2=[[5],[5]]

# attr shape is a list which contains Variable Tensor.
positive_2 = fluid.layers.fill_constant([1], "int32", 2)
data3 = fluid.layers.full(shape=[1, positive_2], dtype='float32', fill_value=1.5) # data3=[1.5, 1.5]

# attr shape is an Variable Tensor.
shape = fluid.layers.fill_constant([1,2], "int32", 2) # shape=[2,2]
data4 = fluid.layers.full(shape=shape, dtype='bool', fill_value=True) # data4=[[True,True],[True,True]]

# attr value is an Variable Tensor.
val = fluid.layers.fill_constant([1], "float32", 2.0) # val=[2.0]
data5 = fluid.layers.full(shape=[2,1], fill_value=val, dtype='float32') #data5=[[2.0],[2.0]]""" .

"DESCRIPTION.This code demonstrates how to enable or disable gradient computation in PaddlePaddle for specific operations." <EXPLAINS> """CODE.import paddle
x = paddle.to_tensor([1.], stop_gradient=False)
is_train = False
with paddle.set_grad_enabled(is_train):
    y = x * 2
assert(y.stop_gradient == True)

paddle.set_grad_enabled(True)
y = x * 2
assert(y.stop_gradient == False)

paddle.set_grad_enabled(False)
y = x * 2
assert(y.stop_gradient == True)""" .

"DESCRIPTION.This code disables gradient calculation for the variable \"Size\" in TensorFlow." <EXPLAINS> """CODE.tf.NotDifferentiable("Size")
""",
        """CODE.tf.no_gradient("Size")
""" .

"DESCRIPTION.This code displays \"Why hello there\" if a button with the label 'Say hello' is pressed, otherwise it displays \"Goodbye\"." <EXPLAINS> """CODE.if st.button('Say hello'):
    st.write('Why hello there')
else:
    st.write('Goodbye')""" .

"DESCRIPTION.This code displays a radio button with options for different movie genres (Comedy, Drama, Documentary). If the user selects \"Comedy\", it will display a message indicating the selection, otherwise it will display a message indicating that comedy was not selected." <EXPLAINS> """CODE.st.radio(
    "What's your favorite movie genre",
    ('Comedy', 'Drama', 'Documentary')
)

if genre == 'Comedy':
    st.write('You selected comedy.')
else:
    st.write("You didn't select comedy.")""" .

"DESCRIPTION.This code displays a subheader with the text \"This is a subheader\" in a streamlit application." <EXPLAINS> "CODE.st.subheader('This is a subheader')" .

"DESCRIPTION.This code displays a subheader with the text \"This is a subheader\" on the screen." <EXPLAINS> "CODE.st.subheader('This is a subheader')" .

"DESCRIPTION.This code displays a success message using the Streamlit library." <EXPLAINS> "CODE.st.success('This is a success message!')" .

"DESCRIPTION.This code displays an explanation and an image of dice numbers that were rolled to ensure they are random." <EXPLAINS> """CODE.with st.beta_expander("See explanation"):
    st.write(\"\"\"
        The chart above shows some numbers I picked for you.
        I rolled actual dice for these, so they're *guaranteed* to
        be random.
    \"\"\")
    st.image("https://static.streamlit.io/examples/dice.jpg")""" .

"DESCRIPTION.This code displays an explanatory text and an image within a beta_expander widget, showing numbers generated from rolling actual dice." <EXPLAINS> """CODE.with st.beta_expander("See explanation"):
    st.write(\"\"\"
        The chart above shows some numbers I picked for you.
        I rolled actual dice for these, so they're *guaranteed* to
        be random.
    \"\"\")
    st.image("https://static.streamlit.io/examples/dice.jpg")""" .

"DESCRIPTION.This code displays an image with a caption \"Sunrise by the mountains\" in a streamlit web application, adjusting the image width to fit the column." <EXPLAINS> "CODE.st.image(image, caption='Sunrise by the mountains', use_column_width=True)" .

"DESCRIPTION.This code displays an informational message." <EXPLAINS> "CODE.st.info('This is a purely informational message')" .

"DESCRIPTION.This code displays the message 'Why hello there' if the button 'Say hello' is clicked, otherwise it displays 'Goodbye'." <EXPLAINS> """CODE.if st.button('Say hello'):
    st.write('Why hello there')
else:
    st.write('Goodbye')""" .

"DESCRIPTION.This code displays three columns, each containing a header with an image of a different animal (cat, dog, owl). The images are displayed with the specified URLs and they are resized to fit the width of their respective columns." <EXPLAINS> """CODE.col1, col2, col3 = st.beta_columns(3)

with col1:
    st.header("A cat")
    st.image("https://static.streamlit.io/examples/cat.jpg", use_column_width=True)

with col2:
    st.header("A dog")
    st.image("https://static.streamlit.io/examples/dog.jpg", use_column_width=True)

with col3:
    st.header("An owl")
    st.image("https://static.streamlit.io/examples/owl.jpg", use_column_width=True)
""" .

"DESCRIPTION.This code downloads an image from a given URL, extracts features from the image using a GroupViT model, and stores the extracted features in the variable image_features." <EXPLAINS> """CODE.from PIL import Image
import requests
from transformers import AutoProcessor, GroupViTModel

model = GroupViTModel.from_pretrained("nvidia/groupvit-gcc-yfcc")
processor = AutoProcessor.from_pretrained("nvidia/groupvit-gcc-yfcc")

url = "http://images.cocodataset.org/val2017/000000039769.jpg"
image = Image.open(requests.get(url, stream=True).raw)

inputs = processor(images=image, return_tensors="pt")

image_features = model.get_image_features(**inputs)
""" .

"DESCRIPTION.This code downloads an image from a given URL, processes the image features using a pretrained AltCLIP model, and extracts the image features." <EXPLAINS> """CODE.from PIL import Image
import requests
from transformers import AutoProcessor, AltCLIPModel

model = AltCLIPModel.from_pretrained("BAAI/AltCLIP")
processor = AutoProcessor.from_pretrained("BAAI/AltCLIP")
url = "http://images.cocodataset.org/val2017/000000039769.jpg"
image = Image.open(requests.get(url, stream=True).raw)
inputs = processor(images=image, return_tensors="pt")
image_features = model.get_image_features(**inputs)
""" .

"DESCRIPTION.This code downloads an image from a given URL, processes the image using a pre-trained AlignModel and AutoProcessor from the kakaobrain library, and extracts features from the processed image using the AlignModel." <EXPLAINS> """CODE.from PIL import Image
import requests
from transformers import AutoProcessor, AlignModel

model = AlignModel.from_pretrained("kakaobrain/align-base")
processor = AutoProcessor.from_pretrained("kakaobrain/align-base")

url = "http://images.cocodataset.org/val2017/000000039769.jpg"
image = Image.open(requests.get(url, stream=True).raw)

inputs = processor(images=image, return_tensors="pt")

image_features = model.get_image_features(**inputs)
""" .

"DESCRIPTION.This code enables and disables the dygraph mode in PaddlePaddle's fluid framework and checks if the code is currently in dygraph mode." <EXPLAINS> """CODE.import paddle.fluid as fluid

fluid.enable_dygraph()  # Now we are in dygragh mode
print(fluid.in_dygraph_mode())  # True
fluid.disable_dygraph()
print(fluid.in_dygraph_mode())  # False""" .

"DESCRIPTION.This code enables distributed computing using the PaddlePaddle framework. It initializes the parallel environment, then sends data from rank 0 to rank 1, while receiving data at other ranks. Finally, it converts the data to a numpy array." <EXPLAINS> """CODE.import paddle
from paddle.distributed import init_parallel_env
init_parallel_env()
if paddle.distributed.ParallelEnv().rank == 0:
    data = paddle.to_tensor([7, 8, 9])
    paddle.distributed.send(data, dst=1)
else:
    data = paddle.to_tensor([1,2,3])
    paddle.distributed.recv(data, src=0)
out = data.numpy()""" .

"DESCRIPTION.This code enables the progress bar for the TensorFlow Datasets (tfds) library." <EXPLAINS> """CODE.
tfds.enable_progress_bar()
""" .

"DESCRIPTION.This code enables the user to input a specific date and displays the inputted date in a message using Streamlit." <EXPLAINS> """CODE.st.date_input('A date to celebrate', datetime.date(2019, 7, 6)
st.write('The date', d)""" .

"DESCRIPTION.This code ensures that only the process with rank 0 downloads the dataset, then all processes wait before reading the dataset, and finally all processes can read the files and start training." <EXPLAINS> """CODE.if self.global_rank == 0:
    # let process 0 download the dataset
    dataset.download_files()

# let all processes wait before reading the dataset
self.barrier()

# now all processes can read the files and start training""" .

"DESCRIPTION.This code ensures that the dtype of the input numpy array is in nanosecond precision." <EXPLAINS> """CODE._ensure_nanosecond_dtype(np.dtype("M8[s]"))
_ensure_nanosecond_dtype(np.dtype("m8[ps]"))""" .

"DESCRIPTION.This code establishes a connection to a TPU cluster, initializes the TPU system, and creates a distribution strategy for TPUs. It then defines a step function that computes the sum of the input tensor and runs this function using the defined strategy with specific options." <EXPLAINS> """CODE.resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='')
tf.config.experimental_connect_to_cluster(resolver)
tf.tpu.experimental.initialize_tpu_system(resolver)
strategy = tf.distribute.experimental.TPUStrategy(tpu='')

options = tf.distribute.RunOptions()
options.experimental_bucketizing_dynamic_shape = True

iterator = iter(inputs)

@tf.function()
def step_fn(inputs):
  output = tf.reduce_sum(inputs)
  return output

strategy.run(step_fn, args=(next(iterator),),
                           options=options)
""" .

"DESCRIPTION.This code establishes a connection to an SQL database using Streamlit, queries the database for all records in the \"pet_owners\" table where the owner is \"barbara\", and then displays the results in a dataframe. The connection has a time-to-live (ttl) of 3600 seconds." <EXPLAINS> """CODE.import streamlit as st

conn = st.experimental_connection("sql")
df = conn.query("select * from pet_owners where owner = :owner", ttl=3600, params={"owner":"barbara"})
st.dataframe(df)""" .

"DESCRIPTION.This code establishes a remote connection to a server named \"remote_machine\" using the provided credentials (username: myname, password: mypassword) and creates a WMI object using that remote connection." <EXPLAINS> """CODE.remote_connetion = wmi.connect_server (
    server="remote_machine", user="myname", password="mypassword"
)
c = wmi.WMI (wmi=remote_connection)""" .

"DESCRIPTION.This code evaluates the performance of a RoBERTa model fine-tuned on ReactionGIF dataset for the task of text classification using the accuracy metric. The evaluation is conducted on the ReactionJPEG dataset's test split." <EXPLAINS> """CODE.from huggingface_hub import metadata_eval_result
metadata_eval_result(
    model_pretty_name="RoBERTa fine-tuned on ReactionGIF",
    task_pretty_name="Text Classification",
    task_id="text-classification",
    metrics_pretty_name="Accuracy",
    metrics_id="accuracy",
    metrics_value=0.2662102282047272,
    dataset_pretty_name="ReactionJPEG",
    dataset_id="julien-c/reactionjpeg",
    dataset_config="default",
    dataset_split="test",
)
""" .

"DESCRIPTION.This code extends a deque object with the values 3 and 4." <EXPLAINS> "CODE.pdeque([1, 2]).extend([3, 4])" .

"DESCRIPTION.This code extends a deque with the elements [3, 4] to the left." <EXPLAINS> "CODE.pdeque([1, 2]).extendleft([3, 4])" .

"DESCRIPTION.This code extends the deque object to the left with the elements [3, 4]." <EXPLAINS> "CODE.pdeque([1, 2]).extendleft([3, 4])" .

"DESCRIPTION.This code extracts and parses a date string in the format of \"year/month/day\", then retrieves specific elements such as the year, month, and day from the parsed date string." <EXPLAINS> """CODE.integer = Word(nums)
date_str = integer("year") + '/' + integer("month") + '/' + integer("day")

result = date_str.parseString("1999/12/31")
print(result.get("year")) # -> '1999'
print(result.get("hour", "not specified")) # -> 'not specified'
print(result.get("hour")) # -> None""" .

"DESCRIPTION.This code extracts text within specified HTML tags \"<b>\" and \"<i>\" from a given string \"src\"." <EXPLAINS> """CODE.src = "this is test <b> bold <i>text</i> </b> normal text "
for tag in ("b","i"):
    opener,closer = makeHTMLTags(tag)
    patt = originalTextFor(opener + SkipTo(closer) + closer)
    print(patt.searchString(src)[0])
""" .

"DESCRIPTION.This code extracts the major version number from the provided version string \"1.2.3\"." <EXPLAINS> "CODE.Version(\"1.2.3\").major" .

"DESCRIPTION.This code filters a grouped dataframe based on the sum of columns 'A' and 'B' being greater than 0." <EXPLAINS> "CODE.grouped.filter(lambda x: x['A'].sum() + x['B'].sum() > 0)" .

"DESCRIPTION.This code filters and returns the names of all worker nodes that are not terminated." <EXPLAINS> """CODE.provider.non_terminated_nodes({TAG_RAY_NODE_TYPE: "worker"})
["node-1", "node-2"]""" .

"DESCRIPTION.This code filters nodes based on the node type \"worker\" in a provider." <EXPLAINS> "CODE.provider.nodes({TAG_RAY_NODE_TYPE: \"worker\"})",
        """CODE.provider.non_terminated_nodes({TAG_RAY_NODE_TYPE: "worker"})
["node-1", "node-2"]""" .

"DESCRIPTION.This code fits a Kernel Ridge regression model to the randomly generated data X and y with an alpha regularization parameter of 1.0." <EXPLAINS> """CODE.from sklearn.kernel_ridge import KernelRidge
import numpy as np
n_samples, n_features = 10, 5
rng = np.random.RandomState(0)
y = rng.randn(n_samples)
X = rng.randn(n_samples, n_features)
clf = KernelRidge(alpha=1.0)
clf.fit(X, y)
""" .

"DESCRIPTION.This code fits a Linear Discriminant Analysis (LDA) model to the data provided in arrays X and y, and then predicts the class label for a new data point [-0.8, -1]." <EXPLAINS> """CODE.from sklearn.lda import LDA
clf = LDA()
clf.fit(X, y)
print(clf.predict([[-0.8, -1]]))""" .

"DESCRIPTION.This code fits a Quadratic Discriminant Analysis (QDA) model to the provided dataset X and corresponding labels y, and then predicts the class label for a new data point [-0.8, -1]." <EXPLAINS> """CODE.from sklearn.qda import QDA
import numpy as np
X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2])
y = np.array([1, 1, 1, 2, 2, 2])
clf = QDA()
clf.fit(X, y)
print(clf.predict([[-0.8, -1]]))
""" .

"DESCRIPTION.This code formats the string 'something' with a width of 5 characters as an integer." <EXPLAINS> """CODE.a = FormatReplace('something')
"{:5d}".format(a)""" .

"DESCRIPTION.This code function flips sequences by reversing the first part of each sequence based on the lengths provided." <EXPLAINS> """CODE.def flip_sequences(inputs, lengths):
    flipped_inputs = []
    for i in range(len(inputs)):
        seq = inputs[i][:lengths[i]]
        flipped_seq = seq[::-1] + inputs[i][lengths[i]:]
        flipped_inputs.append(flipped_seq)
    return flipped_inputs
""" .

"DESCRIPTION.This code function is to remove the role tag (if present) from a given string, which represents a class name or an object in a module." <EXPLAINS> """CODE._strip_rest_role(':class:`ClassName`')
_strip_rest_role(':py:obj:`module.Object`')
_strip_rest_role('ClassName')""" .

"DESCRIPTION.This code generates NumPy arrays of input data and labels from a file, processes each line in the file, and yields a dictionary containing the input data for two inputs ('input_1' and 'input_2') and the output data ('output'). The generated arrays are used to fit a neural network model using the fit_generator function." <EXPLAINS> """CODE.    def generate_arrays_from_file(path):
        while 1:
            f = open(path)
            for line in f:
                # create Numpy arrays of input data
                # and labels, from each line in the file
                x1, x2, y = process_line(line)
                yield ({'input_1': x1, 'input_2': x2, 'output': y})
            f.close()

    graph.fit_generator(generate_arrays_from_file('/my_file.txt'),
                        samples_per_epoch=10000, nb_epoch=10)
""" .

"DESCRIPTION.This code generates Numpy arrays of input data and labels from lines in a file, and uses them to fit a model using a generator function." <EXPLAINS> """CODE.    def generate_arrays_from_file(path):
        while True:
            with open(path) as f:
                for line in f:
                    # create Numpy arrays of input data
                    # and labels, from each line in the file
                    x, y = process_line(line)
                    yield (x, y)

    model.fit_generator(generate_arrays_from_file('/my_file.txt'),
                        steps_per_epoch=1000, epochs=10)
""" .

"DESCRIPTION.This code generates a 3D mesh visualization using tensor data for vertices, colors, and faces, and saves it using TensorBoardX's SummaryWriter." <EXPLAINS> """CODE.from tensorboardX import SummaryWriter
vertices_tensor = np.array([[
    [1, 1, 1],
    [-1, -1, 1],
    [1, -1, -1],
    [-1, 1, -1],
]], dtype=float)
colors_tensor = np.array([[
    [255, 0, 0],
    [0, 255, 0],
    [0, 0, 255],
    [255, 0, 255],
]], dtype=int)
faces_tensor = np.array([[
    [0, 2, 3],
    [0, 3, 1],
    [0, 1, 2],
    [1, 3, 2],
]], dtype=int)

writer = SummaryWriter()
writer.add_mesh('my_mesh', vertices=vertices_tensor, colors=colors_tensor, faces=faces_tensor)

writer.close()""" .

"DESCRIPTION.This code generates a 4x4 matrix filled with random numbers and then calculates the index of the minimum value along each row of the matrix." <EXPLAINS> """CODE.a = torch.randn(4, 4)
torch.argmin(a, dim=1)
""" .

"DESCRIPTION.This code generates a 4x4 tensor with random values and then finds the index of the maximum value along each row of the tensor." <EXPLAINS> """CODE.a = torch.randn(4, 4)
a
tensor([[ 1.3398,  0.2663, -0.2686,  0.2450],
        [-0.7401, -0.8805, -0.3402, -1.1936],
        [ 0.4907, -1.3948, -1.0691, -0.3132],
        [-1.6092,  0.5419, -0.2993,  0.3195]])

torch.argmax(a, dim=1)
tensor([ 0,  2,  0,  1])""" .

"DESCRIPTION.This code generates a DataFrame with 10 rows and 5 columns of randomly generated numbers and displays it in a table format." <EXPLAINS> """CODE.df = pd.DataFrame(
    np.random.randn(10, 5),
    columns=('col %d' % i for i in range(5)))
st.table(df)
""" .

"DESCRIPTION.This code generates a DataFrame with 50 rows and 3 columns filled with random numbers, and then displays a bar chart of the data." <EXPLAINS> """CODE.chart_data = pd.DataFrame(
...     np.random.randn(50, 3),
...     columns=["a", "b", "c"])
...
st.bar_chart(chart_data)""" .

"DESCRIPTION.This code generates a DataFrame with 50 rows and 3 columns filled with random numbers, then plots a bar chart using the data from the DataFrame." <EXPLAINS> """CODE.chart_data = pd.DataFrame(
    np.random.randn(50, 3),
    columns=["a", "b", "c"])
st.bar_chart(chart_data)
""" .

"DESCRIPTION.This code generates a DataFrame with two columns, 'one' and 'two', where the values in 'one' are random integers between 1 and 6 and the values in 'two' are the sum of the values in 'one' and another set of random integers between 1 and 6. It then creates a histogram plot of the values in the DataFrame." <EXPLAINS> """CODE.df = pd.DataFrame(
...     np.random.randint(1, 7, 6000),
...     columns = ['one'])
df['two'] = df['one'] + np.random.randint(1, 7, 6000)
ax = df.plot.hist(bins=12, alpha=0.5)""" .

"DESCRIPTION.This code generates a DatetimeIndex consisting of three consecutive dates starting from '2018-01-01' with a daily frequency. Then, it retrieves the day names corresponding to each date in the DatetimeIndex." <EXPLAINS> """CODE.idx = pd.date_range(start='2018-01-01', freq='D', periods=3)
idx
DatetimeIndex(['2018-01-01', '2018-01-02', '2018-01-03'],
              dtype='datetime64[ns]', freq='D')
idx.day_name()
Index(['Monday', 'Tuesday', 'Wednesday'], dtype='object')""" .

"DESCRIPTION.This code generates a Detection Error Tradeoff (DET) curve based on the true labels 'y' and the predicted probabilities 'pred'. The DET curve is displayed with false positive rate (FPR) on the x-axis and false negative rate (FNR) on the y-axis, using an estimator named 'example estimator'." <EXPLAINS> """CODE.import numpy as np
from sklearn import metrics

y = np.array([0, 0, 1, 1])
pred = np.array([0.1, 0.4, 0.35, 0.8])
fpr, fnr, thresholds = metrics.det_curve(y, pred)
display = metrics.DetCurveDisplay(
    fpr=fpr, fnr=fnr, estimator_name='example estimator'
)
display.plot()
plt.show()      # doctest: +SKIP""" .

"DESCRIPTION.This code generates a TensorFlow dataset from a generator function that yields tuples containing an integer and a list of ones." <EXPLAINS> """CODE.import itertools
tf.enable_eager_execution()

def gen():
  for i in itertools.count(1):
    yield (i, [1] * i)

ds = tf.data.Dataset.from_generator(
    gen, (tf.int64, tf.int64), (tf.TensorShape([]), tf.TensorShape([None])))

for value in ds.take(2):
  print value
# (1, array([1]))
# (2, array([1, 1]))
""" .

"DESCRIPTION.This code generates a Vandermonde matrix using numpy for a range of 5 values. Then it calculates the number of unique elements along the columns and rows of the matrix using librosa." <EXPLAINS> """CODE.x = np.vander(np.arange(5))
librosa.util.count_unique(x, axis=0)
librosa.util.count_unique(x, axis=-1)""" .

"DESCRIPTION.This code generates a confusion matrix plot for the true labels (y_true) versus the predicted probabilities (y_probas) with the specified labels." <EXPLAINS> "CODE.wandb.sklearn.plot_confusion_matrix(y_true, y_probas, labels)" .

"DESCRIPTION.This code generates a dataset with 10 samples, each sample containing random values for 'height', 'age' and 'label'. The generated dataset is then used to create an input function for a TensorFlow session with specified batch size, shuffle behavior, and number of epochs." <EXPLAINS> """CODE.def generator():
    for index in range(10):
        yield {'height': np.random.randint(32,36),
               'age': np.random.randint(18, 80),
               'label': np.ones(1)}

with tf.Session() as session:
    input_fn = generator_io.generator_input_fn(
        generator, target_key="label", batch_size=2, shuffle=False,
        num_epochs=1)
""" .

"DESCRIPTION.This code generates a header with the text \"This is a header\"." <EXPLAINS> "CODE.st.header('This is a header')" .

"DESCRIPTION.This code generates a histogram of dummy data values, calculates the sum of squares of the values, and writes the histogram data along with summary statistics to a file using a SummaryWriter object." <EXPLAINS> """CODE.import numpy as np
dummy_data = []
for idx, value in enumerate(range(30)):
    dummy_data += [idx + 0.001] * value
values = np.array(dummy_data).astype(float).reshape(-1)
counts, limits = np.histogram(values)
sum_sq = values.dot(values)
with SummaryWriter() as summary_writer:
    summary_writer.add_histogram_raw(
            tag='hist_dummy_data',
            min=values.min(),
            max=values.max(),
            num=len(values),
            sum=values.sum(),
            sum_squares=sum_sq,
            bucket_limits=limits[1:].tolist(),
            bucket_counts=counts.tolist(),
            global_step=0)""" .

"DESCRIPTION.This code generates a learning curve display for a decision tree classifier using the Iris dataset. It shows the relationship between training and test scores as the training size increases." <EXPLAINS> """CODE.import matplotlib.pyplot as plt
from sklearn.datasets import load_iris
from sklearn.model_selection import LearningCurveDisplay, learning_curve
from sklearn.tree import DecisionTreeClassifier
X, y = load_iris(return_X_y=True)
tree = DecisionTreeClassifier(random_state=0)
train_sizes, train_scores, test_scores = learning_curve(
    tree, X, y)
display = LearningCurveDisplay(train_sizes=train_sizes,
    train_scores=train_scores, test_scores=test_scores, score_name="Score")
display.plot()
plt.show()""" .

"DESCRIPTION.This code generates a link to the specific line of code within a Python module, using the given package, URL format template, revision, and line number." <EXPLAINS> """CODE._linkcode_resolve('py', {'module': 'tty',
...                          'fullname': 'setraw'},
...                   package='tty',
...                   url_fmt='http://hg.python.org/cpython/file/'
...                           '{revision}/Lib/{package}/{path}#L{lineno}',
...                   revision='xxxx')
'http://hg.python.org/cpython/file/xxxx/Lib/tty/tty.py#L18'""" .

"DESCRIPTION.This code generates a log-frequency filter bank with different parameters such as the sampling rate, number of filters, and spread." <EXPLAINS> """CODE.logfs_fb = librosa.filters.logfrequency(22050, 4096)
logfs_fb = librosa.filters.logfrequency(22050, 4096, fmin=110, fmax=880)
logfs_fb = librosa.filters.logfrequency(22050, 4096, spread=0.05)
logfs_fb = librosa.filters.logfrequency(22050, 4096, spread=0.5)""" .

"DESCRIPTION.This code generates a pandas DataFrame with 1000 rows and 2 columns containing random numbers normalized by dividing by [50, 50] and adding [37.76, -122.4]. It then plots the DataFrame on a map." <EXPLAINS> """CODE.import pandas as pd
import numpy as np

df = pd.DataFrame(
    np.random.randn(1000, 2) / [50, 50] + [37.76, -122.4],
    columns=['lat', 'lon'])

st.map(df)
""" .

"DESCRIPTION.This code generates a plot showing the weightings of CQT frequencies for different weightings (ABCDZ) using librosa library." <EXPLAINS> """CODE.import matplotlib.pyplot as plt
freqs = librosa.cqt_frequencies(108, librosa.note_to_hz('C1'))
weightings = 'ABCDZ'
weights = librosa.multi_frequency_weighting(freqs, weightings)
fig, ax = plt.subplots()
for label, w in zip(weightings, weights):
    ax.plot(freqs, w, label=label)
ax.set(xlabel='Frequency (Hz)', ylabel='Weighting (log10)',
       title='Weightings of CQT frequencies')
ax.legend()""" .

"DESCRIPTION.This code generates a plot with the values from the numpy array 'values' and customizes the y-axis labels using a ChromaFormatter from the librosa display module. The y-axis label is set to 'Pitch class'." <EXPLAINS> """CODE.import matplotlib.pyplot as plt
values = np.arange(12)
plt.figure()
ax = plt.gca()
ax.plot(values)
ax.yaxis.set_major_formatter(librosa.display.ChromaFormatter())
ax.set_ylabel('Pitch class')""" .

"DESCRIPTION.This code generates a random 10x5 DataFrame using NumPy and Pandas, and then displays it as an arrow table using Streamlit." <EXPLAINS> """CODE.import streamlit as st
import pandas as pd
import numpy as np

df = pd.DataFrame(
   np.random.randn(10, 5),
   columns=("col %d" % i for i in range(5)))

st._arrow_table(df)""",
        """CODE.import streamlit as st
import pandas as pd
import numpy as np

df = pd.DataFrame(np.random.randn(10, 5), columns=("col %d" % i for i in range(5)))

st.table(df)""" .

"DESCRIPTION.This code generates a random dataset with a size of 10 and containing lists of integers with a length of 20." <EXPLAINS> "CODE.RandomDataset(size=10, length=20)" .

"DESCRIPTION.This code generates a random normal variable with shape (2,3) using TensorFlow." <EXPLAINS> """CODE.    # TensorFlow example
    kvar = K.random_normal_variable((2,3), 0, 1)
    kvar
    K.eval(kvar)
""" .

"DESCRIPTION.This code generates a random sparse binary matrix of size 5x5, then rolls the matrix by 2 along the first axis. It also creates an equivalent dense roll of the sparse matrix and checks if the two rolled matrices are all close to each other." <EXPLAINS> """CODE.# Generate a random sparse binary matrix
X = scipy.sparse.lil_matrix(np.random.randint(0, 2, size=(5,5)))
X_roll = roll_sparse(X, 2, axis=0)  # Roll by 2 on the first axis
X_dense_r = roll_sparse(X.toarray(), 2, axis=0)  # Equivalent dense roll
np.allclose(X_roll, X_dense_r.toarray())
""" .

"DESCRIPTION.This code generates a random target tensor of shape (10, 25, 25), creates a prediction tensor with the same values as the target tensor, modifies a specific subset of the prediction tensor values, and calculates the Intersection over Union (IoU) between the prediction and target tensors." <EXPLAINS> """CODE.target = torch.randint(0, 1, (10, 25, 25))
pred = torch.tensor(target)
pred[2:5, 7:13, 9:15] = 1 - pred[2:5, 7:13, 9:15]
iou(pred, target)
""" .

"DESCRIPTION.This code generates a random uniform variable with a shape of (2,3) and values between 0 and 1. It then evaluates the variable and prints the result." <EXPLAINS> """CODE.kvar = K.random_uniform_variable((2,3), 0, 1)
kvar
K.eval(kvar)
""" .

"DESCRIPTION.This code generates a random vector 'x' with a normal distribution using a random number generator, and then applies dropout with a probability of 0.5 to create a new vector 'x_drop'." <EXPLAINS> """CODE.with nn.stochastic(rng):
    x = random.normal(nn.make_rng(), shape)
    x_drop = nn.dropout(x, 0.5)
""" .

"DESCRIPTION.This code generates a repository card with English language and saves it to a markdown file at \"/tmp/test.md\"." <EXPLAINS> """CODE.from huggingface_hub.repocard import RepoCard
card = RepoCard("---\\nlanguage: en\\n---\\n# This is a test repo card")
card.save("/tmp/test.md")
""" .

"DESCRIPTION.This code generates a scatter matrix plot for the dataframe \"df\" with a transparency of 0.2 for the points." <EXPLAINS> "CODE.scatter_matrix(df, alpha=0.2)" .

"DESCRIPTION.This code generates a scatter plot using Vega-Lite with circles as marks. The x-axis represents the quantitative values of column 'a', the y-axis represents the quantitative values of column 'b', the size of the circles represents the quantitative values of column 'c', and the color of the circles also represents the values of column 'c'." <EXPLAINS> """CODE.st.vega_lite_chart(df, {
    'mark': {'type': 'circle', 'tooltip': True},
    'encoding': {
        'x': {'field': 'a', 'type': 'quantitative'},
        'y': {'field': 'b', 'type': 'quantitative'},
        'size': {'field': 'c', 'type': 'quantitative'},
        'color': {'field': 'c', 'type': 'quantitative'},
    },
})""" .

"DESCRIPTION.This code generates a sequence of numbers from 10 to 1, groups the numbers into batches of 4, and then flattens the batches into a single sequence." <EXPLAINS> "CODE.next(from_range(10, 1).batch(4).flatten())" .

"DESCRIPTION.This code generates a table by displaying the input list elements in a structured format." <EXPLAINS> "CODE.make_table([0,1,2,5,6,7,9])" .

"DESCRIPTION.This code generates a tensor with values range from 0 to 10 with a step of 1.5." <EXPLAINS> "CODE.tf.keras.backend.arange(start=0, stop=10, step=1.5)" .

"DESCRIPTION.This code generates an array containing values representing infinity, not a number (NaN), and negative infinity." <EXPLAINS> """CODE.mask_zero_div_zero(x, y, result)
array([ inf,  nan, -inf])""" .

"DESCRIPTION.This code generates an identity matrix of size 3 using the Keras backend, and then generates an error as the function K.eye() does not accept a tuple as input." <EXPLAINS> """CODE.from keras import backend as K
K.eval(K.eye(3))
K.eval(K.eye((2, 3)))
""" .

"DESCRIPTION.This code generates and plots the true gradient of a cosine function, the cyclic gradient using the librosa package, and the gradient using the np.gradient function. It then displays these three gradients on a plot with specific limits and styling." <EXPLAINS> """CODE.import numpy as np
import matplotlib.pyplot as plt

x = 2 * np.pi * np.linspace(0, 1, num=64, endpoint=False)
y = np.cos(x)
grad = np.gradient(y)
cyclic_grad = librosa.util.cyclic_gradient(y)
true_grad = -np.sin(x) * 2 * np.pi / len(x)
plt.plot(x, true_grad, label='True gradient', linewidth=5, alpha=0.35)
plt.plot(x, cyclic_grad, label='cyclic_gradient')
plt.plot(x, grad, label='np.gradient', linestyle=':')
plt.legend()
plt.xlim([0, np.pi/16])
plt.ylim([-0.025, 0.025])
plt.show()""" .

"DESCRIPTION.This code generates arrays of input data and labels from each line in a file, using a custom function to process each line. It then fits a model using the generated arrays in batches for a specified number of epochs." <EXPLAINS> """CODE.def generate_arrays_from_file(path):
    while 1:
        f = open(path)
        for line in f:
            # create numpy arrays of input data
            # and labels, from each line in the file
            x, y = process_line(line)
            yield x, y
        f.close()

model.fit_generator(generate_arrays_from_file('/my_file.txt'),
                    samples_per_epoch=10000, nb_epoch=10)
""" .

"DESCRIPTION.This code generates cross-hashed values for the input ragged tensors using a given number of buckets." <EXPLAINS> """CODE.tf.ragged.cross_hashed([tf.ragged.constant([['a'], ['b', 'c']]),
                         tf.ragged.constant([['d'], ['e']]),
                         tf.ragged.constant([['f'], ['g']])],
                        num_buckets=100)""" .

"DESCRIPTION.This code generates model cards with information such as language, license, library name, tags, datasets, metrics, evaluation results, model description, model name, and custom template for model evaluation and documentation." <EXPLAINS> """CODE.from huggingface_hub import ModelCard, ModelCardData, EvalResult

# Using the Default Template
card_data = ModelCardData(
...     language='en',
...     license='mit',
...     library_name='timm',
...     tags=['image-classification', 'resnet'],
...     datasets='beans',
...     metrics=['accuracy'],
... )
card = ModelCard.from_template(
...     card_data,
...     model_description='This model does x + y...'
... )

# Including Evaluation Results
card_data = ModelCardData(
...     language='en',
...     tags=['image-classification', 'resnet'],
...     eval_results=[
...         EvalResult(
...             task_type='image-classification',
...             dataset_type='beans',
...             dataset_name='Beans',
...             metric_type='accuracy',
...             metric_value=0.9,
...         ),
...     ],
...     model_name='my-cool-model',
... )
card = ModelCard.from_template(card_data)

# Using a Custom Template
card_data = ModelCardData(
...     language='en',
...     tags=['image-classification', 'resnet']
... )
card = ModelCard.from_template(
...     card_data=card_data,
...     template_path='./src/huggingface_hub/templates/modelcard_template.md',
...     custom_template_var='custom value',  # will be replaced in template if it exists
... )
""" .

"DESCRIPTION.This code generates numpy arrays of input data and labels from each line in a file and feeds them to a neural network model for training." <EXPLAINS> """CODE.def generate_arrays_from_file(path):
    while 1:
        f = open(path)
        for line in f:
            # create numpy arrays of input data
            # and labels, from each line in the file
            x1, x2, y = process_line(line)
            yield {'input_1': x1, 'input_2': x2, 'output': y}
        f.close()

graph.fit_generator(generate_arrays_from_file('/my_file.txt'),
                    samples_per_epoch=10000, nb_epoch=10)
""" .

"DESCRIPTION.This code generates numpy arrays of input data and labels from each line in a file, and feeds them to a model for training." <EXPLAINS> """CODE.def generate_arrays_from_file(path):
    while 1:
        f = open(path)
        for line in f:
            # create numpy arrays of input data
            # and labels, from each line in the file
            x, y = process_line(line)
            yield x, y
        f.close()

model.fit_generator(generate_arrays_from_file('/my_file.txt'),
                    samples_per_epoch=10000, nb_epoch=10)
""" .

"DESCRIPTION.This code generates numpy arrays of input data and labels from each line in a file, and yields them as dictionaries containing keys 'input_1', 'input_2', and 'output'. This data is used to fit a neural network model using the fit_generator method with specified number of samples per epoch and epochs." <EXPLAINS> """CODE.def generate_arrays_from_file(path):
    while 1:
        f = open(path)
        for line in f:
            # create numpy arrays of input data
            # and labels, from each line in the file
            x1, x2, y = process_line(line)
            yield {'input_1': x1, 'input_2': x2, 'output': y}
        f.close()

graph.fit_generator(generate_arrays_from_file('/my_file.txt'),
                    samples_per_epoch=10000, nb_epoch=10)
""" .

"DESCRIPTION.This code generates one-hot encoded arrays based on input array values and specified depth." <EXPLAINS> """CODE.jax.nn.one_hot(np.array([0, 1, 2]), 3)
DeviceArray([[1., 0., 0.],
             [0., 1., 0.],
             [0., 0., 1.]], dtype=float32)
jax.nn.one_hot(np.array([-1, 3]), 3)
DeviceArray([[0., 0., 0.],
             [0., 0., 0.]], dtype=float32)""" .

"DESCRIPTION.This code generates random data frames, adds rows from a second data frame to the first one, creates a line chart using the first data frame, adds rows from the second data frame to the line chart, and finally creates a Vega-Lite line chart using the first data frame and adds rows from the second data frame to it." <EXPLAINS> """CODE.df1 = pd.DataFrame(
   np.random.randn(50, 20),
   columns=('col %d' % i for i in range(20)))

my_table = st._legacy_table(df1)

df2 = pd.DataFrame(
   np.random.randn(50, 20),
   columns=('col %d' % i for i in range(20)))

my_table._legacy_add_rows(df2)

my_chart = st._legacy_line_chart(df1)
my_chart._legacy_add_rows(df2)

my_chart = st._legacy_vega_lite_chart({
    'mark': 'line',
    'encoding': {'x': 'a', 'y': 'b'},
    'datasets': {
        'some_fancy_name': df1,  # <-- named dataset
    },
    'data': {'name': 'some_fancy_name'},
}),
my_chart._legacy_add_rows(some_fancy_name=df2)  # <-- name used as keyword
""" .

"DESCRIPTION.This code generates random input tensors `x` and `y` with specified sizes `x_len` and `y_len`, where the size of `y` can be either 1 or the same as `x` depending on the specified distribution." <EXPLAINS> """CODE.Fuzzer(
    parameters=[
        FuzzedParameter("x_len", 4, 1024, distribution="uniform"),

        # `y` will either be size one, or match the size of `x`.
        FuzzedParameter("y_len", distribution={
            0.5: 1,
            0.5: ParameterAlias("x_len")
        }),
    ],
    tensors=[
        FuzzedTensor("x", size=("x_len",)),
        FuzzedTensor("y", size=("y_len",)),
    ],
)""" .

"DESCRIPTION.This code generates random samples from a Box space with values between 0 and 1 of shape (3,) and stores them in the 'items' list. Then, it concatenates these samples into a 2D numpy array 'out'." <EXPLAINS> """CODE.from gym.spaces import Box
import numpy as np

space = Box(low=0, high=1, shape=(3,), dtype=np.float32)
out = np.zeros((2, 3), dtype=np.float32)
items = [space.sample() for _ in range(2)]
concatenate(items, out, space)
""" .

"DESCRIPTION.This code generates random samples from a Laplace distribution with loc=0 and scale=1." <EXPLAINS> """CODE.import paddle

m = paddle.distribution.Laplace(paddle.to_tensor(0.0), paddle.to_tensor(1.0))
m.sample()  # Laplace distributed with loc=0, scale=1
# Tensor(shape=[], dtype=float32, place=Place(cpu), stop_gradient=True,
#        3.68546247)""" .

"DESCRIPTION.This code generates samples from a Gumbel distribution with location parameter 0 and scale parameter 1, and then calculates various statistics such as the probability, log probability, cumulative distribution function, entropy, and resampled values." <EXPLAINS> """CODE.import paddle
from paddle.distribution.gumbel import Gumbel

# Gumbel distributed with loc=0, scale=1
dist = Gumbel(paddle.full([1], 0.0), paddle.full([1], 1.0)
dist.sample([2])
value = paddle.full([1], 0.5)
dist.prob(value)
dist.log_prob(value)
dist.cdf(value)
dist.entropy()
dist.rsample([2])""" .

"DESCRIPTION.This code generates split indices based on the complete shape, dimensions mapping, process shape, and process group provided. It returns a list of indices indicating the split points for dividing the complete tensor along the specified dimensions_mapping." <EXPLAINS> """CODE.import numpy as np
def _get_split_indices(complete_shape, dims_mapping, process_shape, process_group):
    split_indices_list = []
    for i in range(len(complete_shape)):
        if dims_mapping[i] == -1:
            split_indices_list.append([])
        else:
            start = sum(process_shape[:process_group.index(dims_mapping[i])])
            end = start + process_shape[process_group.index(dims_mapping[i])]
            split_indices_list.append(list(range(start, end)))
    return split_indices_list

complete_tensor = np.array([[[1.11, 1.12, 1.13, 1.14, 1.15, 1.16]]])
complete_shape = [1, 1, 6]
dims_mapping = [-1, -1, 0]
process_shape = [3]
process_group = [0, 1, 2]

index = _get_split_indices(complete_shape, dims_mapping, process_shape, process_group)
print(index)""" .

"DESCRIPTION.This code generates text using a pre-trained GPT-2 model with a specified input sequence and maximum number of tokens, running the generation process in a separate thread to fetch the generated text in a non-blocking way." <EXPLAINS> """CODE.from transformers import AutoModelForCausalLM, AutoTokenizer, TextIteratorStreamer
from threading import Thread

tok = AutoTokenizer.from_pretrained("gpt2")
model = AutoModelForCausalLM.from_pretrained("gpt2")
inputs = tok(["An increasing sequence: one,"], return_tensors="pt")
streamer = TextIteratorStreamer(tok)

# Run the generation in a separate thread, so that we can fetch the generated text in a non-blocking way.
generation_kwargs = dict(inputs, streamer=streamer, max_new_tokens=20)
thread = Thread(target=model.generate, kwargs=generation_kwargs)
thread.start()
generated_text = ""
for new_text in streamer:
...     generated_text += new_text
generated_text
'An increasing sequence: one, two, three, four, five, six, seven, eight, nine, ten, eleven,'
""" .

"DESCRIPTION.This code generates the Cartesian product of two tensors created from the input lists 'a' and 'b'." <EXPLAINS> """CODE.a = [1, 2, 3]
b = [4, 5]
tensor_a = torch.tensor(a)
tensor_b = torch.tensor(b)
torch.cartesian_prod(tensor_a, tensor_b)
tensor([[1, 4],
        [1, 5],
        [2, 4],
        [2, 5],
        [3, 4],
        [3, 5]])""" .

"DESCRIPTION.This code generates time series data and corresponding targets with a given length and sampling rate. It then creates a generator for batching the data with specified batch size. The code asserts the correctness of the generator output for the first batch." <EXPLAINS> """CODE.from keras.preprocessing.sequence import TimeseriesGenerator
import numpy as np

data = np.array([[i] for i in range(50)])
targets = np.array([[i] for i in range(50)])

data_gen = TimeseriesGenerator(data, targets,
                               length=10, sampling_rate=2,
                               batch_size=2)
assert len(data_gen) == 20

batch_0 = data_gen[0]
x, y = batch_0
assert np.array_equal(x,
                      np.array([[[0], [2], [4], [6], [8]],
                                [[1], [3], [5], [7], [9]]]))
assert np.array_equal(y,
                      np.array([[10], [11]]))
""" .

"DESCRIPTION.This code generates time series data and corresponding targets with specified length, sampling rate, and batch size using the TimeSeriesGenerator class from Keras. It verifies the correctness of the generated data and targets for the first batch." <EXPLAINS> """CODE.from keras.preprocessing.sequence import TimeseriesGenerator
import numpy as np

data = np.array([[i] for i in range(50)])
targets = np.array([[i] for i in range(50)])

data_gen = TimeseriesGenerator(data, targets,
                               length=10, sampling_rate=2,
                               batch_size=2)
assert len(data_gen) == 20

batch_0 = data_gen[0]
x, y = batch_0
assert np.array_equal(x,
                      np.array([[[0], [2], [4], [6], [8]],
                                [[1], [3], [5], [7], [9]]]))
assert np.array_equal(y,
                      np.array([[10], [11]]))
""" .

"DESCRIPTION.This code generates time series data and targets in batches using the TimeseriesGenerator class. It creates batches of input-output pairs with a specified length, sampling rate, and batch size. The generated data batches are then accessed and verified for correctness." <EXPLAINS> """CODE.from keras.preprocessing.sequence import TimeseriesGenerator
import numpy as np

data = np.array([[i] for i in range(50)])
targets = np.array([[i] for i in range(50)])

data_gen = TimeseriesGenerator(data, targets,
                               length=10, sampling_rate=2,
                               batch_size=2)
assert len(data_gen) == 20

batch_0 = data_gen[0]
x, y = batch_0
assert np.array_equal(x,
                      np.array([[[0], [2], [4], [6], [8]],
                                [[1], [3], [5], [7], [9]]]))
assert np.array_equal(y,
                      np.array([[10], [11]]))
""" .

"DESCRIPTION.This code generates time series data and targets using a specified length and sampling rate, and then creates batches of data and targets with a specified batch size. It asserts the length of the data generator and checks the contents of the first batch against expected values." <EXPLAINS> """CODE.from keras.preprocessing.sequence import TimeseriesGenerator
import numpy as np

data = np.array([[i] for i in range(50)])
targets = np.array([[i] for i in range(50)])

data_gen = TimeseriesGenerator(data, targets,
                               length=10, sampling_rate=2,
                               batch_size=2)
assert len(data_gen) == 20

batch_0 = data_gen[0]
x, y = batch_0
assert np.array_equal(x,
                      np.array([[[0], [2], [4], [6], [8]],
                                [[1], [3], [5], [7], [9]]]))
assert np.array_equal(y,
                      np.array([[10], [11]]))
""" .

"DESCRIPTION.This code generates time-series sequences from data and targets, with a specified length, sampling rate, and batch size. It then verifies the length of the generated data is 20, extracts the first batch, and checks if the input (x) and output (y) sequences of the batch match the expected values." <EXPLAINS> """CODE.from keras.preprocessing.sequence import TimeseriesGenerator
import numpy as np
data = np.array([[i] for i in range(50)])
targets = np.array([[i] for i in range(50)])
data_gen = TimeseriesGenerator(data, targets,
                               length=10, sampling_rate=2,
                               batch_size=2)
assert len(data_gen) == 20
batch_0 = data_gen[0]
x, y = batch_0
assert np.array_equal(x,
                      np.array([[[0], [2], [4], [6], [8]],
                                [[1], [3], [5], [7], [9]]]))
assert np.array_equal(y,
                      np.array([[10], [11]]))
""" .

"DESCRIPTION.This code generates two random time series plots using matplotlib, where the x-axis represents time or lag. Each plot has a line plot showing the values over time or lag, with the x-axis labeled accordingly." <EXPLAINS> """CODE.import matplotlib.pyplot as plt
import numpy as np
import librosa.display

times = np.arange(30)
values = np.random.randn(len(times))
plt.figure()
ax = plt.gca()
ax.plot(times, values)
ax.xaxis.set_major_formatter(librosa.display.TimeFormatter())
ax.set_xlabel('Time')

times = np.arange(60)
values = np.random.randn(len(times))
plt.figure()
ax = plt.gca()
ax.plot(times, values)
ax.xaxis.set_major_formatter(librosa.display.TimeFormatter(lag=True))
ax.set_xlabel('Lag')""" .

"""DESCRIPTION.This code has three main functionalities.

1. Print the batch number at the beginning of every batch during model training.
2. Plot the loss after every epoch during model training.
3. Terminate some processes after finishing model training.""" <EXPLAINS> """CODE.# Print the batch number at the beginning of every batch.
batch_print_callback = LambdaCallback(on_batch_begin=lambda batch, logs: print(batch))

# Plot the loss after every epoch.
import numpy as np
import matplotlib.pyplot as plt
plot_loss_callback = LambdaCallback(on_epoch_end=lambda epoch, logs: plt.plot(np.arange(epoch), logs['loss']))

# Terminate some processes after having finished model training.
processes = ...
cleanup_callback = LambdaCallback(on_train_end=lambda logs: [p.terminate() for p in processes if p.is_alive()])

model.fit(..., callbacks=[batch_print_callback, plot_loss_callback, cleanup_callback])
""" .

"DESCRIPTION.This code hashes a pandas object with specific encoding and hash key, without categorizing it." <EXPLAINS> """CODE.pd.array([1, 2])._hash_pandas_object(encoding='utf-8',
                                     hash_key="1000000000000000",
                                     categorize=False
                                     )""" .

"DESCRIPTION.This code implements Hamiltonian Monte Carlo (HMC) sampling algorithm to sample from a multivariate normal distribution. The code runs the Leapfrog integrator for a specified number of iterations and updates the position and momentum of the particles. Finally, it plots the first dimension of the positions over the iterations, showing a sinusoidal pattern." <EXPLAINS> """CODE.tfd = tf.contrib.distributions

dims = 10
num_iter = int(1e3)
dtype = np.float32

position = tf.placeholder(np.float32)
momentum = tf.placeholder(np.float32)

[
    next_momentums,
    next_positions,
] = hmc._leapfrog_integrator(
    current_momentums=[momentum],
    target_log_prob_fn=tfd.MultivariateNormalDiag(
        loc=tf.zeros(dims, dtype)).log_prob,
    current_state_parts=[position],
    step_sizes=0.1,
    num_leapfrog_steps=3)[:2]

sess.graph.finalize()  # No more graph building.

momentum_ = np.random.randn(dims).astype(dtype)
position_ = np.random.randn(dims).astype(dtype)

positions = np.zeros([num_iter, dims], dtype)
for i in xrange(num_iter):
  position_, momentum_ = sess.run(
      [next_momentums[0], next_position[0]],
      feed_dict={position: position_, momentum: momentum_})
  positions[i] = position_

plt.plot(positions[:, 0]);  # Sinusoidal.
""" .

"DESCRIPTION.This code implements a Linear Model trained with L1 prior as a regularizer, also known as Lasso regression. The optimization objective is to minimize the mean squared error between the predicted values and the actual values, while penalizing the absolute sum of the coefficients with an L1 regularization term. The model also has parameters such as alpha (the regularization strength), fit_intercept (to calculate the intercept), normalize (to normalize the input features), and others. It uses the coordinate descent algorithm for fitting the model, and can be used for feature selection and regularization in linear regression problems." <EXPLAINS> """CODE.Linear Model trained with L1 prior as regularizer (aka the Lasso)

The optimization objective for Lasso is::

    (1 / (2 * n_samples)) * ||y - Xw||^2_2 + alpha * ||w||_1

Technically the Lasso model is optimizing the same objective function as
the Elastic Net with ``l1_ratio=1.0`` (no L2 penalty).

Parameters
----------
alpha : float, optional
    Constant that multiplies the L1 term. Defaults to 1.0.
    ``alpha = 0`` is equivalent to an ordinary least square, solved
    by the :class:`LinearRegression` object. For numerical
    reasons, using ``alpha = 0`` is with the Lasso object is not advised
    and you should prefer the LinearRegression object.

fit_intercept : boolean
    whether to calculate the intercept for this model. If set
    to false, no intercept will be used in calculations
    (e.g. data is expected to be already centered).

normalize : boolean, optional, default False
    If ``True``, the regressors X will be normalized before regression.

copy_X : boolean, optional, default True
    If ``True``, X will be copied; else, it may be overwritten.

precompute : True | False | 'auto' | array-like
    Whether to use a precomputed Gram matrix to speed up
    calculations. If set to ``'auto'`` let us decide. The Gram
    matrix can also be passed as argument. For sparse input
    this option is always ``True`` to preserve sparsity.

max_iter: int, optional
    The maximum number of iterations

tol : float, optional
    The tolerance for the optimization: if the updates are
    smaller than ``tol``, the optimization code checks the
    dual gap for optimality and continues until it is smaller
    than ``tol``.

warm_start : bool, optional
    When set to True, reuse the solution of the previous call to fit as
    initialization, otherwise, just erase the previous solution.

positive : bool, optional
    When set to ``True``, forces the coefficients to be positive.

Attributes
----------
``coef_`` : array, shape = (n_features,) | (n_targets, n_features)
    parameter vector (w in the cost function formula)

``sparse_coef_`` : scipy.sparse matrix, shape = (n_features, 1) |             (n_targets, n_features)
    ``sparse_coef_`` is a readonly property derived from ``coef_``

``intercept_`` : float | array, shape = (n_targets,)
    independent term in decision function.

Examples
--------
from sklearn import linear_model
clf = linear_model.Lasso(alpha=0.1)
clf.fit([[0,0], [1, 1], [2, 2]], [0, 1, 2])
Lasso(alpha=0.1, copy_X=True, fit_intercept=True, max_iter=1000,
   normalize=False, positive=False, precompute='auto', tol=0.0001,
   warm_start=False)
print(clf.coef_)
[ 0.85  0.  ]
print(clf.intercept_)
0.15

See also
--------
lars_path
lasso_path
LassoLars
LassoCV
LassoLarsCV
sklearn.decomposition.sparse_encode

Notes
-----
The algorithm used to fit the model is coordinate descent.

To avoid unnecessary memory duplication the X argument of the fit method
should be directly passed as a Fortran-contiguous numpy array.""" .

"DESCRIPTION.This code implements a neural network model with two input layers, each connected to a dense layer with 8 units and ReLU activation function. The output is then calculated as the element-wise subtraction of the outputs from the two dense layers, followed by a dense layer with 4 units. The final model takes two sets of inputs and outputs a single vector." <EXPLAINS> """CODE.import keras
input1 = keras.layers.Input(shape=(16,))
x1 = keras.layers.Dense(8, activation='relu')(input1)
input2 = keras.layers.Input(shape=(32,))
x2 = keras.layers.Dense(8, activation='relu')(input2)
subtracted = keras.layers.Subtract()([x1, x2])
out = keras.layers.Dense(4)(subtracted)
model = keras.models.Model(inputs=[input1, input2], outputs=out)""" .

"DESCRIPTION.This code implements a sequence model using a Basic RNN cell in TensorFlow. It first creates a categorical column 'states' with a vocabulary file containing state names, then embeds this categorical column using a 10-dimensional embedding. The input features are parsed using the specified columns, and a sequence input layer is created. Finally, a Basic RNN cell is initialized with a specified hidden size, and the dynamic RNN function is used to process the input data through the RNN cell, generating outputs and state." <EXPLAINS> """CODE.states = sequence_categorical_column_with_vocabulary_file(
    key='states', vocabulary_file='/us/states.txt', vocabulary_size=50,
    num_oov_buckets=5)
states_embedding = embedding_column(states, dimension=10)
columns = [states_embedding]

features = tf.parse_example(..., features=make_parse_example_spec(columns))
input_layer, sequence_length = sequence_input_layer(features, columns)

rnn_cell = tf.nn.rnn_cell.BasicRNNCell(hidden_size)
outputs, state = tf.nn.dynamic_rnn(
    rnn_cell, inputs=input_layer, sequence_length=sequence_length)
""" .

"DESCRIPTION.This code implements a simple Multi-Layer Perceptron (MLP) model using the PaddlePaddle framework. The gen_data function generates random input data and labels for training. The mlp function defines the structure of the neural network with two fully connected layers and cross-entropy loss function. The code then uses the Adam optimizer to minimize the cost. The model is trained for 10 steps, printing the cost value at each step." <EXPLAINS> """CODE.import paddle.fluid as fluid
import numpy as np

def gen_data(batch_size):
    return {"x": np.random.random(size=(batch_size, 32)).astype('float32'),
            "y": np.random.random(size=(batch_size, 1)).astype('int64')}

def mlp(input_x, input_y, hid_dim=128, label_dim=2):
    fc_1 = fluid.layers.fc(input=input_x, size=hid_dim)
    prediction = fluid.layers.fc(input=[fc_1], size=label_dim, act='softmax')
    cost = fluid.layers.cross_entropy(input=prediction, label=input_y)
    sum_cost = fluid.layers.reduce_mean(cost)
    return sum_cost, fc_1, prediction

input_x = fluid.layers.data(name="x", shape=[32], dtype='float32')
input_y = fluid.layers.data(name="y", shape=[1], dtype='int64')
cost, fc_1, pred = mlp(input_x, input_y)
sgd = fluid.optimizer.Adam(learning_rate=0.01)
sgd = fluid.optimizer.GradientMergeOptimizer(sgd, k_steps=4, avg=True)
sgd.minimize(cost)

place = fluid.CPUPlace()
exe = fluid.Executor(place)
exe.run(fluid.default_startup_program())

for i in range(10):
    cost_val = exe.run(feed=gen_data(32),
               program=fluid.default_main_program(),
               fetch_list=[cost.name])
    print("step=%d, cost=%f" % (i, cost_val[0]))""" .

"DESCRIPTION.This code implements a simple linear regression model using the paddle library in Python. It initializes a linear layer with input size 10 and output size 10, generates random initial values, computes the output by passing the initial values through the linear layer, calculates the mean of the output as the loss, performs backpropagation to calculate gradients, and then uses the SGD optimizer to minimize the loss while applying global norm gradient clipping with a maximum norm value of 5.0." <EXPLAINS> """CODE.import numpy as np
import paddle
import paddle.fluid as fluid

from paddle.fluid.dygraph.base import to_variable
from paddle.fluid.dygraph.nn import Linear

from paddle.fluid.dygraph_grad_clip import GradClipByValue, GradClipByNorm, GradClipByGlobalNorm

from paddle.fluid.optimizer import SGDOptimizer

with fluid.dygraph.guard():
    gloabl_norm_clip = GradClipByGlobalNorm( 5.0 )
    sgd = SGDOptimizer(learning_rate=1.0)

    init_value = np.random.uniform( -1, 1, (10, 10)).astype('float32')

    linear = Linear( 10, 10)

    out = linear( to_variable(init_value) )

    loss = fluid.layers.reduce_mean( out )

    loss.backward()
    sgd.minimize(loss, grad_clip = gloabl_norm_clip)""" .

"DESCRIPTION.This code implements a while loop that iterates until the 'state' variable 'acc' is less than 10. Inside the loop, it increments the 'acc' variable by 1 and creates a new variable 'y' using a neural network dense layer." <EXPLAINS> """CODE.def cond_fn(scope, c):
    return scope.get_variable('state', 'acc') < 10

def body_fn(scope, c):
    acc = scope.variable('state', 'acc')
    acc += 1
    y = scope.child(nn.dense)(c, c.shape[-1])
    return y

c = x
c = body_fn(scope, c)
return lift.while_loop(cond_fn, body_fn, scope, (),
                       carry_variables='state')""" .

"DESCRIPTION.This code implements an exponential distribution transformation using the ExpTransform class in the PaddlePaddle library. The code calculates the forward transformation, inverse transformation, forward log determinant of the Jacobian, and inverse log determinant of the Jacobian for input tensors [1., 2., 3.]." <EXPLAINS> """CODE.import paddle

exp = paddle.distribution.ExpTransform()
print(exp.forward(paddle.to_tensor([1., 2., 3.]))

print(exp.inverse(paddle.to_tensor([1., 2., 3.]))

print(exp.forward_log_det_jacobian(paddle.to_tensor([1., 2., 3.]))

print(exp.inverse_log_det_jacobian(paddle.to_tensor([1., 2., 3.]))""" .

"DESCRIPTION.This code implements average pooling on a 1D input tensor with pool size of 2 and strides of 2." <EXPLAINS> """CODE. y = tf.compat.v1.layers.average_pooling1d(x, pool_size=2, strides=2)

 x = tf.keras.Input((28, 28, 1))
 y = tf.keras.layers.AveragePooling1D(pool_size=2, strides=2)(x)
 model = tf.keras.Model(x, y)
""" .

"DESCRIPTION.This code implements the rectified linear activation function (ReLU) with different parameters such as alpha, max_value, and threshold on a given input array." <EXPLAINS> """CODE.foo = tf.constant([-10, -5, 0.0, 5, 10], dtype = tf.float32)
tf.keras.activations.relu(foo).numpy()
tf.keras.activations.relu(foo, alpha=0.5).numpy()
tf.keras.activations.relu(foo, max_value=5).numpy()
tf.keras.activations.relu(foo, threshold=5).numpy()
""" .

"DESCRIPTION.This code imports the paddle library and lists the available modules from the 'lyuwenyu/paddlehub_demo:main' repository on GitHub without force reloading." <EXPLAINS> """CODE.import paddle

paddle.hub.list('lyuwenyu/paddlehub_demo:main', source='github', force_reload=False)
""" .

"DESCRIPTION.This code initializes a 2x3 matrix with zeros using JAX library in Python." <EXPLAINS> """CODE.import jax, jax.numpy as jnp
jax.nn.initializers.zeros(jax.random.PRNGKey(42), (2, 3), jnp.float32)""" .

"DESCRIPTION.This code initializes a 2x3 matrix with zeros using the Flax library in Python." <EXPLAINS> """CODE.import jax, jax.numpy as jnp
from flax.linen.initializers import zeros_init
zeros_initializer = zeros_init()
zeros_initializer(jax.random.PRNGKey(42), (2, 3), jnp.float32)""" .

"DESCRIPTION.This code initializes a Convolutional Neural Network model using PaddlePaddle, creates an optimizer with stochastic gradient descent (SGD) for training the model, and sets up a gradient scaler for automatic mixed precision training. It generates random data in the shape of [10, 3, 32, 32], performs forward pass through the model, calculates the mean of the output tensor, scales the loss using the gradient scaler, performs backward propagation, and updates the model parameters using the optimizer." <EXPLAINS> """CODE.import paddle

model = paddle.nn.Conv2D(3, 2, 3, bias_attr=True)
optimizer = paddle.optimizer.SGD(learning_rate=0.01, parameters=model.parameters())
scaler = paddle.amp.GradScaler(init_loss_scaling=1024)
data = paddle.rand([10, 3, 32, 32])

with paddle.amp.auto_cast():
    conv = model(data)
    loss = paddle.mean(conv)

scaled = scaler.scale(loss)  # scale the loss
scaled.backward()            # do backward
scaler.minimize(optimizer, scaled)  # update parameters""" .

"DESCRIPTION.This code initializes a Dense layer in a neural network with 3 units and sets the initial weights of the layer to all ones." <EXPLAINS> """CODE.initializer = tf.keras.initializers.Ones()
values = initializer(shape=(2, 2))
initializer = tf.keras.initializers.Ones()
layer = tf.keras.layers.Dense(3, kernel_initializer=initializer)""" .

"DESCRIPTION.This code initializes a FastSpeech2Conformer model with a HifiGan vocoder and accesses the configuration of the model." <EXPLAINS> """CODE.from transformers import (
    FastSpeech2ConformerConfig,
    FastSpeech2ConformerHifiGanConfig,
    FastSpeech2ConformerWithHifiGanConfig,
    FastSpeech2ConformerWithHifiGan,
)

# Initializing FastSpeech2ConformerWithHifiGan sub-modules configurations.
model_config = FastSpeech2ConformerConfig()
vocoder_config = FastSpeech2ConformerHifiGanConfig()

# Initializing a FastSpeech2ConformerWithHifiGan module style configuration
configuration = FastSpeech2ConformerWithHifiGanConfig(model_config.to_dict(), vocoder_config.to_dict())

# Initializing a model (with random weights)
model = FastSpeech2ConformerWithHifiGan(configuration)

# Accessing the model configuration
configuration = model.config
""" .

"DESCRIPTION.This code initializes a FaultInjectEnv environment with specific parameters, including a list of bad indices, worker index, and number of workers." <EXPLAINS> """CODE.from ray.rllib.env.env_context import EnvContext

bad_env = FaultInjectEnv(
    EnvContext({"bad_indices": [1, 2]},
               worker_index=1, num_workers=3))
""" .

"DESCRIPTION.This code initializes a FlaxPegasusForConditionalGeneration model and a PegasusTokenizer for text generation tasks. It then tokenizes input text and generates encoder outputs using the model." <EXPLAINS> """CODE.model = FlaxPegasusForConditionalGeneration.from_pretrained('google/pegasus-large')
tokenizer = PegasusTokenizer.from_pretrained('google/pegasus-large')
inputs = tokenizer(text, max_length=1024, return_tensors='np')
encoder_outputs = model.encode(**inputs)""" .

"DESCRIPTION.This code initializes a FocalNet model with random weights using a specified configuration, and then accesses the configuration of the model." <EXPLAINS> """CODE.from transformers import FocalNetConfig, FocalNetModel

# Initializing a FocalNet microsoft/focalnet-tiny style configuration
configuration = FocalNetConfig()

# Initializing a model (with random weights) from the microsoft/focalnet-tiny style configuration
model = FocalNetModel(configuration)

# Accessing the model configuration
configuration = model.config
""" .

"DESCRIPTION.This code initializes a Google Cloud Storage (GCS) asynchronous subscriber, subscribes to any error messages, then continuously polls for error messages while a boolean variable \"running\" is True. Once it stops polling, it closes the subscriber." <EXPLAINS> """CODE.subscriber = GcsAioSubscriber()
await subscriber.subscribe_error()
while running:
    error_id, error_data = await subscriber.poll_error()
......
await subscriber.close()""" .

"DESCRIPTION.This code initializes a GroupViTVision model with a specific configuration." <EXPLAINS> """CODE.from transformers import GroupViTVisionConfig, GroupViTVisionModel
configuration = GroupViTVisionConfig()
model = GroupViTVisionModel(configuration)
configuration = model.config
""" .

"DESCRIPTION.This code initializes a Lecun uniform initializer using JAX library, and applies it to a random key with shape (2, 3) and data type float32." <EXPLAINS> """CODE.import jax, jax.numpy as jnp
initializer = jax.nn.initializers.lecun_uniform()
initializer(jax.random.PRNGKey(42), (2, 3), jnp.float32)  # doctest: +SKIP""" .

"DESCRIPTION.This code initializes a M-CTC-T model with a configuration and allows access to the model's configuration settings." <EXPLAINS> """CODE.from transformers import MCTCTModel, MCTCTConfig

# Initializing a M-CTC-T mctct-large style configuration
configuration = MCTCTConfig()

# Initializing a model from the mctct-large style configuration
model = MCTCTModel(configuration)

# Accessing the model configuration
configuration = model.config
""" .

"DESCRIPTION.This code initializes a Mean Absolute Error (MAE) loss function using keras, calculates the MAE loss between two sets of values, creates a keras model, and compiles the model using Stochastic Gradient Descent optimizer with MAE loss function." <EXPLAINS> """CODE.mae = keras.losses.MeanAbsoluteError()
loss = mae([0., 0., 1., 1.], [1., 1., 1., 0.])
model = keras.Model(inputs, outputs)
model.compile('sgd', loss=keras.losses.MeanAbsoluteError())
""" .

"DESCRIPTION.This code initializes a Mistral model with a Mistral 7B style configuration and then accesses the configuration of the model." <EXPLAINS> """CODE.from transformers import MistralModel, MistralConfig

# Initializing a Mistral 7B style configuration
configuration = MistralConfig()

# Initializing a model from the Mistral 7B style configuration
model = MistralModel(configuration)

# Accessing the model configuration
configuration = model.config
""" .

"DESCRIPTION.This code initializes a Mixtral model with a configuration and then accesses the model's configuration." <EXPLAINS> """CODE.from transformers import MixtralModel, MixtralConfig

# Initializing a Mixtral 7B style configuration
configuration = MixtralConfig()

# Initializing a model from the Mixtral 7B style configuration
model = MixtralModel(configuration)

# Accessing the model configuration
configuration = model.config
""" .

"DESCRIPTION.This code initializes a OpenLlama model with a specific configuration, then accesses and stores the configuration of the model." <EXPLAINS> """CODE.from transformers import OpenLlamaModel, OpenLlamaConfig

# Initializing a Open-Llama open_llama-7b style configuration
configuration = OpenLlamaConfig()

# Initializing a model from the open_llama-7b style configuration
model = OpenLlamaModel(configuration)

# Accessing the model configuration
configuration = model.config
""" .

"DESCRIPTION.This code initializes a PVT (Vision Transformer) model using a specific configuration (Xrenya/pvt-tiny-224 style) and then accesses the configuration of the model." <EXPLAINS> """CODE.from transformers import PvtModel, PvtConfig

# Initializing a PVT Xrenya/pvt-tiny-224 style configuration
configuration = PvtConfig()

# Initializing a model from the Xrenya/pvt-tiny-224 style configuration
model = PvtModel(configuration)

# Accessing the model configuration
configuration = model.config
""" .

"DESCRIPTION.This code initializes a PaddleCloudRoleMaker as a role, and then initializes a distributed fleet using the role. It then checks if the current process is a server or a worker, and prints a message accordingly. Finally, it creates a barrier for all servers and workers to synchronize and prints a message indicating that all servers and workers have arrived at the barrier point." <EXPLAINS> """CODE.import paddle.distributed.fleet as fleet
from paddle.distributed.fleet import PaddleCloudRoleMaker
import sys
import os

os.environ["PADDLE_WITH_GLOO"] = "2"

def train():
    role = PaddleCloudRoleMaker(
        is_collective=False,
        init_gloo=True,
        path="./tmp_gloo")
    fleet.init(role)

    if fleet.is_server():
        fleet.util.barrier("server")
        print("all server arrive here")
    elif fleet.is_worker():
        fleet.util.barrier("worker")
        print("all server arrive here")
    fleet.util.barrier("all")
    print("all servers and workers arrive here")

if __name__ == "__main__":
    train()""" .

"DESCRIPTION.This code initializes a PoolFormer model using a configuration based on the sail/poolformer_s12 style, and then accesses the configuration of the model." <EXPLAINS> """CODE.from transformers import PoolFormerModel, PoolFormerConfig

# Initializing a PoolFormer sail/poolformer_s12 style configuration
configuration = PoolFormerConfig()

# Initializing a model from the sail/poolformer_s12 style configuration
model = PoolFormerModel(configuration)

# Accessing the model configuration
configuration = model.config
""" .

"DESCRIPTION.This code initializes a PyTorch Lightning Trainer object with a RichProgressBar callback, which will display a progress bar during training." <EXPLAINS> """CODE.from pytorch_lightning import Trainer
from pytorch_lightning.callbacks import RichProgressBar

trainer = Trainer(callbacks=RichProgressBar())
""" .

"DESCRIPTION.This code initializes a Qwen2 style configuration object, then initializes a Qwen2Model object using the configuration, and finally accesses the configuration of the model." <EXPLAINS> """CODE.from transformers import Qwen2Model, Qwen2Config

# Initializing a Qwen2 style configuration
configuration = Qwen2Config()

# Initializing a model from the Qwen2-7B style configuration
model = Qwen2Model(configuration)

# Accessing the model configuration
configuration = model.config
""" .

"DESCRIPTION.This code initializes a REALM model using a pre-trained realm-cc-news configuration and then accesses the configuration of the model." <EXPLAINS> """CODE.from transformers import RealmEmbedder, RealmConfig

# Initializing a REALM realm-cc-news-pretrained-* style configuration
configuration = RealmConfig()

# Initializing a model from the qqaatw/realm-cc-news-pretrained-embedder style configuration
model = RealmEmbedder(configuration)

# Accessing the model configuration
configuration = model.config
""" .

"DESCRIPTION.This code initializes a ResNet-50 backbone with a feature pyramid network (FPN) and loads pre-trained weights. It then generates a random input tensor of shape (1, 3, 64, 64) and passes it through the backbone to obtain a dictionary of output feature maps for different levels of the FPN. The code finally prints the shapes of the output feature maps." <EXPLAINS> """CODE.from torchvision.models.detection.backbone_utils import resnet_fpn_backbone
backbone = resnet_fpn_backbone('resnet50', pretrained=True, trainable_layers=3)
x = torch.rand(1,3,64,64)
output = backbone(x)
print([(k, v.shape) for k, v in output.items()])
""" .

"DESCRIPTION.This code initializes a Rwkv configuration and a Rwkv model with random weights, and then accesses the configuration of the model." <EXPLAINS> """CODE.from transformers import RwkvConfig, RwkvModel

# Initializing a Rwkv configuration
configuration = RwkvConfig()

# Initializing a model (with random weights) from the configuration
model = RwkvModel(configuration)

# Accessing the model configuration
configuration = model.config
""" .

"DESCRIPTION.This code initializes a SeamlessM4T model with a configuration based on the \"facebook/hf-seamless-m4t-medium\" style, and then retrieves the configuration of the model." <EXPLAINS> """CODE.from transformers import SeamlessM4TModel, SeamlessM4TConfig

# Initializing a SeamlessM4T "facebook/hf-seamless-m4t-medium" style configuration
configuration = SeamlessM4TConfig()

# Initializing a model from the "facebook/hf-seamless-m4t-medium" style configuration
model = SeamlessM4TModel(configuration)

# Accessing the model configuration
configuration = model.config
""" .

"DESCRIPTION.This code initializes a SeamlessM4Tv2 model with a specific configuration and then accesses the configuration of the model." <EXPLAINS> """CODE.from transformers import SeamlessM4Tv2Model, SeamlessM4Tv2Config

# Initializing a SeamlessM4Tv2 "" style configuration
configuration = SeamlessM4Tv2Config()

# Initializing a model from the "" style configuration
model = SeamlessM4Tv2Model(configuration)

# Accessing the model configuration
configuration = model.config
""" .

"DESCRIPTION.This code initializes a SidecarEvaluator to evaluate a given model using a specified dataset. It sets up callbacks for evaluating the model's performance and saving the best model based on specified criteria such as loss, and then starts the evaluation process with a maximum number of evaluations set to 1." <EXPLAINS> """CODE.model.compile(loss=..., optimizer=...,
              metrics=['accuracy'])
sidecar_evaluator = keras.utils.SidecarEvaluator(
    model=model,
    data=dataset,
    checkpoint_dir=checkpoint_dir,
    max_evaluations=1,
    callbacks=[
        SidecarEvaluatorModelExport(
            export_filepath=os.path.join(checkpoint_dir,
                                  'best_model_eval',
                                  'best-model-{epoch:04d}'),
            checkpoint_filepath=os.path.join(checkpoint_dir,
            'ckpt-{epoch:04d}'),
            save_freq="eval",
            save_weights_only=True,
            monitor="loss",
            mode="min",
            verbose=1,
        ),
    ],
)
sidecar_evaluator.start()
""" .

"DESCRIPTION.This code initializes a SiglipText model with a configuration based on google/siglip-base-patch16-224 style, then accesses and stores the configuration of the model." <EXPLAINS> """CODE.from transformers import SiglipTextConfig, SiglipTextModel

# Initializing a SiglipTextConfig with google/siglip-base-patch16-224 style configuration
configuration = SiglipTextConfig()

# Initializing a SiglipTextModel (with random weights) from the google/siglip-base-patch16-224 style configuration
model = SiglipTextModel(configuration)

# Accessing the model configuration
configuration = model.config
""" .

"DESCRIPTION.This code initializes a SiglipVision model with a google/siglip-base-patch16-224 style configuration and accesses its configuration." <EXPLAINS> """CODE.from transformers import SiglipVisionConfig, SiglipVisionModel

# Initializing a SiglipVisionConfig with google/siglip-base-patch16-224 style configuration
configuration = SiglipVisionConfig()

# Initializing a SiglipVisionModel (with random weights) from the google/siglip-base-patch16-224 style configuration
model = SiglipVisionModel(configuration)

# Accessing the model configuration
configuration = model.config
""" .

"DESCRIPTION.This code initializes a SweepRun object with the name \"my_run\", sets its state to \"running\", and provides a configuration setting with key \"a\" and value 1." <EXPLAINS> """CODE.run = SweepRun(
  name="my_run",
  state=RunState.running,
  config={"a": {"value": 1}},
)""" .

"DESCRIPTION.This code initializes a TPU cluster resolver, connects to the TPU cluster, initializes the TPU system with a specific topology, creates a device assignment based on the topology, and establishes a TPUStrategy for distributing computations on the TPU. It then defines a step function that splits input data across logical devices, executes a model function on 8 logical devices with the input split in a specific way, and runs the step function using the strategy." <EXPLAINS> """CODE.resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='')
tf.config.experimental_connect_to_cluster(resolver)
topology = tf.tpu.experimental.initialize_tpu_system(resolver)
device_assignment = tf.tpu.experimental.DeviceAssignment.build(
    topology,
    computation_shape=[2, 2, 2],
    num_replicas=1)
strategy = tf.distribute.experimental.TPUStrategy(
    resolver, device_assignment=device_assignment)

iterator = iter(inputs)

@tf.function()
def step_fn(inputs):
  inputs = strategy.experimental_split_to_logical_devices(
    inputs, [1, 2, 4, 1])

  // model() function will be executed on 8 logical devices with `inputs`
  // split 2 * 4  ways.
  output = model(inputs)
  return output

strategy.run(step_fn, args=(next(iterator),))
""" .

"DESCRIPTION.This code initializes a TensorFlow variable 'x' with a constant value specified in the 'value' list. It then prints the values of 'x' for different shapes: fitting shape (2x4), larger shape (3x4), and smaller shape (2x3)." <EXPLAINS> """CODE.import numpy as np
import tensorflow as tf

value = [0, 1, 2, 3, 4, 5, 6, 7]
init = tf.constant_initializer(value)

print('fitting shape:')
tf.reset_default_graph()
with tf.Session():
  x = tf.get_variable('x', shape=[2, 4], initializer=init)
  x.initializer.run()
  print(x.eval())

print('larger shape:')
tf.reset_default_graph()
with tf.Session():
  x = tf.get_variable('x', shape=[3, 4], initializer=init)
  x.initializer.run()
  print(x.eval())

print('smaller shape:')
tf.reset_default_graph()
with tf.Session():
  x = tf.get_variable('x', shape=[2, 3], initializer=init)
""" .

"DESCRIPTION.This code initializes a Vision Transformer Model for Multi-Aspect Extraction (ViT-MAE) using a base style configuration, then creates a model based on that configuration, and finally accesses the configuration of the model." <EXPLAINS> """CODE.from transformers import ViTMAEModel, ViTMAEConfig

# Initializing a ViT MAE vit-mae-base style configuration
configuration = ViTMAEConfig()

# Initializing a model from the vit-mae-base style configuration
model = ViTMAEModel(configuration)

# Accessing the model configuration
configuration = model.config
""" .

"DESCRIPTION.This code initializes a Vision encoder-decoder model using a Vision Transformer (ViT) model as the encoder and a BERT model as the decoder. It then saves the initialized model to a local directory named \"vit-bert\" and reloads the saved model from the same directory." <EXPLAINS> """CODE.from transformers import TFVisionEncoderDecoderModel

model = TFVisionEncoderDecoderModel.from_encoder_decoder_pretrained(
    "google/vit-base-patch16-224-in21k", "bert-base-uncased"
)
model.save_pretrained("./vit-bert")
model = TFVisionEncoderDecoderModel.from_pretrained("./vit-bert")
""" .

"DESCRIPTION.This code initializes a W&B run with a specific project and username, sets the hardware allocation to 1024 megabytes, and logs a loss value of 1 during the run." <EXPLAINS> """CODE.%wandb USERNAME/PROJECT/runs/RUN_ID
---
%%wandb -h 1024
with wandb.init() as run:
    run.log({"loss": 1})""" .

"DESCRIPTION.This code initializes a configuration object and a model object based on the kakaobrain/align-base style configuration for a AlignVisionModel. It then accesses and stores the configuration of the model." <EXPLAINS> """CODE.from transformers import AlignVisionConfig, AlignVisionModel

# Initializing a AlignVisionConfig with kakaobrain/align-base style configuration
configuration = AlignVisionConfig()

# Initializing a AlignVisionModel (with random weights) from the kakaobrain/align-base style configuration
model = AlignVisionModel(configuration)

# Accessing the model configuration
configuration = model.config
""" .

"DESCRIPTION.This code initializes a constant tensor with all elements set to 1." <EXPLAINS> """CODE.import jax, jax.numpy as jnp
from flax.linen.initializers import ones_init
ones_initializer = ones_init()
ones_initializer(jax.random.PRNGKey(42), (3, 2), jnp.float32)""" .

"DESCRIPTION.This code initializes a dataset object for distributed training and sets the HDFS configuration for the dataset." <EXPLAINS> """CODE.import paddle
dataset = paddle.distributed.fleet.DatasetBase()
dataset._set_hdfs_config("my_fs_name", "my_fs_ugi")""",
        """CODE.import paddle.base as base
dataset = base.DatasetFactory().create_dataset()
dataset.set_hdfs_config("my_fs_name", "my_fs_ugi")""" .

"DESCRIPTION.This code initializes a global variable named \"step\" with a shape of [2,3], a value of 1.0, and a data type of 'float32'." <EXPLAINS> """CODE.import paddle.fluid as fluid
if fluid.initializer.force_init_on_cpu():
    step = fluid.layers.create_global_var(
        shape=[2,3], value=1.0, dtype='float32')""" .

"DESCRIPTION.This code initializes a global variable named \"step\" with a shape of [2,3], initial value of 1.0, and data type of float32." <EXPLAINS> """CODE.import paddle.fluid as fluid
with fluid.initializer.init_on_cpu():
    step = fluid.layers.create_global_var(
        shape=[2,3], value=1.0, dtype='float32')""" .

"DESCRIPTION.This code initializes a hash table with a specified number of out-of-vocabulary buckets, looks up values in the table based on input tensor, initializes the hash table, and evaluates and prints the output." <EXPLAINS> """CODE.num_oov_buckets = 3
input_tensor = tf.constant(["emerson", "lake", "palmer", "king", "crimnson"])
table = tf.IdTableWithHashBuckets(
    tf.HashTable(tf.TextFileIdTableInitializer(filename), default_value),
    num_oov_buckets)
out = table.lookup(input_tensor)
table.init.run()
print out.eval()
""" .

"DESCRIPTION.This code initializes a literal autoencoder model with encoder and decoder components." <EXPLAINS> """CODE.LitAutoEncoder()  # doctest: +ELLIPSIS +NORMALIZE_WHITESPACE
LitAutoEncoder(
  (encoder): ...
  (decoder): ...
)""" .

"DESCRIPTION.This code initializes a matrix of shape (2, 3) using Glorot uniform initialization with a PRNG key of 42." <EXPLAINS> """CODE.import jax, jax.numpy as jnp
initializer = jax.nn.initializers.glorot_uniform()
initializer(jax.random.PRNGKey(42), (2, 3), jnp.float32)  # doctest: +SKIP""" .

"DESCRIPTION.This code initializes a model configuration with separate configurations for vision and text components using CLIPVisionConfig and LlamaConfig respectively, and then creates a Llava model for conditional generation based on the combined configuration. Finally, it retrieves the configuration from the created model." <EXPLAINS> """CODE.from transformers import LlavaForConditionalGeneration, LlavaConfig, CLIPVisionConfig, LlamaConfig
vision_config = CLIPVisionConfig()
text_config = LlamaConfig()
configuration = LlavaConfig(vision_config, text_config)
model = LlavaForConditionalGeneration(configuration)
configuration = model.config
""" .

"DESCRIPTION.This code initializes a model with a parameter \"l1\" set to None. It then defines a method called \"prepare_data\" that downloads data and tokenizes it. Lastly, it defines a method called \"setup\" that loads data and sets up a linear layer with input size 28 and number of classes specified by the data." <EXPLAINS> """CODE.class LitModel(...):
    def __init__(self):
        self.l1 = None

    def prepare_data(self):
        download_data()
        tokenize()

        # don't do this
        self.something = else

    def setup(stage):
        data = Load_data(...)
        self.l1 = nn.Linear(28, data.num_classes)""" .

"DESCRIPTION.This code initializes a profiler to track the performance of a TensorFlow program. It then iterates through a specified number of training steps, creating a trace event for each step to monitor and analyze the training process. Finally, the profiler is stopped after all steps are completed." <EXPLAINS> """CODE.tf.profiler.experimental.start('logdir')
for step in range(num_steps):
  # Creates a trace event for each training step with the step number.
  with tf.profiler.experimental.Trace("Train", step_num=step):
    train_fn()
tf.profiler.experimental.stop()
""" .

"DESCRIPTION.This code initializes a replay buffer with a capacity of 5." <EXPLAINS> "CODE.ReplayBuffer(5)" .

"DESCRIPTION.This code initializes a sequential model, adds an embedding layer with input shape (10,), compiles the model using RMSprop optimizer and Mean Squared Error loss function, generates a random input array of shape (32, 10), and finally predicts the output using the model. The code then asserts that the shape of the output array is (32, 10, 64)." <EXPLAINS> """CODE.model = Sequential()
model.add(Embedding(1000, 64, input_length=10))
input_array = np.random.randint(1000, size=(32, 10))
model.compile('rmsprop', 'mse')
output_array = model.predict(input_array)
assert output_array.shape == (32, 10, 64)
""" .

"DESCRIPTION.This code initializes a speech encoder-decoder model using pre-trained Wav2Vec2 and BART models, encodes input data, and obtains encoder outputs." <EXPLAINS> """CODE.from transformers import FlaxSpeechEncoderDecoderModel

model = FlaxSpeechEncoderDecoderModel.from_encoder_decoder_pretrained(
    "facebook/wav2vec2-large-lv60", "facebook/bart-large"
)

inputs = jnp.ones((2, 5000), dtype=jnp.float32)
encoder_outputs = model.encode(inputs)
""" .

"DESCRIPTION.This code initializes a step object with a dataset, loss function, and optimizer, and then initializes it with a distribution and executes the step on the distribution." <EXPLAINS> """CODE.step = step_fn.StandardSingleLossStep(dataset, loss_fn, optimizer)
step.initialize(distribution)
step(distribution)
""" .

"DESCRIPTION.This code initializes a tokenizer and a model for masked language modeling using a pre-trained model. It encodes the input text \"Hello, my dog is cute\" using the tokenizer, passes the encoded input to the model, and calculates the loss and prediction scores based on the masked language modeling task." <EXPLAINS> """CODE.tokenizer = XxxTokenizer.from_pretrained('xxx-base-uncased')
model = XxxForMaskedLM.from_pretrained('xxx-base-uncased')
input_ids = torch.tensor(tokenizer.encode("Hello, my dog is cute")).unsqueeze(0)  # Batch size 1
outputs = model(input_ids, masked_lm_labels=input_ids)
loss, prediction_scores = outputs[:2]""" .

"DESCRIPTION.This code initializes a tokenizer and a token classification model, encodes a given input text using the tokenizer, assigns labels to the tokens, and then passes the input ids and labels to the model to obtain the loss and prediction scores." <EXPLAINS> """CODE.tokenizer = XxxTokenizer.from_pretrained('xxx-base-uncased')
model = XxxForTokenClassification.from_pretrained('xxx-base-uncased')
input_ids = torch.tensor(tokenizer.encode("Hello, my dog is cute")).unsqueeze(0)  # Batch size 1
labels = torch.tensor([1] * input_ids.size(1)).unsqueeze(0)  # Batch size 1
outputs = model(input_ids, labels=labels)
loss, scores = outputs[:2]""" .

"DESCRIPTION.This code initializes a translation dataset builder object from a folder containing manually translated data, retrieves information about the dataset such as splits and number of examples, and creates a dataset object for training by specifying the split and enabling file shuffling." <EXPLAINS> """CODE.
builder = tfds.TranslateFolder(root_dir='path/to/manual_dir/')
print(builder.info)  # Splits, num examples,... are automatically calculated
ds = builder.as_dataset(split='train', shuffle_files=True)
""" .

"DESCRIPTION.This code initializes a variable `kvar` with a 2x3 array of random numbers using TensorFlow's Keras backend. It then initializes another variable `kvar_zeros` with zeros of the same shape as `kvar`, and evaluates `kvar_zeros` to generate a 2x3 array of zeros." <EXPLAINS> """CODE.from tensorflow.keras import backend as K
kvar = K.variable(np.random.random((2,3)))
kvar_zeros = K.zeros_like(kvar)
K.eval(kvar_zeros)
# array([[ 0.,  0.,  0.], [ 0.,  0.,  0.]], dtype=float32)
""" .

"DESCRIPTION.This code initializes a variable `v` using TensorFlow, then loads new values into `v` and prints the updated values." <EXPLAINS> """CODE.v = tf.Variable([1, 2])
init = tf.global_variables_initializer()

with tf.Session() as sess:
    sess.run(init)
    # Usage passing the session explicitly.
    v.load([2, 3], sess)
    print(v.eval(sess)) # prints [2 3]
    # Usage with the default session.  The 'with' block
    # above makes 'sess' the default session.
    v.load([3, 4], sess)
    print(v.eval()) # prints [3 4]
""" .

"DESCRIPTION.This code initializes an EfficientFormer model using a specified configuration and then accesses the configuration of the model." <EXPLAINS> """CODE.from transformers import EfficientFormerConfig, EfficientFormerModel

# Initializing a EfficientFormer efficientformer-l1 style configuration
configuration = EfficientFormerConfig()

# Initializing a EfficientFormerModel (with random weights) from the efficientformer-l3 style configuration
model = EfficientFormerModel(configuration)

# Accessing the model configuration
configuration = model.config
""" .

"DESCRIPTION.This code initializes an IpuStrategy object and sets the options for running on an IPU device, including specifying the number of IPUs to use and enabling mixed precision training with FP16." <EXPLAINS> """CODE.import paddle
import paddle.static as static

paddle.enable_static()

ipu_strategy = static.IpuStrategy()
options = {'num_ipus':1, 'enable_fp16': True}
ipu_strategy.set_options(options)""" .

"DESCRIPTION.This code initializes an accelerator object, then executes a main process in which it prints a message indicating the process index being used." <EXPLAINS> """CODE.from accelerate import Accelerator
accelerator = Accelerator()
with accelerator.main_process_first():
    print(f"This will be printed by process {accelerator.process_index}")
""" .

"DESCRIPTION.This code initializes an iterator for iterating over a range of numbers and a filtered dataset containing only even numbers. It then defines a model function that takes input from the iterator. Finally, it trains the model for a specified number of epochs, iterating over the original dataset first and then over the filtered even dataset in each epoch." <EXPLAINS> """CODE.iterator = Iterator.from_structure(tf.int64, tf.TensorShape([]))

dataset_range = Dataset.range(10)
range_initializer = iterator.make_initializer(dataset_range)

dataset_evens = dataset_range.filter(lambda x: x % 2 == 0)
evens_initializer = iterator.make_initializer(dataset_evens)

# Define a model based on the iterator; in this example, the model_fn
# is expected to take scalar tf.int64 Tensors as input (see
# the definition of 'iterator' above).
prediction, loss = model_fn(iterator.get_next())

# Train for `num_epochs`, where for each epoch, we first iterate over
# dataset_range, and then iterate over dataset_evens.
for _ in range(num_epochs):
  # Initialize the iterator to `dataset_range`
  sess.run(range_initializer)
  while True:
    try:
      pred, loss_val = sess.run([prediction, loss])
    except tf.errors.OutOfRangeError:
      break

  # Initialize the iterator to `dataset_evens`
  sess.run(evens_initializer)
  while True:
    try:
      pred, loss_val = sess.run([prediction, loss])
    except tf.errors.OutOfRangeError:
      break
""" .

"DESCRIPTION.This code initializes an object with the option to compress data and calls the initialization method of the Lexer class with the given options." <EXPLAINS> """CODE.def __init__(self, **options):
    self.compress = options.get('compress', '')
    Lexer.__init__(self, **options)
""" .

"DESCRIPTION.This code initializes an openAI agent with a specified model and API key, then runs the agent to analyze whether the given text (in Spanish) is positive or negative." <EXPLAINS> """CODE.from transformers import OpenAiAgent

agent = OpenAiAgent(model="text-davinci-003", api_key=xxx)
agent.run("Is the following `text` (in Spanish) positive or negative?", text="Â¡Este es un API muy agradable!")
""" .

"DESCRIPTION.This code initializes and accesses a BiT (Big Transfer) model with 50 layers." <EXPLAINS> """CODE.from transformers import BitConfig, BitModel

# Initializing a BiT bit-50 style configuration
configuration = BitConfig()

# Initializing a model (with random weights) from the bit-50 style configuration
model = BitModel(configuration)

# Accessing the model configuration
configuration = model.config
""" .

"DESCRIPTION.This code initializes and accesses the configuration of a MobileViT model using the MobileViTConfig and MobileViTModel classes." <EXPLAINS> """CODE.from transformers import MobileViTConfig, MobileViTModel

# Initializing a mobilevit-small style configuration
configuration = MobileViTConfig()

# Initializing a model from the mobilevit-small style configuration
model = MobileViTModel(configuration)

# Accessing the model configuration
configuration = model.config
""" .

"DESCRIPTION.This code initializes and accesses the configuration of a XGLM model based on the facebook/xglm-564M style." <EXPLAINS> """CODE.from transformers import XGLMModel, XGLMConfig

# Initializing a XGLM facebook/xglm-564M style configuration
configuration = XGLMConfig()

# Initializing a model from the facebook/xglm-564M style configuration
model = XGLMModel(configuration)

# Accessing the model configuration
configuration = model.config
""" .

"DESCRIPTION.This code initializes and configures different models such as Blip2ForConditionalGeneration, Blip2VisionConfig, Blip2QFormerConfig, and OPTConfig based on the Salesforce/blip2-opt-2.7b style configuration. It also allows for the creation of a Blip2Config by combining Blip2VisionConfig, Blip2QFormerConfig, and OPTConfig configurations." <EXPLAINS> """CODE.from transformers import (
...     Blip2VisionConfig,
...     Blip2QFormerConfig,
...     OPTConfig,
...     Blip2Config,
...     Blip2ForConditionalGeneration,
... )

# Initializing a Blip2Config with Salesforce/blip2-opt-2.7b style configuration
configuration = Blip2Config()

# Initializing a Blip2ForConditionalGeneration (with random weights) from the Salesforce/blip2-opt-2.7b style configuration
model = Blip2ForConditionalGeneration(configuration)

# Accessing the model configuration
configuration = model.config

# We can also initialize a Blip2Config from a Blip2VisionConfig, Blip2QFormerConfig and any PretrainedConfig

# Initializing BLIP-2 vision, BLIP-2 Q-Former and language model configurations
vision_config = Blip2VisionConfig()
qformer_config = Blip2QFormerConfig()
text_config = OPTConfig()

config = Blip2Config.from_text_vision_configs(vision_config, qformer_config, text_config)
""" .

"DESCRIPTION.This code initializes and runs a hyperparameter sweep using Weights & Biases (wandb), where the sweep is defined to search for the best hyperparameters based on the metric 'accuracy' using a grid search method for the parameter 'a' with values [1, 2, 3, 4]. The sweep runs the 'my_train_func' function with the parameters specified in the sweep configuration." <EXPLAINS> """CODE.# this line initializes the sweep
sweep_id = wandb.sweep({'name': 'my-awesome-sweep',
                        'metric': 'accuracy',
                        'method': 'grid',
                        'parameters': {'a': {'values': [1, 2, 3, 4]}}})

# this line actually runs it -- parameters are available to
# my_train_func via wandb.config
wandb.agent(sweep_id, function=my_train_func)
""" .

"DESCRIPTION.This code initializes configurations for different sub-modules of a Bark model (semantic, coarse acoustics, fine acoustics, codec) and then uses them to create a overall Bark model configuration. This overall configuration is then used to initialize a Bark model with random weights, and finally the configuration of the model is accessed." <EXPLAINS> """CODE.from transformers import (
...     BarkSemanticConfig,
...     BarkCoarseConfig,
...     BarkFineConfig,
...     BarkModel,
...     BarkConfig,
...     AutoConfig,
... )

# Initializing Bark sub-modules configurations.
semantic_config = BarkSemanticConfig()
coarse_acoustics_config = BarkCoarseConfig()
fine_acoustics_config = BarkFineConfig()
codec_config = AutoConfig.from_pretrained("facebook/encodec_24khz")


# Initializing a Bark module style configuration
configuration = BarkConfig.from_sub_model_configs(
...     semantic_config, coarse_acoustics_config, fine_acoustics_config, codec_config
... )

# Initializing a model (with random weights)
model = BarkModel(configuration)

# Accessing the model configuration
configuration = model.config
""" .

"DESCRIPTION.This code initializes different tensors filled with zeros using TensorFlow's Keras backend, with specific shapes and data types." <EXPLAINS> """CODE.kvar = tf.keras.backend.zeros((3,4))
A = tf.constant([1,2,3])
kvar2 = tf.keras.backend.zeros(A.shape) # [0., 0., 0.]
kvar3 = tf.keras.backend.zeros(A.shape,dtype=tf.int32)
kvar4 = tf.keras.backend.zeros([2,3])""" .

"DESCRIPTION.This code initializes random weights for a dense neural network layer with 3 units using a random uniform distribution with values between 0 and 1." <EXPLAINS> """CODE.initializer = tf.keras.initializers.RandomUniform(minval=0., maxval=1.)
values = initializer(shape=(2, 2))
initializer = tf.keras.initializers.RandomUniform(minval=0., maxval=1.)
layer = tf.keras.layers.Dense(3, kernel_initializer=initializer)""" .

"DESCRIPTION.This code initializes two torch tensors of size 1000x1000 on the GPU using CUDA and then releases the memory occupied by these tensors." <EXPLAINS> """CODE.import torch
from accelerate.utils import release_memory

a = torch.ones(1000, 1000).cuda()
b = torch.ones(1000, 1000).cuda()
a, b = release_memory(a, b)
""" .

"DESCRIPTION.This code instantiates a Hugging Face Hub API object, creates a future object to run the `whoami` method asynchronously, checks if the future task is completed, and retrieves the result of the asynchronous task." <EXPLAINS> """CODE.from huggingface_hub import HfApi
api = HfApi()
future = api.run_as_future(api.whoami) # instant
future.done()
False
future.result() # wait until complete and return result
(...)
future.done()
True
""" .

"DESCRIPTION.This code involves sampling data using ev1, computing gradients using ev2, and applying those gradients back to ev1." <EXPLAINS> """CODE.samples = ev1.sample()
grads = ev2.compute_gradients(samples)
ev1.apply_gradients(grads)""" .

"DESCRIPTION.This code is a Python script that demonstrates how to implement distributed training using PaddlePaddle framework. The script sets up a PaddleCloudRoleMaker for distributed training, initializes the fleet, and then checks if the current process is a server or worker. Depending on the process type, the script gathers input from the server or worker using the fleet.util.all_gather function and prints the output. Finally, it gathers input from all processes and prints the combined output." <EXPLAINS> """CODE.import paddle.distributed.fleet as fleet
from paddle.distributed.fleet import PaddleCloudRoleMaker
import sys
import os

os.environ["PADDLE_WITH_GLOO"] = "2"

def train():
    role = PaddleCloudRoleMaker(
        is_collective=False,
        init_gloo=True,
        path="./tmp_gloo")
    fleet.init(role)

    if fleet.is_server():
        input = fleet.server_index()
        output = fleet.util.all_gather(input, "server")
        print(output)
        # output = [0, 1]
    elif fleet.is_worker():
        input = fleet.worker_index()
        output = fleet.util.all_gather(input, "worker")
        # output = [0, 1]
        print(output)
    output = fleet.util.all_gather(input, "all")
    print(output)
    # output = [0, 1, 0, 1]

if __name__ == "__main__":
    train()""" .

"DESCRIPTION.This code is calling a function called some_experiment_writer_function from an experiment writer object within an experiment object created by the logger object." <EXPLAINS> "CODE.self.logger.experiment.some_experiment_writer_function()" .

"DESCRIPTION.This code is calling a function named \"some_test_tube_function()\" from the \"experiment\" object within the \"logger\" object." <EXPLAINS> "CODE.self.logger.experiment.some_test_tube_function()" .

"DESCRIPTION.This code is checking if the mathematical calculations of addition approximates the expected value within a very small margin of error." <EXPLAINS> """CODE.abs((0.1 + 0.2) - 0.3) < 1e-6
True
0.1 + 0.2 == approx(0.3)
True
(0.1 + 0.2, 0.2 + 0.4) == approx((0.3, 0.6))
True
{'a': 0.1 + 0.2, 'b': 0.2 + 0.4} == approx({'a': 0.3, 'b': 0.6})
True
import numpy as np
np.array([0.1, 0.2]) + np.array([0.2, 0.4]) == approx(np.array([0.3, 0.6]))
True""" .

"DESCRIPTION.This code is creating a deep learning model using the Xception architecture with 1000 classes for image classification. It then distributes the model training on 8 GPUs, each processing 32 samples in a batch size of 256 for 20 epochs using dummy data." <EXPLAINS> """CODE.    import tensorflow as tf
    from keras.applications import Xception
    from keras.utils import multi_gpu_model
    import numpy as np

    num_samples = 1000
    height = 224
    width = 224
    num_classes = 1000

    # Instantiate the base model
    # (here, we do it on CPU, which is optional).
    with tf.device('/cpu:0'):
        model = Xception(weights=None,
                         input_shape=(height, width, 3),
                         classes=num_classes)

    # Replicates the model on 8 GPUs.
    # This assumes that your machine has 8 available GPUs.
    parallel_model = multi_gpu_model(model, gpus=8)
    parallel_model.compile(loss='categorical_crossentropy',
                           optimizer='rmsprop')

    # Generate dummy data.
    x = np.random.random((num_samples, height, width, 3))
    y = np.random.random((num_samples, num_classes))

    # This `fit` call will be distributed on 8 GPUs.
    # Since the batch size is 256, each GPU will process 32 samples.
    parallel_model.fit(x, y, epochs=20, batch_size=256)
""" .

"DESCRIPTION.This code is creating a prefix for an operation name, specifically for the addition operation, resulting in the string 'AddBackward'." <EXPLAINS> """CODE._create_op_prefix('add')
'AddBackward'""" .

"DESCRIPTION.This code is creating and applying a series of commit operations to a model repository using the Hugging Face Hub API. It plans multiple commits involving adding and deleting files, then creates and applies these commits to the specified repository." <EXPLAINS> """CODE.from huggingface_hub import HfApi, plan_multi_commits
addition_commits, deletion_commits = plan_multi_commits(
...     operations=[
...          CommitOperationAdd(...),
...          CommitOperationAdd(...),
...          CommitOperationDelete(...),
...          CommitOperationDelete(...),
...          CommitOperationAdd(...),
...     ],
... )
HfApi().create_commits_on_pr(
...     repo_id="my-cool-model",
...     addition_commits=addition_commits,
...     deletion_commits=deletion_commits,
...     (...)
...     verbose=True,
... )
""" .

"DESCRIPTION.This code is defining a function called test_partial that takes a monkeypatch object as a parameter. Within this function, it sets an attribute of the functools module called \"partial\" to the value 3." <EXPLAINS> """CODE.import functools
def test_partial(monkeypatch):
    with monkeypatch.context() as m:
        m.setattr(functools, "partial", 3)""" .

"DESCRIPTION.This code is for evaluating the translation model on a specific dataset by computing the performance metrics using the given model or pipeline. Specifically, it loads the WMT19 dataset for translation from French to German, maps the data to the required format, and evaluates the performance using the specified model." <EXPLAINS> """CODE.from evaluate import evaluator
from datasets import load_dataset
task_evaluator = evaluator("translation")
data = load_dataset("wmt19", "fr-de", split="validation[:40]")
data = data.map(lambda x: {"text": x["translation"]["de"], "label": x["translation"]["fr"]})
results = task_evaluator.compute(
    model_or_pipeline="Helsinki-NLP/opus-mt-de-fr",
    data=data,
)""" .

"DESCRIPTION.This code is implementing a simple distributed training script using PyTorch's distributed package. It creates two workers and initializes communication between them using the NCCL backend. Each worker then creates a linear model with distributed data parallelism, where Rank 0 receives 10 input tensors and Rank 1 receives 11 input tensors. The script then runs a training loop where each input tensor is fed into the model, the loss is calculated, and gradients are calculated and updated. Lastly, it ensures synchronization between the two workers using the torch.cuda.synchronize() function." <EXPLAINS> """CODE.import torch
import torch.distributed as dist
import os
import torch.multiprocessing as mp
import torch.nn as nn

# On each spawned worker
def worker(rank):
    dist.init_process_group("nccl", rank=rank, world_size=2)
    torch.cuda.set_device(rank)
    model = nn.Linear(1, 1, bias=False).to(rank)
    model = torch.nn.parallel.DistributedDataParallel(
        model, device_ids=[rank], output_device=rank
    )
    # Rank 1 gets one more input than rank 0.
    inputs = [torch.tensor([1]).float() for _ in range(10 + rank)]
    with model.join():
        for _ in range(5):
            for inp in inputs:
                loss = model(inp).sum()
                loss.backward()

# Without the join() API, the below synchronization will hang
# blocking for rank 1's allreduce to complete.
torch.cuda.synchronize(device=rank)""" .

"DESCRIPTION.This code is implementing a training loop using the PaddlePaddle deep learning framework. It creates a linear neural network model, defines an Adam optimizer with a learning rate scheduler, and trains the model for 10 epochs. During training, it calculates the loss, minimizes it using the Adam optimizer, and adjusts the learning rate based on the average loss. Finally, it prints out the current average loss and learning rate at each step." <EXPLAINS> """CODE.import paddle.fluid as fluid
import numpy as np

with fluid.dygraph.guard():
    x = np.random.uniform(-1, 1, [10, 10]).astype("float32")
    linear = fluid.dygraph.Linear(10, 10)
    input = fluid.dygraph.to_variable(x)

    reduce_lr = fluid.dygraph.ReduceLROnPlateau(
                            learning_rate = 1.0,
                            decay_rate = 0.5,
                            patience = 5,
                            verbose = True,
                            cooldown = 3)
    adam = fluid.optimizer.Adam(
        learning_rate = reduce_lr,
        parameter_list = linear.parameters())

    for epoch in range(10):
        total_loss = 0
        for bath_id in range(5):
            out = linear(input)
            loss = fluid.layers.reduce_mean(out)
            total_loss += loss
            adam.minimize(loss)

        avg_loss = total_loss/5

        # adjust learning rate according to avg_loss
        reduce_lr.step(avg_loss)
        lr = adam.current_step_lr()
        print("current avg_loss is %s, current lr is %s" % (avg_loss.numpy()[0], lr))""" .

"DESCRIPTION.This code is manually using a wrapper to ensure compatibility with a new step API on unregistered environments." <EXPLAINS> "CODE.StepAPICompatibility(CustomEnv(), new_step_api=True) # manually using wrapper on unregistered envs" .

"DESCRIPTION.This code is parsing a date string in the format \"yyyy/mm/dd\" and extracting the year, month, and day components from the input string." <EXPLAINS> """CODE.integer = Word(nums)
date_str = integer("year") + '/' + integer("month") + '/' + integer("day")
date_str.parseString("1999/12/31")

ParserElement.inlineLiteralsUsing(Suppress)
date_str = integer("year") + '/' + integer("month") + '/' + integer("day")
date_str.parseString("1999/12/31")""" .

"DESCRIPTION.This code is responsible for managing asynchronous checkpoints during the training process. It deserializes the training state from a global async checkpoint manager, performs training steps at regular intervals, serializes the updated training state, and checks for errors. Finally, it serializes the final training state and waits for the process to finish." <EXPLAINS> """CODE.manager = GlobalAsyncCheckpointManager()

train_state = manager.deserialize(...)

while ...:
  if step % num_steps_between_checkpoints == 0:
    manager.serialize(train_state)
    train_state = train_step(train_state, input)
    manager.check_for_errors()

manager.serialize(train_state)
manager.wait_until_finished()""" .

"DESCRIPTION.This code is used to enumerate the photometric values for TIFF files." <EXPLAINS> """CODE.enumarg(TIFF.PHOTOMETRIC, 2)
enumarg(TIFF.PHOTOMETRIC, 'RGB')""" .

"DESCRIPTION.This code is used to retrieve the image data format setting used in TensorFlow Keras backend." <EXPLAINS> "CODE.tf.keras.backend.image_data_format()" .

"DESCRIPTION.This code is used to train a machine learning model on a batch of samples." <EXPLAINS> "CODE.ev.learn_on_batch(samples)" .

"DESCRIPTION.This code is using a pre-trained Blip model for image captioning from Salesforce to generate captions for an input image. The code loads the model and processor, downloads an image from a URL, processes the image using the AutoProcessor, and extracts image features using the Blip model." <EXPLAINS> """CODE.from PIL import Image
import requests
from transformers import AutoProcessor, BlipModel

model = BlipModel.from_pretrained("Salesforce/blip-image-captioning-base")
processor = AutoProcessor.from_pretrained("Salesforce/blip-image-captioning-base")

url = "http://images.cocodataset.org/val2017/000000039769.jpg"
image = Image.open(requests.get(url, stream=True).raw)

inputs = processor(images=image, return_tensors="pt")

image_features = model.get_image_features(**inputs)
""" .

"DESCRIPTION.This code is validating a given list of items against a schema that specifies the maximum number of items allowed in the list." <EXPLAINS> """CODE.validate([2, 3, 4], {"maxItems" : 2})
""" .

"DESCRIPTION.This code iterates over the demuxers in a dictionary returned by the function get_demuxers() and prints each key-value pair in the format \"key: value\", where the key is the demuxer name and the value is a description of the demuxer format." <EXPLAINS> """CODE.for k, v in get_demuxers().items():
    print(f"{k}: {v}")
aa: Audible AA format files
aac: raw ADTS AAC (Advanced Audio Coding)
aax: CRI AAX
ac3: raw AC-3""" .

"DESCRIPTION.This code iterates over the key-value pairs of the output devices obtained from the function get_output_devices() and prints each key-value pair in the format \"key: value\"." <EXPLAINS> """CODE.for k, v in get_output_devices().items():
    print(f"{k}: {v}")""" .

"DESCRIPTION.This code iterates through a list of requirements parsed from a text file and adjusts each requirement with the string 'none'." <EXPLAINS> """CODE.[r.adjust('none') for r in _parse_requirements(txt)]
[r.adjust('none') for r in _parse_requirements(txt)]""" .

"DESCRIPTION.This code iterates through each optimizer in a list of optimizers, performs a step for each optimizer, and then calls the \"on_before_zero_grad\" method of a model before zeroing out the gradients for that optimizer." <EXPLAINS> """CODE.for optimizer in optimizers:
    optimizer.step()
    model.on_before_zero_grad(optimizer) # < ---- called here
    optimizer.zero_grad""" .

"DESCRIPTION.This code lays out a dashboard with three columns, each displaying a header followed by an image. The first column shows a cat image, the second column shows a dog image, and the third column shows an owl image. Below these columns, there are two new columns created, with the first column displaying a line chart generated from random data and the second column showing the same data in a written format." <EXPLAINS> """CODE.col1, col2, col3 = st.beta_columns(3)

with col1:
    st.header("A cat")
    st.image("https://static.streamlit.io/examples/cat.jpg", use_column_width=True)

with col2:
    st.header("A dog")
    st.image("https://static.streamlit.io/examples/dog.jpg", use_column_width=True)

with col3:
    st.header("An owl")
    st.image("https://static.streamlit.io/examples/owl.jpg", use_column_width=True)

col1, col2 = st.beta_columns([3, 1])
data = np.random.randn(10, 1)

col1.subheader("A wide column with a chart")
col1.line_chart(data)

col2.subheader("A narrow column with the data")
col2.write(data)
""" .

"DESCRIPTION.This code listens for specific events on a Windows system using the Windows Management Instrumentation (WMI) module in Python. It includes monitoring for the creation of new processes, specifically looking for the creation of the \"calc.exe\" process. Additionally, it monitors the Windows event log for error and warning messages, printing out any error or warning logs that are detected." <EXPLAINS> """CODE.c = wmi.WMI ()
raw_wql = "SELECT * FROM __InstanceCreationEvent WITHIN 2 WHERE TargetInstance ISA 'Win32_Process'"
watcher = c.watch_for (raw_wql=raw_wql)
while 1:
  process_created = watcher ()
  print process_created.Name

watcher = c.watch_for (
  notification_type="Creation",
  wmi_class="Win32_Process",
  delay_secs=2,
  Name='calc.exe'
)
calc_created = watcher ()

import pythoncom
import wmi
c = wmi.WMI (privileges=["Security"])
watcher1 = c.watch_for (
  notification_type="Creation",
  wmi_class="Win32_NTLogEvent",
  Type="error"
)
watcher2 = c.watch_for (
  notification_type="Creation",
  wmi_class="Win32_NTLogEvent",
  Type="warning"
)

while 1:
  try:
    error_log = watcher1 (500)
  except wmi.x_wmi_timed_out:
    pythoncom.PumpWaitingMessages ()
  else:
    print error_log

  try:
    warning_log = watcher2 (500)
  except wmi.x_wmi_timed_out:
    pythoncom.PumpWaitingMessages ()
  else:
    print warning_log""" .

"DESCRIPTION.This code loads a dataset called \"rotten_tomatoes\" with a validation split and selects the column 'text' from the dataset, returning a new dataset containing only the 'text' column with 1066 rows." <EXPLAINS> """CODE.from datasets import load_dataset
ds = load_dataset("rotten_tomatoes", split="validation")
ds.select_columns(['text'])
Dataset({
    features: ['text'],
    num_rows: 1066
})
""" .

"DESCRIPTION.This code loads a pre-trained Blip2 model and processor from Salesforce, processes an image fetched from a URL using the processor, and extracts Q-former features from the image using the loaded model." <EXPLAINS> """CODE.import torch
from PIL import Image
import requests
from transformers import Blip2Processor, Blip2Model

device = "cuda" if torch.cuda.is_available() else "cpu"

processor = Blip2Processor.from_pretrained("Salesforce/blip2-opt-2.7b")
model = Blip2Model.from_pretrained("Salesforce/blip2-opt-2.7b", torch_dtype=torch.float16)
model.to(device)  # doctest: +IGNORE_RESULT

url = "http://images.cocodataset.org/val2017/000000039769.jpg"
image = Image.open(requests.get(url, stream=True).raw)
inputs = processor(images=image, return_tensors="pt").to(device, torch.float16)
qformer_outputs = model.get_qformer_features(**inputs)
""" .

"DESCRIPTION.This code loads a pre-trained FlaxBlenderbot model and tokenizer from the Facebook Blenderbot-400M-distill checkpoint. It then encodes the input text \"My friends are cool but they eat too many carbs\", generates decoder input IDs, and uses the model to decode and generate logits for the given input." <EXPLAINS> """CODE.model = FlaxBlenderbotForConditionalGeneration.from_pretrained('facebook/blenderbot-400M-distill')
tokenizer = BlenderbotTokenizer.from_pretrained('facebook/blenderbot-400M-distill')
text = "My friends are cool but they eat too many carbs."
inputs = tokenizer(text, max_length=1024, return_tensors='jax')
encoder_outputs = model.encode(**inputs)
decoder_start_token_id = model.config.decoder_start_token_id
decoder_input_ids = jnp.ones((inputs.input_ids.shape[0], 1), dtype="i4") * decoder_start_token_id
outputs = model.decode(decoder_input_ids, encoder_outputs)
logits = outputs.logits""" .

"DESCRIPTION.This code loads a pre-trained Wav2Vec2 model from Hugging Face, imports it, loads an audio waveform from a file, and then uses the model to generate logits for the input waveform." <EXPLAINS> """CODE.from torchaudio.models.wav2vec2.utils import import_huggingface_model

original = Wav2Vec2ForCTC.from_pretrained("facebook/wav2vec2-base-960h")
model = import_huggingface_model(original)

waveforms, _ = torchaudio.load("audio.wav")
logits, _ = model(waveforms)
""" .

"DESCRIPTION.This code loads a pre-trained model and processor for Wav2Vec2 that uses CTC (Connectionist Temporal Classification) for speech-to-text conversion. It then sets the target language to Spanish and loads a pre-trained adapter for Spanish language." <EXPLAINS> """CODE.from transformers import Wav2Vec2ForCTC, AutoProcessor
ckpt = "facebook/mms-1b-all"
processor = AutoProcessor.from_pretrained(ckpt)
model = Wav2Vec2ForCTC.from_pretrained(ckpt, target_lang="eng")
processor.tokenizer.set_target_lang("spa")
model.load_adapter("spa")
""" .

"DESCRIPTION.This code loads an audio file ('file.mp3') and then extracts frames of 2048 samples from the audio data with a hop length of 64." <EXPLAINS> """CODE.# Load a file
y, sr = librosa.load('file.mp3')
# Extract 2048-sample frames from y with a hop of 64
y_frames = librosa.util.frame(y, frame_length=2048, hop_length=64)
""" .

"DESCRIPTION.This code loads an image from a specified path, converts the image to an array, creates a batch of the image, and then uses a model to make predictions on the image." <EXPLAINS> """CODE.image = tf.keras.preprocessing.image.load_img(image_path)
input_arr = tf.keras.preprocessing.image.img_to_array(image)
input_arr = np.array([input_arr])  # Convert single image to a batch.
predictions = model.predict(input_arr)
""" .

"DESCRIPTION.This code loads the \"rotten_tomatoes\" dataset and selects a contiguous subset of 4 rows starting from row 0." <EXPLAINS> """CODE.from datasets import load_dataset
ds = load_dataset("rotten_tomatoes", split="validation")
ds._select_contiguous(0, 4)
Dataset({
    features: ['text', 'label'],
    num_rows: 4
})
""" .

"DESCRIPTION.This code loads the \"rotten_tomatoes\" dataset for the validation split in streaming mode and retrieves the column names which are 'text' and 'label'." <EXPLAINS> """CODE.from datasets import load_dataset
ds = load_dataset("rotten_tomatoes", split="validation", streaming=True)
ds.column_names
['text', 'label']
""" .

"DESCRIPTION.This code logs a histogram plot of the 'height' column in the 'table' dataset using the Weights & Biases library." <EXPLAINS> "CODE.wandb.log({'histogram-plot1': wandb.plot.histogram(table, \"height\")})" .

"DESCRIPTION.This code logs the training loss and additional metrics with customizable options such as logging frequency, logging to console or file, and reduction function." <EXPLAINS> """CODE.result.log('train_loss', loss)
result.log(
    name,
    value,
    on_step=True,
    on_epoch=False,
    logger=True,
    prog_bar=False,
    reduce_fx=torch.mean,
    enable_graph=False
)""" .

"DESCRIPTION.This code logs the training loss and specified metrics during the training process. The metrics can be logged with various settings such as on_step and on_epoch flags, logging to the logger, displaying in the progress bar, applying a reduction function, and enabling graph visualization." <EXPLAINS> """CODE.result.log('train_loss', loss)
result.log(
    name,
    value,
    on_step=True,
    on_epoch=False,
    logger=True,
    prog_bar=False,
    reduce_fx=torch.mean,
    enable_graph=False
)""" .

"DESCRIPTION.This code maps functions f and h over input arrays x and x, y respectively, using specified axes and axis resources." <EXPLAINS> """CODE.xmap(f, in_axes=['i'], out_axes=[i], axis_resources={'i': SerialLoop(20)})(x)
xmap(h, in_axes=(['i'], ['j']), out_axes=['i', 'j'],
     axis_resources={'i': SerialLoop(20), 'j': SerialLoop(20)})(x, y)
""" .

"DESCRIPTION.This code matches events in two arrays based on the closest values and returns pairs of values from the arrays." <EXPLAINS> """CODE.s_from = np.arange(0, 100, 7)
s_to = np.arange(0, 100, 10)
idx = librosa.util.match_events(s_from, s_to)
zip(s_from, s_to[idx])
""" .

"DESCRIPTION.This code mocks a data access object to run a query with a specific condition (IN (1, 2, 4, 5)) and expects a mocked result to be returned." <EXPLAINS> "CODE.mock_dao.RunQuery(StrContains('IN (1, 2, 4, 5)')).AndReturn(mock_result)" .

"DESCRIPTION.This code multiplies the elements in the specified slice of array x by 6." <EXPLAINS> """CODE.x[idx] *= y
x = jax.numpy.ones((5, 6))
jax.ops.index_mul(x, jnp.index_exp[2:4, 3:], 6.)""" .

"DESCRIPTION.This code normalizes each split of a tensor before splitting it across logical devices using a given strategy." <EXPLAINS> """CODE.def normalize_each_split(split):
  return split - tf.math.reduce_mean(split)

def tpu_computation(x):
  x_split = strategy.experimental_split_to_logical_devices(
              x, [num_cores_per_replica, 1])
  y = experimental_map_outside_compilation(
        normalize_each_split, x_split)
  y_split = strategy.experimental_split_to_logical_devices(
              x, [num_cores_per_replica, 1])
  return y_split
""" .

"DESCRIPTION.This code normalizes keyword aggregation by converting the input dictionary into a list of tuples." <EXPLAINS> """CODE._normalize_keyword_aggregation({'output': ('input', 'sum')})
({'input': ['sum']}, ('output',), [('input', 'sum')])""",
        """CODE.normalize_keyword_aggregation({"output": ("input", "sum")})
(defaultdict(<class 'list'>, {'input': ['sum']}), ('output',), array([0]))""" .

"DESCRIPTION.This code opens a file named \"filename\" and performs some actions inside the context manager." <EXPLAINS> """CODE.with open_file(filename) as f:
    ...
""" .

"DESCRIPTION.This code opens a file using the 'operation' context manager, reads the contents of the file, and assigns it to the variable 'content'." <EXPLAINS> """CODE.with operation.as_file() as file:
    content = file.read()
""" .

"DESCRIPTION.This code opens an image file located at \"/path/to/image.png\" and reads the image data. It then opens a new image file at \"/path/to/output.jpg\" and writes the same image data to that file." <EXPLAINS> """CODE.import imageio.v3 as iio
with iio.imopen("/path/to/image.png", "r") as file:
    im = file.read()

with iio.imopen("/path/to/output.jpg", "w") as file:
    file.write(im)
""" .

"DESCRIPTION.This code optimizes a linear model using stochastic gradient descent and mean squared error loss by calculating the gradients of the weights and applying the gradients to update the model weights." <EXPLAINS> """CODE.model = LinearModel()
model.compile(optimizer='sgd', loss='mse')
model.fit(x, y, epochs=epochs)


model = LinearModel()
opt = tf.keras.optimizers.Adam()
loss_fn = tf.keras.losses.MeanSquaredError()
with tf.GradientTape() as tape:
  output = model(sparse_input)
  loss = tf.reduce_mean(loss_fn(target, output))
grads = tape.gradient(loss, model.weights)
opt.apply_gradients(zip(grads, model.weights))
""" .

"DESCRIPTION.This code parses a report containing lines with an issue number, severity, description, and number of days open. Each line is separated by '|'. It extracts this information from the report and prints out the parsed data for each line." <EXPLAINS> """CODE.integer = Word(nums)
SEP = Suppress('|')
string_data = SkipTo(SEP, ignore=quotedString)
string_data.setParseAction(tokenMap(str.strip))
ticket_expr = (integer("issue_num") + SEP
              + string_data("sev") + SEP
              + string_data("desc") + SEP
              + integer("days_open"))

for tkt in ticket_expr.searchString(report):
    print tkt.dump()""" .

"DESCRIPTION.This code parses a string containing metadata about pixel size and returns a dictionary with the pixel size values in meters." <EXPLAINS> """CODE.pilatus_description_metadata('# Pixel_size 172e-6 m x 172e-6 m')
{'Pixel_size': (0.000172, 0.000172)}""" .

"DESCRIPTION.This code parses a string input containing multiple numbers, extracts the individual numbers as strings, and returns them as a list. Additionally, it adds the sum of the parsed integers to the end of the list using a parse action." <EXPLAINS> """CODE.print(OneOrMore(Word(nums)).parseString("0 123 321")) # -> ['0', '123', '321']

# use a parse action to compute the sum of the parsed integers, and add it to the end
def append_sum(tokens):
    tokens.append(sum(map(int, tokens)))
print(OneOrMore(Word(nums)).addParseAction(append_sum).parseString("0 123 321")) # -> ['0', '123', '321', 444]""" .

"DESCRIPTION.This code parses an HTML table cell content from a given text and prints the stripped body content between the TD tags." <EXPLAINS> """CODE.td,td_end = makeHTMLTags("TD")
table_text = td + SkipTo(td_end).setParseAction(pyparsing_common.stripHTMLTags)("body") + td_end

print(table_text.parseString(text).body) # -> 'More info at the pyparsing wiki page'""" .

"DESCRIPTION.This code parses and extracts all occurrences of the literal string \"CMD\" (case-insensitive) from the input string. It returns a list containing all the extracted strings." <EXPLAINS> "CODE.OneOrMore(CaselessLiteral(\"CMD\")).parseString(\"cmd CMD Cmd10\") # -> ['CMD', 'CMD', 'CMD']" .

"DESCRIPTION.This code parses the input string to find one or more occurrences of the case-insensitive keyword \"CMD\". The result is a list containing all the matched occurrences." <EXPLAINS> "CODE.OneOrMore(CaselessKeyword(\"CMD\")).parseString(\"cmd CMD Cmd10\") # -> ['CMD', 'CMD']" .

"DESCRIPTION.This code performs 3D cropping on the input tensor x, removing 2 units from the start and 4 units from the end of the first dimension, 2 units from the start and 2 units from the end of the second dimension, and 2 units from the start and 1 unit from the end of the third dimension." <EXPLAINS> """CODE.input_shape = (2, 28, 28, 10, 3)
x = np.arange(np.prod(input_shape)).reshape(input_shape)
y = tf.keras.layers.Cropping3D(cropping=(2, 4, 2))(x)
print(y.shape)
(2, 24, 20, 6, 3)""" .

"DESCRIPTION.This code performs GroupKFold cross-validation on a dataset X with corresponding labels y, while taking into account group information provided in the 'groups' array. It splits the data into training and testing sets based on the groups specified, and then trains and tests a model on each split." <EXPLAINS> """CODE.from sklearn.model_selection import GroupKFold
import numpy as np

X = np.array([[1, 2], [3, 4], [5, 6], [7, 8])
y = np.array([1, 2, 3, 4])
groups = np.array([0, 0, 2, 2])
group_kfold = GroupKFold(n_splits=2)

for train_index, test_index in group_kfold.split(X, y, groups):
    X_train, X_test = X[train_index], X[test_index]
    y_train, y_test = y[train_index], y[test_index]

TRAIN: [0 1] TEST: [2 3]
[[1 2]
 [3 4]] [[5 6]
 [7 8]] [1 2] [3 4]
TRAIN: [2 3] TEST: [0 1]
[[5 6]
 [7 8]] [[1 2]
 [3 4]] [3 4] [1 2]
""" .

"DESCRIPTION.This code performs Non-Maximum Suppression (NMS) on bounding boxes and their corresponding scores to filter out redundant detections based on certain thresholds and parameters." <EXPLAINS> """CODE.import paddle.fluid as fluid
boxes = fluid.data(name='bboxes', shape=[None, 81, 8], dtype='float32')
scores = fluid.data(name='scores', shape=[None, 1, 81], dtype='float32')
out = fluid.layers.locality_aware_nms(bboxes=boxes, scores=scores, score_threshold=0.5, nms_top_k=400, nms_threshold=0.3, keep_top_k=200, normalized=False)""" .

"DESCRIPTION.This code performs a cross-validation using a `StratifiedGroupKFold` method from the scikit-learn library. It splits the data into training and testing sets based on the stratified groups provided, ensuring that each group has a proportional representation in both training and testing sets. The code then prints out the groups and corresponding target values for each training and testing fold." <EXPLAINS> """CODE.from sklearn.model_selection import StratifiedGroupKFold
import numpy as np

X = np.ones((17, 2))
y = np.array([0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0])
groups = np.array([1, 1, 2, 2, 3, 3, 3, 4, 5, 5, 5, 5, 6, 6, 7, 8, 8])

cv = StratifiedGroupKFold(n_splits=3)
for train_idxs, test_idxs in cv.split(X, y, groups):
    print("TRAIN:", groups[train_idxs])
    print("      ", y[train_idxs])
    print(" TEST:", groups[test_idxs])
    print("      ", y[test_idxs])

plaintext
TRAIN: [1 1 2 2 4 5 5 5 5 8 8]
       [0 0 1 1 1 0 0 0 0 0 0]
 TEST: [3 3 3 6 6 7]
       [1 1 1 0 0 0]
TRAIN: [3 3 3 4 5 5 5 5 6 6 7]
       [1 1 1 1 0 0 0 0 0 0 0]
 TEST: [1 1 2 2 8 8]
       [0 0 1 1 0 0]
TRAIN: [1 1 2 2 3 3 3 6 6 7 8 8]
       [0 0 1 1 1 1 1 0 0 0...
 TEST: [4...
""" .

"DESCRIPTION.This code performs a fused operation of dropout and element-wise addition on two input tensors x and y with a dropout probability of 0.5." <EXPLAINS> """CODE.import paddle
from paddle.incubate.nn.layer.fused_dropout_add import FusedDropoutAdd

x = paddle.to_tensor([[1,2,3], [4,5,6]], dtype="float32")
y = paddle.to_tensor([[1,2,3], [4,5,6]], dtype="float32")

m = FusedDropoutAdd(p=0.5)

out = m(x, y)""" .

"DESCRIPTION.This code performs a hyperparameter optimization using the Dragonfly library by defining a domain with variables \"LiNO3_vol\", \"Li2SO4_vol\", and \"NaClO4_vol\" with specified ranges. It then sets up an optimization algorithm using EuclideanGPBandit and EuclideanFunctionCaller from Dragonfly. Finally, it runs the optimization using the DragonflySearch algorithm with the specified objective function my_func and maximum concurrency of 4." <EXPLAINS> """CODE.from ray import tune
from dragonfly.opt.gp_bandit import EuclideanGPBandit
from dragonfly.exd.experiment_caller import EuclideanFunctionCaller
from dragonfly import load_config

domain_vars = [{
    "name": "LiNO3_vol",
    "type": "float",
    "min": 0,
    "max": 7
}, {
    "name": "Li2SO4_vol",
    "type": "float",
    "min": 0,
    "max": 7
}, {
    "name": "NaClO4_vol",
    "type": "float",
    "min": 0,
    "max": 7
}]

domain_config = load_config({"domain": domain_vars})
func_caller = EuclideanFunctionCaller(None,
    domain_config.domain.list_of_domains[0])
optimizer = EuclideanGPBandit(func_caller, ask_tell_mode=True)

algo = DragonflySearch(optimizer, max_concurrent=4,
    metric="objective", mode="max")

tune.run(my_func, algo=algo)
""" .

"DESCRIPTION.This code performs a simple linear regression using the SGD optimizer with gradient clipping. It initializes a random 10x10 matrix, applies a linear transformation to it, calculates the mean of the output, and then minimizes the loss function with gradient clipping between -1.0 and 1.0." <EXPLAINS> """CODE.import numpy as np
import paddle
import paddle.fluid as fluid

from paddle.fluid.dygraph.base import to_variable
from paddle.fluid.dygraph.nn import Linear

from paddle.fluid.clip import GradClipByValue, GradClipByNorm, GradClipByGlobalNorm

from paddle.fluid.optimizer import SGDOptimizer

with fluid.dygraph.guard():
    value_clip = GradClipByValue( -1.0, 1.0 )
    sgd = SGDOptimizer(learning_rate=1.0)

    init_value = np.random.uniform( -1, 1, (10, 10)).astype('float32')

    linear = Linear( 10, 10)

    out = linear( to_variable(init_value) )

    loss = fluid.layers.reduce_mean( out )

    loss.backward()
    sgd.minimize(loss, grad_clip = value_clip)""" .

"DESCRIPTION.This code performs a training loop for a machine learning model, specifying a step trace annotation for profiling purposes at each iteration. It increments the global step count until it reaches a specified number of steps (NUM_STEPS)." <EXPLAINS> """CODE.import jax

while global_step < NUM_STEPS:
    with jax.profiler.StepTraceAnnotation("train", step_num=global_step):
        train_step()
        global_step += 1
""" .

"DESCRIPTION.This code performs audio classification using a pre-trained model from the Hugging Face Hub." <EXPLAINS> """CODE.from huggingface_hub import InferenceClient
client = InferenceClient()
client.audio_classification("audio.wav")
[{'score': 0.4976358711719513, 'label': 'hap'}, {'score': 0.3677836060523987, 'label': 'neu'},...]
""" .

"DESCRIPTION.This code performs average pooling on the input tensor x with a pooling window size of 2x2 and a stride of 2." <EXPLAINS> """CODE. y = tf.compat.v1.layers.average_pooling2d(x, pool_size=2, strides=2)

 x = tf.keras.Input((28, 28, 1))
 y = tf.keras.layers.AveragePooling2D(pool_size=2, strides=2)(x)
 model = tf.keras.Model(x, y)
""" .

"DESCRIPTION.This code performs classification using Quadratic Discriminant Analysis on the given input data X and labels y. It fits the model on the data and then predicts the class label for the input [-0.8, -1]." <EXPLAINS> """CODE.from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis
import numpy as np
X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2])
y = np.array([1, 1, 1, 2, 2, 2])
clf = QuadraticDiscriminantAnalysis()
clf.fit(X, y)
print(clf.predict([[-0.8, -1]])""" .

"DESCRIPTION.This code performs convolutional neural network operations on input data using paddle library in Python. It includes convolution, batch normalization, ReLU activation, max pooling, fully connected layer, and mean loss calculation." <EXPLAINS> """CODE.import paddle
import paddle.nn.functional as F
paddle.enable_static()
data = paddle.static.data(name='X', shape=[None, 1, 28, 28], dtype='float32')
conv2d = paddle.static.nn.conv2d(input=data, num_filters=6, filter_size=3)

with paddle.static.amp.bf16_guard():
    bn = paddle.static.nn.batch_norm(input=conv2d, act="relu")
    pool = F.max_pool2d(bn, kernel_size=2, stride=2)
    hidden = paddle.static.nn.fc(pool, size=10)
    loss = paddle.mean(hidden)""" .

"DESCRIPTION.This code performs cross-validation by splitting the data into training and testing sets using the Leave-P-Label-Out method, where P labels are left out in each iteration. It then prints the training and testing data sets for each iteration." <EXPLAINS> """CODE.from sklearn import cross_validation
import numpy as np

X = np.array([[1, 2], [3, 4], [5, 6])
y = np.array([1, 2, 1])
labels = np.array([1, 2, 3])
lpl = cross_validation.LeavePLabelOut(labels, p=2)

for train_index, test_index in lpl:
    X_train, X_test = X[train_index], X[test_index]
    y_train, y_test = y[train_index], y[test_index]
    print(X_train, X_test, y_train, y_test)

sklearn.cross_validation.LeavePLabelOut(labels=[1 2 3], p=2)
""" .

"DESCRIPTION.This code performs feature engineering by creating a bucketized version of the 'price' feature and then using it as input for a linear model to make predictions." <EXPLAINS> """CODE.price = numeric_column('price')
columns = [price, ...]
features = tf.parse_example(..., features=make_parse_example_spec(columns))
dense_tensor = input_layer(features, columns)

bucketized_price = bucketized_column(price, boundaries=[...])
columns = [bucketized_price, ...]
features = tf.parse_example(..., features=make_parse_example_spec(columns))
linear_prediction = linear_model(features, columns)
""" .

"DESCRIPTION.This code performs feature selection using Sequential Feature Selection with a K-Nearest Neighbors classifier on the Iris dataset." <EXPLAINS> """CODE.from sklearn.feature_selection import SequentialFeatureSelector
from sklearn.neighbors import KNeighborsClassifier
from sklearn.datasets import load_iris
X, y = load_iris(return_X_y=True)
knn = KNeighborsClassifier(n_neighbors=3)
sfs = SequentialFeatureSelector(knn, n_features_to_select=3)
sfs.fit(X, y)
sfs.get_support()
sfs.transform(X).shape
""" .

"DESCRIPTION.This code performs gradient boosting regression using the sklearn library to train a model on the provided samples and labels data, and then predicts the target value for a new input data point." <EXPLAINS> """CODE.from sklearn.ensemble import GradientBoostingRegressor
samples = [[0, 0, 2], [1, 0, 0]]
labels = [0, 1]
gb = GradientBoostingRegressor().fit(samples, labels)
print gb.predict([[0, 0, 0]])    # doctest: +ELLIPSIS""" .

"DESCRIPTION.This code performs image segmentation on an input image of a cat, returning a list of dictionaries with information about segmented objects including score, label, and mask." <EXPLAINS> """CODE.from huggingface_hub import InferenceClient
client = InferenceClient()
client.image_segmentation("cat.jpg"):
[{'score': 0.989008, 'label': 'LABEL_184', 'mask': <PIL.PngImagePlugin.PngImageFile image mode=L size=400x300 at 0x7FDD2B129CC0>}, ...]
""" .

"DESCRIPTION.This code performs instance normalization on the input tensor x with a shape of [1, 3, 1, 2]. Instance normalization normalizes the values of each channel independently across each spatial location in the input tensor." <EXPLAINS> """CODE.import paddle.fluid as fluid
from paddle.fluid.dygraph.base import to_variable
import numpy as np
import paddle

# x's shape is [1, 3, 1, 2]
x = np.array([[[[1.0, 8.0]], [[10.0, 5.0]], [[4.0, 6.0]]]]).astype('float32')
with fluid.dygraph.guard():
    x = to_variable(x)
    instanceNorm = paddle.nn.InstanceNorm(3)
    ret = instanceNorm(x)
    # ret's shape is [1, 3, 1, 2]; value is [-1 1 0.999999 -0.999999 -0.999995 0.999995]
    print(ret)""" .

"DESCRIPTION.This code performs matrix multiplication and addition operation with the input, x, and y matrices. The result is calculated as input * x + y, where the input, x, and y matrices are provided as input data and alpha and beta are scalar values used as multiplication factors. The code then prints the resulting matrix after the operation." <EXPLAINS> """CODE.import numpy as np
import paddle
import paddle.fluid as fluid

input = fluid.data(name='input', shape=[2, 2], dtype='float32')
x = fluid.data(name='x', shape=[2, 2], dtype='float32')
y = fluid.data(name='y', shape=[2, 2], dtype='float32')
out = fluid.layers.addmm( input=input, x=x, y=y, alpha=5.0, beta=0.5 )

data_x = np.ones((2, 2)).astype(np.float32)
data_y = np.ones((2, 2)).astype(np.float32)
data_input = np.ones((2, 2)).astype(np.float32)

place =  fluid.CUDAPlace(0) if fluid.core.is_compiled_with_cuda() else fluid.CPUPlace()
exe = fluid.Executor(place)
results = exe.run(fluid.default_main_program(),
                  fetch_list=[out], feed={"input": data_input, 'x': data_x, "y": data_y})
print( np.array(results[0]) )""" .

"DESCRIPTION.This code performs matrix multiplication between matrices \"a\" and \"b\" using TensorFlow, traces the resulting tensor \"c\", then adds 1 to the tensor \"c\" to obtain tensor \"d\"." <EXPLAINS> """CODE.c = tf.MatMul(a, b)
tensor_tracer.trace_tensor(c)
d = tf.add(c, 1)""" .

"DESCRIPTION.This code performs missing value imputation using KNN (K-Nearest Neighbors) algorithm on the input data X." <EXPLAINS> """CODE.import numpy as np
from sklearn.impute import KNNImputer
X = [[1, 2, np.nan], [3, 4, 3], [np.nan, 6, 5], [8, 8, 7]]
imputer = KNNImputer(n_neighbors=2)
imputer.fit_transform(X)""" .

"DESCRIPTION.This code performs one-hot encoding on a given dataset and transforms a new set of features using the fitted encoder." <EXPLAINS> """CODE.from sklearn.preprocessing import OneHotEncoder
enc = OneHotEncoder()
enc.fit([[0, 0, 3], [1, 1, 0], [0, 2, 1], [1, 0, 2]])
enc.n_values_
enc.feature_indices_
enc.transform([[0, 1, 1]]).toarray()""" .

"DESCRIPTION.This code performs ordinal encoding on a dataset." <EXPLAINS> """CODE.from sklearn.preprocessing import OrdinalEncoder
enc = OrdinalEncoder()
X = [['Male', 1], ['Female', 3], ['Female', 2]]
enc.fit(X)
enc.categories_
enc.transform([['Female', 3], ['Male', 1]])
enc.inverse_transform([[1, 0], [0, 1]])""" .

"DESCRIPTION.This code performs power transformation on the input data matrix." <EXPLAINS> """CODE.import numpy as np
from sklearn.preprocessing import power_transform
data = [[1, 2], [3, 2], [4, 5]]
print(power_transform(data))
""" .

"DESCRIPTION.This code performs quantized 1D convolution operation using the provided inputs, filters, and bias with specified padding, scale, and zero point values." <EXPLAINS> """CODE.from torch.nn.quantized import functional as qF
filters = torch.randn(33, 16, 3, dtype=torch.float)
inputs = torch.randn(20, 16, 50, dtype=torch.float)
bias = torch.randn(33, dtype=torch.float)

scale, zero_point = 1.0, 0
dtype_inputs = torch.quint8
dtype_filters = torch.qint8

q_filters = torch.quantize_per_tensor(filters, scale, zero_point, dtype_filters)
q_inputs = torch.quantize_per_tensor(inputs, scale, zero_point, dtype_inputs)
qF.conv1d(q_inputs, q_filters, bias, padding=1, scale=scale, zero_point=zero_point)
""" .

"DESCRIPTION.This code performs sequence classification using the ALBERT model to predict the class label of the input text." <EXPLAINS> """CODE.import tensorflow as tf
from transformers import AlbertTokenizer, TFAlbertForSequenceClassification

tokenizer = AlbertTokenizer.from_pretrained('albert-base-v2')
model = TFAlbertForSequenceClassification.from_pretrained('albert-base-v2')
input_ids = tf.constant(tokenizer.encode("Hello, my dog is cute"))[None, :]  # Batch size 1
outputs = model(input_ids)
logits = outputs[0]""" .

"DESCRIPTION.This code performs tabular classification using the specified table of wine features and a pre-trained model called \"julien-c/wine-quality\" from the Hugging Face model hub." <EXPLAINS> """CODE.from huggingface_hub import InferenceClient
client = InferenceClient()
table = {
    "fixed_acidity": ["7.4", "7.8", "10.3"],
    "volatile_acidity": ["0.7", "0.88", "0.32"],
    "citric_acid": ["0", "0", "0.45"],
    "residual_sugar": ["1.9", "2.6", "6.4"],
    "chlorides": ["0.076", "0.098", "0.073"],
    "free_sulfur_dioxide": ["11", "25", "5"],
    "total_sulfur_dioxide": ["34", "67", "13"],
    "density": ["0.9978", "0.9968", "0.9976"],
    "pH": ["3.51", "3.2", "3.23"],
    "sulphates": ["0.56", "0.68", "0.82"],
    "alcohol": ["9.4", "9.8", "12.6"],
}
client.tabular_classification(table=table, model="julien-c/wine-quality")
""" .

"DESCRIPTION.This code performs text classification on the input text \"I like you\" and returns a list of dictionaries with the classification labels and their corresponding scores including 'POSITIVE' with a score of 0.9998695850372314 and 'NEGATIVE' with a score of 0.0001304351753788069." <EXPLAINS> """CODE.from huggingface_hub import InferenceClient
client = InferenceClient()
client.text_classification("I like you")
[{'label': 'POSITIVE', 'score': 0.9998695850372314}, {'label': 'NEGATIVE', 'score': 0.0001304351753788069}]
""" .

"DESCRIPTION.This code performs the cross product operation between two input arrays x and y using the PaddlePaddle library. It calculates the cross product of the two arrays along specified dimensions and prints the results." <EXPLAINS> """CODE.import paddle
import paddle.fluid as fluid
import numpy as np

data_x = np.array([[1.0, 1.0, 1.0],
                   [2.0, 2.0, 2.0],
                   [3.0, 3.0, 3.0]])
data_y = np.array([[1.0, 1.0, 1.0],
                   [1.0, 1.0, 1.0],
                   [1.0, 1.0, 1.0]])

with fluid.dygraph.guard():
    x = fluid.dygraph.to_variable(data_x)
    y = fluid.dygraph.to_variable(data_y)
    out_z1 = fluid.layers.cross(x, y)
    print(out_z1.numpy())
    #[[-1. -1. -1.]
    # [ 2.  2.  2.]
    # [-1. -1. -1.]]
    out_z2 = fluid.layers.cross(x, y, dim=1)
    print(out_z2.numpy())
    #[[0. 0. 0.]
    # [0. 0. 0.]
    # [0. 0. 0.]]""" .

"DESCRIPTION.This code performs training of a machine learning model using an accelerated computing framework. It initializes the accelerator, prepares the dataloader, model, optimizer, and scheduler for training. Then it iterates over batches of data, calculates the loss, performs backpropagation, and updates the model parameters through optimization." <EXPLAINS> """CODE.from accelerate import Accelerator

accelerator = Accelerator()
dataloader, model, optimizer, scheduler = accelerator.prepare(dataloader, model, optimizer, scheduler)

for (input, target) in accelerator.skip_first_batches(dataloader, num_batches=2):
    optimizer.zero_grad()
    output = model(input)
    loss = loss_func(output, target)
    accelerator.backward(loss)
    optimizer.step()
""" .

"DESCRIPTION.This code performs triangular solving of the system of linear equations where x is a upper triangular matrix and y is a vector, and returns the solution." <EXPLAINS> """CODE.import paddle
import numpy as np

x = paddle.to_tensor([[1, 1, 1],
                      [0, 2, 1],
                      [0, 0,-1]], dtype="float64")
y = paddle.to_tensor([[0], [-9], [5]], dtype="float64")
out = paddle.linalg.triangular_solve(x, y, upper=True)

print(out)""" .

"DESCRIPTION.This code performs validation curve analysis to evaluate the performance of a Logistic Regression model on a classification dataset generated using make_classification function from sklearn. It plots the training and test scores against the specified hyperparameter C values for Logistic Regression model." <EXPLAINS> """CODE.import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import make_classification
from sklearn.model_selection import ValidationCurveDisplay, validation_curve
from sklearn.linear_model import LogisticRegression

X, y = make_classification(n_samples=1_000, random_state=0)
logistic_regression = LogisticRegression()
param_name, param_range = "C", np.logspace(-8, 3, 10)
train_scores, test_scores = validation_curve(
    logistic_regression, X, y, param_name=param_name, param_range=param_range
)
display = ValidationCurveDisplay(
    param_name=param_name, param_range=param_range,
    train_scores=train_scores, test_scores=test_scores, score_name="Score"
)
display.plot()
plt.show()""" .

"DESCRIPTION.This code performs zero padding on a 2D input array using TensorFlow's ZeroPadding2D layer. The input array is defined and printed first, then the ZeroPadding2D layer is applied to the input array, resulting in padding of zeros around the original array." <EXPLAINS> """CODE.input_shape = (1, 1, 2, 2)
x = np.arange(np.prod(input_shape)).reshape(input_shape)
print(x)
[[[[0 1]
   [2 3]]]]
y = tf.keras.layers.ZeroPadding2D(padding=1)(x)
print(y)
tf.Tensor(
  [[[[0 0]
     [0 0]
     [0 0]
     [0 0]]
    [[0 0]
     [0 1]
     [2 3]
     [0 0]]
    [[0 0]
     [0 0]
     [0 0]
     [0 0]]]], shape=(1, 3, 4, 2), dtype=int64)""" .

"DESCRIPTION.This code performs zero-shot image classification using a pre-trained model from the Hugging Face Model Hub. It takes an image URL as input and predicts the most likely labels from the provided list of labels for the image." <EXPLAINS> """CODE.from huggingface_hub import InferenceClient
client = InferenceClient()

client.zero_shot_image_classification(
...     "https://upload.wikimedia.org/wikipedia/commons/thumb/4/43/Cute_dog.jpg/320px-Cute_dog.jpg",
...     labels=["dog", "cat", "horse"],
... )
[{"label": "dog", "score": 0.956}, ...]
""" .

"DESCRIPTION.This code permutes the dimensions of the constant tensor `a` in a specific pattern specified as (1, 0)." <EXPLAINS> """CODE.a = tf.constant([[1, 2, 3], [4, 5, 6], [7, 8, 9], [10, 11, 12]])
tf.keras.backend.permute_dimensions(a, pattern=(1, 0))
""" .

"DESCRIPTION.This code plays an audio file represented by the bytes in the 'audio_bytes' variable, using the specified format 'audio/ogg'." <EXPLAINS> "CODE.st.audio(audio_bytes, format='audio/ogg')" .

"DESCRIPTION.This code pops the 'params' value from a dictionary named 'variables' and assigns it to the variables 'state' and 'params'." <EXPLAINS> "CODE.state, params = variables.pop('params')" .

"DESCRIPTION.This code prepares a translation batch using the MBartTokenizer from the 'facebook/mbart-large-en-ro' model for translating an English phrase to Romanian." <EXPLAINS> """CODE.from transformers import MBartTokenizer
tokenizer = MBartTokenizer.from_pretrained('facebook/mbart-large-en-ro')
example_english_phrase = " UN Chief Says There Is No Military Solution in Syria"
expected_translation_romanian = "Åeful ONU declarÄ cÄ nu existÄ o soluÅ£ie militarÄ Ã®n Siria"
batch: dict = tokenizer.prepare_translation_batch(
...     example_english_phrase, src_lang="en_XX", tgt_lang="ro_RO", tgt_texts=expected_translation_romanian
... )""" .

"DESCRIPTION.This code prepares and sets up data for a task by downloading, cleaning, and caching ImageNet dataset." <EXPLAINS> """CODE.dm.prepare_data()
dm.setup()

def prepare_data(self):
    download_imagenet()
    clean_imagenet()
    cache_imagenet()
""" .

"DESCRIPTION.This code prints out the audio decoders and their corresponding values." <EXPLAINS> """CODE.for k, v in get_audio_decoders().items():
    print(f"{k}: {v}")""" .

"DESCRIPTION.This code prints out the current endpoint address of the parallel environment using the fluid.dygraph module in PaddlePaddle." <EXPLAINS> """CODE.import paddle.fluid as fluid
env = fluid.dygraph.ParallelEnv()
print("The current endpoint are %s" % env.current_endpoint)
# The current endpoint are 127.0.0.1:6170""" .

"DESCRIPTION.This code prints the unsigned hexadecimal representation of the number -2147023174 with leading zeros to fill 8 characters." <EXPLAINS> "CODE.print \"%08X\" % signed_to_unsigned (-2147023174)" .

"DESCRIPTION.This code prints the value of variable x to the console with the message \"x is: \"." <EXPLAINS> """CODE.    x = K.print_tensor(x, message="x is: ")
""" .

"DESCRIPTION.This code prints the value of variable x with the message \"x is: \"." <EXPLAINS> """CODE.    x = K.print_tensor(x, message="x is: ")
""" .

"DESCRIPTION.This code prompts the user to input a number using a number input widget, then displays the inputted number in the webpage using the Streamlit write function." <EXPLAINS> """CODE.number = st.number_input('Insert a number')
st.write('The current number is ', number)""" .

"DESCRIPTION.This code prompts the user to input something and if the user enters any input, it will display a message showing the input provided by the user." <EXPLAINS> """CODE.import streamlit as st

prompt = st.chat_input("Say something")
if prompt:
    st.write(f"User has sent the following prompt: {prompt}")""" .

"DESCRIPTION.This code prompts the user to input their birthday date and then displays the inputted date on the screen." <EXPLAINS> """CODE.d = st.date_input(
    "When's your birthday",
    datetime.date(2019, 7, 6))
st.write('Your birthday is:', d)
""" .

"DESCRIPTION.This code pulls a model from the production Hub using a specified model name and then pushes the model to the staging Hub." <EXPLAINS> """CODE.def test_push_to_hub():
    # Pull from production Hub
    with production_endpoint():
        model = ...from_pretrained("modelname")

    # Push to staging Hub
    model.push_to_hub()
""" .

"DESCRIPTION.This code pushes a dataset to a specified organization on a hub platform with the specified dataset ID and sets the split to \"evaluation\"." <EXPLAINS> """CODE.dataset.push_to_hub("<organization>/<dataset_id>", split="evaluation")
""" .

"DESCRIPTION.This code raises a RuntimeWarning with the message \"my warning\"." <EXPLAINS> """CODE.with warns(RuntimeWarning):
    warnings.warn("my warning", RuntimeWarning)
""" .

"DESCRIPTION.This code randomly generates input and label data, converts them into paddle tensors, and creates a TensorDataset object. It then iterates through the dataset and prints out each input and label pair." <EXPLAINS> """CODE.import numpy as np
import paddle
from paddle.io import TensorDataset

paddle.disable_static()

input_np = np.random.random([2, 3, 4]).astype('float32')
input = paddle.to_tensor(input_np)
label_np = np.random.random([2, 1]).astype('int32')
label = paddle.to_tensor(label_np)

dataset = TensorDataset([input, label])

for i in range(len(dataset)):
    input, label = dataset[i]
    print(input, label)""" .

"DESCRIPTION.This code randomly selects and repeats data from the datasets based on the choices provided by the choice_dataset." <EXPLAINS> """CODE.datasets = [tf.data.Dataset.from_tensors("foo").repeat(),
            tf.data.Dataset.from_tensors("bar").repeat(),
            tf.data.Dataset.from_tensors("baz").repeat()]

choice_dataset = tf.data.Dataset.range(3).repeat(3)

result = tf.contrib.data.choose_from_datasets(datasets, choice_dataset)
""" .

"DESCRIPTION.This code randomly zooms in or out the input image by a factor between .5 and .2 and returns the resulting image as output." <EXPLAINS> """CODE.input_img = np.random.random((32, 224, 224, 3))
layer = tf.keras.layers.RandomZoom(.5, .2)
out_img = layer(input_img)
out_img.shape
""" .

"DESCRIPTION.This code reads a CSV file containing the Iris dataset, then creates a parallel coordinates plot using the 'Name' column and specifies color codes for different categories. Finally, it displays the plot using matplotlib." <EXPLAINS> """CODE.from pandas import read_csv
from pandas.tools.plotting import parallel_coordinates
from matplotlib import pyplot as plt
df = read_csv('https://raw.github.com/pydata/pandas/master/pandas/tests/data/iris.csv')
parallel_coordinates(df, 'Name', colors=('#556270', '#4ECDC4', '#C7F464'))
plt.show()""" .

"DESCRIPTION.This code reads a file named \"config.json\" and sends its contents in a PUT request to a specified URL, displaying the progress of the transfer using tqdm." <EXPLAINS> """CODE.with tqdm_stream_file("config.json") as f:
    requests.put(url, data=f)
config.json: 100%|âââââââââââââââââââââââââ| 8.19k/8.19k [00:02<00:00, 3.72kB/s]
""" .

"DESCRIPTION.This code reads a temporary file that contains two strings: '123456789' and 'an_older_hash'." <EXPLAINS> """CODE._read_manual_review_tmp_file(tmp_path)
['123456789', 'an_older_hash']""" .

"DESCRIPTION.This code reads an image file from \"/path/to/image.png\" and then writes the same image to another file at \"/path/to/output.jpg\"." <EXPLAINS> """CODE.import imageio.v3 as iio
with iio.imopen("/path/to/image.png", "r") as file:
    im = file.read()

with iio.imopen("/path/to/output.jpg", "w") as file:
    file.write(im)""" .

"DESCRIPTION.This code reads an image from a specified file path, then writes the image to another file path in a different format." <EXPLAINS> """CODE.import imageio.v3 as iio
with iio.imopen("/path/to/image.png", "r") as file:
    im = file.read()

with iio.imopen("/path/to/output.jpg", "w") as file:
    file.write(im)""" .

"DESCRIPTION.This code reads chunks of data from a file object with a specified chunk size using a loop until there is no more data left to read." <EXPLAINS> """CODE.with tqdm.wrapattr(file_obj, "read", total=file_obj.size) as fobj:
    while True:
        chunk = fobj.read(chunk_size)
        if not chunk:
            break""" .

"DESCRIPTION.This code reads data from an HDF5 file and uses a trained model to make predictions on the data." <EXPLAINS> """CODE.    x_data = HDF5Matrix('input/file.hdf5', 'data')
    model.predict(x_data)
""" .

"DESCRIPTION.This code rebase_donate_argnums function takes two tuples as inputs and returns a new tuple where each element is the sum of the corresponding elements from the two input tuples." <EXPLAINS> """CODE.rebase_donate_argnums((3, 4), (0, 1))
(1, 2)""" .

"DESCRIPTION.This code reconstructs data by aggregating columns 'a' and 'b' based on the values in column 'h' specified as 'by'." <EXPLAINS> """CODE.reconstruct_data_with_by(df, by='h', cols=['a', 'b'])
   h1      h2
   a     b     a     b
0  1.0   3.0   NaN   NaN
1  3.0   4.0   NaN   NaN
2  NaN   NaN   5.0   6.0""" .

"DESCRIPTION.This code records the values of 'xsinx', 'xcosx', and 'arctanx' at each iteration 'i' in the 'run_14h' log for visualization or analysis." <EXPLAINS> """CODE.writer.add_scalars('run_14h', {'xsinx':i*np.sin(i/r),
                               'xcosx':i*np.cos(i/r),
                               'arctanx': numsteps*np.arctan(i/r)}, i)""" .

"DESCRIPTION.This code redirects console logging to tqdm.write() using logging_redirect_tqdm while iterating through a range of 9, and outputs a console log message when i is equal to 4." <EXPLAINS> """CODE.import logging
from tqdm import trange
from tqdm.contrib.logging import logging_redirect_tqdm

LOG = logging.getLogger(__name__)

if __name__ == '__main__':
    logging.basicConfig(level=logging.INFO)
    with logging_redirect_tqdm():
        for i in trange(9):
            if i == 4:
                LOG.info("console logging redirected to `tqdm.write()`")
    # logging restored
""" .

"DESCRIPTION.This code registers a communication hook for the DDP (DistributedDataParallel) model with the specified process group and quantization per tensor hook." <EXPLAINS> "CODE.ddp_model._register_comm_hook(process_group, quantization_pertensor_hook)" .

"DESCRIPTION.This code registers a custom Pytree node for the set data structure. It defines functions to convert a set into a sorted list for serialization, reconstruct a set from its children, and apply the set constructor to the children." <EXPLAINS> """CODE.register_pytree_node(
    set,
    lambda s: (sorted(s), None, None),
    lambda children, _: set(children),
)""" .

"DESCRIPTION.This code registers two components, 'foo_bar' and 'baz', setting their activation status to False." <EXPLAINS> """CODE.register_component('foo_bar', False)
register_component('baz', False)""" .

"DESCRIPTION.This code removes duplicate tuples from a list based on the first element of each tuple." <EXPLAINS> "CODE._make_unique([('a', '<lambda>'), ('a', '<lambda>'), ('b', '<lambda>')])",
        """CODE.kwarg_list = [('a', '<lambda>'), ('a', '<lambda>'), ('b', '<lambda>')]
_make_unique_kwarg_list(kwarg_list)
[('a', '<lambda>_0'), ('a', '<lambda>_1'), ('b', '<lambda>')]""" .

"DESCRIPTION.This code removes non-words marked by Kaldi from the provided sentences." <EXPLAINS> """CODE.import jiwer

sentences = ["you <unk> like [laugh]"]

print(jiwer.RemoveKaldiNonWords()(sentences))

# prints: ["you  like "]
# note the extra spaces
""" .

"DESCRIPTION.This code removes specific words from a list of sentences and returns the modified sentences with extra spaces." <EXPLAINS> """CODE.import jiwer

sentences = ["yhe awesome", "the apple is not a pear", "yhe"]

print(jiwer.RemoveSpecificWords(["yhe", "the", "a"])(sentences))
# prints: ['  awesome', '  apple is not   pear', ' ']
# note the extra spaces
""" .

"DESCRIPTION.This code renames a discussion in a specified repository to the new title provided, by calling the `rename_discussion` function from the `HfApi` class with the parameters `repo_id`, `discussion_num`, and `new_title`." <EXPLAINS> """CODE.new_title = "New title, fixing a typo"
HfApi().rename_discussion(
    repo_id="username/repo_name",
    discussion_num=34,
    new_title=new_title
)
""" .

"DESCRIPTION.This code renders a template asynchronously with the variable 'knights' set to the value 'that say nih; asynchronously'." <EXPLAINS> "CODE.await template.render_async(knights='that say nih; asynchronously')" .

"DESCRIPTION.This code repeats the elements of a 2D array by a specified number of times in both rows and columns." <EXPLAINS> """CODE.repeat_nd([[1, 2], [3, 4]], (2, 2))
array([[1, 1, 2, 2],
       [1, 1, 2, 2],
       [3, 3, 4, 4],
       [3, 3, 4, 4]])""" .

"DESCRIPTION.This code repeats the elements of tensor b along axis 0 for 2 times." <EXPLAINS> "CODE.tf.keras.backend.repeat_elements(b, rep=2, axis=0)" .

"DESCRIPTION.This code replaces blocks in a Python script with imports from a specified path based on the block type (class or function). It reads a Python file, identifies blocks of code based on the block type, and replaces the blocks with import statements from the specified import path." <EXPLAINS> """CODE.import os

def replace_block_with_imports(lines, import_path, block_type):
    new_lines = []
    in_block = False
    for line in lines:
        if block_type == "class" and "class" in line:
            in_block = True
        elif block_type == "def" and "def" in line:
            in_block = True

        if in_block:
            if line.strip().endswith(":"):
                new_lines.append(f"from {import_path} import {line.strip()[:-1]}")
                in_block = False
            else:
                new_lines.append(line)
        else:
            new_lines.append(line)

    return new_lines

py_file = os.path.join(_PROJECT_ROOT, "src", "pytorch_lightning", "loggers", "logger.py")
import_path = ".".join(["pytorch_lightning", "loggers", "logger"])
with open(py_file, encoding="utf-8") as fp:
    lines = [ln.rstrip() for ln in fp.readlines()]
lines = replace_block_with_imports(lines, import_path, "class")
lines = replace_block_with_imports(lines, import_path, "def")""" .

"DESCRIPTION.This code resamples a time series data to a monthly frequency and calculates the minimum value for each month." <EXPLAINS> "CODE.ser.resample('MS').min()" .

"DESCRIPTION.This code reshapes the data by creating a long format where the columns 'year1' and 'year2' are collected under the 'year' column, and 'hr1' and 'hr2' are collected under the 'hr' column." <EXPLAINS> """CODE.pd.lreshape(data, {'year': ['year1', 'year2'],
                       'hr': ['hr1', 'hr2']})""" .

"DESCRIPTION.This code resolves patterns in a dataset repository located at the specified base path by finding all YAML files matching the given patterns in the source directories." <EXPLAINS> """CODE.from datasets.data_files import resolve_patterns_in_dataset_repository
base_path = "/Users/username/Desktop/hf/datasets"
resolve_patterns_locally_or_by_urls(base_path, ["src/**/*.yaml"])
""" .

"DESCRIPTION.This code resolves patterns locally or by URLs for YAML files in the \"src\" directory and its subdirectories." <EXPLAINS> """CODE.from datasets.data_files import resolve_pattern
base_path = "."
resolve_pattern("docs/**/*.py", base_path)
""",
        """CODE.from datasets.data_files import resolve_patterns_locally_or_by_urls

base_path = "."
resolve_patterns_locally_or_by_urls(base_path, ["src/**/*.yaml"])
""" .

"DESCRIPTION.This code retrieves a collection from the Hugging Face Hub, then updates the last item in the collection by adding a note and changing its position to the top (position=0)." <EXPLAINS> """CODE.from huggingface_hub import get_collection, update_collection_item

# Get collection first
collection = get_collection("TheBloke/recent-models-64f9a55bb3115b4f513ec026")

# Update item based on its ID (add note + update position)
update_collection_item(
...     collection_slug="TheBloke/recent-models-64f9a55bb3115b4f513ec026",
...     item_object_id=collection.items[-1].item_object_id,
...     note="Newly updated model!"
...     position=0,
... )
""" .

"DESCRIPTION.This code retrieves a collection of recent models from TheBloke's repository on the Hugging Face Hub. It displays the title of the collection as \"Recent models\", the total number of items in the collection is 37, and provides information about the first item in the collection such as its ID, author, type, and last modified date." <EXPLAINS> """CODE.from huggingface_hub import get_collection
collection = get_collection("TheBloke/recent-models-64f9a55bb3115b4f513ec026")
collection.title
'Recent models'
len(collection.items)
37
collection.items[0]
CollectionItem: {
    {'item_object_id': '6507f6d5423b46492ee1413e',
    'item_id': 'TheBloke/TigerBot-70B-Chat-GPTQ',
    'author': 'TheBloke',
    'item_type': 'model',
    'lastModified': '2023-09-19T12:55:21.000Z',
    (...)
}}
""" .

"DESCRIPTION.This code retrieves a list of 411 accepted access requests for the model \"Llama-2-7b\" from the Hugging Face model hub, including details such as the username, fullname, email, timestamp, and status of each request." <EXPLAINS> """CODE.from huggingface_hub import list_accepted_access_requests

requests = list_accepted_access_requests("meta-llama/Llama-2-7b")
len(requests)
411
requests[0]
[
    AccessRequest(
        username='clem',
        fullname='Clem ð¤',
        email='***',
        timestamp=datetime.datetime(2023, 11, 23, 18, 4, 53, 828000, tzinfo=datetime.timezone.utc),
        status='accepted',
        fields=None,
    ),
    ...
]
""" .

"DESCRIPTION.This code retrieves a list of available models from the PyTorch Vision repository." <EXPLAINS> "CODE.torch.hub.list('pytorch/vision', force_reload=True)" .

"DESCRIPTION.This code retrieves a list of inference endpoints from the Hugging Face model hub." <EXPLAINS> """CODE.from huggingface_hub import HfApi
api = HfApi()
api.list_inference_endpoints()
[InferenceEndpoint(name='my-endpoint', ...), ...]
""" .

"DESCRIPTION.This code retrieves a list of repositories liked by the user \"julien-c\" from the Hugging Face model hub. It then allows access to the username (user) and a list of liked model repositories (models)." <EXPLAINS> """CODE.from huggingface_hub import list_liked_repos

likes = list_liked_repos("julien-c")

likes.user
"julien-c"

likes.models
["osanseviero/streamlit_1.15", "Xhaheen/ChatGPT_HF", ...]
""" .

"DESCRIPTION.This code retrieves a reference to the master branch on the remote named 'origin'." <EXPLAINS> "CODE.remote.refs.master # yields RemoteReference('/refs/remotes/origin/master')" .

"DESCRIPTION.This code retrieves image features using the CLIP (Contrastive Language-Image Pre-training) model from an image downloaded from a URL." <EXPLAINS> """CODE.from PIL import Image
import requests
from transformers import CLIPProcessor, TFCLIPModel

model = TFCLIPModel.from_pretrained("openai/clip-vit-base-patch32")
processor = CLIPProcessor.from_pretrained("openai/clip-vit-base-patch32")

url = "http://images.cocodataset.org/val2017/000000039769.jpg"
image = Image.open(requests.get(url, stream=True).raw)

inputs = processor(images=image, return_tensors="tf")

image_features = model.get_image_features(**inputs)
""" .

"DESCRIPTION.This code retrieves information about the first parallel port on a Windows system and prints out the relative path of the port." <EXPLAINS> """CODE.pp0 = wmi.WMI ().Win32_ParallelPort ()[0]
print pp0.path ().RelPath""" .

"DESCRIPTION.This code retrieves information about the parallel port device on a Windows system and prints the derivation path of the parallel port." <EXPLAINS> """CODE.pp0 = wmi.WMI ().Win32_ParallelPort ()[0]
print ' <- '.join (pp0.derivation ())""" .

"DESCRIPTION.This code retrieves the allocated base size of the system's page file usage using the Get-CIMInstance Win32_PageFileUsage command and then selects the AllocatedBaseSize attribute." <EXPLAINS> """CODE.powershell(
    "Get-CIMInstance Win32_PageFileUsage | Select AllocatedBaseSize")""" .

"DESCRIPTION.This code retrieves the amount of free physical memory in the Windows operating system." <EXPLAINS> """CODE.wmic("Win32_OperatingSystem", "FreePhysicalMemory")
2134124534""" .

"DESCRIPTION.This code retrieves the default configuration name for the dataset \"openbookqa\"." <EXPLAINS> """CODE.from datasets import get_dataset_default_config_name
get_dataset_default_config_name("openbookqa")
""" .

"DESCRIPTION.This code retrieves the default float data type used by TensorFlow's Keras backend." <EXPLAINS> "CODE.tf.keras.backend.floatx()" .

"DESCRIPTION.This code retrieves the default floating point type used in the Keras backend, which is 'float32'." <EXPLAINS> """CODE.keras.backend.floatx()
'float32'
""" .

"DESCRIPTION.This code retrieves the default floating-point data type used by Keras." <EXPLAINS> """CODE.keras.backend.floatx()
""" .

"DESCRIPTION.This code retrieves the epsilon value used in TensorFlow's backend operations." <EXPLAINS> "CODE.tf.keras.backend.epsilon()" .

"DESCRIPTION.This code retrieves the file tree of the \"lysandre/arxiv-nlp\" repository on Hugging Face Hub and lists the files and their metadata, such as path, size, blob_id, and other information." <EXPLAINS> """CODE.from huggingface_hub import list_repo_tree
repo_tree = list_repo_tree("lysandre/arxiv-nlp")
repo_tree
<generator object HfApi.list_repo_tree at 0x7fa4088e1ac0>
list(repo_tree)
[
    RepoFile(path='.gitattributes', size=391, blob_id='ae8c63daedbd4206d7d40126955d4e6ab1c80f8f', lfs=None, last_commit=None, security=None),
    RepoFile(path='README.md', size=391, blob_id='43bd404b159de6fba7c2f4d3264347668d43af25', lfs=None, last_commit=None, security=None),
    RepoFile(path='config.json', size=554, blob_id='2f9618c3a19b9a61add74f70bfb121335aeef666', lfs=None, last_commit=None, security=None),
    RepoFile(
        path='flax_model.msgpack', size=497764107, blob_id='8095a62ccb4d806da7666fcda07467e2d150218e',
        lfs={'size': 497764107, 'sha256': 'd88b0d6a6ff9c3f8151f9d3228f57092aaea997f09af009eefd7373a77b5abb9', 'pointer_size': 134}, last_commit=None, security=None
    ),
    RepoFile(path='merges.txt', size=456318, blob_id='226b0752cac7789c48f0cb3ec53eda48b7be36cc', lfs=None, last_commit=None, security=None),
    RepoFile(
        path='pytorch_model.bin', size=548123560, blob_id='64eaa9c526867e404b68f2c5d66fd78e27026523',
        lfs={'size': 548123560, 'sha256': '9be78edb5b928eba33aa88f431551348f7466ba9f5ef3daf1d552398722a5436', 'pointer_size': 134}, last_commit=None, security=None
    ),
    RepoFile(path='vocab.json', size=898669, blob_id='b00361fece0387ca34b4b8b8539ed830d644dbeb', lfs=None, last_commit=None, security=None)]
]

""" .

"DESCRIPTION.This code retrieves the frequency group for the resolution 'day'." <EXPLAINS> "CODE.f.Resolution.get_freq_group('day')" .

"DESCRIPTION.This code retrieves the image data format setting used in Keras backend." <EXPLAINS> """CODE.keras.backend.image_data_format()
""" .

"DESCRIPTION.This code retrieves the information of a dataset named \"demo1\" from the Hugging Face model hub and resolves patterns in the dataset repository for CSV files." <EXPLAINS> """CODE.import huggingface_hub
from datasets.data_files import resolve_patterns_in_dataset_repository

dataset_info = huggingface_hub.HfApi().dataset_info("lhoestq/demo1")
resolve_patterns_in_dataset_repository(dataset_info, ["*.csv"])
""" .

"DESCRIPTION.This code retrieves the job ID from the runtime context in Ray." <EXPLAINS> """CODE.ray.get_runtime_context().job_id
ray.get_runtime_context().get()""" .

"DESCRIPTION.This code retrieves the major version number from the input version string \"1.2.3\"." <EXPLAINS> "CODE.Version(\"1.2.3\").major" .

"DESCRIPTION.This code retrieves the micro version number from a given version string." <EXPLAINS> """CODE.Version("1.2.3").micro
3
Version("1").micro""" .

"DESCRIPTION.This code retrieves the minor version number of the input version string." <EXPLAINS> """CODE.Version("1.2.3").minor
Version("1").minor""" .

"DESCRIPTION.This code retrieves the name of the class of the variable 'ba', calls a function named 'hello' with the argument 'michele', and prints 'BEFORE', 'hello michele', and 'AFTER' sequentially." <EXPLAINS> """CODE.ba.__class__.__name__
hello('michele')
BEFORE
hello michele
AFTER""" .

"DESCRIPTION.This code retrieves the query parameters from the URL and returns a dictionary containing the values of the \"show_map\" and \"selected\" keys." <EXPLAINS> """CODE.st.experimental_get_query_params()
{"show_map": ["True"], "selected": ["asia", "america"]}""" .

"DESCRIPTION.This code retrieves the static resources (CPU and GPU) associated with specific IP addresses from a resource manager object called lm." <EXPLAINS> """CODE.lm.get_static_node_resources_by_ip()
{127.0.0.1: {"CPU": 1}, 127.0.0.2: {"CPU": 4, "GPU": 8}}""" .

"DESCRIPTION.This code retrieves the status and URL for an inference endpoint named 'my-text-to-image' and then runs a text-to-image inference using the endpoint." <EXPLAINS> """CODE.from huggingface_hub import HfApi
api = HfApi()
endpoint = api.get_inference_endpoint("my-text-to-image")
endpoint
InferenceEndpoint(name='my-text-to-image', ...)

# Get status
endpoint.status
'running'
endpoint.url
'https://my-text-to-image.region.vendor.endpoints.huggingface.cloud'

# Run inference
endpoint.client.text_to_image(...)
""" .

"DESCRIPTION.This code retrieves the training sequences from the IMDB dataset, retrieves the word index file mapping words to indices, reverses the word index to obtain a dictionary mapping indices to words, and decodes the first sequence in the dataset by converting the indices to words." <EXPLAINS> """CODE.# Retrieve the training sequences.
(x_train, _), _ = keras.datasets.imdb.load_data()
# Retrieve the word index file mapping words to indices
word_index = keras.datasets.imdb.get_word_index()
# Reverse the word index to obtain a dict mapping indices to words
inverted_word_index = dict((i, word) for (word, i) in word_index.items())
# Decode the first sequence in the dataset
decoded_sequence = " ".join(inverted_word_index[i] for i in x_train[0])
""" .

"DESCRIPTION.This code retrieves the weights of a neural network model from a variable named \"ev1\"." <EXPLAINS> "CODE.weights = ev1.get_weights()" .

"DESCRIPTION.This code retrieves training sequences from the IMDb dataset, retrieves a word index mapping words to indices, reverses the word index to obtain a dictionary mapping indices to words, and decodes the first sequence in the dataset into words." <EXPLAINS> """CODE.# Retrieve the training sequences.
(x_train, _), _ = keras.datasets.imdb.load_data()
# Retrieve the word index file mapping words to indices
word_index = keras.datasets.imdb.get_word_index()
# Reverse the word index to obtain a dict mapping indices to words
inverted_word_index = dict((i, word) for (word, i) in word_index.items())
# Decode the first sequence in the dataset
decoded_sequence = " ".join(inverted_word_index[i] for i in x_train[0])
""" .

"DESCRIPTION.This code returns the byte size of a data type, specifically torch.float32, which is 4 bytes." <EXPLAINS> """CODE.dtype_byte_size(torch.float32)
4
""" .

"DESCRIPTION.This code returns the post version number of the software version \"1.2.3.post1\"." <EXPLAINS> "CODE.Version(\"1.2.3.post1\").post" .

"DESCRIPTION.This code returns the resolution string for seconds." <EXPLAINS> "CODE.Resolution.get_str(Resolution.RESO_SEC)" .

"DESCRIPTION.This code returns the three most common elements along with their counts in the string 'abcdeabcdabcaba'." <EXPLAINS> "CODE.Counter('abcdeabcdabcaba').most_common(3)" .

"DESCRIPTION.This code returns the value of the epsilon constant used in the Keras backend." <EXPLAINS> """CODE.keras.backend.epsilon()
1e-07
""",
        """CODE.keras.backend.epsilon()
1e-08
""" .

"DESCRIPTION.This code rotates the elements in a deque object `x` by shifting them by 1 position to the right and then by 2 positions to the left." <EXPLAINS> """CODE.x = pdeque([1, 2, 3])
x.rotate(1)
x.rotate(-2)""" .

"DESCRIPTION.This code rounds up the dataset to the next full batch size by padding it and setting the drop remainder to ensure the batch shape is known statically." <EXPLAINS> """CODE.
ds = ...
# Round up to the next full batch.
target_cardinality = -(-ds.cardinality() // batch_size) * batch_size
ds = ds.apply(tf.data.experimental.pad_to_cardinality(target_cardinality))
# Set `drop_remainder` so that batch shape will be known statically. No data
# will actually be dropped since the batch size divides the cardinality.
ds = ds.batch(batch_size, drop_remainder=True)
""" .

"DESCRIPTION.This code runs a hyperparameter tuning experiment using Ray Tune, where the search space is defined with two parameters (parameter_1 and parameter_2) having choices specified. The results of the experiment are logged to MLflow with the experiment name \"experiment1\" and artifacts saved." <EXPLAINS> """CODE.from ray.tune.integration.mlflow import MLflowLoggerCallback
tune.run(
    train_fn,
    config={
        # define search space here
        "parameter_1": tune.choice([1, 2, 3]),
        "parameter_2": tune.choice([4, 5, 6]),
    },
    callbacks=[MLflowLoggerCallback(
        experiment_name="experiment1",
        save_artifact=True)])""" .

"DESCRIPTION.This code saves a Keras model to a file, then loads the model from the saved file, and finally deletes the saved file." <EXPLAINS> """CODE.with tf_file_io_proxy('keras.engine.saving.tf_file_io') as file_io_proxy:
    gcs_filepath = file_io_proxy.get_filepath(filename='model.h5')
    save_model(model, gcs_filepath)
    file_io_proxy.assert_exists(gcs_filepath)
    new_model_gcs = load_model(gcs_filepath)
    file_io_proxy.delete_file(gcs_filepath)  # cleanup
""" .

"DESCRIPTION.This code saves a Keras model to a specified file path using a file I/O proxy, then loads the saved model from the file path, and finally deletes the file from the file path." <EXPLAINS> """CODE.with tf_file_io_proxy('keras.engine.saving.tf_file_io') as file_io_proxy:
    gcs_filepath = file_io_proxy.get_filepath(filename='model.h5')
    save_model(model, gcs_filepath)
    file_io_proxy.assert_exists(gcs_filepath)
    new_model_gcs = load_model(gcs_filepath)
    file_io_proxy.delete_file(gcs_filepath)  # cleanup
""" .

"DESCRIPTION.This code saves a machine learning model to a specified directory using the Accelerator class." <EXPLAINS> """CODE.from accelerate import Accelerator
accelerator = Accelerator()
model = ...
accelerator.save_model(model, save_directory)
""" .

"DESCRIPTION.This code saves a model created by the fleet object to a specified path." <EXPLAINS> "CODE.fleet.save_model(\"afs:/user/path/\")" .

"DESCRIPTION.This code saves and loads a trained model in PaddlePaddle framework." <EXPLAINS> """CODE.import paddle
import paddle.static as static

paddle.enable_static()

x = static.data(name="x", shape=[10, 10], dtype='float32')
y = static.nn.fc(x, 10)
z = static.nn.fc(y, 10)

place = paddle.CPUPlace()
exe = static.Executor(place)
exe.run(static.default_startup_program())
prog = static.default_main_program()

path = "./temp/model.pdparams"
paddle.save(prog.state_dict(), path)
state_dict_load = paddle.load(path)
prog.set_state_dict(state_dict_load)""" .

"DESCRIPTION.This code saves and loads persistable variables in the program after running a fully connected neural network layer on input data." <EXPLAINS> """CODE.import paddle
import paddle.static as static
import numpy as np

paddle.enable_static()

x = static.data(name="x", shape=[10, 10], dtype='float32')

y = static.nn.fc(x, 10, name='fc')
place = paddle.CPUPlace()
exe = static.Executor(place)
prog = paddle.static.default_main_program()
exe.run(static.default_startup_program())
inputs = np.ones((10, 10), dtype='float32')
exe.run(prog, feed={'x': inputs}, fetch_list=[y, ])
path = 'temp/tensor_'
for var in prog.list_vars():
    if var.persistable:
        t = var.get_value()
        paddle.save(t, path+var.name+'.pdtensor')

for var in prog.list_vars():
    if var.persistable:
        t_load = paddle.load(path+var.name+'.pdtensor')
        var.set_value(t_load)""" .

"DESCRIPTION.This code saves the variable 'data' as a compressed TIFF file named 'temp.tif' with an additional description tag." <EXPLAINS> """CODE.imsave('temp.tif', data, compress=6,
       extratags=[(270, 's', 0, description, True)])""" .

"DESCRIPTION.This code saves variables to a parameter server using a notification method." <EXPLAINS> """CODE._save_pserver_vars_by_notify(executor=exe,
                dirname=param_path, lookup_table=table_name,
                ps_endpoint_list=ps_endpoints)""" .

"DESCRIPTION.This code segment calculates the dynamic time warping (DTW) between two sets of features, X and Y, and visualizes the alignment between them. It first loads an audio file, extracts chroma features from it, generates some random noise, concatenates the noise and chroma features to create a new feature set Y. Then it computes the DTW between X and Y, plots the cost matrix, optimal warping path, and matching cost function." <EXPLAINS> """CODE.import numpy as np
import matplotlib.pyplot as plt
y, sr = librosa.load(librosa.util.example_audio_file(), offset=10, duration=15)
X = librosa.feature.chroma_cens(y=y, sr=sr)
noise = np.random.rand(X.shape[0], 200)
Y = np.concatenate((noise, noise, X, noise), axis=1)
D, wp = librosa.dtw(X, Y, subseq=True)
plt.subplot(2, 1, 1)
librosa.display.specshow(D, x_axis='frames', y_axis='frames')
plt.title('Database excerpt')
plt.plot(wp[:, 1], wp[:, 0], label='Optimal path', color='y')
plt.legend()
plt.subplot(2, 1, 2)
plt.plot(D[-1, :] / wp.shape[0])
plt.xlim([0, Y.shape[1]])
plt.ylim([0, 2])
plt.title('Matching cost function')
plt.tight_layout()""" .

"DESCRIPTION.This code segment calculates the recall at a precision of 0.8 for a given set of predicted values and ground truth labels. It first computes the recall without any sample weights, and then calculates the recall with specified sample weights. The output of each calculation is the recall value in numpy format." <EXPLAINS> """CODE.m = tf.keras.metrics.RecallAtPrecision(0.8, num_thresholds=1)
_ = m.update_state([0, 0, 1, 1], [0, 0.5, 0.3, 0.9])
m.result().numpy()

m.reset_states()
_ = m.update_state([0, 0, 1, 1], [0, 0.5, 0.3, 0.9],
                   sample_weight=[1, 0, 0, 1])
m.result().numpy()
""" .

"DESCRIPTION.This code segment calls the \"some_trains_function\" method from the \"experiment\" attribute of the \"logger\" instance." <EXPLAINS> "CODE.self.logger.experiment.some_trains_function()" .

"DESCRIPTION.This code segment creates a FillTriangular object using TensorFlow Probability and fills a triangular matrix either in lower or upper triangular form based on the set parameter." <EXPLAINS> """CODE.b = tfb.FillTriangular(upper=False)
b.forward([1, 2, 3, 4, 5, 6])
# ==> [[4, 0, 0],
#      [6, 5, 0],
#      [3, 2, 1]]

b = tfb.FillTriangular(upper=True)
b.forward([1, 2, 3, 4, 5, 6])
# ==> [[1, 2, 3],
#      [0, 5, 6],
#      [0, 0, 4]]
""" .

"DESCRIPTION.This code segment creates a neural network model using the Keras library with specified input and output layers, compiles the model using stochastic gradient descent optimizer with mean squared error loss function, and binary accuracy as the evaluation metric." <EXPLAINS> """CODE.model = keras.Model(inputs, outputs)
model.compile('sgd', loss='mse', metrics=[keras.metrics.BinaryAccuracy()])
""" .

"DESCRIPTION.This code segment creates a tensor using PaddlePaddle library, applies the Sigmoid Linear Unit (SiLU) activation function to each element of the tensor, and returns the transformed tensor." <EXPLAINS> """CODE.import paddle

x = paddle.to_tensor([1.0, 2.0, 3.0, 4.0])
m = paddle.nn.Silu()
out = m(x) # [ 0.731059, 1.761594, 2.857722, 3.928055 ]""" .

"DESCRIPTION.This code segment detects beats in an audio signal 'y', calculates the corresponding timestamps, and saves the beat timings in a CSV file named 'beat_times.csv'." <EXPLAINS> """CODE.tempo, beats = librosa.beat.beat_track(y, sr=sr, hop_length=64)
times = librosa.frames_to_time(beats, sr=sr, hop_length=64)
librosa.output.times_csv('beat_times.csv', times)
""" .

"DESCRIPTION.This code segment encodes a given text input using a pre-trained FlaxMarianMTModel model that translates English to German." <EXPLAINS> """CODE.from transformers import MarianTokenizer, FlaxMarianMTModel
tokenizer = MarianTokenizer.from_pretrained('facebook/marian-large-cnn')
model = FlaxMarianMTModel.from_pretrained('Helsinki-NLP/opus-mt-en-de')
text = "My friends are cool but they eat too many carbs."
inputs = tokenizer(text, max_length=64, return_tensors='jax')
encoder_outputs = model.encode(**inputs)""" .

"DESCRIPTION.This code segment encodes the input text using the T5 model to generate encoder outputs." <EXPLAINS> """CODE.from transformers import T5Tokenizer, FlaxLongT5ForConditionalGeneration
tokenizer = T5Tokenizer.from_pretrained("t5-base")
model = FlaxLongT5ForConditionalGeneration.from_pretrained("google/long-t5-local-base")
text = "My friends are cool but they eat too many carbs."
inputs = tokenizer(text, return_tensors="np")
encoder_outputs = model.encode(**inputs)""" .

"DESCRIPTION.This code segment evaluates the performance of a pre-trained model \"nateraw/vit-base-beans\" on the \"beans\" dataset for the task of image classification. It uses the specified label mapping and evaluation metric (accuracy) to compute the model's performance using a bootstrapping strategy." <EXPLAINS> """CODE.from evaluate import evaluator
from datasets import load_dataset
task_evaluator = evaluator("image-classification")
data = load_dataset("beans", split="test[:40]")
results = task_evaluator.compute(
    model_or_pipeline="nateraw/vit-base-beans",
    data=data,
    label_column="labels",
    metric="accuracy",
    label_mapping={'angular_leaf_spot': 0, 'bean_rust': 1, 'healthy': 2},
    strategy="bootstrap"
)
""" .

"DESCRIPTION.This code segment extracts specific fields from a structured pandas Series containing dictionaries with 'version' and 'project' keys. It allows for extracting fields by either field name, field index, or an expression. Additionally, it demonstrates how to handle nested struct types by passing a list of values to index multiple levels and extract specific fields from the nested structures." <EXPLAINS> """CODE.import pyarrow as pa
s = pd.Series(
...     [
...         {"version": 1, "project": "pandas"},
...         {"version": 2, "project": "pandas"},
...         {"version": 1, "project": "numpy"},
...     ],
...     dtype=pd.ArrowDtype(pa.struct(
...         [("version", pa.int64()), ("project", pa.string())]
...     ))
... )

Extract by field name.

s.struct.field("project")
0    pandas
1    pandas
2     numpy
Name: project, dtype: string[pyarrow]

Extract by field index.

s.struct.field(0)
0    1
1    2
2    1
Name: version, dtype: int64[pyarrow]

Or an expression

import pyarrow.compute as pc
s.struct.field(pc.field("project"))
0    pandas
1    pandas
2     numpy
Name: project, dtype: string[pyarrow]

For nested struct types, you can pass a list of values to index
multiple levels:

version_type = pa.struct([
...     ("major", pa.int64()),
...     ("minor", pa.int64()),
... ])
s = pd.Series(
...     [
...         {"version": {"major": 1, "minor": 5}, "project": "pandas"},
...         {"version": {"major": 2, "minor": 1}, "project": "pandas"},
...         {"version": {"major": 1, "minor": 26}, "project": "numpy"},
...     ],
...     dtype=pd.ArrowDtype(pa.struct(
...         [("version", version_type), ("project", pa.string())]
...     ))
... )
s.struct.field(["version", "minor"])
0     5
1     1
2    26
Name: minor, dtype: int64[pyarrow]
s.struct.field([0, 0])
0    1
1    2
2    1
Name: major, dtype: int64[pyarrow]""" .

"DESCRIPTION.This code segment generates binary content, preuploads LFS files one by one, and creates a commit with all the preuploaded files in a given repository." <EXPLAINS> """CODE.from huggingface_hub import CommitOperationAdd, preupload_lfs_files, create_commit, create_repo

repo_id = create_repo("test_preupload").repo_id

# Generate and preupload LFS files one by one
operations = [] # List of all `CommitOperationAdd` objects that will be generated
for i in range(5):
...     content = ... # generate binary content
...     addition = CommitOperationAdd(path_in_repo=f"shard_{i}_of_5.bin", path_or_fileobj=content)
...     preupload_lfs_files(repo_id, additions=[addition]) # upload + free memory
...     operations.append(addition)

# Create commit
create_commit(repo_id, operations=operations, commit_message="Commit all shards")
""" .

"DESCRIPTION.This code segment implements a fused expert-controlled mixture of experts neural network layer in Python using the PaddlePaddle framework. The FusedEcMoe layer takes input tensors x and gate with shapes [batch_size, sequence_length, d_model] and [batch_size, sequence_length, num_experts] respectively, and applies a mixture of experts operation with 8 experts and GELU activation function to produce an output tensor y with shape [batch_size, sequence_length, d_model]." <EXPLAINS> """CODE.import paddle
from paddle.incubate.nn.layer.fused_ec_moe import FusedEcMoe

x = paddle.randn([10, 128, 1024]) # [bsz, seq_len, d_model]
gate = paddle.randn([10, 128, 8]) # [bsz, seq_len, num_experts]
moe = FusedEcMoe(1024, 4096, 8, act_type="gelu")
y = moe(x, gate)
print(y.shape) # [10, 128, 1024]""" .

"DESCRIPTION.This code segment initializes WandB for tracking experiment metrics and adds WandB visualizations to the ML pipeline UI metadata." <EXPLAINS> """CODE.import wandb
from wandb.integration.kfp.helpers import add_wandb_visualization

with wandb.init() as run:
    add_wandb_visualization(run, mlpipeline_ui_metadata_path)

    ... # the rest of your code here""" .

"DESCRIPTION.This code segment initializes a Camembert tokenizer and a Camembert model for token classification. It encodes a French text \"J'aime le camembert !\" using the tokenizer, creates labels for the tokens, and feeds the input_ids and labels into the model to obtain the loss and scores for token classification." <EXPLAINS> """CODE.tokenizer = CamembertTokenizer.from_pretrained('camembert-base')
model = CamembertForTokenClassification.from_pretrained('camembert-base')
input_ids = torch.tensor(tokenizer.encode("J'aime le camembert !", add_special_tokens=True)).unsqueeze(0)  # Batch size 1
labels = torch.tensor([1] * input_ids.size(1)).unsqueeze(0)  # Batch size 1
outputs = model(input_ids, labels=labels)
loss, scores = outputs[:2]
""" .

"DESCRIPTION.This code segment initializes a tokenizer and a model for sequence classification. It encodes a text input, assigns a label, and then feeds the input and labels into the model to obtain the loss and logits for the prediction." <EXPLAINS> """CODE.tokenizer = XxxTokenizer.from_pretrained('xxx-base-uncased')
model = XxxForSequenceClassification.from_pretrained('xxx-base-uncased')
input_ids = torch.tensor(tokenizer.encode("Hello, my dog is cute")).unsqueeze(0)  # Batch size 1
labels = torch.tensor([1]).unsqueeze(0)  # Batch size 1
outputs = model(input_ids, labels=labels)
loss, logits = outputs[:2]
""" .

"DESCRIPTION.This code segment initializes an ALBERT tokenizer and a pre-trained ALBERT model for sequence classification. It then encodes a text input, \"Hello, my dog is cute\", into tokens with the tokenizer and feeds the tokenized input into the model to obtain the loss and logits for binary classification." <EXPLAINS> """CODE.tokenizer = AlbertTokenizer.from_pretrained('albert-base-v2')
model = AlbertForSequenceClassification.from_pretrained('albert-base-v2')
input_ids = torch.tensor(tokenizer.encode("Hello, my dog is cute")).unsqueeze(0)  # Batch size 1
labels = torch.tensor([1]).unsqueeze(0)  # Batch size 1
outputs = model(input_ids, labels=labels)
loss, logits = outputs[:2]
""" .

"DESCRIPTION.This code segment is responsible for creating a distributed program for training a neural network model. It first creates a serial program with forward pass only, then uses auto completion to annotate the main program. It sets up distributed strategy and rank information, before creating a partitioner for the distributed program. Finally, it creates a distributed program with both forward and backward passes for training, using the Adam optimizer for optimization." <EXPLAINS> """CODE.import paddle.distributed.auto_parallel as auto
from paddle.fluid.distributed_attribute import get_default_distributed_context
from paddle.distributed import fleet
from paddle.distributed.auto_parallel.partitioner import Partitioner

# create serial program with forward only
with static.program_guard(serial_main_program, serial_start_program):
    model = create_model(config)
    tokens = static.data(name="tokens", shape=[batch_size, sequence_len], dtype='int64')
    labels = static.data(name="labels", shape=[batch_size, sequence_len], dtype='int64')
    loss_mask = static.data(name="loss_mask", shape=[batch_size, sequence_len], dtype='int64')
    preds = model(tokens)
    loss = criterion(preds, labels, loss_mask)

# auto completion
auto.ProcessMesh(shape=[2, 4], process_group=[0, 1, 2, 3, 4, 5, 6, 7])
annotated_main_program = auto.complete_annotation(serial_main_program)
auto_paralle_context = get_default_distributed_context()

# distributed strategy & rank info
rank_id = paddle.distributed.get_rank()
dist_strategy = fleet.DistributedStrategy()

# create partitioner
Partitioner = Partitioner(dist_strategy, auto_paralle_context, rank_id)

# create dist program with forward only
# for distributed inference, using partitioned_main_prog from here
partitioned_main_prog, partitioned_startup_prog = Partitioner.transpile_forward(complete_train_program, start_program)

# create dist program with forward/backward/update
# for distributed training, using partitioned_main_prog from here
dist_params_grads = Partitioner.apply_backward(loss, complete_train_program, start_program, partitioned_main_prog, partitioned_startup_prog)
optimizer = paddle.fluid.optimizer.AdamOptimizer(
    learning_rate=0.00001,
    beta1=0.9,
    beta2=0.999,
    epsilon=1e-08,
    grad_clip=None)
opt_ops = Partitioner.apply_optimize(optimizer, dist_params_grads, partitioned_main_prog, partitioned_startup_prog)""" .

"DESCRIPTION.This code segment is using a pre-trained DistilBERT model for token classification tasks. It tokenizes a given input text \"Hello, my dog is cute\", assigns labels to the tokens based on the \"1\" label, and then passes the input_ids and labels to the model to obtain loss and token classification scores." <EXPLAINS> """CODE.tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')
model = DistilBertForTokenClassification.from_pretrained('distilbert-base-uncased')
input_ids = torch.tensor(tokenizer.encode("Hello, my dog is cute")).unsqueeze(0)  # Batch size 1
labels = torch.tensor([1] * input_ids.size(1)).unsqueeze(0)  # Batch size 1
outputs = model(input_ids, labels=labels)
loss, scores = outputs[:2]
""" .

"DESCRIPTION.This code segment loads a pre-trained Blip2 model for image processing from the Salesforce/blip2-opt-2.7b checkpoint and a corresponding processor. It then downloads an image from a specified URL, preprocesses it using the processor, and passes it through the model to obtain image features." <EXPLAINS> """CODE.import torch
from PIL import Image
import requests
from transformers import AutoProcessor, Blip2Model

device = "cuda" if torch.cuda.is_available() else "cpu"

model = Blip2Model.from_pretrained("Salesforce/blip2-opt-2.7b", torch_dtype=torch.float16)

model.to(device)  # doctest: +IGNORE_RESULT

processor = AutoProcessor.from_pretrained("Salesforce/blip2-opt-2.7b")
url = "http://images.cocodataset.org/val2017/000000039769.jpg"
image = Image.open(requests.get(url, stream=True).raw)
inputs = processor(images=image, return_tensors="pt").to(device, torch.float16)
image_outputs = model.get_image_features(**inputs)
""" .

"DESCRIPTION.This code segment loads a pre-trained Xxx language model and tokenizer using TensorFlow and transformers library. It then encodes the input text \"Hello, my dog is cute\" using the tokenizer and passes it through the model to generate prediction scores for masked tokens." <EXPLAINS> """CODE.import tensorflow as tf
from transformers import XxxTokenizer, TFXxxForMaskedLM

tokenizer = XxxTokenizer.from_pretrained('xxx-base-uncased')
model = TFXxxForMaskedLM.from_pretrained('xxx-base-uncased')
input_ids = tf.constant(tokenizer.encode("Hello, my dog is cute"))[None, :]  # Batch size 1
outputs = model(input_ids)
prediction_scores = outputs[0]""" .

"DESCRIPTION.This code segment performs bisecting k-means clustering on a dataset X to divide it into 3 clusters. It then assigns labels to the data points in X, predicts the cluster label for new data points [0, 0] and [12, 3], and identifies the cluster centers of the resulting clusters." <EXPLAINS> """CODE.from sklearn.cluster import BisectingKMeans
import numpy as np
X = np.array([[1, 2], [1, 4], [1, 0],
              [10, 2], [10, 4], [10, 0],
              [10, 6], [10, 8], [10, 10]])
bisect_means = BisectingKMeans(n_clusters=3, random_state=0).fit(X)
bisect_means.labels_
bisect_means.predict([[0, 0], [12, 3]])
bisect_means.cluster_centers_""" .

"DESCRIPTION.This code segment performs index sampling on the input data 'x' based on the indices provided in 'index'." <EXPLAINS> """CODE.import paddle.fluid as fluid
import numpy as np

x = fluid.data(name='x', shape=[-1, 5], dtype='float64')
index = fluid.data(name='index', shape=[-1, 3], dtype='int32')
output = fluid.contrib.layers.index_sample(x=x, index=index)""" .

"DESCRIPTION.This code segment performs tree-based data mining sampling using TDM sampler in PaddlePaddle framework. It defines the structure of a tree with specified leaf nodes, layers, and travel paths, and then samples positive and negative examples based on the provided parameters." <EXPLAINS> """CODE.import paddle.fluid as fluid
import numpy as np
x = fluid.data(name="x", shape=[None, 1], dtype="int32", lod_level=1)
travel_list = [[1, 3], [1, 4], [2, 5], [2, 6]] # leaf node's travel path, shape(leaf_node_num, layer_num)
layer_list_flat = [[1], [2], [3], [4], [5], [6]] # shape(node_nums, 1)

neg_samples_num_list = [0, 0] # negative sample nums = 0
layer_node_num_list = [2, 4] #two layer (exclude root node)
leaf_node_num = 4

travel_array = np.array(travel_list)
layer_array = np.array(layer_list_flat)

sample, label, mask = fluid.contrib.layers.tdm_sampler(
    x,
    neg_samples_num_list,
    layer_node_num_list,
    leaf_node_num,
    tree_travel_attr=fluid.ParamAttr(
        initializer=fluid.initializer.NumpyArrayInitializer(
            travel_array)),
    tree_layer_attr=fluid.ParamAttr(
        initializer=fluid.initializer.NumpyArrayInitializer(
            layer_array)),
    output_positive=True,
    output_list=True,
    seed=0,
    tree_dtype='int32')

place = fluid.CPUPlace()
exe = fluid.Executor(place)
exe.run(fluid.default_startup_program())
xx = np.array([[0],[1]]).reshape((2,1)).astype("int32")

exe.run(feed={"x":xx})""" .

"DESCRIPTION.This code segment prepares a batch of translation inputs for a Marian Machine Translation model to translate English sentences to German. It tokenizes the source texts using the MarianTokenizer and prepares a translation batch using the provided source and target texts. The batch includes input_ids, attention_mask, decoder_input_ids, and decoder_attention_mask, which can be used as input for the translation model." <EXPLAINS> """CODE.from transformers import MarianTokenizer
tok = MarianTokenizer.from_pretrained('Helsinki-NLP/opus-mt-en-de')
src_texts = [ "I am a small frog.", "Tom asked his teacher for advice."]
tgt_texts = ["Ich bin ein kleiner Frosch.", "Tom bat seinen Lehrer um Rat."]  # optional
batch_enc: BatchEncoding = tok.prepare_translation_batch(src_texts, tgt_texts=tgt_texts)
# keys  [input_ids, attention_mask, decoder_input_ids,  decoder_attention_mask].
# model(**batch) should work
""" .

"DESCRIPTION.This code segment reduces the learning rate of a model based on the validation loss during training, with a factor of 0.2 and a patience of 5 epochs." <EXPLAINS> """CODE.reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2,
                              patience=5, min_lr=0.001)
model.fit(X_train, Y_train, callbacks=[reduce_lr])
""" .

"DESCRIPTION.This code segment sends and receives data between nodes in a graph structure using the \"sum\" pooling method." <EXPLAINS> """CODE.import paddle

x = paddle.to_tensor([[0, 2, 3], [1, 4, 5], [2, 6, 7]], dtype="float32")
indexes = paddle.to_tensor([[0, 1], [1, 2], [2, 1], [0, 0]], dtype="int32")
src_index = indexes[:, 0]
dst_index = indexes[:, 1]
out = paddle.incubate.graph_send_recv(x, src_index, dst_index, pool_type="sum")""" .

"DESCRIPTION.This code segment updates the STEPS_TRAINED_COUNTER counter and LEARNER_INFO field in the local iterator context when training the policy on one batch using a multi-GPU version of TrainOneStep." <EXPLAINS> """CODE.Multi-GPU version of TrainOneStep.
This should be used with the .for_each() operator. A tuple of the input
and learner stats will be returned.

Examples:
    rollouts = ParallelRollouts(...)
    train_op = rollouts.for_each(MultiGPUTrainOneStep(workers, ...))
    print(next(train_op))  # This trains the policy on one batch.
    SampleBatch(...), {"learner_stats": ...}

Updates the STEPS_TRAINED_COUNTER counter and LEARNER_INFO field in the
local iterator context.""" .

"DESCRIPTION.This code segment uses a pre-trained ALBERT model to predict the masked words in the input sentence \"Hello, my dog is cute\"." <EXPLAINS> """CODE.import tensorflow as tf
from transformers import AlbertTokenizer, TFAlbertForMaskedLM

tokenizer = AlbertTokenizer.from_pretrained('albert-base-v2')
model = TFAlbertForMaskedLM.from_pretrained('albert-base-v2')
input_ids = tf.constant(tokenizer.encode("Hello, my dog is cute"))[None, :]  # Batch size 1
outputs = model(input_ids)
prediction_scores = outputs[0]""" .

"DESCRIPTION.This code segment uses a pre-trained BlipModel to generate text features for descriptions of images identified by the AutoProcessor." <EXPLAINS> """CODE.from transformers import AutoProcessor, BlipModel

model = BlipModel.from_pretrained("Salesforce/blip-image-captioning-base")
processor = AutoProcessor.from_pretrained("Salesforce/blip-image-captioning-base")

inputs = processor(text=["a photo of a cat", "a photo of a dog"], padding=True, return_tensors="pt")
text_features = model.get_text_features(**inputs)
""" .

"DESCRIPTION.This code segment uses a pre-trained T5 model to summarize the input text by generating a summary based on the provided input text. It utilizes the T5Tokenizer to tokenize the input text and the FlaxLongT5ForConditionalGeneration model to encode and decode the input text to generate the summary. The model takes the input text, encodes it, and then decodes it to produce the summary logits." <EXPLAINS> """CODE.from transformers import T5Tokenizer, FlaxLongT5ForConditionalGeneration
import jax.numpy as jnp

tokenizer = T5Tokenizer.from_pretrained("t5-base")
model = FlaxLongT5ForConditionalGeneration.from_pretrained("google/long-t5-local-base")

text = "summarize: My friends are cool but they eat too many carbs."
inputs = tokenizer(text, return_tensors="np")
encoder_outputs = model.encode(**inputs)

decoder_start_token_id = model.config.decoder_start_token_id
decoder_input_ids = jnp.ones((inputs.input_ids.shape[0], 1), dtype="i4") * decoder_start_token_id

outputs = model.decode(decoder_input_ids, encoder_outputs)
logits = outputs.logits""" .

"DESCRIPTION.This code segment uses the CTRL model and tokenizer from the transformers library to encode a given text input, then feeds it into the model to generate outputs. It calculates the loss and logits based on the input text." <EXPLAINS> """CODE.import torch
from transformers import CTRLTokenizer, CTRLLMHeadModel

tokenizer = CTRLTokenizer.from_pretrained('ctrl')
model = CTRLLMHeadModel.from_pretrained('ctrl')

input_ids = torch.tensor(tokenizer.encode("Links Hello, my dog is cute")).unsqueeze(0)  # Batch size 1
outputs = model(input_ids, labels=input_ids)
loss, logits = outputs[:2]""" .

"DESCRIPTION.This code segment uses the LSHForest algorithm from the sklearn library to fit a model on the training data X_train, and then calculates the distances and indices of the nearest neighbors for the test data X_test." <EXPLAINS> """CODE.from sklearn.neighbors import LSHForest

X_train = [[5, 5, 2], [21, 5, 5], [1, 1, 1], [8, 9, 1], [6, 10, 2]]
X_test = [[9, 1, 6], [3, 1, 10], [7, 10, 3]]
lshf = LSHForest()
lshf.fit(X_train)
distances, indices = lshf.kneighbors(X_test, n_neighbors=2)
distances
indices
""" .

"DESCRIPTION.This code segment utilizes a TranslationTool from the transformers package to translate the input text \"This is a super nice API!\" from English to French." <EXPLAINS> """CODE.from transformers.tools import TranslationTool
translator = TranslationTool()
translator("This is a super nice API!", src_lang="English", tgt_lang="French")""" .

"DESCRIPTION.This code segment utilizes a pre-trained RoBERTa model for token classification. It encodes a input text, \"Hello, my dog is cute\", with the RobertaTokenizer, prepares the labels for the input tokens, and feeds the input_ids and labels into the RoBERTa model for token classification. Finally, it extracts the loss and scores from the model outputs." <EXPLAINS> """CODE.tokenizer = RobertaTokenizer.from_pretrained('roberta-base')
model = RobertaForTokenClassification.from_pretrained('roberta-base')
input_ids = torch.tensor(tokenizer.encode("Hello, my dog is cute", add_special_tokens=True)).unsqueeze(0)  # Batch size 1
labels = torch.tensor([1] * input_ids.size(1)).unsqueeze(0)  # Batch size 1
outputs = model(input_ids, labels=labels)
loss, scores = outputs[:2]
""" .

"DESCRIPTION.This code segment utilizes the CTRL model and tokenizer to generate a prediction for the input text \"Links Hello, my dog is cute\" and calculates the loss based on the predicted output." <EXPLAINS> """CODE.import torch
from transformers import CTRLTokenizer, TFCTRLLMHeadModel

tokenizer = CTRLTokenizer.from_pretrained('ctrl')
model = TFCTRLLMHeadModel.from_pretrained('ctrl')

input_ids = torch.tensor(tokenizer.encode("Links Hello, my dog is cute")).unsqueeze(0)  # Batch size 1
outputs = model(input_ids, labels=input_ids)
loss, logits = outputs[:2]""" .

"DESCRIPTION.This code selects the appropriate code implementation based on the platform being used (CPU, TPU, or other platforms) by calling the platform_dependent function with the corresponding code implementations as arguments." <EXPLAINS> """CODE.def cpu_code(*args): ...
def tpu_code(*args): ...
def other_platforms_code(*args): ...
res = platform_dependent(*args, cpu=cpu_code, tpu=tpu_code,
                         default=other_platforms_code)
""" .

"DESCRIPTION.This code selects the top k elements along a specified dimension of a given tensor and creates a binary tensor where the top k elements are represented as 1." <EXPLAINS> """CODE.def select_topk(prob_tensor, topk, dim):
    values, indices = torch.topk(prob_tensor, topk, dim=dim)
    binary_tensor = torch.zeros_like(prob_tensor, dtype=torch.int32)
    binary_tensor.scatter_(dim, indices, 1)
    return binary_tensor

x = torch.tensor([[1.1, 2.0, 3.0], [2.0, 1.0, 0.5]])
result = select_topk(x, topk=2)
print(result)
""" .

"DESCRIPTION.This code sends a POST request using the requests library and checks the response status code. If the status code indicates an error, it raises a HfHubHTTPError and prints the error message along with additional details returned by the server. It then appends a specific error message to the exception and re-raises it." <EXPLAINS> """CODE.        import requests
        from huggingface_hub.utils import hf_raise_for_status, HfHubHTTPError

        response = requests.post(...)
        try:
            hf_raise_for_status(response)
        except HfHubHTTPError as e:
            print(str(e)) # formatted message
            e.request_id, e.server_message # details returned by server

            # Complete the error message with additional information once it's raised
            e.append_to_message("
`create_commit` expects the repository to exist.")
            raise
    """ .

"DESCRIPTION.This code sends a request to a remote server to deploy a model with the specified model ID and method name." <EXPLAINS> """CODE.response: DeploymentResponse = handle.options(
    method_name="other_method",
    multiplexed_model_id="model:v1",
).remote()""" .

"DESCRIPTION.This code sends a response with the data {\"k\": \"v\"} to a client using the provided scope, receive, and send parameters." <EXPLAINS> "CODE.await Response({\"k\": \"v\"}).send(scope, receive, send)" .

"DESCRIPTION.This code serializes different activation functions in TensorFlow Keras." <EXPLAINS> """CODE.tf.keras.activations.serialize(tf.keras.activations.tanh)
tf.keras.activations.serialize(tf.keras.activations.sigmoid)
tf.keras.activations.serialize('abcd')""" .

"DESCRIPTION.This code sets and modifies the display font options for pandas, specifically changing the font color to red and size to 6pt." <EXPLAINS> """CODE.import pandas.core.config as cf
with cf.config_prefix("display.font"):
    cf.register_option("color", "red")
    cf.register_option("size", " 5 pt")
    cf.set_option(size, " 6 pt")
    cf.get_option(size)""" .

"DESCRIPTION.This code sets environment variables `my_var` to `None` and `other_var` to `'some_value'`, then prints whether `my_var` is set and the value of `other_var` in the environment." <EXPLAINS> """CODE.with set_env(my_var=None, other_var='some_value'):
    print("my_var is set:", 'my_var' in os.environ)
    print("other_var =", os.environ['other_var'])""" .

"DESCRIPTION.This code sets query parameters for a Streamlit app, specifying to show a map and selecting the continents \"Asia\" and \"America\"." <EXPLAINS> """CODE.st.experimental_set_query_params(
    show_map=True,
    selected=["asia", "america"],
)""" .

"DESCRIPTION.This code sets query parameters for a Streamlit application, specifying that a map should be displayed and that \"asia\" and \"america\" should be selected on the map." <EXPLAINS> """CODE.st.experimental_set_query_params(
    show_map=True,
    selected=["asia", "america"],
)""" .

"DESCRIPTION.This code sets query parameters for the Streamlit app, showing a map and selecting continents \"asia\" and \"america\"." <EXPLAINS> """CODE.st.experimental_set_query_params(
    show_map=True,
    selected=["asia", "america"],
)""" .

"DESCRIPTION.This code sets the \"server.headless\" configuration option to True temporarily using the patch_config_options context manager and asserts that the option has been successfully set to True. It then runs other test code that depends on these configuration options." <EXPLAINS> """CODE.with patch_config_options({"server.headless": True}):
    assert(config.get_option("server.headless") is True)
    # Other test code that relies on these options""" .

"DESCRIPTION.This code sets the encoding information for an audio file, including the encoding type, bits per sample, compression, and various options like reversing bytes, nibbles, and bits, as well as setting the endianness of the audio data." <EXPLAINS> """CODE.ei = torchaudio.sox_encodinginfo_t()
ei.encoding = torchaudio.get_sox_encoding_t(1)
ei.bits_per_sample = 16
ei.compression = 0
ei.reverse_bytes = torchaudio.get_sox_option_t(2)
ei.reverse_nibbles = torchaudio.get_sox_option_t(2)
ei.reverse_bits = torchaudio.get_sox_option_t(2)
ei.opposite_endian = torchaudio.get_sox_bool(0)
""" .

"DESCRIPTION.This code sets the environmental variables my_var to None and other_var to 'some_value', and then prints whether my_var is set as an environmental variable and the value of other_var in the os environment." <EXPLAINS> """CODE.with set_env(my_var=None, other_var='some_value'):
    print("my_var is set:", 'my_var' in os.environ)
    print("other_var =", os.environ['other_var'])""" .

"DESCRIPTION.This code sets the epsilon value for the Keras backend to 1e-05." <EXPLAINS> """CODE.from keras import backend as K
K.set_epsilon(1e-05)
""" .

"DESCRIPTION.This code sets the floating point data type for numerical computation in Keras to 'float16'." <EXPLAINS> """CODE.from keras import backend as K
K.set_floatx('float16')
""" .

"DESCRIPTION.This code sets the image data format to 'channels_last' in the Keras backend." <EXPLAINS> """CODE.from keras import backend as K
K.set_image_data_format('channels_last')
""" .

"DESCRIPTION.This code sets the names of the index levels in the dataframe to \"bar\" and \"baz\" respectively." <EXPLAINS> """CODE.df._set_axis_name("foo")
     A
foo
0    1
1    2
2    3
df.index = pd.MultiIndex.from_product([['A'], ['a', 'b', 'c']])
df._set_axis_name(["bar", "baz"])
         A
bar baz
A   a    1
    b    2
    c    3""" .

"DESCRIPTION.This code sets the page configuration for a Streamlit web application by providing a title, icon, layout, and initial sidebar state." <EXPLAINS> """CODE.st.beta_set_page_config(
    page_title="Ex-stream-ly Cool App",
    page_icon="ð§",
    layout="wide",
    initial_sidebar_state="expanded",
)""" .

"DESCRIPTION.This code sets the timezone to 'US/Eastern' and retrieves the timezone name for the current datetime." <EXPLAINS> """CODE.from datetime import datetime
from dateutil.tz import tzlocal

with set_timezone('US/Eastern'):
    tzlocal().tzname(datetime.now())""" .

"DESCRIPTION.This code sets the traffic distribution between two versions of a service named \"service-name\" to be 50% each." <EXPLAINS> """CODE.serve.set_traffic("service-name", {
    "backend:v1": 0.5,
    "backend:v2": 0.5
})""" .

"DESCRIPTION.This code sets the traffic distribution for a service named \"service-name\" between two different backend versions, \"backend:v1\" and \"backend:v2\", with a distribution of 50% for each version." <EXPLAINS> """CODE.serve.set_traffic("service-name", {
    "backend:v1": 0.5,
    "backend:v2": 0.5
})""" .

"DESCRIPTION.This code sets the weight of the decoder equal to the weight of the encoder." <EXPLAINS> """CODE.def on_post_move_to_device(self):
    self.decoder.weight = self.encoder.weight""" .

"DESCRIPTION.This code sets up a TPUStrategy for distributing a dataset on a TPU machine. It connects to a TPU cluster resolver, initializes the TPU system, creates a TPUStrategy, and distributes the dataset for training on the TPU machine." <EXPLAINS> """CODE.# Setup TPUStrategy
resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='')
tf.config.experimental_connect_to_cluster(resolver)
tf.tpu.experimental.initialize_tpu_system(resolver)
strategy = tf.distribute.TPUStrategy(resolver)

dataset = tf.data.Dataset.range(16)
distributed_dataset_on_host = (
    strategy.experimental_distribute_dataset(
        dataset,
        tf.distribute.InputOptions(
            experimental_prefetch_to_device=False)))
""" .

"DESCRIPTION.This code sets up a callback function that will run after a future task has finished. The future task is created using torch.futures.Future() and the result is set to 5. When the future task is completed, the callback function prints \"This will run after the future has finished.\" and the result of the future task, which is 5 in this case." <EXPLAINS> """CODE.import torch

    def callback(fut):
        print(f"This will run after the future has finished.")
        print(fut.wait())

    fut = torch.futures.Future()
    fut.add_done_callback(callback)
    fut.set_result(5)

    # Outputs are:
    This will run after the future has finished.
    5""" .

"DESCRIPTION.This code sets up a callback function that will run after a torch future object has finished. The callback function prints a message and the result of the future object." <EXPLAINS> """CODE.import torch

    def callback(fut):
        print(f"This will run after the future has finished.")
        print(fut.wait())

    fut = torch.futures.Future()
    fut.add_done_callback(callback)
    fut.set_result(5)

    # Outputs are:
    This will run after the future has finished.
    5""" .

"DESCRIPTION.This code sets up a convolutional neural network using PaddlePaddle framework on NPU (Ascend AI processor). It trains the network for 8 epochs using random input data and records the profiling information in a file named 'npu.txt'." <EXPLAINS> """CODE.import paddle.fluid as fluid
import paddle.fluid.profiler as profiler
import numpy as np

epoc = 8
dshape = [4, 3, 28, 28]
data = fluid.data(name='data', shape=[None, 3, 28, 28], dtype='float32')
conv = fluid.layers.conv2d(data, 20, 3, stride=[1, 1], padding=[1, 1])

place = fluid.NPUPlace(0)
exe = fluid.Executor(place)
exe.run(fluid.default_startup_program())

output_file = 'npu.txt'
with profiler.npu_profiler(output_file) as npu_prof:
    for i in range(epoc):
        input = np.random.random(dshape).astype('float32')
        exe.run(fluid.default_main_program(), feed={'data': input})""" .

"DESCRIPTION.This code sets up a data parallelism environment using PaddlePaddle's dygraph module. It defines a linear neural network layer with softmax activation, initializes an Adam optimizer, and scales the loss based on the number of trainers. It then computes the gradients, applies collective gradients, minimizes the loss using Adam optimizer, and clears the gradients for the next iteration." <EXPLAINS> """CODE.import numpy as np
import paddle.fluid as fluid
import paddle.fluid.dygraph as dygraph
from paddle.fluid.optimizer import AdamOptimizer
from paddle.fluid.dygraph.nn import Linear
from paddle.fluid.dygraph.base import to_variable

place = fluid.CUDAPlace(fluid.dygraph.ParallelEnv().dev_id)
with fluid.dygraph.guard(place=place):

    # prepare the data parallel context
    strategy=dygraph.prepare_context()

    linear = Linear(1, 10, act="softmax")
    adam = fluid.optimizer.AdamOptimizer()

    # make the module become the data parallelism module
    linear = dygraph.DataParallel(linear, strategy)

    x_data = np.random.random(size=[10, 1]).astype(np.float32)
    data = to_variable(x_data)

    hidden = linear(data)
    avg_loss = fluid.layers.mean(hidden)

    # scale the loss according to the number of trainers.
    avg_loss = linear.scale_loss(avg_loss)

    avg_loss.backward()

    # collect the gradients of trainers.
    linear.apply_collective_grads()

    adam.minimize(avg_loss)
    linear.clear_gradients()""" .

"DESCRIPTION.This code sets up a logger to display information messages using tqdm progress bar, and then logs 'No visual artifacts' message 10 times." <EXPLAINS> """CODE.logger = logging.getLogger()
logger.addHandler(logging.StreamHandler(TqdmStream()))

for _ in tqdm.tqdm(range(10)):
  logger.info('No visual artifacts')
""" .

"DESCRIPTION.This code sets up a neural network model to process sequences of data. It creates input columns for rating and watches data, then parses the input features and constructs an input layer. It defines a basic RNN cell with a specified hidden size, and uses dynamic_rnn to process the input data through the RNN cell and return outputs and states." <EXPLAINS> """CODE.rating = sequence_numeric_column('rating')
watches = sequence_categorical_column_with_identity(
    'watches', num_buckets=1000)
watches_embedding = embedding_column(watches, dimension=10)
columns = [rating, watches]

features = tf.parse_example(..., features=make_parse_example_spec(columns))
input_layer, sequence_length = sequence_input_layer(features, columns)

rnn_cell = tf.nn.rnn_cell.BasicRNNCell(hidden_size)
outputs, state = tf.nn.dynamic_rnn(
    rnn_cell, inputs=input_layer, sequence_length=sequence_length)
""" .

"DESCRIPTION.This code sets up a web server that listens for webhook events. When a dataset is updated, it triggers a training job." <EXPLAINS> """CODE.from huggingface_hub import WebhooksServer, WebhookPayload

app = WebhooksServer()

@app.add_webhook
async def trigger_training(payload: WebhookPayload):
    if payload.repo.type == "dataset" and payload.event.action == "update":
        # Trigger a training job if a dataset is updated
        ...

app.run()
""" .

"DESCRIPTION.This code sets up an IpuStrategy for training a model with specified pipelining configuration, batching per step, and gradient accumulation settings." <EXPLAINS> """CODE.import paddle
import paddle.static as static

paddle.enable_static()

ipu_strategy = static.IpuStrategy()
ipu_strategy.set_pipelining_config(enable_pipelining=False,
                                    batches_per_step=1,
                                    enable_gradient_accumulation=False,
                                    accumulation_factor=1)""" .

"DESCRIPTION.This code sets up the data for training, validation, and testing by loading the data and splitting it into three datasets." <EXPLAINS> """CODE.def setup(self, stage):
    data = load_data(...)
    self.train_ds, self.val_ds, self.test_ds = split_data(data)""" .

"DESCRIPTION.This code simply returns the string 'tensorflow'." <EXPLAINS> """CODE.keras.backend.backend()
'tensorflow'
""" .

"DESCRIPTION.This code snippet aims to perform training of a neural network model using Stochastic Gradient Descent (SGD) optimizer with a specified learning rate. It utilizes a dynamic loss scaling technique to mitigate numerical instability during training. The code calculates the logits output of the model, computes the softmax cross-entropy loss, applies the dynamic loss scaling, computes the gradients, and updates the model parameters using SGD optimization." <EXPLAINS> """CODE.opt = tf.keras.optimizers.SGD(1.0)
model_loss_scale = tf.mixed_precision.experimental.DynamicLossScale()

for step in training_steps:
  with LossScaleGradientTape(model_loss_scale) as tape:
    logits = ...  # Run model and get logits
    loss = tf.nn.softmax_cross_entropy_with_logits(logits=logits,
                                                   labels=labels)
    loss = tf.reduce_mean(loss)
  vars = tape.watched_variables()
  grads = tape.gradient(loss, vars)
  opt.apply_gradients(zip(grads, vars))
""" .

"DESCRIPTION.This code snippet calculates the byte size of the data type torch.float32, which is 4 bytes." <EXPLAINS> """CODE.dtype_byte_size(torch.float32)
4
""" .

"DESCRIPTION.This code snippet calculates the mean squared error (MSE) between the true values (y_true) and the predicted values (y_pred). The code snippet includes examples of calculating MSE with different parameters such as sample weights, sum reduction, and no reduction. Additionally, there is a model compilation using stochastic gradient descent (sgd) optimizer and MSE as the loss function." <EXPLAINS> """CODE.mse = tf.keras.losses.MeanSquaredError()
mse(y_true, y_pred).numpy()
0.5

mse(y_true, y_pred, sample_weight=[0.7, 0.3]).numpy()
0.25

mse = tf.keras.losses.MeanSquaredError(reduction=tf.keras.losses.Reduction.SUM)
mse(y_true, y_pred).numpy()
1.0

mse = tf.keras.losses.MeanSquaredError(reduction=tf.keras.losses.Reduction.NONE)
mse(y_true, y_pred).numpy()
array([0.5, 0.5], dtype=float32)

model.compile(optimizer='sgd', loss=tf.keras.losses.MeanSquaredError())
""" .

"DESCRIPTION.This code snippet clears the custom objects dictionary and then assigns a key-value pair where the key is 'MyObject' and the value is an instance of the MyObject class." <EXPLAINS> """CODE.get_custom_objects().clear()
get_custom_objects()['MyObject'] = MyObject
""" .

"DESCRIPTION.This code snippet constructs a neural network model with two dense layers, the first layer has 4 units and the second layer has 2 units. The activation function used is ReLU for the first layer and log softmax for the second layer. The input data is passed through this model represented by the variable x." <EXPLAINS> """CODE.nn.Sequential([nn.Dense(4),
                            nn.relu,
                            nn.Dense(2),
                            nn.log_softmax])(x)""" .

"DESCRIPTION.This code snippet converts an input tensor into a one-hot encoded tensor." <EXPLAINS> """CODE.to_onehot(x)
    x = torch.tensor([1, 2, 3])
    to_onehot(x)
    tensor([[0, 1, 0, 0],
            [0, 0, 1, 0],
            [0, 0, 0, 1]])""" .

"DESCRIPTION.This code snippet converts musical keys to the corresponding degrees in a scale." <EXPLAINS> """CODE.librosa.key_to_degrees('C:maj')
array([ 0,  2,  4,  5,  7,  9, 11])
librosa.key_to_degrees('C#:maj')
array([ 1,  3,  5,  6,  8, 10,  0])
librosa.key_to_degrees('A:min')
array([ 9, 11,  0,  2,  4,  5,  7])""" .

"DESCRIPTION.This code snippet counts the number of occurrences of a specified element in a list." <EXPLAINS> """CODE.pbag([]).count('non-existent')
0
pbag([1, 1, 2]).count(1)""" .

"DESCRIPTION.This code snippet counts the number of occurrences of the integer 1 in a deque with initial elements [1, 2, 1]." <EXPLAINS> "CODE.pdeque([1, 2, 1]).count(1)" .

"DESCRIPTION.This code snippet counts the occurrences of a specified element within a list." <EXPLAINS> """CODE.pbag([]).count('non-existent')
0
pbag([1, 1, 2]).count(1)""" .

"DESCRIPTION.This code snippet creates a 3x4 tensor filled with zeros using the Keras backend and evaluates the tensor to get the corresponding array of zeros." <EXPLAINS> """CODE.from keras import backend as K
kvar = K.zeros((3,4))
K.eval(kvar)
array([[ 0.,  0.,  0.,  0.],
       [ 0.,  0.,  0.,  0.],
       [ 0.,  0.,  0.,  0.]], dtype=float32)
""" .

"DESCRIPTION.This code snippet creates a CSV logger object to log the training progress of a model using the provided training data. The model is then trained using the training data, and the CSV logger is used as a callback to log the training information to a CSV file named 'training.log'." <EXPLAINS> """CODE.csv_logger = CSVLogger('training.log')
model.fit(X_train, Y_train, callbacks=[csv_logger])
""" .

"DESCRIPTION.This code snippet creates a Qwen2Tokenizer object from the pretrained \"Qwen/Qwen-tokenizer\" model and then uses it to tokenize the input text \"Hello world\" and \" Hello world\", returning the input ids for each tokenized text." <EXPLAINS> """CODE.from transformers import Qwen2Tokenizer

tokenizer = Qwen2Tokenizer.from_pretrained("Qwen/Qwen-tokenizer")
tokenizer("Hello world")["input_ids"]
[9707, 1879]

tokenizer(" Hello world")["input_ids"]
[21927, 1879]
""" .

"DESCRIPTION.This code snippet creates a TensorFlow constant array with specific values, applies the hyperbolic tangent activation function to each element of the array using Keras, and then returns the resulting array as a numpy array." <EXPLAINS> """CODE.a = tf.constant([-3.0,-1.0, 0.0,1.0,3.0], dtype = tf.float32)
b = tf.keras.activations.tanh(a)
b.numpy()""" .

"DESCRIPTION.This code snippet creates a context manager named \"Context\" with the \"cli\" as an argument. Within this context, it retrieves information in the form of a dictionary using the method \"to_info_dict()\" from the context and assigns it to the variable \"info\"." <EXPLAINS> """CODE.with Context(cli) as ctx:
    info = ctx.to_info_dict()""" .

"DESCRIPTION.This code snippet creates a custom HTTP header named 'headers' with the initial key-value pair 'foo'-'bar'. It then adds another key-value pair 'Foo'-'baz' to the 'headers' dictionary. Finally, it accesses the value associated with the key 'foo'." <EXPLAINS> """CODE.headers = HTTPHeaderDict(foo='bar')
headers.add('Foo', 'baz')
headers['foo']""" .

"DESCRIPTION.This code snippet creates a linear model for making predictions using weighted categorical features." <EXPLAINS> """CODE.categorical_column = categorical_column_with_hash_bucket(
    column_name='terms', hash_bucket_size=1000)
weighted_column = weighted_categorical_column(
    categorical_column=categorical_column, weight_feature_key='frequencies')
columns = [weighted_column, ...]
features = tf.parse_example(..., features=make_parse_example_spec(columns))
linear_prediction, _, _ = linear_model(features, columns)
""" .

"DESCRIPTION.This code snippet creates a linear neural network model using PaddlePaddle, trains it using stochastic gradient descent (SGD) optimizer with clipping of gradients by global norm to prevent exploding gradients, and updates the model parameters based on the computed gradients." <EXPLAINS> """CODE.import paddle

x = paddle.uniform([10, 10], min=-1.0, max=1.0, dtype='float32')
linear = paddle.nn.Linear(in_features=10, out_features=10,
                          weight_attr=paddle.ParamAttr(need_clip=True),
                          bias_attr=paddle.ParamAttr(need_clip=False))
out = linear(x)
loss = paddle.mean(out)
loss.backward()

clip = paddle.nn.ClipGradByGlobalNorm(clip_norm=1.0)
sdg = paddle.optimizer.SGD(learning_rate=0.1, parameters=linear.parameters(), grad_clip=clip)
sdg.step()""" .

"DESCRIPTION.This code snippet creates a minimal evaluation result for a model by specifying its task type, dataset type, dataset name, metric type, and metric value. The function `eval_results_to_model_index` then associates these evaluation results with a given model, organizing them into a dictionary format." <EXPLAINS> """CODE.from huggingface_hub.repocard_data import eval_results_to_model_index, EvalResult
# Define minimal eval_results
eval_results = [
...     EvalResult(
...         task_type="image-classification",  # Required
...         dataset_type="beans",  # Required
...         dataset_name="Beans",  # Required
...         metric_type="accuracy",  # Required
...         metric_value=0.9,  # Required
...     )
... ]
eval_results_to_model_index("my-cool-model", eval_results)
[{'name': 'my-cool-model', 'results': [{'task': {'type': 'image-classification'}, 'dataset': {'name': 'Beans', 'type': 'beans'}, 'metrics': [{'type': 'accuracy', 'value': 0.9}]}]}
""" .

"DESCRIPTION.This code snippet creates a neural network model using keras library, setting the optimizer as stochastic gradient descent (sgd), loss function as mean squared error (mse), and evaluation metric as binary accuracy." <EXPLAINS> """CODE.model = keras.Model(inputs, outputs)
model.compile('sgd', loss='mse', metrics=[keras.metrics.BinaryAccuracy()])
""" .

"DESCRIPTION.This code snippet creates a parser for parsing dates in ISO 8601 format and converts the parsed date string into a date object. It then parses the string \"1999-12-31\" and prints out the parsed date as a date object." <EXPLAINS> """CODE.date_expr = pyparsing_common.iso8601_date.copy()
date_expr.setParseAction(pyparsing_common.convertToDate())
print(date_expr.parseString("1999-12-31"))""" .

"DESCRIPTION.This code snippet creates a placeholder tensor with shape (2, 4, 5) using the Keras backend, then initializes a variable tensor with values [[1, 2], [3, 4]] using the same backend." <EXPLAINS> """CODE.from keras import backend as K
input = K.placeholder(shape=(2, 4, 5))
K.int_shape(input)
val = np.array([[1, 2], [3, 4]])
kvar = K.variable(value=val)
K.int_shape(kvar)
""" .

"DESCRIPTION.This code snippet creates a rating sequence numeric column and a watches sequence categorical column with an identity mapping function. It then creates an embedding layer for the watches column with a dimension of 10. The code defines a list of columns and uses them to parse examples and create a feature input layer for a Recurrent Neural Network (RNN). Finally, it sets up an RNN cell with a basic RNN cell type and uses it to perform dynamic RNN computation on the input sequence data." <EXPLAINS> """CODE.rating = sequence_numeric_column('rating')
watches = sequence_categorical_column_with_identity(
    'watches', num_buckets=1000)
watches_embedding = embedding_column(watches, dimension=10)
columns = [rating, watches]

features = tf.parse_example(..., features=make_parse_example_spec(columns))
input_layer, sequence_length = sequence_input_layer(features, columns)

rnn_cell = tf.nn.rnn_cell.BasicRNNCell(hidden_size)
outputs, state = tf.nn.dynamic_rnn(
    rnn_cell, inputs=input_layer, sequence_length=sequence_length)
""" .

"DESCRIPTION.This code snippet creates a remote actor object that inherits from the TaskRunnerBackend class." <EXPLAINS> """CODE.@ray.remote
class TaskRunnerActor(TaskRunnerBackend):
    pass""" .

"DESCRIPTION.This code snippet creates a tensor filled with ones of shape (1,3) using TensorFlow's backend, then casts the tensor to dtype float64 and prints the results of both the original and casted tensors." <EXPLAINS> """CODE.input = tf.keras.backend.ones(shape=(1,3))
print(input)
<tf.Variable 'Variable:0' shape=(1, 3) dtype=float32,
numpy=array([[1., 1., 1.]], dtype=float32)>
cast_input = tf.keras.backend.cast(input, dtype='float64')
print(cast_input)
tf.Tensor([[1. 1. 1.]], shape=(1, 3), dtype=float64)""" .

"DESCRIPTION.This code snippet creates a tensor of embeddings and then calculates the similarity between the embeddings." <EXPLAINS> """CODE.embeddings = torch.tensor([[1., 2., 3., 4.], [1., 2., 3., 4.], [4., 5., 6., 7.]])
embedding_similarity(embeddings)""" .

"DESCRIPTION.This code snippet creates and utilizes the TrueNegatives metric from the tf.keras.metrics module in TensorFlow. The TrueNegatives metric calculates the number of true negative predictions in a binary classification problem. The code first initializes the TrueNegatives metric, updates its state with predicted and true labels, retrieves and prints the result as a numpy array. Then, it resets the metric's state, updates it with predicted and true labels along with sample weights, retrieves and prints the result as a numpy array. Finally, the code compiles a model using stochastic gradient descent optimizer, mean squared error loss function, and the TrueNegatives metric for evaluation." <EXPLAINS> """CODE.m = tf.keras.metrics.TrueNegatives()
m.update_state([0, 1, 0, 0], [1, 1, 0, 0])
m.result().numpy()
2.0

m.reset_state()
m.update_state([0, 1, 0, 0], [1, 1, 0, 0], sample_weight=[0, 0, 1, 0])
m.result().numpy()
1.0
model.compile(optimizer='sgd',
              loss='mse',
              metrics=[tf.keras.metrics.TrueNegatives()])
""" .

"DESCRIPTION.This code snippet defines a class called LitAutoEncoder with an encoder and decoder components inside it." <EXPLAINS> """CODE.LitAutoEncoder()  # doctest: +NORMALIZE_WHITESPACE
LitAutoEncoder(
  (encoder): ...
  (decoder): ...
)""" .

"DESCRIPTION.This code snippet defines a function 'to_tensor' that converts a numpy array batch to a torch tensor. The function then performs permutation of dimensions, normalization of pixel values, and returns the processed tensor. This tensor transformation function is used as part of a data preprocessing pipeline that includes resizing images to (224, 224) dimensions and converting them to TorchVision preprocessed format." <EXPLAINS> """CODE.def to_tensor(batch: np.ndarray) -> torch.Tensor:
    tensor = torch.as_tensor(batch, dtype=torch.float)
    # (B, H, W, C) -> (B, C, H, W)
    tensor = tensor.permute(0, 3, 1, 2).contiguous()
    # [0., 255.] -> [0., 1.]
    tensor = tensor.div(255)
    return tensor
transform = transforms.Compose([
    transforms.Lambda(to_tensor),
    transforms.Resize((224, 224))
])
preprocessor = TorchVisionPreprocessor(
    ["image"], transform=transform, batched=True
)
preprocessor.transform(dataset)  # doctest: +ellipsis
Dataset(num_blocks=..., num_rows=..., schema={image: ArrowTensorType(shape=(3, 224, 224), dtype=float)})
""" .

"DESCRIPTION.This code snippet defines a function called unpackrgb that takes in raw data, data type, bits per sample, and an optional parameter rescale. The function unpacks the raw data into individual samples using struct.unpack, potentially rescales the samples based on the rescale parameter, and returns an array of the unpacked samples. The code then creates some example data using struct.pack and calls the unpackrgb function with different arguments to unpack and potentially rescale the data samples." <EXPLAINS> """CODE.import struct
import numpy as np

def unpackrgb(data, dtype, bitspersample, rescale=True):
    result = []
    for i in range(0, len(data), len(bitspersample)):
        sample = struct.unpack(dtype, data[i:i+len(bitspersample)])[0]
        if rescale:
            sample = sample * (2**len(bitspersample) - 1) // 255
        result.append(sample)
    return np.array(result)

data = struct.pack('BBBB', 0x21, 0x08, 0xff, 0xff)
print(unpackrgb(data, '<B', (5, 6, 5), False))
print(unpackrgb(data, '<B', (5, 6, 5)))
print(unpackrgb(data, '<B', (5, 5, 5)))
""" .

"DESCRIPTION.This code snippet defines a function that checks if the input value is a scalar NaN (Not a Number) by comparing it to np.nan, and returns True if it is a scalar NaN, and False otherwise." <EXPLAINS> """CODE.is_scalar_nan(np.nan)
True
is_scalar_nan(float("nan"))
True
is_scalar_nan(None)
False
is_scalar_nan("")
False
is_scalar_nan([np.nan])
False""" .

"DESCRIPTION.This code snippet defines a function that takes a frequency as input and returns the corresponding resolution abbreviation. It then checks if calling the function with the frequency 'H' returns the abbreviation 'HR'." <EXPLAINS> """CODE.Resolution.get_reso_from_freq('H')
Resolution.get_reso_from_freq('H') == Resolution.RESO_HR""" .

"DESCRIPTION.This code snippet defines a neural network model using the specified inputs and outputs, compiles the model using stochastic gradient descent (sgd) optimizer and SquaredHinge loss function." <EXPLAINS> """CODE.model = keras.Model(inputs, outputs)
model.compile('sgd', loss=keras.losses.SquaredHinge())
""" .

"DESCRIPTION.This code snippet defines and uses the SpecificityAtSensitivity metric from the TensorFlow Keras library. The SpecificityAtSensitivity metric calculates the specificity given a certain sensitivity threshold. It can be updated with predicted and actual values, with an optional sample weight parameter. The metric value can be retrieved using the result method and can be reset with the reset_state method. Additionally, the metric can be used in model compilation as part of the metrics list." <EXPLAINS> """CODE.m = tf.keras.metrics.SpecificityAtSensitivity(0.5)
m.update_state([0, 0, 0, 1, 1], [0, 0.3, 0.8, 0.3, 0.8])
m.result().numpy()
0.66666667

m.reset_state()
m.update_state([0, 0, 0, 1, 1], [0, 0.3, 0.8, 0.3, 0.8],
...                sample_weight=[1, 1, 2, 2, 2])
m.result().numpy()
0.5

model.compile(
    optimizer='sgd',
    loss='mse',
    metrics=[tf.keras.metrics.SpecificityAtSensitivity()])
""" .

"DESCRIPTION.This code snippet defines that the function \"Size\" is not differentiable in TensorFlow." <EXPLAINS> """CODE.tf.NotDifferentiable("Size")
""",
        """CODE.tf.no_gradient("Size")
""" .

"DESCRIPTION.This code snippet defines the softsign function, which computes the softsign transformation of an input tensor x. The forward pass computes x / (1 + abs(x)), while the inverse pass computes x / (1 - abs(x))." <EXPLAINS> """CODE.# Create the Y = softsign(X) transform.
softsign = Softsign()
x = [[[1., 2],
      [3, 4]],
     [[5, 6],
      [7, 8]]]
x / (1 + abs(x)) == softsign.forward(x)
x / (1 - abs(x)) == softsign.inverse(x)
""" .

"DESCRIPTION.This code snippet demonstrates the functionality of Python's Counter class. It allows you to create a counter object to count the occurrences of elements in a given iterable. The code performs operations such as finding the most common elements, sorting the elements, joining and sorting elements, getting the sum of all counts, updating counts, deleting elements, and clearing the counter object." <EXPLAINS> """CODE.c = Counter('abcdeabcdabcaba')
c.most_common(3)
sorted(c)
''.join(sorted(c.elements()))
sum(c.values())
c['a']
for elem in 'shazam':
...     c[elem] += 1
c['a']
del c['b']
c['b']
d = Counter('simsalabim')
c.update(d)
c['a']
c.clear()
c
Counter()
c = Counter('aaabbc')
c['b'] -= 2
c.most_common()                """ .

"DESCRIPTION.This code snippet demonstrates the functionality of popping elements from a custom implementation of a deque data structure. It showcases different scenarios such as popping from an empty deque, popping with different indices, and the resulting deque after each operation." <EXPLAINS> """CODE.pdeque([1, 2]).pop()
pdeque([1])
pdeque([1, 2]).pop(2)
pdeque([])
pdeque([1, 2]).pop(-1)
pdeque([2])""" .

"DESCRIPTION.This code snippet demonstrates the functionality of the QuotedString class in parsing and extracting quoted strings from given input strings." <EXPLAINS> "CODE.[['qs = QuotedString(\\'\"\\')\\nprint(qs.searchString(\\'lsjdf \"This is the quote\" sldjf\\'))\\ncomplex_qs = QuotedString(\\'{{\\', endQuoteChar=\\'}}\\')\\nprint(complex_qs.searchString(\\'lsjdf {{This is the \"quote\"}} sldjf\\'))\\nsql_qs = QuotedString(\\'\"\\', escQuote=\\'\"\"\\')\\nprint(sql_qs.searchString(\\'lsjdf \"This is the quote with \"\"embedded\"\" quotes\" sldjf\\'))']]" .

"DESCRIPTION.This code snippet demonstrates two ways to acquire and release a lock in Python using context manager or try-finally construct. The code ensures that the resources protected by the lock are properly released after they are no longer needed." <EXPLAINS> """CODE.# You can use this method in the context manager (recommended)
with lock.acquire():
    pass

# Or use an equivalent try-finally construct:
lock.acquire()
try:
    pass
finally:
    lock.release()
""" .

"DESCRIPTION.This code snippet enables and disables dynamic graph mode in PaddlePaddle by using the functions enable_dygraph() and disable_dygraph(). It then prints out the current state of dynamic graph mode, which will be True after enabling and False after disabling." <EXPLAINS> """CODE.import paddle.fluid as fluid

fluid.enable_dygraph()  # Now we are in dygragh mode
print(fluid.dygraph.enabled())  # True
fluid.disable_dygraph()
print(fluid.dygraph.enabled())  # False""" .

"DESCRIPTION.This code snippet extracts a substring of length 8 from the input string 'abcdefghijklmnop'." <EXPLAINS> "CODE.snipstr('abcdefghijklmnop', 8)" .

"DESCRIPTION.This code snippet fetches data from a table called \"mytable\" using a Snowpark session, limits the retrieved data to 10 rows, converts it to a pandas DataFrame, and then displays the DataFrame using the Streamlit library." <EXPLAINS> """CODE.import streamlit as st

session = st.experimental_connection("snowpark").session
df = session.table("mytable").limit(10).to_pandas()
st.dataframe(df)""" .

"DESCRIPTION.This code snippet generates a dataset with a single element of value 42, creates an iterator for the dataset, retrieves the next element from the iterator as an optional value, and checks if the optional value has a value. The code then prints the result of whether the optional value has a value and the value of the optional value (42 in this case). Finally, it retrieves the next element from the iterator again and checks if the optional value has a value, printing the result (False)." <EXPLAINS> """CODE.dataset = tf.data.Dataset.from_tensors(42)
iterator = iter(dataset)
optional = iterator.get_next_as_optional()
print(optional.has_value())
tf.Tensor(True, shape=(), dtype=bool)
print(optional.get_value())
tf.Tensor(42, shape=(), dtype=int32)
optional = iterator.get_next_as_optional()
print(optional.has_value())
tf.Tensor(False, shape=(), dtype=bool)""" .

"DESCRIPTION.This code snippet generates metadata for visualization using the tensorboardX library in Python. It creates a list of keywords, appends index numbers to each keyword, generates random image labels using torch, and adds embeddings with metadata and label images to the tensorboard writer for visualization." <EXPLAINS> """CODE.import keyword
import torch
meta = []
while len(meta)<100:
    meta = meta+keyword.kwlist # get some strings
meta = meta[:100]

for i, v in enumerate(meta):
    meta[i] = v+str(i)

label_img = torch.rand(100, 3, 10, 32)
for i in range(100):
    label_img[i]*=i/100.0

writer.add_embedding(torch.randn(100, 5), metadata=meta, label_img=label_img)
writer.add_embedding(torch.randn(100, 5), label_img=label_img)
writer.add_embedding(torch.randn(100, 5), metadata=meta)""" .

"DESCRIPTION.This code snippet generates random data, passes it through a linear neural network, calculates the mean loss, performs backpropagation to update the model weights, clips the gradients for better training stability, and then performs a single step of optimization using the SGD optimizer." <EXPLAINS> """CODE.import paddle

x = paddle.uniform([10, 10], min=-1.0, max=1.0, dtype='float32')
linear = paddle.nn.Linear(in_features=10, out_features=10,
                          weight_attr=paddle.ParamAttr(need_clip=True),
                          bias_attr=paddle.ParamAttr(need_clip=False))
out = linear(x)
loss = paddle.mean(out)
loss.backward()

clip = paddle.nn.ClipGradByNorm(clip_norm=1.0)
sdg = paddle.optimizer.SGD(learning_rate=0.1, parameters=linear.parameters(), grad_clip=clip)
sdg.step()""" .

"DESCRIPTION.This code snippet generates unique identifiers for the given string 'dense' using Keras backend." <EXPLAINS> """CODE.keras.backend.get_uid('dense')
keras.backend.get_uid('dense')
""" .

"DESCRIPTION.This code snippet implements a dropout layer in a neural network model using TensorFlow. The code initializes the random seed, creates a dropout layer with a dropout rate of 0.2 and input shape of (2,), prepares a sample data array, applies the dropout layer to the data while training the model, and finally prints the modified output data after dropout." <EXPLAINS> """CODE.tf.random.set_seed(0)
layer = tf.keras.layers.Dropout(.2, input_shape=(2,))
data = np.arange(10).reshape(5, 2).astype(np.float32)
print(data)
[[0. 1.]
 [2. 3.]
 [4. 5.]
 [6. 7.]
 [8. 9.]]
outputs = layer(data, training=True)
print(outputs)
tf.Tensor(
[[ 0.    1.25]
 [ 2.5   3.75]
 [ 5.    6.25]
 [ 7.5   8.75]
 [10.    0.  ]], shape=(5, 2), dtype=float32)""" .

"DESCRIPTION.This code snippet implements a while loop where the value of 'n' decrements by 1 in each iteration until it reaches 0, and the variable 's' is assigned the value of 'n'." <EXPLAINS> """CODE.while n > 0:
    n = n - 1
    s = n

s = Undefined('s')
init_state = (s,)
s = while_loop(cond, body, init_state)
""" .

"DESCRIPTION.This code snippet implements the Tanh transform function using the PaddlePaddle library in Python to transform input tensors using the hyperbolic tangent function and compute the forward and inverse transformations as well as the log determinant Jacobian values." <EXPLAINS> """CODE.import paddle
tanh = paddle.distribution.TanhTransform()
x = paddle.to_tensor([[1., 2., 3.], [4., 5., 6.]])
print(tanh.forward(x))
print(tanh.inverse(tanh.forward(x)))
print(tanh.forward_log_det_jacobian(x))
print(tanh.inverse_log_det_jacobian(tanh.forward(x)))""" .

"DESCRIPTION.This code snippet initializes Cauchy distributions with specified parameters and calculates the cumulative distribution function (CDF) of the Cauchy distribution for given input values. It demonstrates the usage of Cauchy distributions in probability and statistics calculations with PaddlePaddle." <EXPLAINS> """CODE.import paddle
from paddle.distribution import Cauchy

# init Cauchy with float
rv = Cauchy(loc=0.1, scale=1.2)
print(rv.cdf(paddle.to_tensor(1.5)))

# broadcast to value
rv = Cauchy(loc=0.1, scale=1.2)
print(rv.cdf(paddle.to_tensor([1.5, 5.1])))

# init Cauchy with N-Dim tensor
rv = Cauchy(loc=paddle.to_tensor([0.1, 0.1]), scale=paddle.to_tensor([1.0, 2.0]))
print(rv.cdf(paddle.to_tensor([1.5, 5.1])))

# init Cauchy with N-Dim tensor with broadcast
rv = Cauchy(loc=paddle.to_tensor(0.1), scale=paddle.to_tensor([1.0, 2.0]))
print(rv.cdf(paddle.to_tensor([1.5, 5.1])))

""" .

"DESCRIPTION.This code snippet initializes a Bayesian Ridge regression model with specified hyperparameters such as alpha values, lambda values, number of iterations, and other settings." <EXPLAINS> """CODE.BayesianRidge(alpha_1=1e-06, alpha_2=1e-06, compute_score=False,
        copy_X=True, fit_intercept=True, lambda_1=1e-06, lambda_2=1e-06,
        n_iter=300, normalize=False, tol=0.001, verbose=False)""" .

"DESCRIPTION.This code snippet initializes a LitClassifier object with a Backbone object as its argument." <EXPLAINS> """CODE.LitClassifier(Backbone())  # doctest: +ELLIPSIS +NORMALIZE_WHITESPACE
LitClassifier(
  (backbone): ...
)""" .

"DESCRIPTION.This code snippet initializes a tokenizer using SeamlessM4TTokenizer from the facebook/hf-seamless-m4t-medium model. It then translates an example English phrase \"UN Chief Says There Is No Military Solution in Syria\" to French, with the expected translation given as \"Le chef de l'ONU affirme qu'il n'y a pas de solution militaire en Syrie.\" Finally, it processes the input using the tokenizer to return tokenized tensors." <EXPLAINS> """CODE.from transformers import SeamlessM4TTokenizer

tokenizer = SeamlessM4TTokenizer.from_pretrained(
...     "facebook/hf-seamless-m4t-medium", src_lang="eng", tgt_lang="fra"
... )
example_english_phrase = " UN Chief Says There Is No Military Solution in Syria"
expected_translation_french = "Le chef de l'ONU affirme qu'il n'y a pas de solution militaire en Syrie."
inputs = tokenizer(example_english_phrase, text_target=expected_translation_french, return_tensors="pt")
""" .

"DESCRIPTION.This code snippet initializes and loads trainer arguments from a specified checkpoint directory, with provided serial number, trainer ID, and a list of specific trainer arguments." <EXPLAINS> """CODE.param_path = "./checkpoint/"
serial = 7
trainer_id = 2
trainer_args = ["epoch_id", "step_id"]

_load_trainer_args(checkpoint_dir=param_path, serial=serial,
trainer_id=trainer_id, trainer_args=trainer_args)""" .

"DESCRIPTION.This code snippet inserts a number 'n' into a table called 'numbers' in a database using an active session and then commits the changes." <EXPLAINS> """CODE.with conn.session as session:
    session.execute("INSERT INTO numbers (val) VALUES (:n);", {"n": n})
    session.commit()""" .

"DESCRIPTION.This code snippet is creating a time series dataset from an array where the input data is sliced with the last 10 elements removed, the targets are created by taking the last 10 elements of the original data. The dataset is then generated using TensorFlow's time series dataset utility function with a sequence length of 10. Finally, it is validating that the first batch from the dataset contains the correct input and target sequences by comparing them to the original data." <EXPLAINS> """CODE.input_data = data[:-10]
targets = data[10:]
dataset = tf.keras.preprocessing.timeseries_dataset_from_array(
    input_data, targets, sequence_length=10)
for batch in dataset:
  inputs, targets = batch
  assert np.array_equal(inputs[0], data[:10])  # First sequence: steps [0-9]
  assert np.array_equal(targets[0], data[10])  # Corresponding target: step 10
  break
""" .

"DESCRIPTION.This code snippet is performing stratified shuffle splitting, which is a technique used for cross-validation. It splits the input data X and corresponding labels y into multiple train-test sets while maintaining the same class distribution in each set. Each iteration of the loop prints the training and testing indices as well as the corresponding data splits." <EXPLAINS> """CODE.from sklearn.cross_validation import StratifiedShuffleSplit
X = np.array([[1, 2], [3, 4], [1, 2], [3, 4]])
y = np.array([0, 0, 1, 1])
sss = StratifiedShuffleSplit(y, 3, test_size=0.5, random_state=0)
len(sss)
3
print sss       # doctest: +ELLIPSIS
StratifiedShuffleSplit(labels=[0 0 1 1], n_iterations=3, ...)
for train_index, test_index in sss:
...    print "TRAIN:", train_index, "TEST:", test_index
...    X_train, X_test = X[train_index], X[test_index]
...    y_train, y_test = y[train_index], y[test_index]
TRAIN: [0 3] TEST: [1 2]
TRAIN: [0 2] TEST: [1 3]
TRAIN: [1 2] TEST: [0 3]""" .

"DESCRIPTION.This code snippet is used to calculate the Sensitivity at Specificity metric for a binary classification model. The Sensitivity at Specificity metric measures the true positive rate when the false positive rate is at a specific value (in this case, 0.5). The code snippet involves initializing the metric, updating its state with predicted and true labels, and finally computing the metric value. Additionally, it demonstrates resetting the state of the metric and updating it with sample weights. Finally, it shows how to include the Sensitivity at Specificity metric in the list of metrics to be used during model compilation." <EXPLAINS> """CODE.m = tf.keras.metrics.SensitivityAtSpecificity(0.5)
m.update_state([0, 0, 0, 1, 1], [0, 0.3, 0.8, 0.3, 0.8])
m.result().numpy()
0.5

m.reset_state()
m.update_state([0, 0, 0, 1, 1], [0, 0.3, 0.8, 0.3, 0.8],
...                sample_weight=[1, 1, 2, 2, 1])
m.result().numpy()
0.333333

model.compile(
    optimizer='sgd',
    loss='mse',
    metrics=[tf.keras.metrics.SensitivityAtSpecificity()])
""" .

"DESCRIPTION.This code snippet is used to get the frequency code for a given input, which can be a single character or a tuple of character and its frequency." <EXPLAINS> """CODE.get_freq_code('3D')
get_freq_code('D')
get_freq_code(('D', 3))""" .

"DESCRIPTION.This code snippet is used to link a model named \"my-portfolio\" to a variable named \"sm\"." <EXPLAINS> "CODE.link_model(sm, \"my-portfolio\")" .

"DESCRIPTION.This code snippet is using PyTorch library to ensure that input tensors are at least one-dimensional arrays by using the torch.atleast_1d() function." <EXPLAINS> """CODE.x = torch.randn(2)
x
tensor([1.4584, 0.7583])
torch.atleast_1d(x)
tensor([1.4584, 0.7583])
x = torch.tensor(1.)
x
tensor(1.)
torch.atleast_1d(x)
tensor([1.])
x = torch.tensor(0.5)
y = torch.tensor(1.)
torch.atleast_1d((x,y))
(tensor([0.5000]), tensor([1.]))""" .

"DESCRIPTION.This code snippet iterates over each example in the dataset after applying filtering and processing functions to the data. The dataset can be split into multiple shards for parallel processing, shuffled for randomness, and loaded using multiple workers for efficient processing." <EXPLAINS> """CODE.ids = ds.to_iterable_dataset()
for example in ids:
...     pass


ids = ds.to_iterable_dataset()
ids = ids.filter(filter_fn).map(process_fn)  # will filter and process on-the-fly when you start iterating over the iterable dataset
for example in ids:
...     pass


ids = ds.to_iterable_dataset(num_shards=64)  # the dataset is split into 64 shards to be iterated over
ids = ids.shuffle(buffer_size=10_000)  # will shuffle the shards order and use a shuffle buffer for fast approximate shuffling when you start iterating
for example in ids:
...     pass


import torch
ids = ds.to_iterable_dataset(num_shards=64)
ids = ids.filter(filter_fn).map(process_fn)
dataloader = torch.utils.data.DataLoader(ids, num_workers=4)  # will assign 64 / 4 = 16 shards to each worker to load, filter and process when you start iterating
for example in ids:
...     pass


import torch
ids = ds.to_iterable_dataset(num_shards=64)
ids = ids.shuffle(buffer_size=10_000)  # will shuffle the shards order and use a shuffle buffer when you start iterating
dataloader = torch.utils.data.DataLoader(ids, num_workers=4)  # will assign 64 / 4 = 16 shards from the shuffled list of shards to each worker when you start iterating
for example in ids:
...     pass


from datasets.distributed import split_dataset_by_node
ids = ds.to_iterable_dataset(num_shards=512)
ids = ids.shuffle(buffer_size=10_000)  # will shuffle the shards order and use a shuffle buffer when you start iterating
ids = split_dataset_by_node(ds, world_size=8, rank=0)  # will keep only 512 / 8 = 64 shards from the shuffled lists of shards when you start iterating
dataloader = torch.utils.data.DataLoader(ids, num_workers=4)  # will assign 64 / 4 = 16 shards from this node's list of shards to each worker when you start iterating
for example in ids:
...     pass


ids = ds.to_iterable_dataset(num_shards=64)
ids = ids.shuffle(buffer_size=10_000, seed=42)  # will shuffle the shards order and use a shuffle buffer when you start iterating
for epoch in range(n_epochs):
...     ids.set_epoch(epoch)  # will use effective_seed = seed + epoch to shuffle the shards and for the shuffle buffer when you start iterating
...     for example in ids:
...         pass

""" .

"DESCRIPTION.This code snippet likely infers the data type of the object \"Foo\" based on the input data type \"i8\" (int64)." <EXPLAINS> "CODE._maybe_infer_dtype_type(Foo(np.dtype(\"i8\")))" .

"DESCRIPTION.This code snippet lists the files in the 'gs://some-bucket' directory by temporarily mocking the function to use gfile." <EXPLAINS> """CODE.with tfds.core.community.mock_builtin_to_use_gfile():
  files = os.listdir('gs://some-bucket')
""" .

"DESCRIPTION.This code snippet loads an audio file using the librosa library." <EXPLAINS> "CODE.librosa.load(librosa.util.example_audio_file())" .

"DESCRIPTION.This code snippet mocks the behavior of a data access object (DAO) method called GetUsersInfo, specifying that it should return a mock result when it receives an input parameter of 'expectedUserName'." <EXPLAINS> "CODE.mock_dao.GetUsersInfo(In('expectedUserName')).AndReturn(mock_result)" .

"DESCRIPTION.This code snippet parses and converts numeric values followed by a unit identifier (K for kilobytes, M for megabytes) into their corresponding byte values. It then prints the parsed and converted values for the input string \"5K 100 640K 256M\"." <EXPLAINS> """CODE.integer = Word(nums).setParseAction(lambda toks: int(toks[0]))
integerK = integer.copy().addParseAction(lambda toks: toks[0]*1024) + Suppress("K")
integerM = integer.copy().addParseAction(lambda toks: toks[0]*1024*1024) + Suppress("M")
print(OneOrMore(integerK | integerM | integer).parseString("5K 100 640K 256M"))
integerM = integer().addParseAction(lambda toks: toks[0]*1024*1024) + Suppress("M")""" .

"DESCRIPTION.This code snippet performs constrained beam search translation from English to German using the T5-base model. The input sentence \"translate English to German: How old are you?\" is encoded, constraints are set, and beam search is run with 3 beams to generate the translated sentence \"Wie alter sind Sie?\"." <EXPLAINS> """CODE.from transformers import (
...     AutoTokenizer,
...     AutoModelForSeq2SeqLM,
...     LogitsProcessorList,
...     MinLengthLogitsProcessor,
...     ConstrainedBeamSearchScorer,
...     PhrasalConstraint,
... )
import torch

tokenizer = AutoTokenizer.from_pretrained("t5-base")
model = AutoModelForSeq2SeqLM.from_pretrained("t5-base")

encoder_input_str = "translate English to German: How old are you?"
encoder_input_ids = tokenizer(encoder_input_str, return_tensors="pt").input_ids


# lets run beam search using 3 beams
num_beams = 3
# define decoder start token ids
input_ids = torch.ones((num_beams, 1), device=model.device, dtype=torch.long)
input_ids = input_ids * model.config.decoder_start_token_id

# add encoder_outputs to model keyword arguments
model_kwargs = {
...     "encoder_outputs": model.get_encoder()(
...         encoder_input_ids.repeat_interleave(num_beams, dim=0), return_dict=True
...     )
... }

constraint_str = "sind"
constraint_token_ids = tokenizer.encode(constraint_str)[:-1]  # slice to remove eos token
constraints = [PhrasalConstraint(token_ids=constraint_token_ids)]


# instantiate beam scorer
beam_scorer = ConstrainedBeamSearchScorer(
...     batch_size=1, num_beams=num_beams, device=model.device, constraints=constraints
... )

# instantiate logits processors
logits_processor = LogitsProcessorList(
...     [
...         MinLengthLogitsProcessor(5, eos_token_id=model.config.eos_token_id),
...     ]
... )

outputs = model.constrained_beam_search(
...     input_ids, beam_scorer, constraints=constraints, logits_processor=logits_processor, **model_kwargs
... )

print("Generated:", tokenizer.batch_decode(outputs, skip_special_tokens=True))
# => ['Wie alter sind Sie?']
""" .

"DESCRIPTION.This code snippet performs distributed matrix multiplication using xmap with specified axes and axis resources." <EXPLAINS> """CODE.devices = np.array(jax.devices())[:4].reshape((2, 2)
with mesh(devices, ('x', 'y')):
    distributed_out = xmap(
      jnp.vdot,
      in_axes=({0: 'left', 1: 'right'}),
      out_axes=['left', 'right', ...],
      axis_resources={'left': 'x', 'right': 'y'})(x, x.T)""" .

"DESCRIPTION.This code snippet prepares input data for a linear model and an embedding layer for a neural network model in TensorFlow." <EXPLAINS> """CODE.video_id = categorical_column_with_identity(
    key='video_id', num_buckets=1000000, default_value=0)
columns = [video_id, ...]
features = tf.parse_example(..., features=make_parse_example_spec(columns))
linear_prediction, _, _ = linear_model(features, columns)

columns = [embedding_column(video_id, 9),...]
features = tf.parse_example(..., features=make_parse_example_spec(columns))
dense_tensor = input_layer(features, columns)
""" .

"DESCRIPTION.This code snippet reads data from an HDF5 file and uses a pre-trained model to make predictions on the data." <EXPLAINS> """CODE.X_data = HDF5Matrix('input/file.hdf5', 'data')
model.predict(X_data)
""" .

"DESCRIPTION.This code snippet removes and returns the first element of a deque object initialized with values 1 and 2." <EXPLAINS> "CODE.pdeque([1, 2]).popleft()" .

"DESCRIPTION.This code snippet removes the role prefix and backticks from the input string." <EXPLAINS> """CODE._strip_rest_role(':class:`ClassName`')
_strip_rest_role(':py:obj:`module.Object`')
_strip_rest_role('ClassName')""",
        """CODE._strip_rst_role(':class:`ClassName`')
_strip_rst_role(':py:obj:`module.Object`')
_strip_rst_role('ClassName')""" .

"DESCRIPTION.This code snippet retrieves a collection from the Hugging Face Hub using a given slug, and then deletes the last item in that collection based on its object ID." <EXPLAINS> """CODE.from huggingface_hub import get_collection, delete_collection_item

# Get collection first
collection = get_collection("TheBloke/recent-models-64f9a55bb3115b4f513ec026")

# Delete item based on its ID
delete_collection_item(
...     collection_slug="TheBloke/recent-models-64f9a55bb3115b4f513ec026",
...     item_object_id=collection.items[-1].item_object_id,
... )
""" .

"DESCRIPTION.This code snippet retrieves all model variables and then filters out convolutional weight variables by including patterns containing 'Conv' and excluding patterns containing 'biases' and 'Logits'." <EXPLAINS> """CODE.variables = tf.contrib.framework.get_model_variables()
conv_weight_variables = tf.contrib.framework.filter_variables(
    variables,
    include_patterns=['Conv'],
    exclude_patterns=['biases', 'Logits'])
""" .

"DESCRIPTION.This code snippet retrieves the default floating point data type used by Keras backend." <EXPLAINS> """CODE.keras.backend.floatx()
'float32'
""" .

"DESCRIPTION.This code snippet retrieves the image data format currently set in the Keras backend, which is 'channels_first'." <EXPLAINS> """CODE.keras.backend.image_data_format()
'channels_first'
""" .

"DESCRIPTION.This code snippet retrieves the image data format setting used by Keras, which is 'channels_first'." <EXPLAINS> """CODE.keras.backend.image_data_format()
'channels_first'
""" .

"DESCRIPTION.This code snippet retrieves the minor version number from a given version string." <EXPLAINS> """CODE.Version("1.2.3").minor
Version("1").minor""" .

"DESCRIPTION.This code snippet retrieves the status of a specific model named \"bigcode/starcoder\" using the InferenceClient from the Hugging Face Hub. The model is identified as loaded, running on GPU compute type, belongs to the text-generation-inference framework, and is in the loaded state." <EXPLAINS> """CODE.from huggingface_hub import InferenceClient
client = InferenceClient()
client.get_model_status("bigcode/starcoder")
ModelStatus(loaded=True, state='Loaded', compute_type='gpu', framework='text-generation-inference')
""" .

"DESCRIPTION.This code snippet retrieves the weights of a model or neural network using ev1.get_weights() and then sets these weights in another model or neural network using ev2.set_weights()." <EXPLAINS> """CODE.weights = ev1.get_weights()
ev2.set_weights(weights)""" .

"DESCRIPTION.This code snippet sets up an environment that enables 64-bit precision and then prints the data type of an array created using JAX NumPy's arange function." <EXPLAINS> """CODE.with enable_x64():
  ...   print(jnp.arange(10.0).dtype)""" .

"DESCRIPTION.This code snippet takes a categorical column named 'name' with predefined vocabulary list ['bob', 'george', 'wanda'], and converts it into a dense tensor representation using TensorFlow. The resulting dense tensor contains binary values based on the presence of each category in the 'name' feature in the input data." <EXPLAINS> """CODE.name = indicator_column(categorical_column_with_vocabulary_list('name',
    ['bob', 'george', 'wanda'])
columns = [name, ...]
features = tf.parse_example(..., features=make_parse_example_spec(columns))
dense_tensor = input_layer(features, columns)

dense_tensor == [[1, 0, 0]]  # If "name" bytes_list is ["bob"]
dense_tensor == [[1, 0, 1]]  # If "name" bytes_list is ["bob", "wanda"]
dense_tensor == [[2, 0, 0]]  # If "name" bytes_list is ["bob", "bob"]
""" .

"DESCRIPTION.This code snippet takes a list of dictionaries, converts them into a feature matrix, and then performs transformations on the feature matrix using a DictVectorizer object. It allows for fitting and transforming data into a format suitable for machine learning algorithms." <EXPLAINS> """CODE.from sklearn.feature_extraction import DictVectorizer
v = DictVectorizer(sparse=False)
D = [{'foo': 1, 'bar': 2}, {'foo': 3, 'baz': 1}]
X = v.fit_transform(D)
X
array([[ 2.,  0.,  1.],
       [ 0.,  1.,  3.])
v.inverse_transform(X) == [{'bar': 2.0, 'foo': 1.0}, {'baz': 1.0, 'foo': 3.0}]
True
v.transform({'foo': 4, 'unseen_feature': 3})
array([[ 0.,  0.,  4.]]))""" .

"DESCRIPTION.This code snippet trains a Linear Classifier model using TensorFlow Estimator with specified feature columns (age and language). It then exports the trained model to a timestamped directory as a SavedModel, which can be used for serving, analysis with TFMA, or direct loading. Finally, it loads the exported model and retrieves the weights of the 'age' feature for further analysis or inference." <EXPLAINS> """CODE.classifier = tf.estimator.LinearClassifier(
    feature_columns=[age, language])
classifier.train(input_fn=input_fn, steps=1000)

feature_spec = {
    'age': tf.placeholder(dtype=tf.int64),
    'language': array_ops.placeholder(dtype=tf.string)
}
label_spec = tf.placeholder(dtype=dtypes.int64)

train_rcvr_fn = tf.contrib.estimator.build_raw_supervised_input_receiver_fn(
    feature_spec, label_spec)

export_dir = tf.contrib.estimator.export_saved_model_for_mode(
    classifier,
    export_dir_base='my_model/',
    input_receiver_fn=train_rcvr_fn,
    mode=model_fn_lib.ModeKeys.TRAIN)

# export_dir is a timestamped directory with the SavedModel, which
# can be used for serving, analysis with TFMA, or directly loaded in.
with ops.Graph().as_default() as graph:
  with session.Session(graph=graph) as sess:
    loader.load(sess, [tag_constants.TRAINING], export_dir)
    weights = graph.get_tensor_by_name(''linear/linear_model/age/weights')
    ...
""" .

"DESCRIPTION.This code snippet transforms the data by standardizing it, i.e., by subtracting the mean of the data and dividing by its standard deviation." <EXPLAINS> "CODE.resampled.transform(lambda x: (x - x.mean()) / x.std())" .

"DESCRIPTION.This code snippet uses JAX to parallelize a given function across different devices specified in the input array. The function is decorated with pjit to perform parallel just-in-time compilation. The code then utilizes Mesh to distribute the computation across the specified devices with different configurations, such as setting in_axis_resources and out_axis_resources. This allows for efficient parallel processing of the input data on the specified devices." <EXPLAINS> """CODE.from jax.experimental.maps import Mesh
from jax.experimental.pjit import pjit
from jax.experimental import PartitionSpec as P
import numpy as np

inp = np.arange(16).reshape((8, 2))
devices = np.array(jax.devices()).reshape(4, 2)

global_mesh = Mesh(devices, ('x', 'y'))
with global_mesh:
  out = pjit(lambda x: x, in_axis_resources=None, out_axis_resources=None)(inp)

with Mesh(devices, ('x', 'y')) as global_mesh:
  out = pjit(lambda x: x, in_axis_resources=None, out_axis_resources=None)(inp)

global_mesh = Mesh(devices, ('x', 'y'))
with global_mesh as m:
  out = pjit(lambda x: x, in_axis_resources=None, out_axis_resources=None)(inp)

with Mesh(devices, ('x', 'y')):
  out = pjit(lambda x: x, in_axis_resources=None, out_axis_resources=None)(inp)""" .

"DESCRIPTION.This code snippet uses a pre-trained MBart tokenizer to prepare a sequence-to-sequence batch for translation from English to Romanian. The English phrase \"UN Chief Says There Is No Military Solution in Syria\" is tokenized and prepared as input with the expected translation \"Åeful ONU declarÄ cÄ nu existÄ o soluÅ£ie militarÄ Ã®n Siria\" for the model." <EXPLAINS> """CODE.from transformers import MBartTokenizerFast
tokenizer = MBartTokenizerFast.from_pretrained('facebook/mbart-large-en-ro')
example_english_phrase = " UN Chief Says There Is No Military Solution in Syria"
expected_translation_romanian = "Åeful ONU declarÄ cÄ nu existÄ o soluÅ£ie militarÄ Ã®n Siria"
batch: dict = tokenizer.prepare_seq2seq_batch(
    example_english_phrase, src_lang="en_XX", tgt_lang="ro_RO", tgt_texts=expected_translation_romanian
)
""" .

"""DESCRIPTION.This code snippet uses the PaddlePaddle library to perform rolling operation on a numpy array. The `roll` function shifts the elements of the input array along the specified dimensions. It demonstrates two examples:
1. Rolling the elements of the array by 1 shift without specifying dimension.
2. Rolling the elements of the array by 1 shift along the 0th dimension.""" <EXPLAINS> """CODE.import numpy as np
import paddle.fluid as fluid

data = np.array([[1.0, 2.0, 3.0],
                 [4.0, 5.0, 6.0],
                 [7.0, 8.0, 9.0]])
with fluid.dygraph.guard():
    x = fluid.dygraph.to_variable(data)
    out_z1 = fluid.layers.roll(x, shifts=1)
    print(out_z1.numpy())
    #[[9. 1. 2.]
    # [3. 4. 5.]
    # [6. 7. 8.]]
    out_z2 = fluid.layers.roll(x, shifts=1, dims=0)
    print(out_z2.numpy())
    #[[7. 8. 9.]
    # [1. 2. 3.]
    # [4. 5. 6.]]""" .

"DESCRIPTION.This code snippet uses the SeamlessM4TTokenizerFast from the transformers library to tokenize and encode an English phrase and its expected French translation for translation tasks." <EXPLAINS> """CODE.from transformers import SeamlessM4TTokenizerFast

tokenizer = SeamlessM4TTokenizerFast.from_pretrained(
...     "facebook/hf-seamless-m4t-medium", src_lang="eng", tgt_lang="fra"
... )
example_english_phrase = " UN Chief Says There Is No Military Solution in Syria"
expected_translation_french = "Le chef de l'ONU affirme qu'il n'y a pas de solution militaire en Syrie."
inputs = tokenizer(example_english_phrase, text_target=expected_translation_french, return_tensors="pt")
""" .

"DESCRIPTION.This code snippet uses the `parseString` method to match the literal string 'blah' in the input string. If the entire input string matches 'blah', it returns a list containing the string 'blah'. If 'blah' is not found in the input string, it raises an exception indicating that 'blah' was expected." <EXPLAINS> """CODE.Literal('blah').parseString('blah')  # -> ['blah']
Literal('blah').parseString('blahfooblah')  # -> ['blah']
Literal('blah').parseString('bla')  # -> Exception: Expected "blah\"""" .

"DESCRIPTION.This code snippet utilizes a pretrained GroupViT model and CLIPTokenizer from the transformers library to extract text features from input phrases describing images of a cat and a dog." <EXPLAINS> """CODE.from transformers import CLIPTokenizer, GroupViTModel

model = GroupViTModel.from_pretrained("nvidia/groupvit-gcc-yfcc")
tokenizer = CLIPTokenizer.from_pretrained("nvidia/groupvit-gcc-yfcc")

inputs = tokenizer(["a photo of a cat", "a photo of a dog"], padding=True, return_tensors="pt")
text_features = model.get_text_features(**inputs)
""" .

"DESCRIPTION.This code snippet utilizes the JAX library to distribute arrays across multiple devices, stack arrays, and retrieve individual elements from the distributed arrays." <EXPLAINS> """CODE.from jax import api, numpy as jnp
devices = api.local_devices()
x = [jnp.ones(5) for device in devices]
y = api.device_put_sharded(x, devices)
np.allclose(y, jnp.stack(x))
x = [(i, jnp.arange(i, i + 4)) for i in range(len(devices))]
y = api.device_put_sharded(x, devices)
type(y)
<class 'tuple'>
y0 = api.device_put_sharded([a for a, b in x], devices)
y1 = api.device_put_sharded([b for a, b in x], devices)
np.allclose(y[0], y0)
np.allclose(y[1], y1)
""" .

"DESCRIPTION.This code snippet utilizes the PLBartTokenizer from the transformers library to tokenize a Python code snippet and its expected English translation. It tokenizes the input Python code phrase \"def maximum(a,b,c):NEW_LINE_INDENTreturn max([a,b,c])\" and its expected translation \"Returns the maximum value of a b c.\" into tensors for model training." <EXPLAINS> """CODE.from transformers import PLBartTokenizer

tokenizer = PLBartTokenizer.from_pretrained("uclanlp/plbart-python-en_XX", src_lang="python", tgt_lang="en_XX")
example_python_phrase = "def maximum(a,b,c):NEW_LINE_INDENTreturn max([a,b,c])"
expected_translation_english = "Returns the maximum value of a b c."
inputs = tokenizer(example_python_phrase, return_tensors="pt")
with tokenizer.as_target_tokenizer():
...     labels = tokenizer(expected_translation_english, return_tensors="pt")
inputs["labels"] = labels["input_ids"]
""" .

"DESCRIPTION.This code sorts and returns all elements in the Counter object 'c'." <EXPLAINS> """CODE.c = Counter('ABCABC')
sorted(c.elements())""" .

"DESCRIPTION.This code sorts the array 'arr' in ascending order and returns the bottom 3 elements." <EXPLAINS> """CODE.import pyarrow as pa
import pyarrow.compute as pc
arr = pa.array(["a", "b", "c", None, "e", "f"])
pc.bottom_k_unstable(arr, k=3)""" .

"DESCRIPTION.This code specifies the version operator \"==1.2.3\"." <EXPLAINS> "CODE.Specifier(\"==1.2.3\").operator" .

"DESCRIPTION.This code splits a list of elements between two processes and prints the inputs assigned to each process. The second split includes padding the inputs to make the assignment equal." <EXPLAINS> """CODE.# Assume there are two processes
from accelerate import PartialState

state = PartialState()
with state.split_between_processes(["A", "B", "C"]) as inputs:
    print(inputs)
# Process 0
["A", "B"]
# Process 1
["C"]

with state.split_between_processes(["A", "B", "C"], apply_padding=True) as inputs:
    print(inputs)
# Process 0
["A", "B"]
# Process 1
["C", "C"]
""" .

"DESCRIPTION.This code splits a list of items into two separate processes using the Accelerator class. The first split_between_processes call splits the list [\"A\", \"B\", \"C\"] without padding, resulting in Process 0 getting [\"A\", \"B\"] and Process 1 getting [\"C\"]. The second split_between_processes call splits the same list with padding (apply_padding=True), resulting in Process 0 getting [\"A\", \"B\"] and Process 1 getting [\"C\", \"C\"]." <EXPLAINS> """CODE.# Assume there are two processes
from accelerate import Accelerator

accelerator = Accelerator()
with accelerator.split_between_processes(["A", "B", "C"]) as inputs:
    print(inputs)
# Process 0
["A", "B"]
# Process 1
["C"]

with accelerator.split_between_processes(["A", "B", "C"], apply_padding=True) as inputs:
    print(inputs)
# Process 0
["A", "B"]
# Process 1
["C", "C"]
""" .

"DESCRIPTION.This code splits a sentence based on various punctuation marks and returns a list of the separated parts." <EXPLAINS> """CODE.Example::
    punc = oneOf(list(".,;:/-!?"))
    print(list(punc.split("This, this?, this sentence, is badly punctuated!"))
prints::
    ['This', ' this', '', ' this sentence', ' is badly punctuated', '']""" .

"DESCRIPTION.This code splits the arrays 'a' and 'b' into training and testing sets using the train_test_split function from the sklearn library. It divides the data into a training set (a_train and b_train) and a testing set (a_test and b_test) based on the specified test_size and random_state parameters. The training and testing sets are then displayed for both 'a' and 'b'." <EXPLAINS> """CODE.import numpy as np
from sklearn.cross_validation import train_test_split
a, b = np.arange(10).reshape((5, 2)), range(5)
a
array([[0, 1],
       [2, 3],
       [4, 5],
       [6, 7],
       [8, 9]])
b
[0, 1, 2, 3, 4]

a_train, a_test, b_train, b_test = train_test_split(
...     a, b, test_size=0.33, random_state=42)
...
a_train
array([[4, 5],
       [0, 1],
       [6, 7]])
b_train
array([2, 0, 3])
a_test
array([[2, 3],
       [8, 9]])
b_test
array([1, 4])""" .

"DESCRIPTION.This code splits the complete_tensor into three parts based on the specified indices [2, 4] along the third dimension." <EXPLAINS> """CODE.import numpy as np
complete_tensor = np.array([[[1.11, 1.12, 1.13, 1.14, 1.15, 1.16]]])
rank = 2
complete_shape = [1, 1, 6]
dims_mapping = [-1, -1, 0]
process_shape = [3]
process_group = [0, 1, 2]

sliced_tensor_list = split(complete_tensor, [[], [], [2, 4]], 3)""" .

"DESCRIPTION.This code splits the traffic for the \"service-name\" between two backend versions, \"backend:v1\" and \"backend:v2\", with a 50/50 distribution." <EXPLAINS> """CODE.serve.set_traffic("service-name", {
    "backend:v1": 0.5,
    "backend:v2": 0.5
})""",
        """CODE.serve.split("service-name", {
    "backend:v1": 0.5,
    "backend:v2": 0.5
})""" .

"DESCRIPTION.This code squeezes the specified axes in a tensor with shape (5, 1, 2, 1, 1) to produce a new tensor with shape (5, 2, 1) and corresponding new axis order 'TYX'." <EXPLAINS> """CODE.squeeze_axes((5, 1, 2, 1, 1), 'TZYXC')
((5, 2, 1), 'TYX')""" .

"DESCRIPTION.This code starts a server using Flask framework with the specified host and port number." <EXPLAINS> """CODE.def start_server(self, host, port):
    self._process = subprocess.Popen(["flask", "run" "--host", host, "--port", str(port)])""" .

"DESCRIPTION.This code starts a server using Flask framework with the specified host and port." <EXPLAINS> """CODE.def start_server(self, host, port):
    self._process = subprocess.Popen(["flask", "run" "--host", host, "--port", str(port)])""" .

"DESCRIPTION.This code starts a server using Flask on the specified host and port." <EXPLAINS> """CODE.def start_server(self, host, port):
    self._process = subprocess.Popen(["flask", "run" "--host", host, "--port", str(port)])""" .

"DESCRIPTION.This code stops the server by killing the process running it." <EXPLAINS> """CODE.def stop_server(self):
    self._process.kill()""" .

"DESCRIPTION.This code stops the server by killing the running process." <EXPLAINS> """CODE.def stop_server(self):
    self._process.kill()""" .

"DESCRIPTION.This code substitutes certain words in the input sentences with specific replacements and returns the modified sentences." <EXPLAINS> """CODE.import jiwer

sentences = ["you're pretty", "your book", "foobar"]

print(jiwer.SubstituteWords({"pretty": "awesome", "you": "i", "'re": " am", 'foo': 'bar'})(sentences))

# prints: ["i am awesome", "your book", "foobar"]
""" .

"DESCRIPTION.This code takes a list as input and returns an iterator that yields pairs of adjacent elements from the input list." <EXPLAINS> "CODE.iterpairs([1, 2, 3, 4])" .

"DESCRIPTION.This code takes a list of percentiles as input and formats them for display." <EXPLAINS> """CODE.format_percentiles([0.01999, 0.02001, 0.5, 0.666666, 0.9999])
format_percentiles([0, 0.5, 0.02001, 0.5, 0.666666, 0.9999])""" .

"DESCRIPTION.This code trains a machine learning model using stochastic gradient descent (SGD) with the Torch backend. It loads a checkpoint, runs the training function, prints the iterations from the checkpoint epoch to 4, and then shuts down the Trainer." <EXPLAINS> """CODE.from ray.util import sgd

def train_func():
    checkpoint = sgd.load_checkpoint()
    for iter in range(checkpoint["epoch"], 5):
        print(iter)

trainer = Trainer(backend="torch")
trainer.start()
trainer.run(train_func, checkpoint={"epoch": 3})
# 3
# 4
trainer.shutdown()
""" .

"DESCRIPTION.This code unflattens a dictionary by nesting keys with underscores into nested dictionaries." <EXPLAINS> """CODE.unflatten({
  foo_bar_baz: 123,
  foo_bar_biz: 456,
  x_bonks: 'hi',
})""" .

"DESCRIPTION.This code unflattens a tensor of shape [4, 6, 8] into a tensor of shape [2, 3, 4, 6, 8] along the given axis 1." <EXPLAINS> """CODE.import paddle

x = paddle.randn(shape=[4, 6, 8])
shape = [2, 3]
axis = 1
unflatten = paddle.nn.Unflatten(axis, shape)
res = unflatten(x)
print(res.shape)""" .

"DESCRIPTION.This code updates the user information for the user with the key 'stevepm' in a mock data access object (DAO) using the user information provided in the variable 'stevepm_user_info'." <EXPLAINS> "CODE.mock_dao.UpdateUsers(ContainsKeyValue('stevepm', stevepm_user_info))" .

"DESCRIPTION.This code updates user information for the user with the key 'stevepm'." <EXPLAINS> "CODE.mock_dao.UpdateUsers(ContainsKeyValue('stevepm', stevepm_user_info))" .

"DESCRIPTION.This code uses JAX library to parallelize the all_gather operation on the input array x along the axis 'i'." <EXPLAINS> """CODE.x = np.arange(4)
y = jax.pmap(lambda x: jax.lax.all_gather(x, 'i'), axis_name='i')(x)
print(y)
[[0 1 2 3]
 [0 1 2 3]
 [0 1 2 3]
 [0 1 2 3]]
""" .

"DESCRIPTION.This code uses PyTorch's autograd profiler to profile the computation graph. It first calculates y = x^2, then calculates z = y^3 with the specified label \"label-z\", and finally performs backpropagation on y. The profiler is then used to print a table of the average timings of each function call, sorted by the total self CPU time." <EXPLAINS> """CODE.with torch.autograd.profiler.profile() as prof:
    y = x ** 2
    with torch.autograd.profiler.record_function("label-z"): # label the block
        z = y ** 3
    y.backward()

print(prof.key_averages().table(sort_by="self_cpu_time_total"))
""" .

"DESCRIPTION.This code uses TensorFlow's profiler to profile the performance of the code within the context block and then generates profiling data in the specified log directory." <EXPLAINS> """CODE.with tf.profiler.experimental.Profile("/path/to/logdir"):
  # do some work
""" .

"DESCRIPTION.This code uses XLNet tokenizer and model for token classification to process the input sentence \"Hello, my dog is cute\" and generate scores for the sequence classification." <EXPLAINS> """CODE.import tensorflow as tf
from transformers import XLNetTokenizer, TFXLNetForTokenClassification

tokenizer = XLNetTokenizer.from_pretrained('xlnet-large-cased')
model = TFXLNetForSequenceClassification.from_pretrained('xlnet-large-cased')
input_ids = tf.constant(tokenizer.encode("Hello, my dog is cute"))[None, :]  # Batch size 1
outputs = model(input_ids)
scores = outputs[0]""" .

"DESCRIPTION.This code uses a pre-trained ALBERT model to generate the last hidden states for the input text \"Hello, my dog is cute\"." <EXPLAINS> """CODE.import tensorflow as tf
from transformers import AlbertTokenizer, TFAlbertModel

tokenizer = AlbertTokenizer.from_pretrained('bert-base-uncased')
model = TFAlbertModel.from_pretrained('bert-base-uncased')
input_ids = tf.constant(tokenizer.encode("Hello, my dog is cute"))[None, :]  # Batch size 1
outputs = model(input_ids)
last_hidden_states = outputs[0]  # The last hidden-state is the first element of the output tuple""" .

"DESCRIPTION.This code uses a pre-trained DistilBERT model to perform token classification on the input text \"Hello, my dog is cute\" and outputs the scores for each token in the input text." <EXPLAINS> """CODE.import tensorflow as tf
from transformers import DistilBertTokenizer, TFDistilBertForTokenClassification

tokenizer = DistilBertTokenizer.from_pretrained('bert-base-uncased')
model = TFDistilBertForTokenClassification.from_pretrained('bert-base-uncased')
input_ids = tf.constant(tokenizer.encode("Hello, my dog is cute"))[None, :]  # Batch size 1
outputs = model(input_ids)
scores = outputs[0]""" .

"DESCRIPTION.This code uses a pre-trained FlaxMT5Model and T5Tokenizer to generate a summary of an input article about negotiations in Syria. It tokenizes the input article and summary, uses the tokenizer to prepare the decoder input ids, then passes the input ids to the model to generate hidden states as the output." <EXPLAINS> """CODE.from transformers import FlaxMT5Model, T5Tokenizer
model = FlaxMT5Model.from_pretrained("google/mt5-small")
tokenizer = T5Tokenizer.from_pretrained("google/mt5-small")
article = "UN Offizier sagt, dass weiter verhandelt werden muss in Syrien."
summary = "Weiter Verhandlung in Syrien."
inputs = tokenizer(article, return_tensors="np")
with tokenizer.as_target_tokenizer():
...     decoder_input_ids = tokenizer(summary, return_tensors="np").input_ids
outputs = model(input_ids=inputs["input_ids"], decoder_input_ids=decoder_input_ids)
hidden_states = outputs.last_hidden_state""" .

"DESCRIPTION.This code uses a pre-trained RoBERTa model to perform token classification on the input text \"Hello, my dog is cute\" and stores the resulting scores." <EXPLAINS> """CODE.import tensorflow as tf
from transformers import RobertaTokenizer, TFRobertaForTokenClassification

tokenizer = RobertaTokenizer.from_pretrained('roberta-base')
model = TFRobertaForTokenClassification.from_pretrained('roberta-base')
input_ids = tf.constant(tokenizer.encode("Hello, my dog is cute", add_special_tokens=True))[None, :]  # Batch size 1
outputs = model(input_ids)
scores = outputs[0]""" .

"DESCRIPTION.This code uses a pre-trained model for image captioning to generate a textual description of an image containing two cats laying on a couch." <EXPLAINS> """CODE.from PIL import Image
import requests
from transformers import AutoProcessor, BlipForConditionalGeneration

model = BlipForConditionalGeneration.from_pretrained("Salesforce/blip-image-captioning-base")
processor = AutoProcessor.from_pretrained("Salesforce/blip-image-captioning-base")

url = "http://images.cocodataset.org/val2017/000000039769.jpg"
image = Image.open(requests.get(url, stream=True).raw)

inputs = processor(images=image, return_tensors="pt")

outputs = model.generate(**inputs)
print(processor.decode(outputs[0], skip_special_tokens=True))
two cats are laying on a couch
""" .

"DESCRIPTION.This code uses a pre-trained model to answer a question based on an image. It first loads an image from a URL, then uses a language and image processor to generate an answer to the question \"How many cats are in the picture?\" The model generates a response based on the provided image and question, and the code prints the decoded answer." <EXPLAINS> """CODE.from PIL import Image
import requests
from transformers import AutoProcessor, BlipForQuestionAnswering

model = BlipForQuestionAnswering.from_pretrained("Salesforce/blip-vqa-base")
processor = AutoProcessor.from_pretrained("Salesforce/blip-vqa-base")

url = "http://images.cocodataset.org/val2017/000000039769.jpg"
image = Image.open(requests.get(url, stream=True).raw)
text = "How many cats are in the picture?"

inputs = processor(images=image, text=text, return_tensors="pt")

outputs = model.generate(**inputs)
print(processor.decode(outputs[0], skip_special_tokens=True))
2
""" .

"DESCRIPTION.This code uses a pre-trained transformer model to perform question answering on the input text \"Hello, my dog is cute\". The model tokenizes the input text, generates input_ids, and then outputs start_scores and end_scores for the answer span in the input text." <EXPLAINS> """CODE.import tensorflow as tf
from transformers import XxxTokenizer, TFXxxForQuestionAnswering

tokenizer = XxxTokenizer.from_pretrained('xxx-base-uncased')
model = TFXxxForQuestionAnswering.from_pretrained('xxx-base-uncased')
input_ids = tf.constant(tokenizer.encode("Hello, my dog is cute"))[None, :]  # Batch size 1
outputs = model(input_ids)
start_scores, end_scores = outputs[:2]""" .

"DESCRIPTION.This code uses a pre-trained xxx-base-uncased model from the Transformers library to perform token classification on the input text \"Hello, my dog is cute\"." <EXPLAINS> """CODE.import tensorflow as tf
from transformers import XxxTokenizer, TFXxxForTokenClassification

tokenizer = XxxTokenizer.from_pretrained('xxx-base-uncased')
model = TFXxxForTokenClassification.from_pretrained('xxx-base-uncased')
input_ids = tf.constant(tokenizer.encode("Hello, my dog is cute"))[None, :]  # Batch size 1
outputs = model(input_ids)
scores = outputs[0]""" .

"DESCRIPTION.This code uses a pretrained model to generate audio from a given input text with a specified voice preset. The audio is then converted into an array and the code extracts the audio data as a numpy array." <EXPLAINS> """CODE.from transformers import AutoProcessor, BarkModel
processor = AutoProcessor.from_pretrained("ylacombe/bark-small")
model = BarkModel.from_pretrained("ylacombe/bark-small")
voice_preset = "v2/en_speaker_6"
inputs = processor("Hello, my dog is cute, I need him in my life", voice_preset=voice_preset)
audio_array = model.generate(**inputs, semantic_max_new_tokens=100)
audio_array = audio_array.cpu().numpy().squeeze()
""" .

"DESCRIPTION.This code uses k-nearest neighbors regression to predict the value of a given input based on the nearest neighbors in the training data." <EXPLAINS> """CODE.X = [[0], [1], [2], [3]]
y = [0, 0, 1, 1]
from sklearn.neighbors import NeighborsRegressor
neigh = NeighborsRegressor(n_neighbors=2)
neigh.fit(X, y)
NeighborsRegressor(algorithm='auto', classification_type='knn_vote',
          leaf_size=30, n_neighbors=2, radius=1.0)
print neigh.predict([[1.5]])
[ 0.5]
""" .

"DESCRIPTION.This code uses the Counter class from the collections module to count the occurrences of characters in a string. It then performs various operations such as finding the most common characters, sorting the characters, joining the elements into a string, summing the values, updating the counter with a new string, and deleting specific elements." <EXPLAINS> """CODE.c = Counter('abcdeabcdabcaba')
c.most_common(3)
sorted(c)
''.join(sorted(c.elements()))
sum(c.values())
c['a']
for elem in 'shazam':
    c[elem] += 1
c['a']
del c['b']
c['b']
d = Counter('simsalabim')
c.update(d)
c['a']
c.clear()""" .

"DESCRIPTION.This code uses the Hugging Face Hub's InferenceClient to perform visual question answering on an image of a tiger. It takes the image URL and a question as input and returns a list of possible answers along with their corresponding confidence scores." <EXPLAINS> """CODE.from huggingface_hub import InferenceClient
client = InferenceClient()
client.visual_question_answering(
...     image="https://huggingface.co/datasets/mishig/sample_images/resolve/main/tiger.jpg",
...     question="What is the animal doing?"
... )
[{'score': 0.778609573841095, 'answer': 'laying down'},{'score': 0.6957435607910156, 'answer': 'sitting'}, ...]
""" .

"DESCRIPTION.This code uses the Jedi library to infer the full name of the function 'os.path.join' in the given Python source code." <EXPLAINS> """CODE.import os
os.path.join

from jedi import Script
source = '''
import os
os.path.join'''
script = Script(source, path='example.py')
print(script.infer(3, len('os.path.join'))[0].full_name)""" .

"DESCRIPTION.This code uses the MarianMTModel and MarianTokenizer from the transformers library to translate a sample text from a source language to a target language specified by the variables src and trg. The model translates the sample text \"oÃ¹ est l'arrÃªt de bus ?\" from French to English using the pretrained model specified by mname. The translated text is \"Where is the bus stop ?\" and is stored in the words variable as a list of strings." <EXPLAINS> """CODE.from transformers import MarianTokenizer, MarianMTModel
from typing import List
src = 'fr'  # source language
trg = 'en'  # target language
sample_text = "oÃ¹ est l'arrÃªt de bus ?"
mname = f'Helsinki-NLP/opus-mt-{src}-{trg}'

model = MarianMTModel.from_pretrained(mname)
tok = MarianTokenizer.from_pretrained(mname)
batch = tok.prepare_seq2seq_batch(src_texts=[sample_text])  # don't need tgt_text for inference
gen = model.generate(**batch)  # for forward pass: model(**batch)
words: List[str] = tok.batch_decode(gen, skip_special_tokens=True)  # returns "Where is the bus stop ?\"""" .

"DESCRIPTION.This code uses the PaddlePaddle framework to implement a function that performs a switch case operation. The switch case checks the value of the provided index and calls the corresponding function based on the index value. The code defines three functions (fn_1, fn_2, fn_3) that return constant values. It then sets up switch case operations with different branch functions and default values. The code executes the switch case operations and prints the results." <EXPLAINS> """CODE.import paddle.fluid as fluid
import paddle.fluid.layers as layers

def fn_1():
    return layers.fill_constant(shape=[1, 2], dtype='float32', value=1)

def fn_2():
    return layers.fill_constant(shape=[2, 2], dtype='int32', value=2)

def fn_3():
    return layers.fill_constant(shape=[3], dtype='int32', value=3)

main_program = fluid.default_startup_program()
startup_program = fluid.default_main_program()
with fluid.program_guard(main_program, startup_program):
    index_1 = layers.fill_constant(shape=[1], dtype='int32', value=1)
    index_2 = layers.fill_constant(shape=[1], dtype='int32', value=2)

    out_1 = layers.switch_case(
        branch_index=index_1,
        branch_fns={1: fn_1, 2: fn_2},
        default=fn_3)

    out_2 = layers.switch_case(
        branch_index=index_2,
        branch_fns=[(1, fn_1), (2, fn_2)],
        default=fn_3)

    # Argument default is None and no index matches. fn_3 will be called because of the max index 7.
    out_3 = layers.switch_case(
        branch_index=index_2,
        branch_fns=[(0, fn_1), (4, fn_2), (7, fn_3)])

exe = fluid.Executor(fluid.CPUPlace())
res_1, res_2, res_3 = exe.run(main_program,
                              fetch_list=[out_1, out_2, out_3])
print(res_1)  # [[1. 1.]]
print(res_2)  # [[2 2] [2 2]]
print(res_3)  # [3 3 3]
""" .

"DESCRIPTION.This code uses the PaddlePaddle framework to perform index-based sampling and top-k operation on tensors. The index_sample function selects elements from the input tensor based on the specified indices, while the topk function returns the top k maximum values and their corresponding indices in the input tensor. The code then uses the indices obtained from the topk operation to extract elements from another target tensor. Finally, the code prints out the results of the index sampling and top-k operations." <EXPLAINS> """CODE.import paddle.fluid as fluid
import numpy as np

data = np.array([[1.0, 2.0, 3.0, 4.0],
                    [5.0, 6.0, 7.0, 8.0],
                    [9.0, 10.0, 11.0, 12.0]]).astype('float32')

data_index = np.array([[0, 1, 2],
                        [1, 2, 3],
                        [0, 0, 0]]).astype('int32')

target_data = np.array([[100, 200, 300, 400],
                        [500, 600, 700, 800],
                        [900, 1000, 1100, 1200]]).astype('int32')

with fluid.dygraph.guard():
    x = fluid.dygraph.to_variable(data)
    index = fluid.dygraph.to_variable(data_index)
    target = fluid.dygraph.to_variable(target_data)

    out_z1 = fluid.layers.index_sample(x, index)
    print(out_z1.numpy())
    #[[1. 2. 3.]
    # [6. 7. 8.]
    # [9. 9. 9.]]

    # Use the index of the maximum value by topk op
    # get the value of the element of the corresponding index in other tensors
    top_value, top_index = fluid.layers.topk(x, k=2)
    out_z2 = fluid.layers.index_sample(target, top_index)
    print(top_value.numpy())
    #[[ 4.  3.]
    # [ 8.  7.]
    # [12. 11.]]

    print(top_index.numpy())
    #[[3 2]
    # [3 2]
    # [3 2]]

    print(out_z2.numpy())
    #[[ 400  300]
    # [ 800  700]
    # [1200 1100]]""" .

"DESCRIPTION.This code uses the Ray Tune library to run a training function with a configuration that synchronizes data to the driver using DockerSyncer." <EXPLAINS> """CODE.from ray.tune.integration.docker import DockerSyncer
tune.run(train,
         sync_config=tune.SyncConfig(
             sync_to_driver=DockerSyncer))""" .

"DESCRIPTION.This code uses the Ray library to create an iterator that generates batches of numbers from 0 to 999,999, and then prints each batch." <EXPLAINS> """CODE.import ray
for batch in ray.data.range(
    1000000
).iterator().iter_batches():
    print(batch)
""" .

"DESCRIPTION.This code uses the new pattern matching syntax introduced in Python 3.10 to match the values of the arguments and perform different actions based on the matched patterns." <EXPLAINS> """CODE.with match(args) as m:
    if   m.case(('x', 'y')):
        # use m.x and m.y
    elif m.case(('x', 'y', 'z')):
        # use m.x, m.y and m.z

Equivalent native code for Python >= 3.10:
match args:
    case (x, y):
        # use x and y
    case (x, y, z):
        # use x, y and z""" .

"DESCRIPTION.This code uses the psutil module to create a subprocess running a Python command that prints 'hi'. It retrieves information such as the process name, user IDs, and username. It also communicates with the subprocess, terminates it, and waits for it to finish within a specified timeout period." <EXPLAINS> """CODE.import psutil
from subprocess import PIPE
p = psutil.Popen(["/usr/bin/python", "-c", "print 'hi'"], stdout=PIPE)
p.name
p.uids
p.username
p.communicate()
p.terminate()
p.wait(timeout=2)""" .

"DESCRIPTION.This code uses the pyparsing module to test parsing of various types of numbers and formats such as integers, real numbers, hex numbers, fractions, mixed fractions, and UUIDs. The code runs tests for each type of number and format to ensure proper parsing and conversion." <EXPLAINS> """CODE.Example::
    pyparsing_common.number.runTests('''
        # any int or real number, returned as the appropriate type
        100
        -100
        +100
        3.14159
        6.02e23
        1e-12
        ''')

    pyparsing_common.fnumber.runTests('''
        # any int or real number, returned as float
        100
        -100
        +100
        3.14159
        6.02e23
        1e-12
        ''')

    pyparsing_common.hex_integer.runTests('''
        # hex numbers
        100
        FF
        ''')

    pyparsing_common.fraction.runTests('''
        # fractions
        1/2
        -3/4
        ''')

    pyparsing_common.mixed_integer.runTests('''
        # mixed fractions
        1
        1/2
        -3/4
        1-3/4
        ''')

    import uuid
    pyparsing_common.uuid.setParseAction(tokenMap(uuid.UUID))
    pyparsing_common.uuid.runTests('''
        # uuid
        12345678-1234-5678-1234-567812345678
        ''')""" .

"DESCRIPTION.This code utilizes a pre-trained Camembert model for masked language modeling. It tokenizes the input text \"J'aime le camembert !\" using the Camembert tokenizer, then passes the input through the Camembert model to perform masked language modeling, with the model predicting the masked tokens and calculating the loss based on the actual tokens in the input." <EXPLAINS> """CODE.tokenizer = CamembertTokenizer.from_pretrained('camembert-base')
model = CamembertForMaskedLM.from_pretrained('camembert-base')
input_ids = torch.tensor(tokenizer.encode("J'aime le camembert !")).unsqueeze(0)  # Batch size 1
outputs = model(input_ids, masked_lm_labels=input_ids)
loss, prediction_scores = outputs[:2]""" .

"DESCRIPTION.This code will iterate through the numbers 0 to 4, adding 1 to the variable \"s.out\" in each iteration." <EXPLAINS> """CODE.for i in range(5):
    s.out += 1
""" .

"DESCRIPTION.This function 'foldr' takes in a function, a tensor of elements, an optional initializer tensor, and an optional name. It applies the function cumulatively from right to left on the elements of the input tensor. The 'initializer' parameter is used to initialize the accumulator before applying the function. The function returns the final accumulated result after applying the function to all elements of the input tensor from right to left." <EXPLAINS> """CODE.def foldr(fn, elems, initializer=None, name=None):
    if not callable(fn):
        raise TypeError("fn must be callable")
    if not isinstance(initializer, tf.Tensor) and initializer is not None:
        raise TypeError("initializer must be a tensor or None value")
    if not isinstance(elems, tf.Tensor):
        raise TypeError("elems must be a tensor")

    accumulator = initializer
    for elem in reversed(elems):
        if accumulator is None:
            accumulator = elem
        else:
            accumulator = fn(accumulator, elem)

    return accumulator

# Example usage
result = foldr(lambda acc, x: acc + x, tf.constant([1, 2, 3, 4]), initializer=tf.constant(0))
print(result)
""" .

"DESCRIPTION.This function adds n-grams to sequences based on the token_indice dictionary, where n-grams are consecutive sequences of n items." <EXPLAINS> """CODE.def add_ngram(sequences, token_indice, ngram_range):
...     for i in range(len(sequences)):
...         for n in range(2, ngram_range + 1):
...             for j in range(len(sequences[i]) - n + 1):
...                 ngram = tuple(sequences[i][j:j + n])
...                 if ngram in token_indice:
...                     sequences[i].append(token_indice[ngram])
...     return sequences
...
sequences = [[1, 3, 4, 5], [1, 3, 7, 9, 2]]
token_indice = {(1, 3): 1337, (9, 2): 42, (4, 5): 2017}
add_ngram(sequences, token_indice, ngram_range=2)
[[1, 3, 4, 5, 1337, 2017], [1, 3, 7, 9, 2, 1337, 42]]

sequences = [[1, 3, 4, 5], [1, 3, 7, 9, 2]]
token_indice = {(1, 3): 1337, (9, 2): 42, (4, 5): 2017, (7, 9, 2): 2018}
add_ngram(sequences, token_indice, ngram_range=3)
[[1, 3, 4, 5, 1337, 2017], [1, 3, 7, 9, 2, 1337, 42, 2018]]""" .

"DESCRIPTION.This function builds HTTP headers for a request based on the input parameters such as authorization token, write action flag, library name, library version, and user agent. It sets the authorization header with a token if provided, checks for write access restrictions, and includes user agent information if library name and version are provided." <EXPLAINS> """CODE.def build_hf_headers(use_auth_token=None, is_write_action=False, library_name=None, library_version=None, user_agent=None):
    headers = {}

    if use_auth_token is not None:
        if isinstance(use_auth_token, str):
            headers["authorization"] = f"Bearer {use_auth_token}"
        elif use_auth_token is True:
            # token is read from the machine (cache or env variable)
            pass
        elif use_auth_token is False:
            # authorization header is not set
            pass
        else:
            # token is read from the machine only except if HF_HUB_DISABLE_IMPLICIT_TOKEN env variable is set
            pass

    if is_write_action:
        if use_auth_token is None or use_auth_token.startswith("api_org"):
            raise ValueError("You must use your personal account token for write-access methods.")

    if library_name is not None and library_version is not None:
        headers["user-agent"] = f"{library_name}/{library_version}; hf_hub/0.10.2; python/3.10.4; tensorflow/1.55"

    return headers

""" .

"DESCRIPTION.This function calculates a decayed learning rate based on the given step, using a combination of linear and cosine decay methods. The decayed learning rate is calculated by incorporating the initial learning rate, a linear decay factor, a cosine decay factor, and additional parameters." <EXPLAINS> """CODE.def decayed_learning_rate(step):
  step = min(step, decay_steps)
  linear_decay = (decay_steps - step) / decay_steps)
  cosine_decay = 0.5 * (
      1 + cos(pi * 2 * num_periods * step / decay_steps))
  decayed = (alpha + linear_decay + eps_t) * cosine_decay + beta
  return initial_learning_rate * decayed


decay_steps = 1000
lr_decayed_fn = (
  tf.keras.experimental.NoisyLinearCosineDecay(
    initial_learning_rate, decay_steps))
""" .

"DESCRIPTION.This function calculates a decayed learning rate based on the step number provided as input. It uses a cosine decay formula with a specified decay steps parameter, alpha value, and initial learning rate." <EXPLAINS> """CODE.def decayed_learning_rate(step):
  step = min(step, decay_steps)
  cosine_decay = 0.5 * (1 + cos(pi * step / decay_steps))
  decayed = (1 - alpha) * cosine_decay + alpha
  return initial_learning_rate * decayed


decay_steps = 1000
lr_decayed_fn = tf.keras.optimizers.schedules.CosineDecay(
    initial_learning_rate, decay_steps)
""" .

"DESCRIPTION.This function calculates a decayed learning rate based on the step, decay_steps, alpha, beta, num_periods, pi, and initial_learning_rate parameters provided. It combines linear decay and cosine decay formulas to compute the decayed learning rate." <EXPLAINS> """CODE.def decayed_learning_rate(step):
  step = min(step, decay_steps)
  linear_decay = (decay_steps - step) / decay_steps
  cosine_decay = 0.5 * (
      1 + cos(pi * 2 * num_periods * step / decay_steps))
  decayed = (alpha + linear_decay) * cosine_decay + beta
  return initial_learning_rate * decayed


decay_steps = 1000
lr_decayed_fn = (
  tf.keras.experimental.LinearCosineDecay(
    initial_learning_rate, decay_steps))
""" .

"DESCRIPTION.This function calculates the BLEU score of machine translated text with one or more reference texts using n-gram comparison up to a specified value (default is 4) and optionally applying smoothing based on Lin et al. 2004." <EXPLAINS> """CODE.def bleu_score(translate_corpus, reference_corpus, n_gram=4, smooth=False):
    # Calculate BLEU score of machine translated text with one or more references
    # Args:
    #     translate_corpus: An iterable of machine translated corpus
    #     reference_corpus: An iterable of iterables of reference corpus
    #     n_gram: Gram value ranged from 1 to 4 (Default 4)
    #     smooth: Whether or not to apply smoothing â Lin et al. 2004
    # Return:
    #     Tensor with BLEU Score

    # Example:
    #     translate_corpus = ['the cat is on the mat'.split()]
    #     reference_corpus = [['there is a cat on the mat'.split(), 'a cat is on the mat'.split()]]
    #     bleu_score(translate_corpus, reference_corpus)
    #     tensor(0.7598)

    # Your code goes here
    pass
""" .

"DESCRIPTION.This function calculates the confusion matrix between the predicted values and the target values. If normalize is set to True, it normalizes the confusion matrix." <EXPLAINS> """CODE.def confusion_matrix(pred, target, normalize=False):
    num_classes = max(max(pred), max(target)) + 1
    C = torch.zeros(num_classes, num_classes)
    for p, t in zip(pred, target):
        C[p, t] += 1
    if normalize:
        C = C / C.sum(1, keepdim=True)
    return C
""" .

"DESCRIPTION.This function calculates the hash value of a file specified by the file path using the specified hashing algorithm (default is 'auto'). It reads the file in chunks, updates the hash value with each chunk, and returns the hexadecimal digest of the hash value." <EXPLAINS> """CODE.def _hash_file(fpath, algorithm='auto', chunk_size=65536):
    import hashlib

    if algorithm == 'auto':
        hasher = hashlib.md5()
    elif algorithm == 'sha256':
        hasher = hashlib.sha256()
    elif algorithm == 'md5':
        hasher = hashlib.md5()
    else:
        raise ValueError("Invalid algorithm. Use 'auto', 'sha256', or 'md5'.")

    with open(fpath, 'rb') as file:
        for chunk in iter(lambda: file.read(chunk_size), b''):
            hasher.update(chunk)

    return hasher.hexdigest()
""" .

"DESCRIPTION.This function calculates the mean squared error between the predicted values (pred) and the target values (target)." <EXPLAINS> """CODE.def mse(pred, target, reduction='elementwise_mean'):
    return torch.mean((pred - target) ** 2)

x = torch.tensor([0., 1, 2, 3])
y = torch.tensor([0., 1, 2, 2])
mse(x, y)
""" .

"DESCRIPTION.This function calculates the precision of a model by taking the ratio of true positive predictions to the total predicted positive instances." <EXPLAINS> "CODE.precision(x, y)" .

"DESCRIPTION.This function calculates the recall score for a classification model." <EXPLAINS> """CODE.def recall(pred, target, num_classes, reduction='elementwise_mean'):
    # Computes recall score.

    # Your code here

    return recall_tensor
""" .

"DESCRIPTION.This function checks whether the filepath points to an unrequested hidden file or is inside an unrequested hidden directory." <EXPLAINS> """CODE.def _is_unrequested_hidden_file_or_is_inside_unrequested_hidden_dir(filepath, pattern):
    # code here
    pass
""" .

"DESCRIPTION.This function computes the Receiver Operating Characteristic (ROC) for a multiclass predictor and returns the ROC curve for each class." <EXPLAINS> """CODE.def multiclass_roc(pred, target):
    # code for computing ROC for multiclass predictors
    # return roc for each class
    # Number of classes, false-positive rate (fpr), true-positive rate (tpr), thresholds
    pass
""" .

"DESCRIPTION.This function configures automatic mixed precision (AMP) training for the input model and optimizers at a specified AMP level." <EXPLAINS> """CODE.def configure_apex(amp, model, optimizers, amp_level):
    model, optimizers = amp.initialize(
        model, optimizers, opt_level=amp_level,
    )

    return model, optimizers""" .

"DESCRIPTION.This function converts a file size string to an integer, where the input size can be in bytes (B), kilobytes (KB), megabytes (MB), gigabytes (GB), or terabytes (TB)." <EXPLAINS> """CODE.def convert_file_size_to_int(size):
    if isinstance(size, int):
        return size
    units = {"B": 1, "KB": 1024, "MB": 1024 ** 2, "GB": 1024 ** 3, "TB": 1024 ** 4}
    number, unit = int(size[:-2]), size[-2:]
    return number * units[unit]

""" .

"DESCRIPTION.This function converts block indices to time values using the specified block length, hop length, and sampling rate." <EXPLAINS> "CODE.librosa.blocks_to_time(n, block_length=16, hop_length=512, sr=sr)" .

"DESCRIPTION.This function converts elements in a tuple to strings." <EXPLAINS> """CODE.name = 'foo'
_column_name_to_strings(name)
'foo'
name = ('foo', 'bar')
_column_name_to_strings(name)
('foo', 'bar')
import pandas as pd
name = (1, pd.Timestamp('2017-02-01 00:00:00'))
_column_name_to_strings(name)
('1', '2017-02-01 00:00:00')""" .

"DESCRIPTION.This function creates a data loader for the MNIST dataset with specified transformation and parameters for testing." <EXPLAINS> """CODE.def val_dataloader(self):
    dataset = MNIST(root=PATH, train=False, transform=transforms.ToTensor(), download=False)
    loader = torch.utils.data.DataLoader(dataset=dataset, shuffle=False)
    return loader""" .

"DESCRIPTION.This function creates a data loader for training MNIST dataset with specified batch size and shuffling the data before each epoch." <EXPLAINS> """CODE.def train_dataloader(self):
    transform = transforms.Compose([transforms.ToTensor(),
                                    transforms.Normalize((0.5,), (1.0,))])
    dataset = MNIST(root='/path/to/mnist/', train=True, transform=transform,
                    download=True)
    loader = torch.utils.data.DataLoader(
        dataset=dataset,
        batch_size=self.batch_size,
        shuffle=True
    )
    return loader""" .

"DESCRIPTION.This function creates a data loader for training data for the MNIST dataset in PyTorch with specified transformation and batch size." <EXPLAINS> """CODE.def train_dataloader(self):
    transform = transforms.Compose([transforms.ToTensor(),
                                    transforms.Normalize((0.5,), (1.0,))])
    dataset = MNIST(root='/path/to/mnist/', train=True, transform=transform,
                    download=True)
    loader = torch.utils.data.DataLoader(
        dataset=dataset,
        batch_size=self.batch_size,
        shuffle=True
    )
    return loader""" .

"DESCRIPTION.This function creates a training data loader for the MNIST dataset without downloading it." <EXPLAINS> """CODE.def train_dataloader(self):
    dataset = MNIST(root=PATH, train=True, transform=transforms.ToTensor(), download=False)
    loader = torch.utils.data.DataLoader(dataset=dataset)
    return loader""" .

"DESCRIPTION.This function defines two sub-functions that update true_count and false_count variables respectively based on the value of the pred parameter. It then uses a conditional statement to return the result of calling either true_fn or false_fn on the inputs scope and x." <EXPLAINS> """CODE.def cond_example(scope, x, pred):
    scope.variable('state', 'true_count', lambda: 0)
    scope.variable('state', 'false_count', lambda: 0)
    def true_fn(scope, x):
      scope.variable('state', 'true_count').value += 1
      return scope.child(nn.dense)(x, 2)
    def false_fn(scope, x):
      scope.variable('state', 'false_count').value += 1
      return -scope.child(nn.dense)(x, 2)
    return lift.cond(pred, true_fn, false_fn, scope, x)""" .

"DESCRIPTION.This function eliminates consecutive duplicate values from the input tensor while optionally returning the unique elements, indices of original elements in the unique list, and counts of occurrences for each unique element." <EXPLAINS> """CODE.def torch.unique_consecutive(input, return_inverse=False, return_counts=False, dim=None):
    # Eliminates all but the first element from every consecutive group of equivalent elements.
    # This function is different from torch.unique in the sense that it only eliminates consecutive duplicate values.
    # Arguments:
    # input (Tensor): the input tensor
    # return_inverse (bool): Whether to also return the indices for where elements in the original input ended up in the returned unique list.
    # return_counts (bool): Whether to also return the counts for each unique element.
    # dim (int): the dimension to apply unique. If None, the unique of the flattened input is returned. default: None
    # Returns:
    # (Tensor, Tensor (optional), Tensor (optional)): A tensor or a tuple of tensors containing
    # - output (Tensor): the output list of unique scalar elements.
    # - inverse_indices (Tensor): (optional) if return_inverse is True, there will be an additional returned tensor (same shape as input) representing the indices for where elements in the original input map to in the output;
    # otherwise, this function will only return a single tensor.
    # - counts (Tensor): (optional) if return_counts is True, there will be an additional returned tensor (same shape as output or output.size(dim), if dim was specified) representing the number of occurrences for each unique value or tensor.

    # Example:
    x = torch.tensor([1, 1, 2, 2, 3, 1, 1, 2])
    output = torch.unique_consecutive(x)
    output
    output, inverse_indices = torch.unique_consecutive(x, return_inverse=True)
    output
    inverse_indices
    output, counts = torch.unique_consecutive(x, return_counts=True)
    output
    counts
""" .

"DESCRIPTION.This function executes the given 'body' function for a specified number of 'nsteps' times using an initial state 'init_state', and returns the final state after applying the 'body' function iteratively." <EXPLAINS> """CODE.def for_loop(nsteps, body, init_state):
  refs = tree_map(make_ref, init_state)
  for i in range(nsteps):
    body(i, refs)
  return tree_map(ref_get, refs)
""" .

"DESCRIPTION.This function extracts metadata information about a software library and returns a dictionary with the software library name as key and its version as the value." <EXPLAINS> """CODE.svs_description_metadata('Aperio Image Library v1.0')
{'Aperio Image Library': 'v1.0'}""" .

"DESCRIPTION.This function flattens a given gym space into a Box space, which is a high-dimensional space with shape information." <EXPLAINS> """CODE.def flatten_space(space):
    if isinstance(space, Box):
        return Box(np.prod(space.shape),)
    elif isinstance(space, Discrete):
        return Box(space.n,)
    elif isinstance(space, Dict):
        return Box(sum([flatten_space(subspace).shape[0] for subspace in space.spaces]),)
    else:
        raise NotImplementedError
""" .

"DESCRIPTION.This function generates a description of an image with shape (256, 256, 3) and axes orientation 'YXS'." <EXPLAINS> """CODE.image_description((256, 256, 3), axes='YXS')  # doctest: +SKIP
b'{"shape": [256, 256, 3], "axes": "YXS"}'""" .

"DESCRIPTION.This function generates a mask for a sequence based on the lengths of the sequences and a maximum length, where each sequence is represented as a boolean array with True values for the actual length of the sequence and False values for the padding." <EXPLAINS> """CODE.def sequence_mask(lengths, max_length):
    mask = []
    for length in lengths:
        mask.append([True] * length + [False] * (max_length - length))
    return mask
""" .

"DESCRIPTION.This function generates slices from an index array, with optional start and end indices, as well as a specified step size." <EXPLAINS> """CODE.librosa.util.index_to_slice(np.arange(20, 100, 15))
librosa.util.index_to_slice(np.arange(20, 100, 15),
                            idx_min=0, idx_max=100)
librosa.util.index_to_slice(np.arange(20, 100, 15),
                            idx_min=0, idx_max=100, step=5)""" .

"DESCRIPTION.This function initializes a module with optional output and returns the initialized module along with any variables generated during initialization." <EXPLAINS> """CODE.def init_with_output(fn, module, mutable=False):
    def init_fn(rngs, *args, **kwargs):
        variables = module.init(rngs)
        return fn(module, *args, **kwargs), variables
    return init_fn
""" .

"DESCRIPTION.This function is called when saving a checkpoint in a model training process. It adds a key and value pair to the checkpoint dictionary, allowing the user to save a pickable object along with the checkpoint." <EXPLAINS> """CODE.def on_save_checkpoint(self, checkpoint):
    # 99% of use cases you don't need to implement this method
    checkpoint['something_cool_i_want_to_save'] = my_cool_pickable_object
""" .

"DESCRIPTION.This function is responsible for computing the gradients of the loss function with respect to the model parameters for backpropagation." <EXPLAINS> """CODE.def backward(self, loss, optimizer, optimizer_idx):
    loss.backward()""" .

"DESCRIPTION.This function is used to display frequency ticks on a plot. It allows for specifying the locations of the ticks, the frequencies to display, the format of the frequency labels, and the axis on which to display the ticks." <EXPLAINS> """CODE.librosa.display.frequency_ticks()
librosa.display.frequency_ticks(locations, frequencies)
librosa.display.frequency_ticks(frequencies, freq_fmt='Hz')
librosa.display.frequency_ticks(frequencies, axis='y')""" .

"DESCRIPTION.This function is used to override the default behavior of saving a checkpoint in a model training process. It adds a custom key-value pair to the checkpoint dictionary before it is saved." <EXPLAINS> """CODE.def on_save_checkpoint(self, checkpoint):
    # 99% of use cases you don't need to implement this method
    checkpoint['something_cool_i_want_to_save'] = my_cool_pickable_object
""" .

"DESCRIPTION.This function is used to solve for the identity matrix when given two linear operator identity matrices as input." <EXPLAINS> """CODE.@linear_operator_algebra.RegisterSolve(
  lin_op.LinearOperatorIdentity,
  lin_op.LinearOperatorIdentity)
def _solve_identity(a, b):
  # Return the identity matrix.""" .

"DESCRIPTION.This function pads the sequences in the input data dictionary with zeros to match a specified maximum length, excluding certain states if specified." <EXPLAINS> """CODE.def right_zero_pad(self, max_len, exclude_states=False):
    self.zero_padded = True
    self.max_seq_len = max_len

    if self[SampleBatch.SEQ_LENS] is None:
        raise ValueError

    for key in self.keys():
        if key != SampleBatch.SEQ_LENS and (not exclude_states or "state_in" not in key):
            self[key] += [0] * (max_len - len(self[key]))

    return self
""" .

"DESCRIPTION.This function parses the command-line arguments passed to the Python script." <EXPLAINS> "CODE.parse_args()" .

"DESCRIPTION.This function performs GPU transforms on the input batch data before returning the modified batch." <EXPLAINS> """CODE.def on_after_batch_transfer(self, batch, dataloader_idx):
    batch['x'] = gpu_transforms(batch['x'])
    return batch""" .

"DESCRIPTION.This function performs a forward pass of a recurrent neural network (RNN) with input sequences, initial states, and sequence lengths. It returns the output of the RNN model and the final hidden and cell states." <EXPLAINS> """CODE.def forward_rnn(self, inputs, state, seq_lens):
    model_out, h, c = self.rnn_model([inputs, seq_lens] + state)
    return model_out, [h, c]""" .

"DESCRIPTION.This function performs antirectification on the input data by subtracting the mean along axis 1, normalizing using L2 norm along axis 1, and then splitting the data into positive and negative parts before concatenating them back together." <EXPLAINS> """CODE.def antirectifier(x):
    x -= K.mean(x, axis=1, keepdims=True)
    x = K.l2_normalize(x, axis=1)
    pos = K.relu(x)
    neg = K.relu(-x)
    return K.concatenate([pos, neg], axis=1)
model.add(Lambda(antirectifier))
""" .

"DESCRIPTION.This function preprocesses the input batch data by applying transformations to the 'x' key of the batch dictionary before transferring it to the dataloader." <EXPLAINS> """CODE.def on_before_batch_transfer(self, batch, dataloader_idx):
    batch['x'] = transforms(batch['x'])
    return batch""" .

"DESCRIPTION.This function records a property with the key \"example_key\" and the value 1." <EXPLAINS> """CODE.def test_function(record_property):
    record_property("example_key", 1)""" .

"DESCRIPTION.This function resolves the identity of resources by updating the resource inputs based on the operation type. It checks if the resources are read-only or read-write and then updates the inputs accordingly by removing and adding resources." <EXPLAINS> """CODE.@register_acd_resource_resolver
def ResolveIdentity(op, resource_reads, resource_writes):
  # op: The `Operation` being processed by ACD currently.
  # resource_reads: An `ObjectIdentitySet` of read-only resources.
  # resource_writes: An `ObjectIdentitySet` of read-write resources.
  if not resource_reads or resource_writes:
    return False
  def update(resource_inputs):
    to_add = []
    to_remove = []
    for t in resource_inputs:
      if t.op.type == "Identity":
        to_remove.append(t)
        to_add.append(t.op.inputs[0])
    if not to_add and not to_remove:
      return False
    for t in to_remove:
      resource_inputs.discard(t)
    resource_inputs.update(to_add)
    return True
  return update(resource_reads) or update(resource_writes)
""" .

"DESCRIPTION.This function retrieves metrics from a trainer and model, removes the \"v_num\" key from the metrics dictionary, and returns the modified metrics." <EXPLAINS> """CODE.def get_metrics(self, trainer, model):
    # don't show the version number
    items = super().get_metrics(trainer, model)
    items.pop("v_num", None)
    return items
""" .

"DESCRIPTION.This function returns a dataloader for the MNIST dataset with specified parameters for validation data." <EXPLAINS> """CODE.def val_dataloader(self):
    dataset = MNIST(root=PATH, train=False, transform=transforms.ToTensor(), download=False)
    loader = torch.utils.data.DataLoader(dataset=dataset, shuffle=False)
    return loader""" .

"DESCRIPTION.This function returns an initial state consisting of two arrays filled with zeroes of size self.cell_size and data type np.float32." <EXPLAINS> """CODE.def get_initial_state(self):
    return [
        np.zeros(self.cell_size, np.float32),
        np.zeros(self.cell_size, np.float32),
    ]""" .

"DESCRIPTION.This function returns the identity matrix when provided with a linear operator that is an identity operator." <EXPLAINS> """CODE.@linear_operator_algebra.RegisterInverse(lin_op.LinearOperatorIdentity)
def _inverse_identity(lin_op_a):
  # Return the identity matrix.""" .

"DESCRIPTION.This function returns the port number to connect to a proxy, with a default value of 8501." <EXPLAINS> """CODE.@ConfigOption('proxy.port')
def _proxy_port():
    \"\"\"Connect to the proxy at this port.

    Defaults to 8501.
    \"\"\"
    return 8501""" .

"DESCRIPTION.This function returns the value stored in the attribute _value_out of the object." <EXPLAINS> """CODE.def value_function(self):
    return self._value_out""" .

"DESCRIPTION.This function saves a specific value from a checkpoint data, which is 'something_cool_i_want_to_save', into an attribute 'self.something_cool_i_want_to_save'." <EXPLAINS> """CODE.def on_load_checkpoint(self, checkpoint):
    # 99% of the time you don't need to implement this method
    self.something_cool_i_want_to_save = checkpoint['something_cool_i_want_to_save']""" .

"DESCRIPTION.This function serves the input tensor x and returns a dictionary containing the predictions computed by calling the self function on x." <EXPLAINS> """CODE.def serve_step(self, x: torch.Tensor) -> Dict[str, torch.Tensor]:
    return {"predictions": self(x)}
""" .

"DESCRIPTION.This function splits a given path using \"::\" as a separator, then extracts the base name and extension from the first part of the split path. Finally, it concatenates the base name with the rest of the split parts and returns a tuple of the modified path and extension." <EXPLAINS> """CODE.def xsplitext(path):
    parts = path.split("::")
    first_part, *rest = parts
    base, ext = os.path.splitext(first_part)
    return (base + "".join(rest), ext)
""" .

"DESCRIPTION.This function splits downloaded data into training and testing sets, generates examples for each set, and returns them in a dictionary." <EXPLAINS> """CODE.def _split_generators(self, dl_manager):
  path = dl_manager.download_and_extract('http://dataset.org/my_data.zip')
  return {
      'train': self._generate_examples(path=f'{path}/train/'),
      'test': self._generate_examples(path=f'{path}/test/'),
  }
""" .

"DESCRIPTION.This function takes a list of indices and returns the corresponding values from the input data, with an option to fill missing values if specified." <EXPLAINS> """CODE.def take(self, indices, allow_fill=False, fill_value=None):
    from pandas.core.algorithms import take

    data = self.astype(object)

    if allow_fill and fill_value is None:
        fill_value = self.dtype.na_value

    result = take(data, indices, fill_value=fill_value,
                  allow_fill=allow_fill)
    return self._from_sequence(result, dtype=self.dtype)
""" .

"DESCRIPTION.This function takes a list of lists as input and returns a new list with the length of each sublist, excluding any occurrences of the number 0." <EXPLAINS> """CODE.def get_length(y):
    return [len([i for i in x if i != 0]) for x in y]
""" .

"DESCRIPTION.This function takes a tuple representing the dimensions of an image and a string representing the order of axes and returns a JSON-formatted byte string containing the image description." <EXPLAINS> """CODE.image_description((256, 256, 3), axes='YXS')  # doctest: +SKIP
b'{"shape": [256, 256, 3], "axes": "YXS"}'""" .

"DESCRIPTION.This function takes an index, a list of branches, and an operand as input, and returns the result of calling the function at the specified index in the branches list with the operand as an argument. The index is clamped to ensure it is within the bounds of the branches list." <EXPLAINS> """CODE.def switch(index, branches, operand):
    index = clamp(0, index, len(branches) - 1)
    return branches[index](operand)""" .

"DESCRIPTION.This function takes an input dict containing observations, feeds the observations into a base model, and returns the model's output along with the input state." <EXPLAINS> """CODE.def forward(self, input_dict, state, seq_lens):
    model_out, self._value_out = self.base_model(input_dict["obs"])
    return model_out, state""" .

"DESCRIPTION.This function takes specified indices from the data, with an option to fill missing values with a specified fill value. It coerces the data to object type if necessary, assigns a fill value if specified and then uses the take function from pandas to get the desired elements. Finally, it returns a new object with the selected elements." <EXPLAINS> """CODE.   def take(self, indices, allow_fill=False, fill_value=None):
       from pandas.core.algorithms import take

       # If the ExtensionArray is backed by an ndarray, then
       # just pass that here instead of coercing to object.
       data = self.astype(object)

       if allow_fill and fill_value is None:
           fill_value = self.dtype.na_value

       # fill value should always be translated from the scalar
       # type for the array, to the physical storage type for
       # the data, before passing to take.

       result = take(data, indices, fill_value=fill_value,
                     allow_fill=allow_fill)
       return self._from_sequence(result)
""" .

"DESCRIPTION.This function tokenizes the given input sentence into individual words." <EXPLAINS> "CODE.tokenize('Bob dropped the apple. Where is the apple?')" .

"DESCRIPTION.This function transfers a batch of data to a specified device, either moving all tensors in a custom data structure to the device or calling the superclass function to transfer the batch if it is not a custom batch." <EXPLAINS> """CODE.def transfer_batch_to_device(self, batch, device):
    if isinstance(batch, CustomBatch):
        # move all tensors in your custom data structure to the device
        batch.samples = batch.samples.to(device)
        batch.targets = batch.targets.to(device)
    else:
        batch = super().transfer_batch_to_device(data, device)
    return batch
""" .

"DESCRIPTION.This function transfers a batch of data to a specified device, moving all tensors in a custom data structure to the device if the batch is an instance of CustomBatch, otherwise it transfers the batch using the superclass's transfer method." <EXPLAINS> """CODE.def transfer_batch_to_device(self, batch, device):
    if isinstance(batch, CustomBatch):
        # move all tensors in your custom data structure to the device
        batch.samples = batch.samples.to(device)
        batch.targets = batch.targets.to(device)
    else:
        batch = super().transfer_batch_to_device(data, device)
    return batch""" .

"DESCRIPTION.This function transfers a batch of data to a specified device, such as a GPU or CPU. If the batch is an instance of a custom data structure called CustomBatch, then it moves all tensors within the batch to the specified device. Otherwise, it uses the superclass method to transfer the data." <EXPLAINS> """CODE.    def transfer_batch_to_device(self, batch, device):
        if isinstance(batch, CustomBatch):
            # move all tensors in your custom data structure to the device
            batch.samples = batch.samples.to(device)
            batch.targets = batch.targets.to(device)
        else:
            batch = super().transfer_batch_to_device(data, device)
        return batch
""" .

"DESCRIPTION.This function transfers a batch of data to a specified device. If the batch is an instance of a custom data structure called CustomBatch, it moves all tensors within the batch to the specified device. Otherwise, it uses the superclass method to transfer the batch to the device." <EXPLAINS> """CODE.def transfer_batch_to_device(self, batch, device):
    if isinstance(batch, CustomBatch):
        # move all tensors in your custom data structure to the device
        batch.samples = batch.samples.to(device)
        batch.targets = batch.targets.to(device)
    else:
        batch = super().transfer_batch_to_device(data, device)
    return batch
""" .

"DESCRIPTION.This function transfers a batch of data to the specified device. If the batch is an instance of CustomBatch, it moves all tensors in the custom data structure to the device. Otherwise, it calls the superclass method to transfer the batch to the device." <EXPLAINS> """CODE.def transfer_batch_to_device(self, batch, device):
    if isinstance(batch, CustomBatch):
        # move all tensors in your custom data structure to the device
        batch.samples = batch.samples.to(device)
        batch.targets = batch.targets.to(device)
    else:
        batch = super().transfer_batch_to_device(data, device)
    return batch""" .

"DESCRIPTION.This function updates a configuration dictionary by replacing a key 'hidden_cls' with a custom object obtained from another function using the key 'my_custom_object_name' from the input configuration." <EXPLAINS> """CODE.def from_config(cls, config, custom_objects=None):
    if 'my_custom_object_name' in config:
        config['hidden_cls'] = tf.keras.utils.get_registered_object(
            config['my_custom_object_name'], custom_objects=custom_objects)""" .

"DESCRIPTION.This function updates the dictionary `kwargs` with the keys and values provided in the arguments `one` and `two`. If `one` is not provided, its value is set to `None`. If `two` is not provided, its value is set to `2`." <EXPLAINS> "CODE.update_kwargs(kwargs, one=None, two=2)" .

"DESCRIPTION.This function will print the message \"This will be printed by process 0 only.\" when executed by the main process." <EXPLAINS> """CODE.@state.on_main_process
def print_something():
    print("This will be printed by process 0 only.")""" .

"DESCRIPTION.This method is used to load a checkpoint and retrieve a specific value ('something_cool_i_want_to_save') from it, which is then assigned to a class attribute." <EXPLAINS> """CODE.def on_load_checkpoint(self, checkpoint):
    # 99% of the time you don't need to implement this method
    self.something_cool_i_want_to_save = checkpoint['something_cool_i_want_to_save']""" .

"DESCRIPTION.This piece of code appends the element 3 to the left side of a deque containing the elements 1 and 2." <EXPLAINS> "CODE.pdeque([1, 2]).appendleft(3)" .

"DESCRIPTION.This python code demonstrates the functionality of a Trie data structure by storing and splitting strings based on specific substrings." <EXPLAINS> """CODE.trie = Trie()
trie.split("[CLS] This is a extra_id_100")
["[CLS] This is a extra_id_100"]
trie.add("[CLS]")
trie.add("extra_id_1")
trie.add("extra_id_100")
trie.split("[CLS] This is a extra_id_100")
["[CLS]", " This is a ", "extra_id_100"]
""" .

"DESCRIPTION.This python code initializes a replay buffer with a size of 5." <EXPLAINS> "CODE.ReplayBuffer(5)" .

"DESCRIPTION.This python code prepares data by downloading ImageNet, cleaning ImageNet, and caching ImageNet." <EXPLAINS> """CODE.def prepare_data(self):
    download_imagenet()
    clean_imagenet()
    cache_imagenet()""" .

